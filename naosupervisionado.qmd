# Aprendizado não supervisionado

## Alguns conceitos básicos de algebra

Para melhor introduzir o campo do aprendizado não supervisionado, alguns conceitos de álgebra são necessários para compreender o que se passa por trás de cada algoritmo da análise de dados multivariada. Vamos introduzir com vetores e matriz, seguindo com decomposição espectral para então darmos inicio a área da estatística multivariada ou aprendizado não supervisionado. Não é objetivo desse livro demonstrar conceitos algébricos e nem se aprofundar demais no assunto [veja @anton2001algebra, e @johnson2002applied para mais demonstrações e conceitos].

### Definições importantes

**Vetor Aleatório** : Seja X um vetor contendo *p* componentes, onde cada componente é uma variável aleatória, isto é, $X_i$ é uma variável aleatória, $\forall\quad i =1,2,...,p$. Então *X* é chamado de vetor aleatório e é denotado por:

$$
 \begin{align}
  X &= \begin{bmatrix}
           X_{1} \\
           X_{2} \\
           \vdots \\
           X_{p}
         \end{bmatrix}
  \end{align}
$$

O vetor transposto do vetor aleatório *X* é denotadopor $X' = [X_1 X_2 X_3 ...X_p]$

**Vetor de Médias** : O vetor $\mu$ é chamado vetor de médias quando $E(X) = \mu$ onde *X* é um vetor aleatório. Dessa forma

$$
\begin{align}
  E(X) &= \begin{bmatrix}
           E(X_{1}) \\
           E(X_{2}) \\
           \vdots \\
           E(X_{p})
         \end{bmatrix}
  \end{align} = \mu = \begin{bmatrix}
           \mu_1 \\
           \mu_2 \\
           \vdots \\
           \mu_p
         \end{bmatrix}
$$

**Matriz de covariâncias** : A matriz de variâncias e covariâncias do vetor *X* é denotada por,

$$
Cov(X) = V(X) = Var(X) = \Sigma_{p\times p} = \begin{bmatrix}
           \sigma_{11} & \sigma_{12} & ... & \sigma_{1p}  \\
          \sigma_{21} & \sigma_{22} & ... & \sigma_{2p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \sigma_{p1} & \sigma_{p2} & ... & \sigma_{pp}
         \end{bmatrix}
$$

Onde $\sigma_{ii}$ representa a variância do elemento $X_i$ do vetor aleatório e $\sigma_{ij} = E[(X_i- \mu_i)(X_j - \mu_j)]$ $\forall\quad i,j = 1,\dots,p$. A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja $\Sigma = '\Sigma$. Sendo tambem não negativa definida, $a'\Sigma a \geq 0$ para todo vetor de constantes pertencentes aos reais.

**Matriz de correlação** : A matriz de correlação do vetor *X* é denotada por,

$$
P_{p\times p} = \begin{bmatrix}
           1 & \rho_{12} & \rho_{13}& ... & \rho_{1p}  \\
          \rho_{21} & 1 & \rho_{23}&... & \rho_{2p}  \\
          \rho_{31} & \rho_{32} & 1 &... & \rho_{3p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \rho_{p1} & \rho_{p2} &\rho_{p3}& ... & 1
         \end{bmatrix}
$$

Em que

$$
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} = \frac{\sigma_{ij}}{\sigma_i\sigma_j}
$$

**Auto Valores e Auto Vetores** : Se $\Sigma$ for uma matriz quadrada, ou seja $\Sigma_{p\times p}$, então um vetore não nulo $e$ em $R^n$ é denominado *autovetor* de $\Sigma$ se $\Sigma e$ for um múltiplo escalar de $e$, isto é,

$$
\Sigma e = \lambda e
$$

com algum escalar $\lambda$. O escalar $\lambda$ é denominado de *autovalor* de $\Sigma$, e dizemos que $e$ é um *autovetor associado a* $\lambda$. Por $\Sigma$ ser uma matriz não negativa definida seus autovalores $\lambda_i$ associados tambem serão não negativos. Os autovetores e autovalores serão necessários para a análise de componentes principais mais a frente abordada.

#### Equação característica

Ainda é necessário uma forma de encontrar os autovetores e autovalores associados a uma matriz $\Sigma$. Se $\Sigma$ for uma matriz quadrada, então $\lambda$ se, e somente se, $\lambda$ satisfaz a equação

$$
det(\lambda I - \Sigma) = 0
$$

Onde *det* é o determinante e $I$ a matriz identidade. Para esclarecimento, suponha como exemplo que,

$$
\Sigma = \begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
$$

Então,

$$
\begin{split}
det\left(\begin{bmatrix}
\lambda& 0\\
0 & \lambda
\end{bmatrix}
-  
\begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
\right) = 0\\
det\left(\begin{bmatrix}
\lambda - 8 & 2 \\
2 & \lambda-5
\end{bmatrix}
\right) = 0 \\
(\lambda - 8)\times(\lambda-5) - (2)\times(2) = 0
\end{split}
$$

Resolvendo a equação obtemos os valores de $\lambda_1 = 9$ e $lambda_2 = 4$, podemos encontrar os autovetores $v$ associados seguindo a definição:

$$
\begin{bmatrix}
8&-2\\
-2 & 5
\end{bmatrix}
\begin{bmatrix}
v_{11}\\
v_{12}
\end{bmatrix} = 
9\begin{bmatrix}
v_{11}\\
v_{12}
\end{bmatrix} \rightarrow v_{11} =- 2v_{12}
$$

Note que para cada autovalor temos infinitos possíveis autovetores dentro dos reais. Nos restringiremos aos autovetores normalizados.Dizemos que um vetor $e_i$ é normalizado quando:

$$
e_i = \begin{bmatrix}
e_{i1}\\
e_{i2}\\
\vdots\\
e_{ip}
\end{bmatrix}
$$

Em que

$$
||e_i|| = \sqrt{e^2_{i1} + e^2_{i2} + \dots + e^2_{ip}} = 1
$$

### Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.

O teorema da decomposição espectral é de extrema importância em álgebra matricial e estatística multivariada, ele relaciona a matriz com seus autovalores e autovetores normalizados.

Suponha $\Sigma$ a matriz de covariâncias. Então existe uma matriz ortogonal $O$(matriz no qual sua transposta é igual a sua inversa) tal que,

$$
O'\Sigma O = \begin{bmatrix}
\lambda_1 & 0 & 0 &\dots & 0\\
0&\lambda_2& 0 & \dots & 0 \\
0 & 0 &\lambda_3 &\dots & 0\\
\vdots& \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \dots& \lambda_p
\end{bmatrix} = \Lambda
$$

Onde $\lambda\_1 \geq \lambda\_2 \geq \dots \lambda\_p\geq0$ são os autovalores ordenados em ordem decrescente da matriz $\Sigma$. Nesse caso, dizemos que a matriz $\Sigma$ é similar à matriz $\Lambda$, que implica em:

-   $det(\Sigma) = det(\Lambda) = \prod^p_{i=1} \lambda_i$

-   traço$(\Sigma) =$ traço$(\Lambda) = \lambda_1 +\dots+\lambda_p$

Tem-se que a i-ésima coluna da matriz $O$ é o autovetor normalizado $e_i$ relacionado ao autovalor $\lambda_i$. Então a matriz $O$ é dada por $O = [e_1,e_2,\dots,e_p]$ e pelo teorema da decomposição espectral, podemos ver que:

$$
\Sigma = O \Lambda O' = \sum_{i=1}^p \lambda_i e_i e_i'
$$

Dentro do **R** é possível realizar a decomposição espectral usando a função `eigen()`,

```{r echo=TRUE}
sigma <- matrix(c(8,-2,-2,5),nrow = 2)
sigma
eigen(sigma)
```

## Análise de Componentes Principais (PCA)

A análise de componentes principais se preocupa em conseguir explicar a variância e covariância de uma estrutura de variáveis através de algumas poucas combinações lineares. Tendo como principal objetivo dessa análise a redução de dimensionalidade e interpretação das relações. Essas combinações lineares são os componentes principais e são não correlacionadas entre sí. Quando assumimos que as variáveis originiais possuem distribuição normal, as componentes, além de não correlacionadas são normalmente distribuidas e idependentes. Os componentes principais são extraidos através da decomposição da matriz de covariância do vetor aleatório. Caso alguma trasnformação seja realizada nesse vetor, a decomposição será realizada na matriz de covariância do vetor transformado. Um caso muito utilizado, suponha que nossas variáveis estão em escalas muito diferentes, o PCA pode acabar por dar mais variabilidade a essa variável com escala superior, para isso então padronizamos o vetor. Utilizar a matriz de covariância do vetor transformado e a matriz de correlação do vetor originais são ações equivalentes nessa situação.

**Definição**: Seja *X* um vetor aleatório com $\mu = E(X)$ e $\Sigma = Var(X)$ e $(\lambda_i,e_i), i = 1,\dots,p$ os pares de autovalores e autovetores normalizados associados de $\Sigma$. Então,

$$
\begin{split}
Y = O'X,\quad \textrm{com}\quad O = [e_1,e_2,\dots,e_p],\textrm{ os componentes principais de X}\\
\textrm{ou seja}\\
Y = 
\begin{bmatrix}
Y_1\\
\vdots\\
Y_d
\end{bmatrix} \textrm{ com  } \quad Y_1 = e_1'X = e_{11}X_1 + e_{12}X_2 +  \dots + e_{1p}X_p 
\end{split}
$$

O primeiro componente principal. Os componentes principais de *X*, *Y*, são tais que,

$$
\begin{split}
\mu_y = E(Y) = E(O'X) = O'E(X) = O'\mu_x\\
\Sigma_y = Var(Y) = Var(O'X) = O'Var(X)O = O'\Sigma_xO = \Lambda
\end{split}
$$

ou seja

$$
cov(Y_i,Y_j) = 0, \forall i \neq j \textrm{ e } Var(Y_i) = \lambda_i
$$

A prova desse resultado pode ser vista em [@johnson2002applied, pp.432].

Descrevemos a variância total da população como sendo o somatório de todos os autovalores $\lambda$. A partir disso, podemos descrever a proporção da variância total explicada pela j-ésima componente como sendo:

$$
\frac{\lambda_j}{\sum_{i=1}^p \lambda_i} \qquad \forall j =1,\dots,p
$$

Para algum $p$ significativamente grande, podemos utilizar $d<p$ componentes ao invés das $p$ variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas $d$ componentes.

Se $Y_i = e'_iX, i =1\dots,p$ são as componentes principais obtidas da matriz de covariância, então

$$
\rho_{Y_i,X_j} = \frac{e_{ij}\sqrt{\lambda_i}}{\sigma_{jj}}, \quad \forall i,j=1,\dots p 
$$

São os coeficientes de correlação entre a componente $Y_i$ e a variável $X_j$

### Exemplo.

Para realmente entender a aplicabilidade da análise de componentes, vamos pegar um subconjunto do banco de dados mtcars, conjunto de dados no R base, consiste nas características de modelos de carros. Selecionaremos um subconjunto de colunas numéricas para conseguirmos trabalhar, considerando que PCA funciona melhor com variáveis numéricas. Há possibilidade de transformação de variáveis categoricas em variáveis *dummy*, porem o algoritmo não será tão preciso, também não sendo possível trabalhar com variáveis categóricas ordinais nesse caso.

(deixei um exemplo com mtcars pois o banco de dados trabalhado no livro tem poucas variaveis numericas)

```{r echo=FALSE}
dados <- mtcars[,c(1:7,10,11)]
dados |> head()
```

Podemos obter de forma simples no *R* as componentes, bem como a proporção da variância explicada, com a função `prcomp()`. Bem como citado tambem é comum padronização das variáveis devido a escala de cada característica, para isso basta informar o parâmetro `scale.` como *TRUE* dentro da função.

```{r echo=TRUE}
dados.pca <- dados |> 
  prcomp()
paste('dados não padronizados: ',sep = "\n")
dados.pca |> summary()

#padronizando as variaveis devido a diferenca de escalas

dados.pca.padr <- dados |>
  prcomp(scale. = T)
paste('dados padronizados:',sep = "\n")
dados.pca.padr |> summary()

#outras informacoes

dados.pca.padr |> print()
```

Da informação obtida por `print(dados.pca.padr)`, podemos identificar os *_loadings_* da análise. Os _loadings_ podem ser definidos como os coeficientes da combinação linear das variáveis originais de onde as componentes principais são construidas. De um ponto de vista matemático os _loadings_ são iguais às coordenadas das variáveis divididas pela raiz quadrada do autovalor associado ao componente. são úteis quando você deseja entender os resultados. Lembre-se de que cada nova variável  Y é uma combinação linear de todas as variáveis. A matriz de _loadings_ representa verticalmente quanto da variância de cada componente é explicada por cada variável original. Vemos por exemplo que, conforme `mpg` aumenta, a `PC1` tem um descrécimo. Os _loadings_ são muito úteis na hora de nomear nossas componentes por essa relação que faz com cada uma das variáveis.


### Número de Componentes Principais

Até agora foi descrito que podemos utilizar um número $d < p$ de componentes principais que contenha uma explicabilidade aproximada dos dados originais, mas qual seria esse valor $d$? Há um conjunto de técnicas para essa tomada de decisão, sendo uma delas por exemplo a proporção de variância acumulada total explicada pelas componentes $Y_1,\dots,Y_p$:

$$
\frac{\sum^d_{j=1}\lambda_j}{\sum_{i=1}^p \lambda_i}
$$

Esse valor é observado na função `prcomp()` já citada, como _cumulative Proportion_ no resultado do `summary()` da função.

Podemos utilizar como apoio gráfico e auxílio na tomada de decisão para o número de componentes é o *scree plot*, conhecido também como gráfico do cotovelo. Consiste na ordenação dos autovalores do maior para o menor, procurando por uma espécie de cotovelo dentro do gráfico. Selecionamos o número $i $ de componentes em que há um grande valor para observação $\lambda_{i-1}$ em comparação a observação $\lambda_i$ e uma pequena alteração da observação $\lambda_i$ para a observação $\lambda_{i+1}$. Observe a seguir

```{r}
#variancia explicada por cada componente
var_explicada = dados.pca.padr$sdev^2 / sum(dados.pca.padr$sdev^2)
library(ggplot2)

qplot(c(1:9), var_explicada) + 
  geom_line() + 
  xlab("Principal Componente") + 
  ylab("variancia explicada") +
  ggtitle("Scree Plot") +
  ylim(0, 1) + 
  scale_x_discrete(limits=c(1:9))
```

Podemos por meio, tanto do _scree plot_, quanto pelo valor da variância explicada acumulada, selecionar $d= 3$ componentes para reter, pela queda de 2 para 3 ser significante, enquanto a de 3 para 4 nem tanto. Reduzindo número de variáveis a 3.

##Métodos de Agrupamentos

A análise de agrupamentos ou *clusterização*, tem como objetivo, agrupar indivíduos da população usando como base medidas de similaridade entre eles, formando grupos heterogêneos entre sí com homogenuidade entre indivíduos de mesmo *cluster*. Muito utilizado na classificação de tipos de clientes de mercado, usuários de aplicativos, ou até mesmo em áres como psicologia, para agrupamentos de perfis de personalidade. Outro exemplo pode ser visto no trabalho (colocar link mariana).

### Medidas de dissimilaridade

De forma mais intutitiva, essas medidas de dissimilaridade seriam formas de numerar o quão próximo ou distânte a característica de um indivíduo (Idade por exemplo), se aproxima da mesma característica de outro indivíduo da mesma população. Não possuimos uma única forma de medida. Aqui apresentaremos as mais conhecidas e mais trabalhadas. Não existe uma métrica melhor, a eficácia de uma medida dependerá do caso em que a mesma será aplicada. Suponha que para cada elemento amostral será obtido o vetor $X = [X_{1},X_{2},\dots,X_{p}]'$ de medidas, onde $X_{i}$ representa a medida da i-ésima característica para a unidade amostral.

**Distância Euclidiana**: Essa é provavelmente a mais conhecida e usada medida de distância. Ela simplesmente é a distância geométrica no espaço multidimensional. Considere o i-ésimo e o j-ésimo indivíduo:

$$
d(X,Y) = \sqrt{\sum^p_{i=1}(X_i - Y_i)^2}
$$ 

**Distância de Canberra**: A distância de Camberra examina a soma das séries de diferenças fracionárias entre as coordenadas do par de observações.

$$
d(X,Y) = \sum^p_{i=1}\frac{|X_i - Y_i|}{|X_i| + |Y_i|}
$$ 

**Distância de Manhattan**: A distância de Manhattan ("City Block" ou "Geometria do Táxi") é uma forma de geometria em que a distância entre dois pontos é a soma das diferenças absolutas de suas coordenadas.

$$
d(X,Y) = \sum^p_{i=1}|X_i- Y_i|
$$

**Distância de Chebyshev**: Em matemática, distância de Chebyshev (ou distância de Tchebychev), métrica máxima ou $L_{\infty}$ métrica, é uma métrica definida em um espaço vetorial onde a distância entre dois vetores é a maior de suas diferenças ao longo de qualquer característica.

$$
d(X,Y) = \max_i(|X_i - Y_i|)
$$

**Distância de Minkowski**: A distância de Minkowski de ordem $k$, sendo $k$ inteiro, pode ser considerada uma generalização tanto da distância euclidiana quanto da distância de manhattan.

$$
d(X,Y) = \left(\sum^p_{i=1}|X_i - Y_i|^k\right)^\frac{1}{k}
$$ Todas essas distâncias aqui citadas podem ser acessadas pela função `dist()` do *R*, alterando o parãmetro `method` para a distância desejada, da seguinte forma :

```{r include=FALSE}
dados <- readRDS("~/LIVRO OOBR/dados/dados_covid[LIMPO].rds")
```

```{r}
db<- dados[1:10,c('sem_pri','idade_anos','dt_evoluca_2','ano','dt_sint')]
db$ano <- db
db |> na.omit() |> dist(method = 'euclidean')

```

Consguindo a distância euclidiana entre cada uma das 10 primeiras observações para as características selecionadas.

### Técnicas Hierárquicas

Dentro da estatística multivariada dividimos frequêntemente as técnicas aglomerativas em dois tipos: hierárquicos e não hierárquicos, sendo as hierárquicas classificadas em aglomerativa e divisivas. Métodos hierárquicos são geralmente utilizados na análise exploratória afim de encontrar um número ótimo de *clusters* para o conjunto de variáveis, para as técnicas não hierárquicas é necessário um valor prévio de grupos.

#### Técnicas Hierárquicas Aglomerativas

Considere cada observação como um grupo único, nos métodos aglomerativos vamos anexando cada grupo um ao outro em cada passo, usando suas medidas de similaridade para esse agrupamentos. Em cada instância do processo o par de grupos com a menor medida de dissimilaridade. Suponha a distância euclidiana por exemplo, em cada passo, verificaremos os $p$ grupos e anexamos o par com a menor distância euclidiana, seguindo para o próximo passo realizamos o mesmo com os $p-1$ grupos, até q sobre apenas 1 grupo com todas as observações. Mas esse é apenas um dos metodos de ligamento entre os grupos, observaremos outros nesse capítulo.

**Ligamento Simples**:

