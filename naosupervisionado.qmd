# Aprendizado não supervisionado

## Alguns conceitos básicos de algebra

Para melhor introduzir o campo do aprendizado não supervisionado, alguns conceitos de álgebra são necessários para compreender o que se passa por trás de cada algoritmo da análise de dados multivariada. Vamos introduzir com vetores e matriz, seguindo com decomposição espectral para então darmos inicio a área da estatística multivariada ou aprendizado não supervisionado. Não é objetivo desse livro demonstrar conceitos algébricos e nem se aprofundar demais no assunto [veja @anton2001algebra, e @johnson2002applied para mais demonstrações e conceitos].

### Definições importantes

**Vetor Aleatório** : Seja X um vetor contendo *p* componentes, onde cada componente é uma variável aleatória, isto é, $X_i$ é uma variável aleatória, $\forall\quad i =1,2,...,p$. Então *X* é chamado de vetor aleatório e é denotado por:

$$
 \begin{align}
  X &= \begin{bmatrix}
           X_{1} \\
           X_{2} \\
           \vdots \\
           X_{p}
         \end{bmatrix}
  \end{align}
$$

O vetor transposto do vetor aleatório *X* é denotadopor $X' = [X_1 X_2 X_3 ...X_p]$

**Vetor de Médias** : O vetor $\mu$ é chamado vetor de médias quando $E(X) = \mu$ onde *X* é um vetor aleatório. Dessa forma

$$
\begin{align}
  E(X) &= \begin{bmatrix}
           E(X_{1}) \\
           E(X_{2}) \\
           \vdots \\
           E(X_{p})
         \end{bmatrix}
  \end{align} = \mu = \begin{bmatrix}
           \mu_1 \\
           \mu_2 \\
           \vdots \\
           \mu_p
         \end{bmatrix}
$$

**Matriz de covariâncias** : A matriz de variâncias e covariâncias do vetor *X* é denotada por,

$$
Cov(X) = V(X) = Var(X) = \Sigma_{p\times p} = \begin{bmatrix}
           \sigma_{11} & \sigma_{12} & ... & \sigma_{1p}  \\
          \sigma_{21} & \sigma_{22} & ... & \sigma_{2p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \sigma_{p1} & \sigma_{p2} & ... & \sigma_{pp}
         \end{bmatrix}
$$

Onde $\sigma_{ii}$ representa a variância do elemento $X_i$ do vetor aleatório e $\sigma_{ij} = E[(X_i- \mu_i)(X_j - \mu_j)]$ $\forall\quad i,j = 1,\dots,p$. A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja $\Sigma = '\Sigma$. Sendo tambem não negativa definida, $a'\Sigma a \geq 0$ para todo vetor de constantes pertencentes aos reais.

**Matriz de correlação** : A matriz de correlação do vetor *X* é denotada por,

$$
P_{p\times p} = \begin{bmatrix}
           1 & \rho_{12} & \rho_{13}& ... & \rho_{1p}  \\
          \rho_{21} & 1 & \rho_{23}&... & \rho_{2p}  \\
          \rho_{31} & \rho_{32} & 1 &... & \rho_{3p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \rho_{p1} & \rho_{p2} &\rho_{p3}& ... & 1
         \end{bmatrix}
$$

Em que

$$
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} = \frac{\sigma_{ij}}{\sigma_i\sigma_j}
$$

**Auto Valores e Auto Vetores** : Se $\Sigma$ for uma matriz quadrada, ou seja $\Sigma_{p\times p}$, então um vetore não nulo $e$ em $R^n$ é denominado *autovetor* de $\Sigma$ se $\Sigma e$ for um múltiplo escalar de $e$, isto é,

$$
\Sigma e = \lambda e
$$

com algum escalar $\lambda$. O escalar $\lambda$ é denominado de *autovalor* de $\Sigma$, e dizemos que $e$ é um *autovetor associado a* $\lambda$. Por $\Sigma$ ser uma matriz não negativa definida seus autovalores $\lambda_i$ associados tambem serão não negativos. Os autovetores e autovalores serão necessários para a análise de componentes principais mais a frente abordada.

#### Equação característica

Ainda é necessário uma forma de encontrar os autovetores e autovalores associados a uma matriz $\Sigma$. Se $\Sigma$ for uma matriz quadrada, então $\lambda$ se, e somente se, $\lambda$ satisfaz a equação

$$
det(\lambda I - \Sigma) = 0
$$

Onde *det* é o determinante e $I$ a matriz identidade. Para esclarecimento, suponha como exemplo que,

$$
\Sigma = \begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
$$

Então,

$$
\begin{split}
det\left(\begin{bmatrix}
\lambda& 0\\
0 & \lambda
\end{bmatrix}
-  
\begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
\right) = 0\\
det\left(\begin{bmatrix}
\lambda - 8 & 2 \\
2 & \lambda-5
\end{bmatrix}
\right) = 0 \\
(\lambda - 8)\times(\lambda-5) - (2)\times(2) = 0
\end{split}
$$

Resolvendo a equação obtemos os valores de $\lambda_1 = 9$ e $lambda_2 = 4$, podemos encontrar os autovetores $v$ associados seguindo a definição:

$$
\begin{bmatrix}
8&-2\\
-2 & 5
\end{bmatrix}
\begin{bmatrix}
v_{11}\\
v_{12}
\end{bmatrix} = 
9\begin{bmatrix}
v_{11}\\
v_{12}
\end{bmatrix} \rightarrow v_{11} =- 2v_{12}
$$

Note que para cada autovalor temos infinitos possíveis autovetores dentro dos reais. Nos restringiremos aos autovetores normalizados.Dizemos que um vetor $e_i$ é normalizado quando:

$$
e_i = \begin{bmatrix}
e_{i1}\\
e_{i2}\\
\vdots\\
e_{ip}
\end{bmatrix}
$$

Em que

$$
||e_i|| = \sqrt{e^2_{i1} + e^2_{i2} + \dots + e^2_{ip}} = 1
$$

### Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.

O teorema da decomposição espectral é de extrema importância em álgebra matricial e estatística multivariada, ele relaciona a matriz com seus autovalores e autovetores normalizados.

Suponha $\Sigma$ a matriz de covariâncias. Então existe uma matriz ortogonal $O$(matriz no qual sua transposta é igual a sua inversa) tal que,

$$
O'\Sigma O = \begin{bmatrix}
\lambda_1 & 0 & 0 &\dots & 0\\
0&\lambda_2& 0 & \dots & 0 \\
0 & 0 &\lambda_3 &\dots & 0\\
\vdots& \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \dots& \lambda_p
\end{bmatrix} = \Lambda
$$

Onde $\lambda\_1 \geq \lambda\_2 \geq \dots \lambda\_p\geq0$ são os autovalores ordenados em ordem decrescente da matriz $\Sigma$. Nesse caso, dizemos que a matriz $\Sigma$ é similar à matriz $\Lambda$, que implica em:

-   $det(\Sigma) = det(\Lambda) = \prod^p_{i=1} \lambda_i$

-   traço$(\Sigma) =$ traço$(\Lambda) = \lambda_1 +\dots+\lambda_p$

Tem-se que a i-ésima coluna da matriz $O$ é o autovetor normalizado $e_i$ relacionado ao autovalor $\lambda_i$. Então a matriz $O$ é dada por $O = [e_1,e_2,\dots,e_p]$ e pelo teorema da decomposição espectral, podemos ver que:

$$
\Sigma = O \Lambda O' = \sum_{i=1}^p \lambda_i e_i e_i'
$$

Dentro do **R** é possível realizar a decomposição espectral usando a função `eigen()`,

```{r}
sigma <- matrix(c(8,-2,-2,5),nrow = 2)
sigma
eigen(sigma)
```

## Análise de Componentes Principais (PCA)

A análise de componentes principais se preocupa em conseguir explicar a variância e covariância de uma estrutura de variáveis através de algumas poucas combinações lineares. Tendo como principal objetivo dessa análise a redução de dimensionalidade e interpretação das relações. Essas combinações lineares são os componentes principais e são não correlacionadas entre sí. Quando assumimos que as variáveis originiais possuem distribuição normal, as componentes, além de não correlacionadas são normalmente distribuidas e idependentes. Os componentes principais são extraidos através da decomposição da matriz de covariância do vetor aleatório. Caso alguma trasnformação seja realizada nesse vetor, a decomposição será realizada na matriz de covariância do vetor transformado. Um caso muito utilizado, suponha que nossas variáveis estão em escalas muito diferentes, o PCA pode acabar por dar mais variabilidade a essa variável com escala superior, para isso então padronizamos o vetor. Utilizar a matriz de covariância do vetor transformado e a matriz de correlação do vetor originais são ações equivalentes nessa situação.

**Definição**: Seja *X* um vetor aleatório com $\mu = E(X)$ e $\Sigma = Var(X)$ e $(\lambda_i,e_i), i = 1,\dots,p$ os pares de autovalores e autovetores normalizados associados de $\Sigma$. Então,

$$
\begin{split}
Y = O'X,\quad \textrm{com}\quad O = [e_1,e_2,\dots,e_p],\textrm{ os componentes principais de X}\\
\textrm{ou seja}\\
Y = 
\begin{bmatrix}
Y_1\\
\vdots\\
Y_d
\end{bmatrix} \textrm{ com  } \quad Y_1 = e_1'X = e_{11}X_1 + e_{12}X_2 +  \dots + e_{1p}X_p 
\end{split}
$$

O primeiro componente principal. Os componentes principais de *X*, *Y*, são tais que,

$$
\begin{split}
\mu_y = E(Y) = E(O'X) = O'E(X) = O'\mu_x\\
\Sigma_y = Var(Y) = Var(O'X) = O'Var(X)O = O'\Sigma_xO = \Lambda
\end{split}
$$

ou seja

$$
cov(Y_i,Y_j) = 0, \forall i \neq j \textrm{ e } Var(Y_i) = \lambda_i
$$

A prova desse resultado pode ser vista em [@johnson2002applied, pp.432].

Descrevemos a variância total da população como sendo o somatório de todos os autovalores $\lambda$. A partir disso, podemos descrever a proporção da variância total explicada pela j-ésima componente como sendo:

$$
\frac{\lambda_j}{\sum_{i=1}^p \lambda_i} \qquad \forall j =1,\dots,p
$$  

Para algum $p$ significativamente grande, podemos utilizar $d<p$ componentes ao invés das $p$ variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas $d$ componentes.

Se $Y_i = e'_iX, i =1\dots,p$ são as componentes principais obtidas da matriz de covariância, então

$$
\rho_{Y_i,X_j} = \frac{e_{ij}\sqrt{\lambda_i}}{\sigma_{jj}}, \quad \forall i,j=1,\dots p 
$$

São os coeficientes de correlação entre a componente $Y_i$ e a variável $X_j$

### Exemplo.
Para realmente entender a aplicabilidade da análise de componentes, vamos pegar um subconjunto do conjunto de dados anteriormente trabalhado.

```{r}

```

##Métodos de Agrupamentos

A análise de agrupamentos ou _clusterização_, tem como objetivo, agrupar indivíduos da população usando como base medidas de similaridade entre eles, formando grupos heterogêneos entre sí com homogenuidade entre indivíduos de mesmo _cluster_. 
Muito utilizado na classificação de tipos de clientes de mercado, usuários de aplicativos, ou até mesmo em áres como psicologia, para agrupamentos de perfis de personalidade. Outro exemplo pode ser visto no trabalho (colocar link mariana).

### Medidas de similaridade

De forma mais intutitiva, essas medidas de similaridade seriam formas de numerar o quão próximo a característica de um indivíduo (Idade por exemplo), se aproxima da mesma característica de outro indivíduo da mesma população.
Não possuimos uma única forma de medida. Aqui apresentaremos as mais conhecidas e mais trabalhadas. Não existe uma métrica melhor, a eficácia de uma medida dependerá do caso em que a mesma será aplicada.
Suponha que para cada elemento amostral _j_ será obtido o vetor $X_j = [X_{1j},X_{2j},\dots,X_{pj}]'$ de medidas, onde $X_{ij}$ representa a medida da i-ésima característica para a j-ésima unidade amostral.

**Distância Euclidiana**: