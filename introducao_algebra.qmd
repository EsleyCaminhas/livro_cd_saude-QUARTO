# Alguns conceitos básicos de algebra

Para introduzir o campo do aprendizado não supervisionado, é importante entender alguns conceitos básicos de álgebra que são fundamentais para a compreensão dos algoritmos de análise de dados multivariada. Começaremos com vetores e matrizes e, em seguida, abordaremos a decomposição espectral. A partir daí, estaremos prontos para explorar a área da estatística multivariada ou aprendizado não supervisionado. Vale ressaltar que este livro não tem como objetivo demonstrar todos os conceitos algébricos necessários e nem se aprofundar demais no assunto. Para mais demonstrações e conceitos, consulte as obras de referência como [@anton2001algebra, e @johnson2002applied]

## Definições importantes

### Vetor Aleatório

Um vetor aleatório $\boldsymbol{X}$ é um vetor contendo $p$ componentes, onde cada componente é uma variável aleatória $X_i$, para $i = 1, 2, ..., p$.

$$
 \begin{align}
  \boldsymbol{X} &= \begin{bmatrix}
           X_{1} \\
           X_{2} \\
           \vdots \\
           X_{p}
         \end{bmatrix}
  \end{align}
$$

Suponha que o objeto em estudo é saúde sanguínea, o vetor de variáveis em estudo poderia vir a ser o número de vitaminas do tipo A, B e D no sangue, ou seja, um vetor com três componentes. O vetor transposto do vetor aleatório $\boldsymbol{X}$ é denotado por $\boldsymbol{X}' = [X_1 X_2 X_3 ...X_p]$.

### Vetor de Médias

O vetor $\boldsymbol{\mu}$ é chamado vetor de médias quando $E(\boldsymbol{X}) = \boldsymbol{\mu}$, onde $\boldsymbol{X}$ é um vetor aleatório. Dessa forma,

$$
\begin{align}
  E(\boldsymbol{X}) &= \begin{bmatrix}
           E(X_{1}) \\
           E(X_{2}) \\
           \vdots \\
           E(X_{p})
         \end{bmatrix}
  \end{align} = \boldsymbol{\mu} = \begin{bmatrix}
           \mu_1 \\
           \mu_2 \\
           \vdots \\
           \mu_p
         \end{bmatrix}.
$$

Ou seja, sendo cada componente $X_i$ do vetor uma variável aleatória, $\mu_i$ representa a respectiva esperança dessa variável.

### Matriz de Covariâncias

A matriz de variâncias e covariâncias do vetor $\boldsymbol{X}$ é denotada por

$$
Cov(\boldsymbol{X}) = V(\boldsymbol{X}) = Var(\boldsymbol{X}) = 
\boldsymbol{\Sigma}_{p\times p} = \begin{bmatrix}
           \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p}  \\
          \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp}
         \end{bmatrix}.
$$

Onde $\sigma_{ii}$ representa a variância do elemento $X_i$ do vetor aleatório e $\sigma_{ij} = E[(X_i- \mu_i)(X_j - \mu_j)]$ a covariância entre as componentes $X_i$ e $X_j$, $\forall\quad i,j = 1,\dots,p$. A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja $\boldsymbol\Sigma = \boldsymbol\Sigma'$. Sendo tambem não negativa definida, $a'\boldsymbol\Sigma a \geq 0$ para todo vetor de constantes pertencentes aos reais. A matriz de covariância pode ser facilmente calcula utilizando a função `cov()` da linguagem **R**.

### Matriz de correlação

A matriz de correlação do vetor $\boldsymbol{X}$ é denotada por,

$$
\boldsymbol{P}_{p\times p} = \begin{bmatrix}
           1 & \rho_{12} & \rho_{13}& ... & \rho_{1p}  \\
          \rho_{21} & 1 & \rho
          _{23}&... & \rho_{2p}  \\
          \rho_{31} & \rho_{32} & 1 &... & \rho_{3p}  \\
            \vdots &\vdots & \ddots &\vdots \\
           \rho_{p1} & \rho_{p2} &\rho_{p3}& ... & 1
         \end{bmatrix}.
$$

Em que,

$$
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii}\sigma_{jj}}} = \frac{\sigma_{ij}}{\sigma_i\sigma_j}.
$$

Sendo assim, $\rho_{ij}$ a correlação entre a i-ésima e a j-ésima componente do vetor aleatório $\boldsymbol{X}$, $\forall i,j = 1,\dots, p$, em que, se $j = i$ a correlação assume o valor máximo de 1. De forma análoga à matriz de covariância, pode-se utilizar a função `cor()`, para obter a matriz de correlação $\boldsymbol{P}$ do vetor aleatório $\boldsymbol{X}$.

Um dos conceitos fundamentais da álgebra linear, bem como da estatística multivariada é o de autovalores e autovetores de uma matriz. A decomposição espectral de uma matriz é a representação de uma matriz simétrica em termos de seus autovetores e autovalores. Os autovetores são vetores especiais que não mudam de direção quando multiplicados pela matriz original, mas apenas são escalados por um fator conhecido, como autovalor correspondente.

### Auto Valores e Auto Vetores

Em estatística multivariada, trabalhamos habitualmente com matrizes quadradas de covariância e correlação, ou seja, matrizes em que o número de colunas e linhas é o mesmo. Nesse contexto, um vetor não nulo $\boldsymbol{e}$ é denominado autovetor de uma matriz quadrada $\boldsymbol\Sigma_{p\times p}$ se $\boldsymbol\Sigma \boldsymbol{e}$ for um múltiplo escalar de $\boldsymbol{e}$, isto é,

$$
\boldsymbol\Sigma \boldsymbol{e} = \lambda \boldsymbol{e},
$$

com algum escalar $\lambda$. O escalar $\lambda$ é denominado de autovalor de $\boldsymbol\Sigma$, e dizemos que $\boldsymbol{e}$ é um autovetor associado a $\lambda$. Os autovetores são usados para reduzir a dimensionalidade de um conjunto de dados e extrair informações importantes sobre as relações entre as variáveis.

Note que é possível descrever a equação acima da seguinte forma:

$$
\begin{split}
\boldsymbol\Sigma \boldsymbol{e} = \lambda \boldsymbol{e}\\
\lambda\boldsymbol{e}-\boldsymbol\Sigma\boldsymbol{e}  = 0 \\
(\lambda \boldsymbol I - \boldsymbol\Sigma)\boldsymbol{e} = 0.
\end{split}
$$

em que $\boldsymbol I$ é a matriz identidade de mesma dimensão que $\boldsymbol\Sigma$.

Uma importante relação entre autovalores e autovetores é dada pela equação característica de $\boldsymbol\Sigma$, que é obtida resolvendo a equação $(\lambda \boldsymbol I - \boldsymbol\Sigma)\boldsymbol{e} = 0$ para $\lambda$. Para que a equação tenha solução não trivial, ou seja, que o vetor $\boldsymbol{e}$ não seja o vetor nulo, é necessário que o determinante da matriz $(\lambda \boldsymbol I - \boldsymbol\Sigma)$ seja igual a zero. Essa equação polinomial é definida como equação característica de $\boldsymbol\Sigma$. Os autovalores podem ser encontrados resolvendo a equação para $\lambda$. Uma definição mais formal seria:

### Equação característica

A equação característica de uma matriz quadrada $\boldsymbol{\Sigma}$ de ordem $p$ é definida como a equação polinomial de grau $p$ dada por:

$$
det(\lambda \boldsymbol I - \boldsymbol\Sigma) = 0.
$$

onde os valores de $\lambda$ que satisfazem a equação são determinados como os autovalores de $\boldsymbol\Sigma$ e *det* representa o determinante da matriz resultante da equação. Para esclarecimento, suponha como exemplo que,

$$
\boldsymbol\Sigma = \begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
$$

Então,

$$
\begin{split}
det\left(\begin{bmatrix}
\lambda& 0\\
0 & \lambda
\end{bmatrix}
-  
\begin{bmatrix}
8 & -2 \\
-2 & 5
\end{bmatrix}
\right) = 0\\
det\left(\begin{bmatrix}
\lambda - 8 & 2 \\
2 & \lambda-5
\end{bmatrix}
\right) = 0 \\
(\lambda - 8)\times(\lambda-5) - (2)\times(2) = 0\\
\lambda^2 - 13\lambda + 36 = 0
\end{split}
$$

Resolvendo a equação obtemos os valores de $\lambda_1 = 9$ e $lambda_2 = 4$, podemos encontrar os autovetores $\boldsymbol{e}_i$ associados seguindo a definição. Para $\lambda_1$ temos:

$$
\begin{bmatrix}
\lambda_1-8&2\\
2 & \lambda_1-5
\end{bmatrix}
\begin{bmatrix}
e_{11}\\
e_{12}
\end{bmatrix} = 
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
1&2\\
2 & 4
\end{bmatrix}
\begin{bmatrix}
e_{11}\\
e_{12}
\end{bmatrix} = 
\begin{bmatrix}
0\\
0
\end{bmatrix}
\rightarrow e_{11} = - 2e_{12}.
$$

Da mesma forma para $\lambda_2$:

$$
\begin{bmatrix}
\lambda_2-8&2\\
2 & \lambda_2-5
\end{bmatrix}
\begin{bmatrix}
e_{21}\\
e_{22}
\end{bmatrix} = 
\begin{bmatrix}
0\\
0
\end{bmatrix}\\
\begin{bmatrix}
-4&2\\
2 & -1
\end{bmatrix}
\begin{bmatrix}
e_{21}\\
e_{22}
\end{bmatrix} = 
\begin{bmatrix}
0\\
0
\end{bmatrix}
\rightarrow 2e_{21} = e_{22}.
$$

Após encontrar os autovalores de uma matriz $\boldsymbol{\Sigma}$, é importante observar que para cada autovalor há infinitos autovetores possíveis. Para evitar essa ambiguidade, é comum restringir os autovetores a um conjunto de vetores normalizados. Um vetor $\boldsymbol{e}_i$ é normalizado quando a sua norma euclidiana é igual a 1, isto é, quando:

$$
||\boldsymbol{e}_i|| = \sqrt{e^2_{i1} + e^2_{i2} + \dots + e^2_{ip}} = 1.
$$

Dessa forma, garantimos que os autovetores estão no mesmo espaço de dimensão $p$ que as variáveis originais. Para o exemplo então, temos:

$$
\begin{split}
e_{11} =- 2e_{12}\\
\sqrt{e_{11}^2 + e_{12}^2 } = 1\\
\sqrt{(-2e_{12})^2 + e_{12}^2 } = 1\\
\sqrt{5e_{12}^2 } = 1\\
e_{12} = \frac{1}{\sqrt{5}}  \approx 0.45\\
e_{11} \approx - 0.90
\end{split}
$$

O mesmo se segue para $\lambda_2$, onde obtemos o vetor $\boldsymbol{e}_2' = [-0.45,-0.90]$, assim então os vetores normalizados:

$$
\boldsymbol{e}_1 = \begin{bmatrix}
 -0.90\\
 0.45
\end{bmatrix};\qquad\boldsymbol{e}_2 = \begin{bmatrix}
-0.45\\
-0.90
\end{bmatrix}.
$$

Para entender melhor a relação entre matriz de covariância/correlação e seus autovetores e autovalores normalizados, utilizamos o conceito de decomposição espectral. Através desse teorema, podemos decompor a matriz em questão em seus autovetores e autovalores normalizados, o que nos permite obter informações valiosas sobre as relações entre as variáveis originais.

## Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.

Para uma matriz quadrada simétrica $\boldsymbol\Sigma$ de ordem $p$, é possível encontrar uma matriz $\boldsymbol{O}$ cujas colunas consistem nos autovetores normalizados de $\boldsymbol\Sigma$, e uma matriz diagonal $\boldsymbol \Lambda$ cujos elementos diagonais são os autovalores correspondentes, de tal forma que $\boldsymbol\Sigma$ pode ser escrito como $\boldsymbol\Sigma = \boldsymbol O \boldsymbol \Lambda \boldsymbol O'$. Da mesma forma, podemos dizer que:

$$
\boldsymbol O'\boldsymbol\Sigma \boldsymbol O = \begin{bmatrix}
\lambda_1 & 0 & 0 &\dots & 0\\
0&\lambda_2& 0 & \dots & 0 \\
0 & 0 &\lambda_3 &\dots & 0\\
\vdots& \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \dots& \lambda_p
\end{bmatrix} = \boldsymbol\Lambda
$$

Onde $\lambda_1 \geq \lambda_2 \geq \dots \lambda_p\geq0$ são os autovalores da matriz $\boldsymbol\Sigma$. Nesse caso, dizemos que a matriz $\boldsymbol \Sigma$ é similar à matriz $\boldsymbol\Lambda$, que implica em:

-   $det(\boldsymbol\Sigma) = det(\boldsymbol\Lambda) = \prod^p_{i=1} \lambda_i$;

-   traço$(\boldsymbol\Sigma) =$ traço$(\boldsymbol\Lambda) = \lambda_1 +\dots+\lambda_p$.

Tem-se que a i-ésima coluna da matriz $\boldsymbol O$ é o autovetor normalizado $\boldsymbol{e}_i$ relacionado ao autovalor $\lambda_i$. Então a matriz $\boldsymbol O$ é dada por $O = [\boldsymbol{e}_1,\boldsymbol{e}_2,\dots,\boldsymbol{e}_p]$. De forma simples, podemos ver que:

$$
\Sigma = \boldsymbol{O \Lambda O'}= \sum_{i=1}^p \lambda_i \boldsymbol{e}_i \boldsymbol{e}_i'
$$

Os autovetores normalizados em $\mathbf{O}$ são ortogonais entre si e formam uma base para o espaço de dimensão $p$ em que a matriz $\mathbf{\Sigma}$ atua. Cada autovetor representa uma direção no espaço em que a variabilidade dos dados é máxima. Os autovalores em $\mathbf{\Lambda}$ representam a variância explicada por cada direção (ou autovetor) correspondente. Os autovalores são ordenados em ordem decrescente, de forma que o primeiro autovalor corresponde à direção de maior variabilidade e assim por diante. Observe na figura abaixo, onde é representado um conjunto de dados gerados aleatoriamente de 3 componentes ($X_1,X_2,X_3$), e o comportamento de cada um dos autovetores para cada uma das dimensões.

```{r echo=FALSE,warning=FALSE,message=FALSE}
library(plotly)
library(tidyverse)

visualize_eigen <- function(dados) {

  # Covariance matrix
  cov_matrix <- cov(dados)

  # Eigenvectors and eigenvalues
  decomp <- eigen(cov_matrix)
  autovetor <- decomp$vectors
  autovalor <- decomp$values

  # Scatter plot of original data
  x1 <- dados[,1]
  x2 <- dados[,2]
  x3 <- dados[,3]

  df <- data.frame(x1, x2, x3)

  fig <- plot_ly(df, x = x1, y = x2, z = x3, type = "scatter3d", mode = "markers",
                 marker = list(color = "lightblue", size = 2),
                 name = "Observações")

  # Plot eigenvectors as arrows
  for (i in 1:length(autovalor)) {
    arrow_start <- df %>% summarise_all(mean) %>% as.numeric()
    arrow_end <- autovetor[,i] * sqrt(autovalor[i]) + arrow_start

    fig <- fig %>%
      add_trace(x = c(arrow_start[1], arrow_end[1]),
                y = c(arrow_start[2], arrow_end[2]),
                z = c(arrow_start[3], arrow_end[3]),
                type = "scatter3d",
                mode = "lines",
                line = list(width = 4),
                name = paste0("autovetor ", i))
  }

  # Create text explaining eigenvalues and eigenvectors
  text <- paste0("A primeira componente explica ", round((autovalor[1] / sum(autovalor)) * 100, 2),
                 "% da variância.\nA segunda componente explica ", round((autovalor[2] / sum(autovalor)) * 100, 2),
                 "% da variância.")

  # Add text to plot
  fig <- fig %>%
    layout(
           scene = list(xaxis = list(title = "X1"),
                        yaxis = list(title = "X2"),
                        zaxis = list(title = "X3")),
           annotations = list(x = 0, y = -0.01, text = text,
                              showarrow = FALSE,
                              font = list(size = 10)))

  return(fig)
}

# Gerando dados de exemplo com média zero e matriz de covariância específica
set.seed(123)
data <- mvtnorm::rmvnorm(n = 1000, mean = c(0,0,0), sigma = matrix(c(1, 0.5, 0.2,
                                                            0.5, 1, 0.3,
                                                            0.2, 0.3, 1), nrow = 3))

# Visualizando dados e autovetores/autovalores
visualize_eigen(data)
```

Os autovetores são representados por setas que partem do centro de massa dos dados e indicam a direção e a magnitude da variação dos dados nessa direção. A magnitude da variação é dada pelo valor do autovalor correspondente a cada autovetor, que indica a importância daquele eixo.

Dentro do **R** é possível realizar a decomposição espectral usando a função `eigen()`, da seguinte forma:

```{r echo=TRUE, warning=FALSE}
sigma <-matrix(c(8,-2,-2,5),nrow = 2)
sigma
eigen(sigma)
```

Nos devolvendo os autovalores e autovetores normalizados para a matriz inserida na função, os mesmos calculados de forma manual anteriormente.
