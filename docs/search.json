[
  {
    "objectID": "tutorialr.html#sobre-o-software-r",
    "href": "tutorialr.html#sobre-o-software-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.1 Sobre o software R",
    "text": "A.1 Sobre o software R\nR é um ambiente computacional e uma linguagem de programação para manipulação, análise e visualização de dados. Para essas finalidades, ele é considerado um dos melhores e um dos mais utilizados dentre os ambientes computacionais disponíveis. O R é mantido pela R Development Core Team e está disponível para diferentes sistemas operacionais: Linux, Mac e Windows.\nO software é livre, ou seja, gratuito, com código aberto em uma linguagem acessível. Nele, estão implementadas muitas metodologias estatísticas. Muitas dessas fazem parte do ambiente base do R e outras acompanham o ambiente sob a forma de pacotes, o que o torna altamente flexível. Os pacotes são bibliotecas com funções extras devidamente documentadas criadas para ajudar a resolver problemas de diferentes áreas do conhecimento.\nO R possui uma comunidade extremamente ativa, engajada desde o aprimoramento de ferramentas e desenvolvimento de novas bibliotecas, até o suporte aos usuários. Sobre o desenvolvimento de novas bibliotecas, um pesquisador em Estatística que desenvolve um novo modelo estatístico pode disponibilizá-lo em um pacote acessível aos usuários que se interessem pelo modelo, por exemplo. Além disso, a disponibilidade e compartilhamento da pesquisa em um pacote no R é uma boa prática quando falamos de reprodutibilidade na ciência. Ainda nesse ponto, realizar as análises de uma pesquisa aplicada em um programa livre e acessível a todos é um dos principais pontos para permitir reprodutibilidade.\nOptar por programar em R também implica na escolha de uma IDE (Integrated Development Environment). Uma IDE é um ambiente de desenvolvimento integrado onde podem ser combinadas ferramentas utilizadas no desenvolvimento de aplicações, como um editor de código ou uma ferramenta de preenchimento inteligente de código. Para o R, a IDE mais popular entre os usuários é o RStudio. O RStudio é um conjunto de ferramentas integradas projetadas para editar e executar os códigos em R. Assim, quando for o interesse utilizar o R, basta abrir o RStudio (R é automaticamente carregado)."
  },
  {
    "objectID": "tutorialr.html#instalação-do-r",
    "href": "tutorialr.html#instalação-do-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.2 Instalação do R",
    "text": "A.2 Instalação do R\nA seguir, será apresentado o passo a passo de como instalar o R e o RStudio para os três sistemas operacionais: Windows, MAC e Linux, respectivamente.\n\nA.2.1 R no Windows\nA forma mais simples de instalar o R consiste em primeiramente acessar a página do software pelo endereço https://cloud.r-project.org/. Ao acessar a página haverão três opções para download, sendo cada uma referente a um sistema operacional em específico. Assim, para conseguir instalar o software em um sistema operacional Windows basta primeiramente clicar no link Download R for Windows.\n\n\n\n\n\nPasso 1\n\n\n\n\nQuatro subdiretórios irão surgir, dentre eles é necessário clicar na base, pois este contém a distribuição base do R para instalação.\n\n\n\n\n\nPasso 2\n\n\n\n\nO subdiretório base irá redirecionar para uma página que contém o link de download do arquivo de instalação do software. Este por sua vez, pode ser identificado como Download + versão atual do R + for Windows.\n\n\n\n\n\nPasso 3\n\n\n\n\nFeito isso, um arquivo executável será baixado no computador, o qual, ao abri-lo, deverá escolher o idioma (português brasileiro) e simplesmente clicar em Avançar toda vez que o cliente de instalação requerer.\n\n\n\n\n\nPasso 4\n\n\n\n\n\n\n\n\n\nPasso 5\n\n\n\n\nAssim, uma instalação padrão do software será instalada no computador.\n\n\nA.2.2 R no MAC\nDa mesma forma a qual iniciamos a instalação do R no Windows também iniciaremos no MAC, onde é necessário acessar o endereço https://cloud.r-project.org/ e clicar no link Download R for macOS.\n\n\n\n\n\nPasso 1\n\n\n\n\nO link irá redirecionar para uma página com arquivos de extensão .pkg típicos de macOS. É importante verificar qual versão disponível é a ideal para seu sistema. A versão do tipo arm64.pkg é referente a versão mais recente do macOS na data deste material.\n\n\n\n\n\nPasso 2\n\n\n\n\nTendo feito o download do arquivo, basta abri-lo para um cliente de instalação ficar disponível, e então, para efetuar uma instalação padrão deve-se seguir as instruções do cliente sem customizações aditivas assim como foi feito para o Windows.\n\n\nA.2.3 R no Linux\nA instalação do R no Linux depende da distribuição sendo utilizada. Basta acessar o mesmo endereço https://cloud.r-project.org/ utilizado na instalação dos outros sistemas, e clicar no link Download R for Linux.\n\n\n\n\n\nPasso 1\n\n\n\n\nFeito isso irá aparecer as opções de distribuições para Linux em que o software está disponível para download, basta selecionar a distribuição compatível. Caso sua distribuição for Ubuntu por exemplo, clicamos nela no respectivo link.\n\n\n\n\n\nPasso 2\n\n\n\n\nAssim, irá ser redirecionado para uma página com as devidas instruções de instalação do R para a distribuição escolhida. Basta seguir as instruções para efetuar uma instalação padrão do software."
  },
  {
    "objectID": "tutorialr.html#instalação-do-rstudio",
    "href": "tutorialr.html#instalação-do-rstudio",
    "title": "Appendix A — Tutorial de R",
    "section": "A.3 Instalação do RStudio",
    "text": "A.3 Instalação do RStudio\nO RStudio é um conjunto de ferramentas integradas projetadas (IDE - Integrated Development Environment) da linguagem R para auxiliar na produtividade ao utilizar o R. Embora não seja obrigatório o seu uso, é um consenso na comunidade de que o uso do RStudio facilita o aprendizado enquanto acelera a produtividade do usuário, tornando-o indispensável principalmente para iniciantes.\nNo ano de 2022, RStudio iniciou um processo de transição de nome onde passou a se chamar Posit. O objetivo por de trás desse processo se dá na inclusão da comunidade de Python ao R, dado o crescimento notório do Python na área de análise de dados nos últimos anos e que ambas as linguagens se complementam.\nO primeiro passo para instalar o RStudio é acessar o site da Posit e ir até a página de download que pode ser acessada pelo endereço https://posit.co/download/rstudio-desktop/. Feito isso, a página irá apresentar algumas opções, dentre elas uma breve tabela com arquivos executáveis mais recentes disponíveis de instalação do RStudio.\n\n\n\n\n\nArquivos executáveis de instalação\n\n\n\n\nDentre os arquivos executáveis está a versão mais recente para Windows (retângulo vermelho), macOS (retângulo azul) e para diferentes distribuições do Linux (retângulo verde). É preciso fazer o download conforme o seu sistema operacional.\nApós o download basta abrir o arquivo executável baixado e seguir as instruções do cliente para que a instalação seja feita.\n\n\n\n\n\nRStudio aberto pela primeira vez"
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-rstudio",
    "href": "tutorialr.html#primeiros-passos-no-rstudio",
    "title": "Appendix A — Tutorial de R",
    "section": "A.4 Primeiros passos no RStudio",
    "text": "A.4 Primeiros passos no RStudio\nO RStudio é uma ferramenta que por padrão é dividida em quatro painéis, sendo que cada um deles contêm abas com diferentes utilidades.\n\n\n\n\n\nPainéis do RStudio\n\n\n\n\nA seguir descrevemos melhor os painéis e algumas abas comumente utilizadas do RStudio:\n Editor/Scripts: local para escrever códigos (principalmente arquivos em formato .R).\n Console: onde se executa os códigos e visualiza resultados.\n Aqui, é possível acessar todos os objetos criados em Environment e o histórico de códigos executados em History e conectar fonte de dados em Connections.\n Nessa área, temos diversas utilidades frequentemente utilizadas:\n\npodemos acessar arquivos e pastas do computador pela aba Files;\nna aba Plots, visualizamos resultados em que são gerados figuras (como gráficos e tabelas), caso um comando desse tipo tenha sido executado;\nem Packages, podemos manusear pacotes (instalar, atualizar ou deletar);\nna aba Help temos acesso à documentação de uma determinada função quando utilizado o comando help() ou ?. Uma função nada mais é do que uma estrutura de código pronta com a forma de acesso nome(argumento) que recebe argumentos de entrada e retorna uma resposta. O próprio comando help() é uma função.\n\nO usuário pode alterar as configurações padrões do RStudio ao acessar as opções globais.\n\n\n\n\n\nOpções globais\n\n\n\n\nPara usuários iniciantes, é recomendável configurar a aparência e estrutura (layout) dos painéis conforme a própria preferência para tornar a experiência de uso mais confortável.\n\n\n\n\n\nMenu de aparência\n\n\n\n\nPodemos alterar o layout pelo menu Panel Layout. Usualmente, os painéis são estruturados de forma que o painel Console fique ao lado do painel de Script (Source/Editor), facilitando a visualização dos comandos rodados.\n\n\n\n\n\nMenu de estruturação dos painéis\n\n\n\n\n\nA.4.1 Projetos\nUma funcionalidade importante é a criação de projetos, permitindo dividir o trabalho em múltiplos ambientes, cada um com o seu diretório, documentos e workspace.\nPara criar um projeto, os seguintes passos podem ser seguidos:\n\nClique na opção File do menu, e então em New Project.\nClique em New Directory.\nClique em New Project.\nEscreva o nome do diretório (pasta) onde deseja manter seu projeto, exemplo: “my_project”.\nClique no botão Create Project.\n\nPara criar um novo script para escrever os códigos, vá em File -> New File -> R Script.\n\n\nA.4.2 Boas práticas\nComente bem o seu código: é possível fazer comentários usando o símbolo #. É sempre bom explicar o que uma variável armazena, o que uma função faz, por que alguns parâmetros são passados para uma determinada função, qual é o objetivo de um trecho de código, etc.\nEvite linhas de código muito longas: usar linhas de código mais curtas ajuda na leitura do código.\nEscreva um código organizado. Por exemplo, adote um padrão no uso de minúsculas e maiúsculas, uma lógica única na organização de pastas e arquivos, pode ser adotada uma breve descrição (como comentário) indicando o que um determinado script faz.\nCarregue todos os pacotes que irá usar sempre no início do arquivo: quando alguém abrir o seu código será fácil identificar quais são os pacotes que devem ser instalados e quais dependências podem existir."
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-r",
    "href": "tutorialr.html#primeiros-passos-no-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.5 Primeiros passos no R",
    "text": "A.5 Primeiros passos no R\nO código pode ser escrito no Script e então ser executado ao apertar o botão Run (localizado no painel de Script) ou com o atalho no teclado Ctrl + Enter. É importante salientar que, apenas a linha em que o símbolo de inserção de código (barra vertical do cursor) estiver é que será executada. Para executar múltiplas linhas simultaneamente, é necessário selecionar as linhas desejadas e então utilizar o comando de execução mencionado.\nOutra forma de escrever e executar códigos é através do painel Console. Normalmente, o Console é utilizado para executar códigos sem muitas linhas de estruturação ou para fazer testes rápidos (ex: uso do R como calculadora). Para rodar o código diretamente pelo painel Console, basta escrevê-lo na linha em que contém o símbolo >, o qual indica que o R está pronto para receber comandos, e então pressionar a tecla Enter.\n\nA.5.1 R como calculadora\nUma das utilidades do R é utilizá-lo como uma calculadora, onde podemos realizar contas matemáticas simples até as mais complexas.\nPor padrão, o R entende as linhas de códigos da esquerda para a direita e de cima para baixo. No entanto, ao se deparar com operações matemáticas, ele respeita algumas prioridades. A operação com maior para a menor prioridade é: potenciação > multiplicação ou divisão > adição ou subtração. Caso haja a necessidade de alterar essa ordem, isso pode ser feito utilizando parênteses.\n\n# Adição.\n10 + 15\n\n[1] 25\n\n# Subtração.\n10 - 2\n\n[1] 8\n\n# Multiplicação.\n2 * 10\n\n[1] 20\n\n# Divisão.\n30/2\n\n[1] 15\n\n# Raiz quadrada.\nsqrt(4)\n\n[1] 2\n\n# Potência.\n2^2\n\n[1] 4\n\n# Potência > Multiplicação > Soma.\n2^2 + 5 * 2\n\n[1] 14\n\n# Multiplicação > Potência > Soma.\n2^2 + (5 * 2)\n\n[1] 14\n\n# Potência > Soma > Multiplicação.\n2 * (2^2 + 5) \n\n[1] 18\n\n\nCaso um comando incompleto seja dado, como 10 ^, o R mostrará um +. Isso não tem a ver com a soma e apenas que o R está esperando que o comando que estava sendo escrito seja finalizado. Para recomeçar, basta terminar a escrita do comando ou apenas pressionar Esc.\nVale também ressaltar que se um comando que o R não reconhece for dado, ele retornará uma mensagem de erro.\n\n\nA.5.2 Atribuição\nOs objetos (também chamados de variáveis) são “locais” onde são guardadas informações (números, textos etc). O ato de “guardar” informações dentro de objetos é chamado de atribuição, e pode ser feito com <- ou =. Embora ambas as formas funcionem, na prática, o sinal <- é usualmente utilizado para atribuições enquanto que o sinal = é utilizado para configurar argumentos de funções.\n\n# Variável x recebe o número 5 de diferentes formas.\nx <- 5 \n\nx = 5\n\ny = (2^2 + 6) - 4\nx <- y - 1\n\nUm ponto importante a se atentar é que o R é case sensitive, isto é, faz a diferenciação entre as letras minúsculas e maiúsculas. Portanto, x é diferente de X.\n\n# Dica: Podemos obter o output do comando ao colocá-lo em volta de ().\n(x <- 10/2)\n\n[1] 5\n\n# Ao chamar X obteremos um erro, pois a variável criada era minúscula.\nX\n\nError in eval(expr, envir, enclos): object 'X' not found\n\n\n\n\nA.5.3 Objetos em R\nExistem cinco classes básicas de objetos no R:\n\nCharacter: “UAH!”\nNumeric: 0.95 (números reais)\nInteger: 100515 (inteiros)\nComplex: 2 + 5i (números complexos, a + bi)\nLogical: TRUE (booleanos, TRUE/FALSE)\n\nApós realizar a atribuição, podemos verificar a classe do objeto com a função class().\n\n# Character/texto, deve estar entre aspas \"\".\nx <- \"gestante\"; \nclass(x) \n\n[1] \"character\"\n\n# Numeric/números reais.\nx <- 0.9 \nclass(x) \n\n[1] \"numeric\"\n\n# Integer/números inteiros, tem que ser atribuído com o valor acompanhado de um ‘L’.\nx <- 5L\nclass(x)\n\n[1] \"integer\"\n\n# Complex/números complexos.\nx <- 2 + 5i\nclass(x)\n\n[1] \"complex\"\n\n# logical/valores lógicos.\nx <- TRUE\nclass(x)\n\n[1] \"logical\"\n\n\nOs valores lógicos são apresentados em letra maiúscula. Isso é muito importante, pois o R diferencia letras maiúsculas de minúsculas. Então, valores lógicos só são reconhecidos se escritos como TRUE ou FALSE. Além disso, cada valor lógico assume um valor numérico, sendo TRUE referente ao valor 1 e FALSE referente ao valor 0.\n\n# Operações matemáticas com valores lógicos.\n(TRUE*2)^2 + TRUE + FALSE + 2*TRUE\n\n[1] 7\n\n\nMuitas vezes é do interesse do usuário apagar objetos que foram criados, principalmente se for rodar códigos prontos em um ambiente que outra pessoa estava trabalhando, pois pode haver objetos já criados com os mesmos nomes dos que se encontram no código/script de interesse, o que poderá levar a erros e dificuldades de execução. A remoção de objetos pode ser feito com a função rm() ou remove().\n\n# Criando o objeto x.\nx <- 20\nx\n\n[1] 20\n\n# Removendo o objeto x.\nrm(x)\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\n# Removendo todos os objetos criados.\n(x <- 1)\n\n[1] 1\n\n(y <- 2)\n\n[1] 2\n\nrm(list=ls())\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\ny\n\nError in eval(expr, envir, enclos): object 'y' not found\n\n\nVale notar que ao utilizar a função rm() ou a função remove() para remover todos os objetos criados, é necessário incluir um argumento chamado list onde utilizamos o sinal de = para especificar os objetos a serem deletados. A função ls() lista todos os objetos criados até o momento.\n\n\nA.5.4 Vetores\nNo R a estrutura mais básica de dados é chamada de Vector (vetor), podendo aparecer no formado Atomic (atômico) ou no formado de list (lista). Dentre os vetores atômicos existem quatro tipos, sendo eles: Character, Integer, Double e Logical.\n\nCom vetores podemos atribuir vários valores a um mesmo objeto. Para entrar com vários números (ou nomes, ou qualquer outro grupo de coisas), precisamos usar uma função para dizer ao programa que os valores serão combinados em um único vetor. Para criar vetores atômicos a função c() é a mais usual por podermos criar vetores atômicos de todos os tipos diretamente. Também podemos utilizar a função seq() e o símbolo : para criar vetores do tipo Integer, e a função rep() que é capaz de criar vetores Double, por exemplo. Além disso, podemos verificar o tipo do vetor com a função typeof().\n\n# Vetor Double com a função c().\n(vetor1 <- c(2.5, 3, 4/5))\n\n[1] 2.5 3.0 0.8\n\ntypeof(vetor1)\n\n[1] \"double\"\n\n# Vetor Integer com a função c().\n(vetor2 <- c(5L, 7L, 9L))\n\n[1] 5 7 9\n\ntypeof(vetor2)\n\n[1] \"integer\"\n\n# Vetor Character com a função c().\n(vetor3 <- c(\"hospital1\", \"hospital2\"))\n\n[1] \"hospital1\" \"hospital2\"\n\ntypeof(vetor3)\n\n[1] \"character\"\n\n# Vetor Logical com a função c().\n(vetor4 <- c(TRUE, FALSE, FALSE, TRUE))\n\n[1]  TRUE FALSE FALSE  TRUE\n\ntypeof(vetor4)\n\n[1] \"logical\"\n\n# Vetor Integer com a função seq().\n(vetor5 <- seq(1, 5))\n\n[1] 1 2 3 4 5\n\ntypeof(vetor5)\n\n[1] \"integer\"\n\n# Vetor Integer com o símbolo :.\n(vetor6 <- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ntypeof(vetor6)\n\n[1] \"integer\"\n\n# Vetor Double com a função rep(). \n(vetor7 <- rep(1,10))\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\ntypeof(vetor7)\n\n[1] \"double\"\n\n\nÉ comum o usuário querer saber o tamanho do vetor que ele está trabalhando, isso pode ser feito com a função length(). Além disso, é importante ter certeza de que estamos trabalhando com um vetor atômico, o que pode ser verificado com a função is.vector().\n\n# Podemos construir um vetor com vetores dentro da função c().\n(vetor <- c(c(1, 2), rep(1, 2), seq(1, 2), 1:2))\n\n[1] 1 2 1 1 1 2 1 2\n\nis.vector(vetor)\n\n[1] TRUE\n\ntypeof(vetor)\n\n[1] \"double\"\n\nlength(vetor)\n\n[1] 8\n\n\nObserve que é possível criar um vetor com elementos de diferentes tipos. Sabemos que a função rep() gera um vetor de tipo Double e a seq() gera um vetor de tipo Integer, e ao criar um vetor utilizando a função c() em conjunto com estas obtemos um vetor de tipo Double, de forma que o R priorizou este tipo ao invés do Integer. No R isso é chamado de coerção, onde o vetor sendo criado irá manter o tipo de maior prioridade dentre os seus elementos, e os elementos de tipos com menor prioridade serão convertidos para o tipo prioritário. Isso ocorre, pois todos os elementos de um vetor atômico devem ter o mesmo tipo. Para os tipos apresentados temos como o de menor prioridade para o maior: Logical < Integer < Double < Character. Além disso, se considerarmos Complex e List, teremos List com maior prioridade seguido de Character e Complex.\nPode ser do interesse do usuário visualizar elementos específicos que existem dentro de um vetor, isso pode ser feito ao especificar a posição do elemento dentro do vetor entre os símbolos [].\n\n# vetor com varios elementos.\nvet <- c(TRUE, 5, 7L, \"hospital\")\ntypeof(vet)\n\n[1] \"character\"\n\n# elemento de posição 3.\nvet[3]\n\n[1] \"7\"\n\n# elementos das posições 2, 3 e 4.\nvet[2:4]\n\n[1] \"5\"        \"7\"        \"hospital\"\n\n\nAs operações vetoriais podem ser realizadas de maneira bastante intuitiva, pois em vetores atômicos as operações são realizadas elemento a elemento.\n\n# Operações com vetores.\nvetor1 <- c(4, 9, 16)\n(vetor1_menos1 <- vetor1 - 1)\n\n[1]  3  8 15\n\n(vetor1_vezes2 <- vetor1 * 2)\n\n[1]  8 18 32\n\n(vetor1_dividido2 <- vetor1/2)\n\n[1] 2.0 4.5 8.0\n\n(vetor1_raiz <- sqrt(vetor1))\n\n[1] 2 3 4\n\nvetor2 <- c(1, 2, 3)\n(vetor1_mais_vetor2 <- vetor1 + vetor2)\n\n[1]  5 11 19\n\n\nVamos agora considerar vetores de pesos (quilos) e alturas (metros) de 6 pessoas.\n\n# Vetores de peso e de quilo.\n(peso <- c(62, 70, 52, 98, 90, 70))\n\n[1] 62 70 52 98 90 70\n\n(altura <- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61))\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Obs: note que o separador decimal do R é um . (ponto).\n\nPodemos a partir dessas informações calcular o IMC. Vale lembrar que o IMC é dado pelo peso (em kg) dividido pela altura (em metros) ao quadrado.\n\n(imc <- peso/(altura^2))\n\n[1] 21.45329 21.13271 16.97959 26.03890 26.58318 27.00513\n\n\nÉ importante saber que, no R, vetores são a base dos demais objetos. Objetos com apenas um elemento, por exemplo, não são considerados escalares, mas vetores de tamanho um. Em outras palavras, os próprios elementos de um vetor são também vetores.\n\nelemento1 <- \"\"\nis.vector(elemento1)\n\n[1] TRUE\n\nlength(elemento1)\n\n[1] 1\n\nelemento2 <- 5\nis.vector(elemento2)\n\n[1] TRUE\n\nlength(elemento2)\n\n[1] 1\n\nelemento3 <- TRUE\nis.vector(elemento3)\n\n[1] TRUE\n\nlength(elemento3)\n\n[1] 1\n\n\nAlém dos vetores de formato atômico também existem os de formado lista, que diferente dos atômicos, as listas podem ter elementos de tipos diferentes de forma que não há necessidade do R efetuar coerções. Para criar listas no R podemos utilizar a função list().\n\n# Lista com vários tipos de elementos (inclusive listas).\n(lista <- list(5, \"hospital\", list(1:5), c(rep(1, 2)), seq(1, 2)))\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] \"hospital\"\n\n[[3]]\n[[3]][[1]]\n[1] 1 2 3 4 5\n\n\n[[4]]\n[1] 1 1\n\n[[5]]\n[1] 1 2\n\nis.vector(lista)\n\n[1] TRUE\n\ntypeof(lista)\n\n[1] \"list\"\n\nlength(lista)\n\n[1] 5\n\n# Dica: podemos verificar a estrutura de qualquer objeto com a função str().\nstr(lista)\n\nList of 5\n $ : num 5\n $ : chr \"hospital\"\n $ :List of 1\n  ..$ : int [1:5] 1 2 3 4 5\n $ : num [1:2] 1 1\n $ : int [1:2] 1 2\n\n# Dica: podemos retornar uma lista para vetor atômico com a função unlist().\nunlist(lista)\n\n [1] \"5\"        \"hospital\" \"1\"        \"2\"        \"3\"        \"4\"       \n [7] \"5\"        \"1\"        \"1\"        \"1\"        \"2\"       \n\n\n\n\nA.5.5 Matrizes\nMatrizes são vetores numéricos com duas dimensões, sendo estas a linha e a coluna às quais o elemento pertence. No R podemos criar matrizes com a função matrix().\n\n# Criando uma matriz de 16 elementos com 4 linhas e 4 colunas.\n(matri <- matrix(seq(1,16), nrow = 4, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nstr(matri)\n\n int [1:4, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Podemos verificar se é uma matriz com a função is.matrix().\nis.matrix(matri)\n\n[1] TRUE\n\n\nNote que os números de 1 a 16 foram dispostos na matriz coluna por coluna, ou seja, preenchendo de cima para baixo e depois da esquerda para a direita. Isso ocorre por padrão, pois a função matrix() possui um argumento chamado byrow = FALSE em que, para criar uma matriz em que é preenchida de elementos por linha, basta alterar o argumento para byrow = TRUE. Além disso, a função seq() está gerando os elementos da matriz enquanto o argumento nrow indica o número de linhas e ncol o número de colunas.\nPara visualizar um elemento específico de uma matriz podemos utilizar o mesmo método que usamos com vetores. Lembrando que matrizes ainda são vetores, porém, com uma dimensão a mais. Então, para visualizar um elemento específico devemos indicar a posição do elemento para todas as dimensões existentes, no caso das matrizes, para linha e coluna.\n\n# Obtendo linhas, colunas e elementos específicos.\nmatri[3,  ]   # seleciona a 3ª linha.\n\n[1]  3  7 11 15\n\nmatri[ , 2]   # seleciona a 2ª coluna.\n\n[1] 5 6 7 8\n\nmatri[1, 2]   # seleciona o elemento da primeira linha e segunda coluna.\n\n[1] 5\n\n\nPerceba que cada linha e cada coluna de uma matriz é um vetor (uma dimensão). Assim, podemos alterar uma linha ou uma coluna atribuindo um vetor de interesse, por exemplo.\n\n# substituindo a primeira linha e quarta coluna da matriz.\nmatri[1, ] <- c(9, 9, 9, 9)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    9\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nmatri[, 4] <- rep(1, 4)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    1\n[2,]    2    6   10    1\n[3,]    3    7   11    1\n[4,]    4    8   12    1\n\n\nÉ de importância para o usuário verificar o tamanho (número de elementos) quando se trata de vetores. Porém, quando se trata de matrizes, é importante conhecer as dimensões além do número de elementos. Para verificar as dimensões de uma matriz podemos utilizar a função dim(), enquanto para o tamanho (número de elementos) ainda podemos utilizar a função length().\n\n# Verificando o tamanho e dimensões da matriz.\nlength(matri)\n\n[1] 16\n\ndim(matri)\n\n[1] 4 4\n\n\nComo sabemos que as linhas e colunas de uma matriz são vetores, podemos adicionar mais linhas e colunas a ela com os elementos que queremos. Para concatenar linhas e colunas em uma matriz podemos utilizar as funções rbind() e cbind() respectivamente.\n\nvet1 <- c(99, 98, 97, 95)\nvet2 <- c(0, 5, 7, 9, 99) \n(matri <- rbind(matri, vet1))\n\n     [,1] [,2] [,3] [,4]\n        9    9    9    1\n        2    6   10    1\n        3    7   11    1\n        4    8   12    1\nvet1   99   98   97   95\n\n(matri <- cbind(matri, vet2))\n\n                 vet2\n      9  9  9  1    0\n      2  6 10  1    5\n      3  7 11  1    7\n      4  8 12  1    9\nvet1 99 98 97 95   99\n\n\nOperações matemáticas entre matrizes e elementos são realizadas elemento a elemento assim como vetores. Porém, quando se trata de matrizes, é de interesse efetuar a multiplicação matricial clássica, o que pode ser feito com a operação %*% respeitando a equidade do número de colunas da matriz que pré-multiplica e o número de linhas da matriz que pós-multiplica.\n\n# Criando duas matrizes 2x2 (duas linhas e duas colunas).\n(matriz1 <- matrix(c(rep(1, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    2\n\n(matriz2 <- matrix(c(rep(2, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n# Soma duas matrizes (elemento a elemento).\nmatriz1 + matriz2\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    3    4\n\n# Subtrai duas matrizes (elemento a elemento).\nmatriz1 - matriz2\n\n     [,1] [,2]\n[1,]   -1    0\n[2,]   -1    0\n\n# Divide duas matrizes (elemento a elemento).\nmatriz1/matriz2\n\n     [,1] [,2]\n[1,]  0.5    1\n[2,]  0.5    1\n\n# Multiplica duas matrizes (elemento a elemento).\nmatriz1 * matriz2\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    2    4\n\n# Multiplicação matricial clássica.\nmatriz1 %*% matriz2\n\n     [,1] [,2]\n[1,]    6    6\n[2,]    6    6\n\n# Potência de uma matriz (elemento a elemento).\n(matriz3 <- matriz2^2)\n\n     [,1] [,2]\n[1,]    4    4\n[2,]    4    4\n\n# Raiz quadrada de uma matriz (elemento a elemento).\nsqrt(matriz3)\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nA.5.6 Fatores\nÉ muito comum termos que lidar com variáveis categóricas, ou seja, variáveis que possuem categorias intrínsecas em sua natureza. No R, existe uma classe de objetos chamada Fatores especificamente para representar esse tipo de variável (nominal e ordinal). Os fatores podem ser vistos como vetores de elementos numéricos inteiros (pois são assim internamente representados no R) e possuem rótulos (labels). Consequentemente, são vetores do tipo Double.\n\n# Criando um vetor/variável com a informação do sexo de 7 pessoas. \n(sexo1 <- c(\"Mulher\", \"Homem\", \"Homem\", \"Mulher\", \"Mulher\", \"Mulher\", \"Homem\"))\n\n[1] \"Mulher\" \"Homem\"  \"Homem\"  \"Mulher\" \"Mulher\" \"Mulher\" \"Homem\" \n\n# Verificando a classe da variável sexo1.\nclass(sexo1)\n\n[1] \"character\"\n\n# Transformando em fator.\n(sexo2 <- as.factor(sexo1))\n\n[1] Mulher Homem  Homem  Mulher Mulher Mulher Homem \nLevels: Homem Mulher\n\nclass(sexo2)\n\n[1] \"factor\"\n\n# Verificando os levels da variável de classe factor (sexo2).\nlevels(sexo2)\n\n[1] \"Homem\"  \"Mulher\"\n\n\nPodemos verificar que a variável é representada internamente por elementos numéricos inteiros ao tentar transformá-la em um vetor numérico com a função as.numeric().\n\n# Ao transformar sexo1 obteremos um vetor de dados faltantes (NA) por coerção.\nas.numeric(sexo1)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA NA NA NA NA\n\n# Ao transformar sexo2 obteremos um vetor double com valores inteiros.\n(sexo2_num <- as.numeric(sexo2))\n\n[1] 2 1 1 2 2 2 1\n\ntypeof(sexo2_num)\n\n[1] \"double\"\n\n\nFatores possuem levels em ordem alfabética, e isso pode influenciar diretamente na hora de construir gráficos e realizar aplicações de modelos.\n\n\nA.5.7 Data Frame\nTrata-se de uma “tabela de dados” onde as colunas são as variáveis e as linhas são os registros, e as colunas podem ser de classes diferentes. Logo, a principal diferença entre data frame e matriz é que matrizes só podem conter elementos da mesma classe.\nPara criar data frame no R é utilizado a função data.frame().\n\n# Colunas/variáveis para o data frame.\nID <- seq(1,6)\npes <- c(62, 70, 52, 98, 90, 70)\nalt <- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61)\nimc <- pes/(alt^2)\n\n# Criando o data frame.\n(dados <- data.frame(ID = ID, peso = pes, altura = alt, imc = imc))\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nPodemos pensar na estrutura de um data frame da mesma forma que de uma matriz. Se por acaso for do interesse olhar os dados de altura, por exemplo, basta acessar a coluna três do data frame.\n\n# Selecionando a variável \"altura\".\ndados[, 3]\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n\nEmbora possamos usar os mesmos métodos discutidos na seção de matrizes, quando se trata de data frames, usualmente selecionamos as variáveis de interesse sem ter que saber em qual coluna ela está. Isso pode ser feito ao utilizar o símbolo $, dessa forma a coluna será selecionada em forma de vetor.\n\n# Selecionando a variável \"altura\".\ndados$altura\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Dica: também é possível fazer a seleção de colunas da seguinte forma:\ndados[, c(\"altura\", \"peso\")]\n\n  altura peso\n1   1.70   62\n2   1.82   70\n3   1.75   52\n4   1.94   98\n5   1.84   90\n6   1.61   70\n\n\nUtilizando o mesmo símbolo podemos adicionar ou deletar colunas.\n\n# Adicionando a variável \"grupo\".\ngr <- c(rep(1,3),rep(2,3))\ndados$grupo <- gr\ndados\n\n  ID peso altura      imc grupo\n1  1   62   1.70 21.45329     1\n2  2   70   1.82 21.13271     1\n3  3   52   1.75 16.97959     1\n4  4   98   1.94 26.03890     2\n5  5   90   1.84 26.58318     2\n6  6   70   1.61 27.00513     2\n\n# Deletando a variável \"grupo\".\ndados$grupo <- NULL\ndados\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nNote que ao adicionar variáveis a um data frame essa variável tem que ter o mesmo número de elementos que as demais variáveis, caso isso não seja respeitado o R ira retornar um erro.\nA estrutura de data frame é provavelmente a mais utilizada no dia a dia de quem analisa dados. Sabendo disso, existem algumas funções que são importantes de um usuário de R ter em mente.\n\nhead() - Mostra as primeiras 6 linhas.\ntail() - Mostra as últimas 6 linhas.\ndim() - Número de linhas e de colunas.\nnames() - Os nomes das colunas (variáveis).\nstr() - Estrutura do data frame. Mostra, entre outras coisas, a classe de cada coluna.\n\nAlgumas dessas funções já foram abordadas ao longo do texto. As funções de visualização head() e tail() possuem um argumento chamado n o qual podemos customizar o número de linhas que queremos visualizar.\n\nhead(dados, n = 4)\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n\ntail(dados, n = 4)\n\n  ID peso altura      imc\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\ndim(dados)\n\n[1] 6 4\n\nnames(dados)\n\n[1] \"ID\"     \"peso\"   \"altura\" \"imc\"   \n\nstr(dados)\n\n'data.frame':   6 obs. of  4 variables:\n $ ID    : int  1 2 3 4 5 6\n $ peso  : num  62 70 52 98 90 70\n $ altura: num  1.7 1.82 1.75 1.94 1.84 1.61\n $ imc   : num  21.5 21.1 17 26 26.6 ...\n\n\nCada coluna do data frame pode ser interpretada como um vetor. Dessa forma, as operações de vetores discutidas anteriormente são válidas.\n\n# Cria uma coluna do produto de peso por altura.\ndados$pesovezesaltura <- dados$peso * dados$altura\ndados\n\n  ID peso altura      imc pesovezesaltura\n1  1   62   1.70 21.45329          105.40\n2  2   70   1.82 21.13271          127.40\n3  3   52   1.75 16.97959           91.00\n4  4   98   1.94 26.03890          190.12\n5  5   90   1.84 26.58318          165.60\n6  6   70   1.61 27.00513          112.70\n\n# Cria uma coluna de peso + cinco.\ndados$peso5 <- dados$peso + 5\ndados\n\n  ID peso altura      imc pesovezesaltura peso5\n1  1   62   1.70 21.45329          105.40    67\n2  2   70   1.82 21.13271          127.40    75\n3  3   52   1.75 16.97959           91.00    57\n4  4   98   1.94 26.03890          190.12   103\n5  5   90   1.84 26.58318          165.60    95\n6  6   70   1.61 27.00513          112.70    75\n\n# Cria uma coluna da metade do peso original.\ndados$pesometade <-  dados$peso/2\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n\n\n\nA.5.8 Operadores lógicos\nSabemos que TRUE e FALSE são objetos que pertencem à classe logical, além de terem representação numérica de 1 e 0 respectivamente.\nA operação lógica nada mais é do que um teste que retorna verdadeiro (TRUE) ou falso (FALSE). Assim, podemos realizar comparações entre valores utilizando alguns operadores específicos.\n\n# Verifica se 9 é igual a 12.\n9 == 12\n\n[1] FALSE\n\n# Verifica se 12 é igual a 12.\n12 == 12\n\n[1] TRUE\n\n# Verifica se 9 é diferente de 12.\n9 != 12\n\n[1] TRUE\n\n# Verifica se 9 é maior que 5.\n9 > 5\n\n[1] TRUE\n\n# Verifica se 9 é maior ou igual a 9.\n9 >= 9\n\n[1] TRUE\n\n# Verifica se 4 é menor que 4.\n4 < 4\n\n[1] FALSE\n\n# Verifica se 4 é menor ou igual que 4.\n4 <= 4\n\n[1] TRUE\n\n\nPodemos notar que estes operadores funcionam bem com números, mas isso não é verdade quando se trata de objetos do tipo character (texto). Dentre esses, o operador == apenas funciona com números e o != funciona normalmente tanto com números quanto para textos. Os operadores >, >=, < e <= funcionam com textos pensando na ordem alfabética destes.\nPodemos utilizar operadores de comparação múltipla mais usuais em conjunto com estes discutidos para tornar as comparações ainda mais dinâmicas.\n\nE: & - será verdadeiro se todas operações forem TRUE.\n\n\nx <- 17\n\n# Verifica se x > 9 é verdadeiro E x < 50 é verdadeiro.\n(x > 9) & (x < 50)\n\n[1] TRUE\n\n# Verifica se x < 9 é verdadeiro E x < 50 é verdadeiro E x > 17 é verdadeiro.\n(x > 9) & (x < 50) & (x > 17)\n\n[1] FALSE\n\n\n\nOU: | - será verdadeiro se pelomenos uma operação for TRUE.\n\n\nx <- 17\n\n# Verifica se x < 9 é verdadeiro OU x < 50 é verdadeiro.\n(x < 9) | (x < 50)\n\n[1] TRUE\n\n# Verifica se x < 9 é verdadeiro OU x > 50 é verdadeiro OU x <= 17 é verdadeiro.\n(x < 9) | (x > 50) | (x <= 17)\n\n[1] TRUE\n\n\n\nNegação: ! - nega a resposta lógica da comparação.\n\n\nx <- 17\n\n# Retorna TRUE se x < 50 for FALSE, e FALSE caso contrário. \n!(x < 50)\n\n[1] FALSE\n\n\nPodemos verificar se um valor (ou conjunto de valores) está contido em um vetor utilizando o operador %in%.\n\nex <- 1:15\n\n# Verifica se os valores 3 e 5 fazem parte dos elementos do vetor ex.\nc(3, 5) %in% ex\n\n[1] TRUE TRUE\n\n# Dica: o operador %in% também funciona com character:\ntexto <- c(\"hospital1\", \"hospital2\", \"hospital3\", \"hospital4\", \"hospital5\")\nc(\"hospital5\", \"UTI\") %in% texto\n\n[1]  TRUE FALSE\n\n\nTodos esses operadores podem ser utilizados ao manipular data frames. Iremos aproveitar o data frame criado anteriormente e adicionar mais duas colunas de textos para realizar alguns testes.\n\n# Visualizando o data frame criado anteriormente\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n# Adicionando a coluna sexo.\ndados$sexo <- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"M\")\n\n# Adicionando a coluna olhos (preenchimento impreciso = F).\ndados$olhos <- c(\"preto\", \"castanho\", \"F\", \"preto\", \"azul\", \"F\")\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Utilizando o operador %in% para obter as linhas com a cor dos olhos imprecisa.\ndados[dados$olhos %in% dados$sexo, ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo olhos\n3  3   52   1.75 16.97959            91.0    57         26    M     F\n6  6   70   1.61 27.00513           112.7    75         35    M     F\n\n# %in% com ! para obter as linhas com a cor dos olhos correta.\ndados[!(dados$olhos %in% dados$sexo), ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n\n# Linhas onde o peso é menor que o imc + 40. Retorna apenas colunas peso e imc.\ndados[(dados$peso < (dados$imc + 40)), c(\"peso\", \"imc\")]\n\n  peso      imc\n3   52 16.97959\n\n\n\n\nA.5.9 Dados faltantes, infinitos e indefinições matemáticas\nDados faltantes é uma das coisas mais comuns em bases de dados, podendo surgir por diferentes fatores. No R, dados faltantes são representados por NA e é um símbolo que todo usuário deve conhecer e saber lidar. Além do NA, símbolos como NaN e Inf também são muito comuns no dia a dia.\n\nNA (Not Available): dado faltante/indisponível.\nNaN (Not a Number): indefinições matemáticas. Como 0/0 e log(-1).\nInf (Infinito): número muito grande ou o limite matemático. Aceita sinal negativo (-Inf).\n\n\nx <- c(1, 6, 9)\n\n# Retorna NA\nx[4]\n\n[1] NA\n\n# Retorna NaN\nlog(-10)\n\nWarning in log(-10): NaNs produced\n\n\n[1] NaN\n\n# Retorna Inf\n10^14321\n\n[1] Inf\n\n\nAo lidar com bases de dados é necessário saber verificar se ela apresenta dados faltantes.\n\n# Base de dados que estamos usando.\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Adiciona linhas com dados faltantes.\ndados <- rbind(dados, c(6, NA, 1.75, NA, 125, 99, 50, \"M\", \"castanho\"))\ndados <- rbind(dados, c(9, 50, NA, 50, 127, 97, 55, \"F\", \"azul\"))\n\n# Deleta colunas que não iremos mais usar.\ndados[, c(\"pesovezesaltura\", \"peso5\", \"pesometade\")] <- NULL\ndados\n\n  ID peso altura              imc sexo    olhos\n1  1   62    1.7 21.4532871972318    M    preto\n2  2   70   1.82 21.1327134404057    F castanho\n3  3   52   1.75 16.9795918367347    M        F\n4  4   98   1.94 26.0388989265597    F    preto\n5  5   90   1.84 26.5831758034026    F     azul\n6  6   70   1.61 27.0051309748852    M        F\n7  6 <NA>   1.75             <NA>    M castanho\n8  9   50   <NA>               50    F     azul\n\n# Ao incluir NA a variável imc passou a apresentar mais casas decimais.\n# Dica: podemos arredondar os valores do vetor alterados com a função round().\ndados[1:6, \"imc\"] <- round(as.numeric(dados[1:6, \"imc\"]), digits = 2) \ndados\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n7  6 <NA>   1.75  <NA>    M castanho\n8  9   50   <NA>    50    F     azul\n\n# Avalia se os elementos da coluna peso são NA ou não.\nis.na(dados$peso)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n# Verifica se existe pelomenos 1 dado faltante no data frame.\nany(is.na(dados))\n\n[1] TRUE\n\n# Filtra apenas as linhas com NA na variável peso.\ndados[is.na(dados$peso),]\n\n  ID peso altura  imc sexo    olhos\n7  6 <NA>   1.75 <NA>    M castanho\n\n# Dica: as funções na.omit() e complete.cases() podem remover linhas com NA.\nna.omit(dados)\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\ndados[complete.cases(dados), ]\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\n\nPara lidar com dados faltantes é importante ter pelo menos uma ideia do motivo para eles existirem na base de dados sendo analisada. Muitas vezes não temos ideia desse motivo, e a melhor estratégia acaba sendo analisar os dados, incluindo e reportando com transparência os dados faltantes. Ao analisar dados sem excluir os casos faltantes, muitas vezes nos deparamos com erros inesperados que ocorrem por tentarmos usar funções que não estão considerando esses casos. Situações como essas exigem uma atenção a mais do usuário, tendo que pesquisar e ler documentações de funções para ter certeza do que a função sendo usada está fazendo.\n\n# Criando um vetor com dados faltante.\nvetor1 <- c(NA, 1, 1, 1, 5)\n\n# mean() calcula a média do vetor.\nmean(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nmean(vetor1, na.rm = TRUE)\n\n[1] 2\n\n# sum() calcula a soma dos elementos do vetor.\nsum(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nsum(vetor1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\nA.5.10 Condicionamento: If e else\nAs estruturas if e else, também chamadas de condicionais, servem para executar códigos apenas se uma condição (teste lógico) for satisfeita.\n\nvalor1 <- 224\nvalor2 <- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta <- 10 # resposta é 10.\n} else { # caso contrário.\n  resposta <- 15 # respota é 15.\n  }\nresposta\n\n[1] 15\n\n\nVeja que o R só executa o conteúdo das chaves {} se a expressão dentro dos parênteses () retornar TRUE. Além disso, note que a condição de igualdade é representada por dois iguais (==). Como dito anteriormente, apenas um igual (=) é símbolo de atribuição (preferível <-), em argumentos de estruturas condicionais queremos realizar comparações.\nPara utilizar mais condições podemos utilizar o else if ().\n\nvalor1 <- 224\nvalor2 <- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta <- 10 # resposta é 10.\n} else if (valor1 > valor2) { # Se não, então valor1 é maior que valor2 ?\n  resposta <- 15 # então a resposta é 15.\n  } else { # caso contrário.\n    resposta <- 25 # respota é 25.\n    }\nresposta\n\n[1] 25\n\n\n\n\nA.5.11 Iterador for\nO for serve para repetir uma mesma tarefa para um conjunto de valores diferentes (realiza um loop). Cada repetição é chamada de iteração.\nComo exemplo, considere o vetor atribuído ao objeto vetor1 como segue:\n\nvetor1 <- c(1,20,50,60,100)\n\nPodemos criar um novo vetor que seja formado por cada elemento do vetor1 dividido por sua posição.\n\nvetor2 <- NULL\nfor (i in 1: length(vetor1)){\n  vetor2[i] <- vetor1[i]/i\n}\nvetor2\n\n[1]  1.00000 10.00000 16.66667 15.00000 20.00000\n\n\nNote que primeiro definimos o objeto vetor2, recebendo NULL. O NULL representa a ausência de um objeto e serve para já declarar algum objeto que receberá valor na sequência. Ao rodar o for, o vetor2 passa a ser um vetor de tamanho 5 (tamanho do vetor1).\nNo exemplo, temos 5 iterações e para cada valor de i, correndo de 1 até 5 (tamanho do vetor1), pegamos o valor do vetor1 na posição i e dividimos por i. Assim, formamos o vetor2.\n\n\nA.5.12 Funções\nFunções no R são nomes que guardam um código de R. A ideia é que sempre que rodar a função com os seus argumentos, o código que ela guarda será executado e o resultado será retornado.\nJá usamos anteriormente algumas funções que estão na base do R. Por exemplo, quando usamos class() para entender a classe do objeto que o R está entendendo. Colocamos um argumento dentro do parêntese e o R retornou qual a classe do objeto em questão.\nImportantes:\n\nSe a função tiver mais de um argumento, eles são sempre separados por vírgulas;\nCada função tem os seus próprios argumentos. Para saber quais são e como usar os argumentos de uma função, basta acessar a sua documentação. Uma forma de fazer isso é pela função help(), cujo argumento é o nome da função em questão.\n\n\nhelp(mean)\n\nVeja que abrirá a documentação sobre a função mean no menu “Help” do RStudio. Assim, é possível ver os argumentos e exemplos de uso da função.\nAinda sobre funções já presentes no R, vamos considerar agora a função sample. Veja a documentação dessa função para ver o que ela faz.\n\nhelp(sample)\n\nA função sample retorna uma amostra de um vetor com tamanho especificado em um de seus argumentos com ou sem reposição. Ela apresenta quatro argumentos na forma sample(x, size, replace = FALSE, prob = NULL), em que: x é o vetor do qual será amostrado o número de elementos especificado no argumento size, replace indica se é com ou sem reposição e prob é para especificar probabilidades de seleção.\nPodemos usar essa função para amostrar de um objeto dois elementos (size = 2) em uma seleção com reposição (replace = TRUE) e que a probabilidade de seleção seja a mesma para todos os elementos do vetor. No caso da probabilidade, como podemos ver na documentação da função sample, o default (padrão se o usuário não mudar o argumento) é ser a mesma probabilidade de seleção para todos os elementos. Assim, se o usuário nada especificar para esse argumento, o R entenderá o seu default. O mesmo vale para o argumento replace: caso fosse o interesse fazer a seleção sem reposição, não precisaríamos colocar esse argumento por seu default ser FALSE.\n\n\n[1] 20 20\n\n\nTambém poderíamos usar a mesma função sem colocar o nome dos argumentos, desde que o usuário entenda o que ela está fazendo.\n\nsample(vetor_am, 2 , TRUE) \n\n[1]  0.5 10.0\n\n\nNesse caso, é importante que se respeite a ordem dos argumentos: o vetor tem que ser o primeiro, o segundo argumento é size e assim por diante.\nVale ressaltar que as duas últimas saídas não necessariamente serão as mesmas, porque é feito um sorteio aleatório de dois elementos de vetor_am em cada uma delas.\nAlém de usar funções já prontas, podemos criar novas funções. Suponha que queremos criar uma função de dois argumentos que retorna o primeiro mais três vezes o segundo argumento.\n\nf_conta <- function(x, y) {\n  out <- x + 3 * y\n  return(out)\n}\n\nA função acima possui:\n\nnome: f_conta.\nargumentos: x e y.\no corpo out: <- x + 3 * y.\no que retorna: return(out).\n\nPara chamar a função e utilizá-la basta chamar pelo nome com os devidos argumentos, assim como temos feito até então.\n\nf_conta(x = 10, y = 20)\n\n[1] 70\n\n\nVeja que o cálculo acima retorna exatamente o mesmo que o seguinte:\n\nf_conta(y = 20, x = 10)\n\n[1] 70\n\n\nIsso acontece porque a ordem dos argumentos foi alterada, porém, mantendo seus devidos nomes. Se não quiser colocar os nomes dos argumentos, precisa tomar cuidado para não errar a ordem deles. Isso porque:\n\nf_conta(10,20)\n\n[1] 70\n\n\né diferente de:\n\nf_conta(20,10)\n\n[1] 50\n\n\n\n\nA.5.13 Como obter ajuda no R\nListamos aqui 3 maneiras para buscar ajuda no R:\n\nHelp/documentação do R (comandos help(nome_da_funcao) ou ?nome_da_funcao). Como exemplo:\n\n\nhelp(mean) \n?mean\n\n\nGoogle: especificar a linguagem é de suma importância na pesquisa, além de deixar o problema ou a função bem claro.\n\n\n\n\n\n\nPesquisa no Google\n\n\n\n\n\nComunidade: O Stack Overflow e o Stack Overflow em Português são sites de perguntas e respostas amplamente utilizados por todas as linguagens de programação.\n\n\n\nA.5.14 Pacotes\nComo dito quando falamos “Sobre o R”, o R apresenta funções na sua base e também em forma de pacotes (conjunto de funções bem documentado), que precisam ser instalados (uma vez no seu computador) e carregados na sessão de utilização do R (carregado em toda sessão aberta).\nDificilmente uma análise será feita apenas com as funções básicas do R e dificilmente não vai existir um pacote com as funções que você precisa. Por esse motivo, falamos a seguir em como instalar e carregar pacotes.\n\nA.5.14.1 Instalação de pacotes\n\nVia CRAN:\n\n\ninstall.packages(\"nome-do-pacote\")\n\nExemplo: Instalação do pacote dplyr.\n\ninstall.packages(\"dplyr\")\n\nNote que o nome do pacote está entre aspas.\n\nVia Github: Para instalar via Github precisa primeiramente instalar o pacote devtools.\n\n\ndevtools::install_github(\"nome-do-repo/nome-do-pacote\")\n\nExemplo:\n\ndevtools::install_github(\"tidyverse/dplyr\")\n\n\n\nA.5.14.2 Carregar pacotes\nUma vez que um pacote de interesse está instalado em sua máquina, para carregá-lo na sessão atual do R é só rodar a seguinte linha de comando:\n\nlibrary(nome-do-pacote)\n\nVeja que para carregar o pacote não se usa aspas.\nComo exemplo, o carregamento do pacote dplyr:\n\nlibrary(dplyr)\n\nSó é necessário instalar o pacote uma vez, mas é necessário carregá-lo toda vez que começar uma nova sessão.\nDado que o pacote está carregado ao rodar a função library(), todas as funções desse pacote podem ser usadas sem problemas.\nCaso você não queira carregar o pacote e apenas usar uma função específica do pacote, você pode usar nome-do-pacote::nome-da-funcao. Por exemplo:\n\ndplyr::distinct(...)\n\nTendo carregado o pacote dplyr anteriormente (pela função library()), não seria necessário colocar dplyr:: antes da função distinct do pacote.\n\n\n\nA.5.15 Materiais complementares\nLivros e Artigos:\n\nCritical Thinking in Clinical Research. Felipe Fregni & Ben M. W. Illigens. 2018.\nCHAPTER 3: Selecting the Study Population. In: Critical Thinking in Clinical Research by Felipe Fregni and Ben Illigens. Oxford University Press 2018.\nFandino W. Formulating a good research question: Pearls and pitfalls. Indian J Anaesth. 2019;63(8):611–616. doi:10.4103/ija.IJA_198_19\nRiva JJ, Malik KM, Burnie SJ, Endicott AR, Busse JW. What is your research question? An introduction to the PICOT format for clinicians. J Can Chiropr Assoc. 2012;56(3):167–171.\nExternal validity, generalizability, and knowledge utilization. Ferguson L1. J Nurs Scholarsh. 2004;36(1):16-22.\nPeter M Rothwell; Commentary: External validity of results of randomized trials: disentangling a complex concept, International Journal of Epidemiology, Volume 39, Issue 1, 1 February 2010, Pages 94–96, https://doi.org/10.1093/ije/dyp305\n\nSites:\n\nhttps://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/1-data-display-and-summary\nhttp://www.sthda.com/english/wiki/statistical-tests-and-assumptions"
  },
  {
    "objectID": "supervisionada.html",
    "href": "supervisionada.html",
    "title": "5  Aprendizado supervisionado",
    "section": "",
    "text": "Tópicos:\n\nconceitos iniciais\nalgoritmos supervisionados\naplicações e interpretabilidade\n\n\n5.0.1 Machine learning\n\nfalar sobre ml\nmecionar um pouco sobre Aprendizado supervisionado VS não supervisionado\n“neste capítulo vamos falar sobre aprendizado supervisionado\n\n\n\n5.0.2 Aprendizado supervisionado (conceitos iniciais)\nAprendizado supervisionado pode ser definido como a tarefa de aprender uma função que mapeia uma entrada em uma saída e isso é feito com base em exemplos e treinos. Em outras palavras, uma máquina é treinada para encontrar soluções chamadas rótulos, onde esses rótulos identificam alguma característica. Apesar de também poder ser usada para regressão, o aprendizado supervisionado tem como tafera típica a classificação. Um exemplo bem simples de classificação é: suponha que eu queira classificar imagens de animais, nesse caso possuo um banco de dados com imagens de cachorros e gatos. Quero que meu algoritmo classifique as imagens identificando o tipo do animal na imagem. Para isso o algoritmo é treinado utilizando vários exemplos para que ele consiga classificar novas imagens posteriormente. Outra tafera é predizer um valor com base em características, por exemplo, prever o valor de um carro dado um conjunto de características (quilometragem, idade, marca, etc.) chamadas preditores. Este tipo de tarefa é chamada regressão. Para treinar o sistema é preciso incluir diversos exemplos, assim o banco de dado é separado em treino e teste, onde o é feito o treinamento na base treino para posteriormente serem feitos os testes de predição e avaliação da qualidade do ajuste na base teste.\n\n5.0.2.1 Dificuldades gerais do machine learnig\nComo dito ateriormente, a idéia geral do aprendizado de máquina é contruir um algorito para solucionar os meus problemas, onde esse algoritmo será treinado com dados. Mas, o que acontece se o meu algorito for ruim ou os dados serem ruins.\n\n5.0.2.1.1 Quantidade insuficiente de dados\nFalando sobre dados ruins, o primeiro problema é a quantidade de dados. Já parou pra pensar em quão difícil é treinar uma máquina? voltando no exemplo anterior, para você aprender a diferenciar um cachorro de um gato quando era criança, bastou alguém lhe apontar qual era qual algumas vezes e você se tornou capaz de diferenciar cães de gatos independente das caracteristicas. Uma máquina não consegue fazer isso fácilmente, é necessário uma quantidade grande de dados para a maioria dos algoritmos, até mesmo para problemas simples como o do exemplo citado e para problemas complexos, como reconhecimento de imagem ou fala você pode precisar de milhões de exemplos.\n\n\n5.0.2.1.2 Dados de treino não representativos\nComo mencionado anteriormente, o treinamento de um algoritmo é feito por meio de uma base de dados, onde está é separada em dados de treinamento e de teste, para que eu possa usá-lo e generalizá-lo em dados futuros. Dados de treinamento que não representem bem os dados que serão uzados no futuro podem um modelo que não funcionará bem. Utilizando o exemplo do algoritmo de regressão onde o objetivo era prever os valores dos carros com base em suas características. Digamos que meu algoritmo foi treinado com uma da base de dados de carros apenas do estado de São Paulo, mas meu algoritmo será utilizado para prever carros de todo o país, pode ser que não funcione tão bem. O estados podem alterar significamente os preços dos carros por meio de impostos por exemplo. É de extrema importância utilizar um conjunto de dados de treino que represente bem os dados que você deseja generalizar. Isso pode não ser uma tarefa fácil, pode encontrar problemas com amostras, principalmente se ela for muito pequena e até mesmo uma amostra grande pode não ser representativa.\n\n\n5.0.2.1.3 Qualidade dos dados\nComo pode ter imaginado, a qualidade dos dados também é de extrema importância. Dados com discrenpâncias, vários erros, e gerados a partir de medições de baixa qualidade fará com que fique mais difícil o seu algoritmo identificar padrões e tomar decisões. Se você convive com pessoas do ramo da ciência de dados em geral, é bem provável que ja tenha ouvido alguém dizer algo o tipo: “gastamos a maior parte do nosso tempo para limpar os dados”. Isso não é em vão. Na maioria dos casos, principalmente no ramo de aprendizagem de máquinas é gasto um enorme tempo para limpar os dados pois pode influênciar muito na qualidade do modelo. Por exemplo, se algumas informações forem muito discrepantes, é preciso decidir entre tentar corrigir ou excluí-las. Se uma variável tiver uma quantidade significativa de valores faltantes, deverá ser decidido se essas observações serão excluídas ou se será possível utilizar métodos de imputação de dados. Treinar mais de um modelo com diferentes decisões tomadas sobre os dados também pode ser efetivo.\n\n\n5.0.2.1.4 Sobreajustamento dos dados (Overfitting)\nO sobreajustamento é um conceito que ocorre quando nosso modelo (não só um modelo de aprendizado de máquinas), se ajusta exatamento aos nossos dados de treinamento. Ouvir isso uma primeira vez pode parecer excelente, ou até mesmo o cenário ideal, afinal, queremos que o nosso modelo se ajuste o máximo possível, certo? bom.. não exatamente. O que acontece neste caso, é que o modelo mostra-se adequado apenas para os dados de treino, como se o modelo tivesse apenas decorado os dados de treino e não fosse capaz de generalizar para outros dados nunca vistos antes. Assim, o desempenho do nosso modelo quando usado em novos dados cai drasticamente. Algumas razões que podem levar a um sobreajustamento: base de treino muito pequena, não contendo dados suficientes para representar bem todos valores de entrada possíveis; grande quantidade de informações irrelevantes (dados ruidosos); treinamento excessivo em um único conjunto de amostra; modelo muito complexo, fazendo com que ele aprenda os ruídos nos dados de treinamento. Agora que sabemos o problema que é um sobreajustamento e as razões que podem levar a isso, precisamos falar sobre como evitar que isso aconteça. Existem algumas tecnicas comumente utilizadas.\n\nRegularização: Foi dito anteriormente que uma razão para o sobreajustamento é a complexidade do modelo, então, faz sentido diminuírmos sua complexidade. Isso pode ser feito removendo ou dimuindo o número de parâmetros.\nParada antecipada: Quando um modelo está sendo treinado por radadas de repetição, é possível avaliar cara uma dessa repetição. Nomermalmente o desempenho de um modelo melhora a cada repetição, mas chega um momento em que começa a acontecer o sobreajustamento. A ideia da parada antecipada é pausar o treinamento anter que chegue a esse ponto.\nAumento de dados: Essa técnica consiste em aumentar ligeiramente os dados da amostra toda vez que o modelo os processa, ou seja, injevar dados limpos e relevantes nos dados de treino. Isso faz com que os conjuntos de treino pareçam “exclusivos” do modelo, impedindo que ele aprenda suas características. Mas isso deve ser feito com moderação, pode injetar dados que não estão limpos pode fazer mais mal do que bem. Além disso, não é um método garantido.\n\n\n\n5.0.2.1.5 Existem outras técnicas que podem ser utilizadas para evitar o sobreajustamento. Mas precisamos falar também sobre como detectá-los.\nUma forma não técnica e que não deve ser a sua única forma de tentar identificar o sobreajustamento é por meio da visualização gráfica. A visualização gráfica pode ser usada apenas para levantar hipotéses, nunca para tomar uma decisão final. Até mesmo porque nem sempre é possível verificar esse problema visualmente. Tavelz a técnica mais eficiente para isso é a Validação Cruzada k-fold (k-fold Cross Validation). Valos falar sobre posteriormente.\n\n\n5.0.2.1.6 Subajustamento dos dados (Underfitting)\nComo pode ter imaginado, subajustamento é o oposto do sobreajustamto. Ocorre quando seu modelo é muito simples para aprender a estrutura dos dados. O subjastumento leva à um erro elevado tanto nos dados detreino quanto nos dados de teste. Pode ocorrer quando o modelo não foi treinado por tempo suficiente ou as variáveis ​​de entrada não são significativas o suficiente para determinar uma relação significativa entre as variáveis ​​de entrada e saída. Aqui também estamos em um cenário a ser evitado e apesar de ser contrário ao sobreajustamento, as téncias tanto para identificar quanto para evitar o problema são semelhantes. Um adendo, geralmente, identificar um subajustamento é mais fácil que identificar um sobreajustamento.\n\n\n\n5.0.2.2 Modelo de Regressão Linear\nJá temos uma breve noção sobre o que é aprendizado supervisionado, agora vamos aprofundar um pouco dentro dos modelos. Como foi mencionado aprendizado supervisionado é usado principalmente para métodos de classificação e regressão. Modelo de regressão linear, como o próprio nome já diz, se enquadra nos métodos de regressão. A regressão consiste em modelar um valor de previsão com base em variáveis independentes. De forma mais geral, o modelo conquiste em fazer uma previsão “simples” calculando uma ponderação entre as somas dos recusrso de entrada e uma constante chamada intercepto. Assim, obtemos uma relação linear entre a variável de saída e as variáveis de entrada. A linha de regressão é a linha de melhor ajuste para o modelo\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nonde:\n\n\\(\\hat y\\) é o valor predito\n\\(n\\) o número de características\n\\(x_i\\) é a \\(i^{th}\\) característica\n\\(\\beta_j\\) é o \\(j^{th}\\) parâmetro do modelo\n\nCerto, temos uma definição matemática do nosso modelo, mas como posso treiná-lo? Treinar um modelo significa também definir os parâmetros para que o modelo se ajuste melhor aos meus dados. Em outras palavras, um modelo treinado irá se ajustar a melhor linha para prever o valor de \\(y\\) para um dado valor de \\(x\\). Assim, ao encontrar os melhores valores de \\(\\beta 's\\) obtemos a melhor linha de ajuste. Para isso, primeiro precisamos de uma medida de quão bem (ou mal) o modelo se ajusta aos meus dados. A medida mais comum usada em um modelo de regressão é a Raiz do Erro Quadrático Médio (REQM). Em resumo, o REQM é uma medida de quão espalhados estão esses resíduos. Ele avalia a diferença média quadrática entre os valores observados e previstos. Portanto, treinar um modelo consiste em encontrar os valores de \\(\\beta's\\) que irá minimizar o REQM.\n\\[\nREQM = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}  (\\hat y_i - y_i)^2}\n\\]\nExistem algumas suposições importantes quue devem ser feitas para utilizar um modelo de regressão linear. Estas são algumas verificações formais durante a construção de um modelo de regressão linear, o que garante a obtenção do melhor resultado possível do conjunto de dados fornecido.\n\nSuposição de linearidade: A regressão linear assume que a relação entre a entrada e saída é linear. Pode parecer um pouco óbvio, mas em alguns casos onde, em um primeiro olhar, faça sentido usar uma regressão linar, nossos dados não permitam isso. Pode ser necessário transformar os dados.\nHomocedasticidade: Homocedasticidade é uma situação em que o termo de erro é o mesmo para todos os valores de variáveis ​​independentes. Com homocedasticidade, não deve haver uma distribuição padrão clara de dados no gráfico de dispersão.\nErros normalmente distribuídos: A regressão linear assume que o termo de erro deve seguir o padrão de distribuição normal. Se os termos de erro não forem normalmente distribuídos, os intervalos de confiança se tornarão muito amplos ou muito estreitos, o que pode causar dificuldades em encontrar coeficientes. Você pode obter algum benefício usando transformações (por exemplo, log ou BoxCox) em suas variáveis ​​para tornar sua distribuição mais gaussiana.\nMulticolinearidade: O modelo de regressão linear não assume nenhuma autocorrelação em termos de erro. Se houver alguma correlação no termo de erro, isso reduzirá drasticamente a precisão do modelo. A autocorrelação geralmente ocorre se houver uma dependência entre os erros residuais. Considere calcular correlações pareadas para seus dados de entrada e remover os mais correlacionados.\n\n\n\n5.0.2.3 Modelo de Regressão logística\nAlguns algoritmos de regressão podem ser usados para classificação (o contrário também é valido). A regressão logística é um dos algoritmos mais populares do machine leranrning e geralmente é usada para estimar a probabilidade de que uma instância pertença a uma classe. Por exemplo, qual a probalidade de que o objeto de uma imgagem seja um cachorro? ou um gato? neste caso, se a probabilidade estimada for maior que 50%, então o modelo pode prever que naquela imagem tem um cachorro (classe rotulada como “1”), se for menor, prevê que é um gato (classe rotulada como “0”). Este tipo de regressão pode retornar valores categóricos ou discretos, como: Sim ou Não, 0 ou 1, verdadeiro ou falso, entre outros. Mas aqui, ela fornece os valores probabilísticos que estão entre 0 e 1. Apresar de ser semelhante a regressão linear, aqui não ajustamos uma linha de regressão, mas sim uma função logistica em forma de “S” que prevê os dois valores máximos (0 ou 1).\n\ninserir imagem\n\nA equação de regressão Logística pode ser obtida a partir da equação de Regressão Linear.\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nO problema de usar essa abordagem é que podemos prever probalidades negativas em alguns casos e valores maiores que 1 em outros. Essas previsões não são sensatas, pois sabemos que a verdadeira probabilidade deve ser um número entre 0 e 1. Para resolver esse problema, devemos modelar \\(\\hat y\\) usando uma função que fornceça saídas entre 0 e 1 para todos os valores de \\(\\hat y\\). Na regressão logística usamos a função logística como sendo:\n\\[\n\\hat y = \\frac{e^{\\beta_0+\\beta_1X}}{1 + e^{\\beta_0+\\beta_1X}}\n\\]\nDepois de algumas manipulações, chegamos que\n\\[\n\\frac{\\hat y}{1- \\hat y} = e^{\\beta_0+\\beta_1X}\n\\]\nMas precisamos variar de \\(-\\infty\\) até \\(\\infty\\), então pegue o logaritmo da equação e temos:\n\\[\n\\log\\bigg[\\frac{\\hat y}{1- \\hat y} \\bigg ] = {\\beta_0+\\beta_1X}\n\\]\nExistem alguns tipos de regressão logística:\n\nBinomial: Aqui deve haver apenas dois tipos de possível variáveis, como 0 ou 1, Falso ou Verdadeiro, etc.\nMultinomial: Pode também haver 3 ou mais tipos não ordenados possíveis da variável dependende, como, cachorro, gato ou tigre.\nOrdinal: Na regressão logística ordinal, pode haver 3 ou mais tipos ordenados possíveis de variáveis ​​dependentes, como “baixo”, “médio” ou “alto”.\n\n\n\n5.0.2.4 Validação Cruzada (Cross-Validation)\nAté aqui falamos um pouco sobre alguns problemas que podem ser encontrados no aprendizado de máquinas e superficialmente sobre dois modelos de regressão. Vamos falar agora sobre um método que é bem utilizado para validar a estabilidade do seu modelo. Como mencionamos anteriormente, não podemos simplismente ajustar um modelo aos meus dados de treino e esperar que ele funcione perfeitamente, ou até mesmo esperar que aquele seja o melhor modelo possível ser fazer nenhuma validação. Falamos um pouco sobre isso quando discutimos sobre sobreajustamento e subajustamento. Então, vamos nos aprofundar sobre um método que nos garanta que o nosso modelo obteve a maioria dos padrões dos dados corretos sem captar muitos ruídos.\n\n5.0.2.4.1 O que é validação cruzada?\nValidação cruzada é uma técnica para avaliar um modelo de aprendizado de máquina e tester o seu desempenho. Pode ajudar a comparar e selecionar um modelo mais apropriado para o nosso problema. É bem fácil de entender, de implementar e tende a ter um viés menor do que outros métodos usados para o mesmo objetivo. Por isso é uma ferramenta tão utilizada. Tanto a vaildação cruzada quanto outros algoritmos funcionam de maneira semelhantes, consistem em: divider o conjunto de dados em treino e teste; treinar o modelo no conjunto treino; validar o modelo no conjunto teste e repetir as etapas anteriores algumas vezes. Dentro da validação cruzada existem diversas técnicas onde umas são mais utilizadas. Já mencionamos anteriormente o método k-fold, mas exite também os métodos, hold-out, leave-p-out, k-fold stratified, entre outros. Vamos falar sobre alguns deles.\n\nHold-Out Cross Validation: Está é a tecnica mais simples e comum. Ele consiste em remover uma parte dos dados de treinamento e usá-la para obter previsões do modelo treinado no restante dos dados. A estimativa de erro informa como nosso modelo está se saindo em dados não vistos ou no conjunto de vailidação. A implementação é extremamente fácil e existem pacotes que podem ajudar nisso. Mas apesar disto, esse método tem um grande desvantagem. Se estivermos trabalhando com um conjunto de dados que não é completamente uniforme, podemos acabar em uma situação difícil após a separação. O conjunto de treino pode não representar muito bem o conjunto de teste, ou seja, os conjuntos podem ser bem diferentes, onde um é mais fácil do que o outro.\nK-Fold Cross Validation: O K-Fold pode se apresentar como um técnica que minimiza a desvantagens do método Hold-Out apresentando uma nova maneira de difidir o banco de dados. Neste método os dados são divididos em k subconjuntos (daí o nome). O método de validação é repetido k vezes, onde, a cade vez, um dos k subconjuntos é usado como conjunto de teste e os outros k-1 conjuntos são unidos para formar o conjunto de treinamento. A estimativa de erro é a média de todas as k tentativas. Como cada ponto de dados chega a um conjunto de validação exatamente uma vez e a um conjunto de treinamento k-1 vezes, isso reduz significativamente o viés. Como “regra geral”, k=5 ou k=10 é escolhido, mas não existe nada fixo. Comparando diretamente ao método Hold-Out, o método K-Fold tende a ser melhor, mas também possui uma desvantagem. Aumentar o k resulta no treinamento de mais modelos e o processo de treinamento pode ser custoso e demorado.\nLeave-P-Out Cross Validation: Este método consiste em criar todos os conjuntos de treinamento e testes possíveis usando p amostras como conjunto de teste. Em outras palavras, deixa p pontos de dados fora dos dados de treino, ou seja, se houver n pontos de dados na amostra original, np amostras são usadas para treinar o modelo p pontos são usadas como conjunto teste. Como pode imaginar, este método e extremamente exaustivo, tento em vista que é preciso validar o modelo para todas as comibanções possíveis e para um p demasiadamente grande, pode ser computacionalmente inviável.\n\nO método de validação cruzada também pode nos ajudar a ajustar hiperparâmetros, falaremos sobre isso posteriormente.\n\n\n\n5.0.2.5 Precisão vs intrerpretabilidade\nAté aqui, discutimos muito sobre métodos para obter modelos precisos, com desempenhos ótimos. Obter um modelo que irá prever com excelência um evento em dados não é visto como um modelo valioso, mas, onde entra a interpretabilidade?\na interpretabilidade fornece informações sobre o relacionamento entre as entradas e a saída de um modelo. Um modelo que pode ser interpretado permite responder perguntas sobre por que os recursos independentes predizer aquele atributo dependente. Por exemplo: Um modelo extremamente preciso preciso pode permitir que eu saiba quais os meus clientes “podem” receber créditos ou não, mas se ele não for interpretável, eu nunca saberei o porque. Pense no cenário da saúde, onde eu tenho diversas informações sobre paciêntes e meu modelo pode predizer precisamente quais tem mais probalidade de ser diagnosticado com um doença. Mas já pensou em qual importante é saber quais fatores influênciam isso? porque aquele paciente é mais provável de ser diagnostico com tal doença em relação a outro. Em diversos cenários a interpretabilidade torna-se indispensável dentro dos modelos. A depender do cenário, a interpretabilidade é mais importante que a precisão e vice-versa.\nSaber qual priorizar vai depender muito do cenário em que se encontra. Normalmente a escolha de um determinado algoritmo em detrimento de outro e como a seleção do algoritmo está relacionada ao caso de uso que estamos tentando resolver e ao objetivo de negócios que queremos alcançar. Um modelo com menos parâmetros é mais fácil de interpretar. Isso é intuitivo. Um modelo de regressão linear tem um coeficiente por recurso de entrada e um termo de interceptação. Por exemplo, você pode examinar cada termo e entender como eles contribuem para a saída. Uma árvore de decisões (falaremos mais sobre elas) também costuma ser de fácil interpretação, mas mesmo modelos considerados “interpretáveis” podem se tornar rapidamente não interpretáveis."
  },
  {
    "objectID": "naosupervisionado.html#alguns-conceitos-básicos-de-algebra",
    "href": "naosupervisionado.html#alguns-conceitos-básicos-de-algebra",
    "title": "6  Aprendizado não supervisionado",
    "section": "6.1 Alguns conceitos básicos de algebra",
    "text": "6.1 Alguns conceitos básicos de algebra\nPara melhor introduzir o campo do aprendizado não supervisionado, alguns conceitos de álgebra são necessários para compreender o que se passa por trás de cada algoritmo da análise de dados multivariada. Vamos introduzir com vetores e matriz, seguindo com decomposição espectral para então darmos inicio a área da estatística multivariada ou aprendizado não supervisionado. Não é objetivo desse livro demonstrar conceitos algébricos e nem se aprofundar demais no assunto Johnson, Wichern, et al. (2002).\n\n6.1.1 Definições importantes\nVetor Aleatório : Seja X um vetor contendo p componentes, onde cada componente é uma variável aleatória, isto é, \\(X_i\\) é uma variável aleatória, \\(\\forall\\quad i =1,2,...,p\\). Então X é chamado de vetor aleatório e é denotado por:\n\\[\n\\begin{align}\n  X &= \\begin{bmatrix}\n           X_{1} \\\\\n           X_{2} \\\\\n           \\vdots \\\\\n           X_{p}\n         \\end{bmatrix}\n  \\end{align}\n\\]\nO vetor transposto do vetor aleatório X é denotadopor \\(X' = [X_1 X_2 X_3 ...X_p]\\)\nVetor de Médias : O vetor \\(\\mu\\) é chamado vetor de médias quando \\(E(X) = \\mu\\) onde X é um vetor aleatório. Dessa forma\n\\[\n\\begin{align}\n  E(X) &= \\begin{bmatrix}\n           E(X_{1}) \\\\\n           E(X_{2}) \\\\\n           \\vdots \\\\\n           E(X_{p})\n         \\end{bmatrix}\n  \\end{align} = \\mu = \\begin{bmatrix}\n           \\mu_1 \\\\\n           \\mu_2 \\\\\n           \\vdots \\\\\n           \\mu_p\n         \\end{bmatrix}\n\\]\nMatriz de covariâncias : A matriz de variâncias e covariâncias do vetor X é denotada por,\n\\[\nCov(X) = V(X) = Var(X) = \\Sigma_{p\\times p} = \\begin{bmatrix}\n           \\sigma_{11} & \\sigma_{12} & ... & \\sigma_{1p}  \\\\\n          \\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n         \\end{bmatrix}\n\\]\nOnde \\(\\sigma_{ii}\\) representa a variância do elemento \\(X_i\\) do vetor aleatório e \\(\\sigma_{ij} = E[(X_i- \\mu_i)(X_j - \\mu_j)]\\) \\(\\forall\\quad i,j = 1,\\dots,p\\). A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja \\(\\Sigma = '\\Sigma\\). Sendo tambem não negativa definida, \\(a'\\Sigma a \\geq 0\\) para todo vetor de constantes pertencentes aos reais.\nMatriz de correlação : A matriz de correlação do vetor X é denotada por,\n\\[\nP_{p\\times p} = \\begin{bmatrix}\n           1 & \\rho_{12} & \\rho_{13}& ... & \\rho_{1p}  \\\\\n          \\rho_{21} & 1 & \\rho_{23}&... & \\rho_{2p}  \\\\\n          \\rho_{31} & \\rho_{32} & 1 &... & \\rho_{3p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\rho_{p1} & \\rho_{p2} &\\rho_{p3}& ... & 1\n         \\end{bmatrix}\n\\]\nEm que\n\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}} = \\frac{\\sigma_{ij}}{\\sigma_i\\sigma_j}\n\\]\nAuto Valores e Auto Vetores : Se \\(\\Sigma\\) for uma matriz quadrada, ou seja \\(\\Sigma_{p\\times p}\\), então um vetore não nulo \\(e\\) em \\(R^n\\) é denominado autovetor de \\(\\Sigma\\) se \\(\\Sigma e\\) for um múltiplo escalar de \\(e\\), isto é,\n\\[\n\\Sigma e = \\lambda e\n\\]\ncom algum escalar \\(\\lambda\\). O escalar \\(\\lambda\\) é denominado de autovalor de \\(\\Sigma\\), e dizemos que \\(e\\) é um autovetor associado a \\(\\lambda\\). Por \\(\\Sigma\\) ser uma matriz não negativa definida seus autovalores \\(\\lambda_i\\) associados tambem serão não negativos. Os autovetores e autovalores serão necessários para a análise de componentes principais mais a frente abordada.\n\n6.1.1.1 Equação característica\nAinda é necessário uma forma de encontrar os autovetores e autovalores associados a uma matriz \\(\\Sigma\\). Se \\(\\Sigma\\) for uma matriz quadrada, então \\(\\lambda\\) se, e somente se, \\(\\lambda\\) satisfaz a equação\n\\[\ndet(\\lambda I - \\Sigma) = 0\n\\]\nOnde det é o determinante e \\(I\\) a matriz identidade. Para esclarecimento, suponha como exemplo que,\n\\[\n\\Sigma = \\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\]\nEntão,\n\\[\n\\begin{split}\ndet\\left(\\begin{bmatrix}\n\\lambda& 0\\\\\n0 & \\lambda\n\\end{bmatrix}\n-  \n\\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\right) = 0\\\\\ndet\\left(\\begin{bmatrix}\n\\lambda - 8 & 2 \\\\\n2 & \\lambda-5\n\\end{bmatrix}\n\\right) = 0 \\\\\n(\\lambda - 8)\\times(\\lambda-5) - (2)\\times(2) = 0\n\\end{split}\n\\]\nResolvendo a equação obtemos os valores de \\(\\lambda_1 = 9\\) e \\(lambda_2 = 4\\), podemos encontrar os autovetores \\(v\\) associados seguindo a definição:\n\\[\n\\begin{bmatrix}\n8&-2\\\\\n-2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nv_{11}\\\\\nv_{12}\n\\end{bmatrix} =\n9\\begin{bmatrix}\nv_{11}\\\\\nv_{12}\n\\end{bmatrix} \\rightarrow v_{11} =- 2v_{12}\n\\]\nNote que para cada autovalor temos infinitos possíveis autovetores dentro dos reais. Nos restringiremos aos autovetores normalizados.Dizemos que um vetor \\(e_i\\) é normalizado quando:\n\\[\ne_i = \\begin{bmatrix}\ne_{i1}\\\\\ne_{i2}\\\\\n\\vdots\\\\\ne_{ip}\n\\end{bmatrix}\n\\]\nEm que\n\\[\n||e_i|| = \\sqrt{e^2_{i1} + e^2_{i2} + \\dots + e^2_{ip}} = 1\n\\]\n\n\n\n6.1.2 Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.\nO teorema da decomposição espectral é de extrema importância em álgebra matricial e estatística multivariada, ele relaciona a matriz com seus autovalores e autovetores normalizados.\nSuponha \\(\\Sigma\\) a matriz de covariâncias. Então existe uma matriz ortogonal \\(O\\)(matriz no qual sua transposta é igual a sua inversa) tal que,\n\\[\nO'\\Sigma O = \\begin{bmatrix}\n\\lambda_1 & 0 & 0 &\\dots & 0\\\\\n0&\\lambda_2& 0 & \\dots & 0 \\\\\n0 & 0 &\\lambda_3 &\\dots & 0\\\\\n\\vdots& \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\dots& \\lambda_p\n\\end{bmatrix} = \\Lambda\n\\]\nOnde \\(\\lambda\\_1 \\geq \\lambda\\_2 \\geq \\dots \\lambda\\_p\\geq0\\) são os autovalores ordenados em ordem decrescente da matriz \\(\\Sigma\\). Nesse caso, dizemos que a matriz \\(\\Sigma\\) é similar à matriz \\(\\Lambda\\), que implica em:\n\n\\(det(\\Sigma) = det(\\Lambda) = \\prod^p_{i=1} \\lambda_i\\)\ntraço\\((\\Sigma) =\\) traço\\((\\Lambda) = \\lambda_1 +\\dots+\\lambda_p\\)\n\nTem-se que a i-ésima coluna da matriz \\(O\\) é o autovetor normalizado \\(e_i\\) relacionado ao autovalor \\(\\lambda_i\\). Então a matriz \\(O\\) é dada por \\(O = [e_1,e_2,\\dots,e_p]\\) e pelo teorema da decomposição espectral, podemos ver que:\n\\[\n\\Sigma = O \\Lambda O' = \\sum_{i=1}^p \\lambda_i e_i e_i'\n\\]\nDentro do R é possível realizar a decomposição espectral usando a função eigen(),\n\nsigma <- matrix(c(8,-2,-2,5),nrow = 2)\nsigma\n\n     [,1] [,2]\n[1,]    8   -2\n[2,]   -2    5\n\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 9 4\n\n$vectors\n           [,1]       [,2]\n[1,] -0.8944272 -0.4472136\n[2,]  0.4472136 -0.8944272"
  },
  {
    "objectID": "naosupervisionado.html#análise-de-componentes-principais-pca",
    "href": "naosupervisionado.html#análise-de-componentes-principais-pca",
    "title": "6  Aprendizado não supervisionado",
    "section": "6.2 Análise de Componentes Principais (PCA)",
    "text": "6.2 Análise de Componentes Principais (PCA)\nA análise de componentes principais se preocupa em conseguir explicar a variância e covariância de uma estrutura de variáveis através de algumas poucas combinações lineares. Tendo como principal objetivo dessa análise a redução de dimensionalidade e interpretação das relações. Essas combinações lineares são os componentes principais e são não correlacionadas entre sí. Quando assumimos que as variáveis originiais possuem distribuição normal, as componentes, além de não correlacionadas são normalmente distribuidas e idependentes. Os componentes principais são extraidos através da decomposição da matriz de covariância do vetor aleatório. Caso alguma trasnformação seja realizada nesse vetor, a decomposição será realizada na matriz de covariância do vetor transformado. Um caso muito utilizado, suponha que nossas variáveis estão em escalas muito diferentes, o PCA pode acabar por dar mais variabilidade a essa variável com escala superior, para isso então padronizamos o vetor. Utilizar a matriz de covariância do vetor transformado e a matriz de correlação do vetor originais são ações equivalentes nessa situação.\nDefinição: Seja X um vetor aleatório com \\(\\mu = E(X)\\) e \\(\\Sigma = Var(X)\\) e \\((\\lambda_i,e_i), i = 1,\\dots,p\\) os pares de autovalores e autovetores normalizados associados de \\(\\Sigma\\). Então,\n\\[\n\\begin{split}\nY = O'X,\\quad \\textrm{com}\\quad O = [e_1,e_2,\\dots,e_p],\\textrm{ os componentes principais de X}\\\\\n\\textrm{ou seja}\\\\\nY =\n\\begin{bmatrix}\nY_1\\\\\n\\vdots\\\\\nY_d\n\\end{bmatrix} \\textrm{ com  } \\quad Y_1 = e_1'X = e_{11}X_1 + e_{12}X_2 +  \\dots + e_{1p}X_p\n\\end{split}\n\\]\nO primeiro componente principal. Os componentes principais de X, Y, são tais que,\n\\[\n\\begin{split}\n\\mu_y = E(Y) = E(O'X) = O'E(X) = O'\\mu_x\\\\\n\\Sigma_y = Var(Y) = Var(O'X) = O'Var(X)O = O'\\Sigma_xO = \\Lambda\n\\end{split}\n\\]\nou seja\n\\[\ncov(Y_i,Y_j) = 0, \\forall i \\neq j \\textrm{ e } Var(Y_i) = \\lambda_i\n\\]\nA prova desse resultado pode ser vista em (Johnson, Wichern, et al. 2002, 5:432).\nDescrevemos a variância total da população como sendo o somatório de todos os autovalores \\(\\lambda\\). A partir disso, podemos descrever a proporção da variância total explicada pela j-ésima componente como sendo:\n\\[\n\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i} \\qquad \\forall j =1,\\dots,p\n\\]\nPara algum \\(p\\) significativamente grande, podemos utilizar \\(d<p\\) componentes ao invés das \\(p\\) variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas \\(d\\) componentes.\nSe \\(Y_i = e'_iX, i =1\\dots,p\\) são as componentes principais obtidas da matriz de covariância, então\n\\[\n\\rho_{Y_i,X_j} = \\frac{e_{ij}\\sqrt{\\lambda_i}}{\\sigma_{jj}}, \\quad \\forall i,j=1,\\dots p\n\\]\nSão os coeficientes de correlação entre a componente \\(Y_i\\) e a variável \\(X_j\\)\n\n6.2.1 Exemplo.\nPara realmente entender a aplicabilidade da análise de componentes, vamos pegar um subconjunto do banco de dados mtcars, conjunto de dados no R base, consiste nas características de modelos de carros. Selecionaremos um subconjunto de colunas numéricas para conseguirmos trabalhar, considerando que PCA funciona melhor com variáveis numéricas. Há possibilidade de transformação de variáveis categoricas em variáveis dummy, porem o algoritmo não será tão preciso, também não sendo possível trabalhar com variáveis categóricas ordinais nesse caso.\n(deixei um exemplo com mtcars pois o banco de dados trabalhado no livro tem poucas variaveis numericas)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n3\n1\n\n\n\n\n\nPodemos obter de forma simples no R as componentes, bem como a proporção da variância explicada, com a função prcomp(). Bem como citado tambem é comum padronização das variáveis devido a escala de cada característica, para isso basta informar o parâmetro scale. como TRUE dentro da função.\n\ndados.pca <- dados |> \n  prcomp()\npaste('dados não padronizados: ',sep = \"\\n\")\n\n[1] \"dados não padronizados: \"\n\ndados.pca |> summary()\n\nImportance of components:\n                           PC1      PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     136.532 38.14735 3.06642 1.27492 0.90474 0.64734 0.3054\nProportion of Variance   0.927  0.07237 0.00047 0.00008 0.00004 0.00002 0.0000\nCumulative Proportion    0.927  0.99938 0.99985 0.99993 0.99997 0.99999 1.0000\n                          PC8    PC9\nStandard deviation     0.2859 0.2159\nProportion of Variance 0.0000 0.0000\nCumulative Proportion  1.0000 1.0000\n\n#padronizando as variaveis devido a diferenca de escalas\n\ndados.pca.padr <- dados |>\n  prcomp(scale. = T)\npaste('dados padronizados:',sep = \"\\n\")\n\n[1] \"dados padronizados:\"\n\ndados.pca.padr |> summary()\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.3782 1.4429 0.71008 0.51481 0.42797 0.35184 0.32413\nProportion of Variance 0.6284 0.2313 0.05602 0.02945 0.02035 0.01375 0.01167\nCumulative Proportion  0.6284 0.8598 0.91581 0.94525 0.96560 0.97936 0.99103\n                          PC8     PC9\nStandard deviation     0.2419 0.14896\nProportion of Variance 0.0065 0.00247\nCumulative Proportion  0.9975 1.00000\n\n#outras informacoes\n\ndados.pca.padr |> print()\n\nStandard deviations (1, .., p=9):\n[1] 2.3782219 1.4429485 0.7100809 0.5148082 0.4279704 0.3518426 0.3241326\n[8] 0.2418962 0.1489644\n\nRotation (n x k) = (9 x 9):\n            PC1         PC2         PC3          PC4        PC5         PC6\nmpg  -0.3931477  0.02753861 -0.22119309 -0.006126378 -0.3207620  0.72015586\ncyl   0.4025537  0.01570975 -0.25231615  0.040700251  0.1171397  0.22432550\ndisp  0.3973528 -0.08888469 -0.07825139  0.339493732 -0.4867849 -0.01967516\nhp    0.3670814  0.26941371 -0.01721159  0.068300993 -0.2947317  0.35394225\ndrat -0.3118165  0.34165268  0.14995507  0.845658485  0.1619259 -0.01536794\nwt    0.3734771 -0.17194306  0.45373418  0.191260029 -0.1874822 -0.08377237\nqsec -0.2243508 -0.48404435  0.62812782 -0.030329127 -0.1482495  0.25752940\ngear -0.2094749  0.55078264  0.20658376 -0.282381831 -0.5624860 -0.32298239\ncarb  0.2445807  0.48431310  0.46412069 -0.214492216  0.3997820  0.35706914\n             PC7         PC8         PC9\nmpg  -0.38138068 -0.12465987  0.11492862\ncyl  -0.15893251  0.81032177  0.16266295\ndisp -0.18233095 -0.06416707 -0.66190812\nhp    0.69620751 -0.16573993  0.25177306\ndrat  0.04767957  0.13505066  0.03809096\nwt   -0.42777608 -0.19839375  0.56918844\nqsec  0.27622581  0.35613350 -0.16873731\ngear -0.08555707  0.31636479  0.04719694\ncarb -0.20604210 -0.10832772 -0.32045892\n\n\nDa informação obtida por print(dados.pca.padr), podemos identificar os loadings da análise. Os loadings podem ser definidos como os coeficientes da combinação linear das variáveis originais de onde as componentes principais são construidas. De um ponto de vista matemático os loadings são iguais às coordenadas das variáveis divididas pela raiz quadrada do autovalor associado ao componente. são úteis quando você deseja entender os resultados. Lembre-se de que cada nova variável Y é uma combinação linear de todas as variáveis. A matriz de loadings representa verticalmente quanto da variância de cada componente é explicada por cada variável original. Vemos por exemplo que, conforme mpg aumenta, a PC1 tem um descrécimo. Os loadings são muito úteis na hora de nomear nossas componentes por essa relação que faz com cada uma das variáveis.\n\n\n6.2.2 Número de Componentes Principais\nAté agora foi descrito que podemos utilizar um número \\(d < p\\) de componentes principais que contenha uma explicabilidade aproximada dos dados originais, mas qual seria esse valor \\(d\\)? Há um conjunto de técnicas para essa tomada de decisão, sendo uma delas por exemplo a proporção de variância acumulada total explicada pelas componentes \\(Y_1,\\dots,Y_p\\):\n\\[\n\\frac{\\sum^d_{j=1}\\lambda_j}{\\sum_{i=1}^p \\lambda_i}\n\\]\nEsse valor é observado na função prcomp() já citada, como cumulative Proportion no resultado do summary() da função.\nPodemos utilizar como apoio gráfico e auxílio na tomada de decisão para o número de componentes é o scree plot, conhecido também como gráfico do cotovelo. Consiste na ordenação dos autovalores do maior para o menor, procurando por uma espécie de cotovelo dentro do gráfico. Selecionamos o número $i $ de componentes em que há um grande valor para observação \\(\\lambda_{i-1}\\) em comparação a observação \\(\\lambda_i\\) e uma pequena alteração da observação \\(\\lambda_i\\) para a observação \\(\\lambda_{i+1}\\). Observe a seguir\n\n#variancia explicada por cada componente\nvar_explicada = dados.pca.padr$sdev^2 / sum(dados.pca.padr$sdev^2)\nlibrary(ggplot2)\n\n\nqplot(c(1:9), var_explicada) + \n  geom_line() + \n  xlab(\"Principal Componente\") + \n  ylab(\"variancia explicada\") +\n  ggtitle(\"Scree Plot\") +\n  ylim(0, 1) + \n  scale_x_discrete(limits=c(1:9))\n\n\nPodemos por meio, tanto do scree plot, quanto pelo valor da variância explicada acumulada, selecionar \\(d= 3\\) componentes para reter, pela queda de 2 para 3 ser significante, enquanto a de 3 para 4 nem tanto. Reduzindo número de variáveis a 3.\n##Métodos de Agrupamentos\nA análise de agrupamentos ou clusterização, tem como objetivo, agrupar indivíduos da população usando como base medidas de similaridade entre eles, formando grupos heterogêneos entre sí com homogenuidade entre indivíduos de mesmo cluster. Muito utilizado na classificação de tipos de clientes de mercado, usuários de aplicativos, ou até mesmo em áres como psicologia, para agrupamentos de perfis de personalidade. Outro exemplo pode ser visto no trabalho (colocar link mariana).\n\n\n6.2.3 Medidas de dissimilaridade\nDe forma mais intutitiva, essas medidas de dissimilaridade seriam formas de numerar o quão próximo ou distânte a característica de um indivíduo (Idade por exemplo), se aproxima da mesma característica de outro indivíduo da mesma população. Não possuimos uma única forma de medida. Aqui apresentaremos as mais conhecidas e mais trabalhadas. Não existe uma métrica melhor, a eficácia de uma medida dependerá do caso em que a mesma será aplicada. Suponha que para cada elemento amostral será obtido o vetor \\(X = [X_{1},X_{2},\\dots,X_{p}]'\\) de medidas, onde \\(X_{i}\\) representa a medida da i-ésima característica para a unidade amostral.\nDistância Euclidiana: Essa é provavelmente a mais conhecida e usada medida de distância. Ela simplesmente é a distância geométrica no espaço multidimensional. Considere o i-ésimo e o j-ésimo indivíduo:\n\\[\nd(X,Y) = \\sqrt{\\sum^p_{i=1}(X_i - Y_i)^2}\n\\]\nDistância de Canberra: A distância de Camberra examina a soma das séries de diferenças fracionárias entre as coordenadas do par de observações.\n\\[\nd(X,Y) = \\sum^p_{i=1}\\frac{|X_i - Y_i|}{|X_i| + |Y_i|}\n\\]\nDistância de Manhattan: A distância de Manhattan (“City Block” ou “Geometria do Táxi”) é uma forma de geometria em que a distância entre dois pontos é a soma das diferenças absolutas de suas coordenadas.\n\\[\nd(X,Y) = \\sum^p_{i=1}|X_i- Y_i|\n\\]\nDistância de Chebyshev: Em matemática, distância de Chebyshev (ou distância de Tchebychev), métrica máxima ou \\(L_{\\infty}\\) métrica, é uma métrica definida em um espaço vetorial onde a distância entre dois vetores é a maior de suas diferenças ao longo de qualquer característica.\n\\[\nd(X,Y) = \\max_i(|X_i - Y_i|)\n\\]\nDistância de Minkowski: A distância de Minkowski de ordem \\(k\\), sendo \\(k\\) inteiro, pode ser considerada uma generalização tanto da distância euclidiana quanto da distância de manhattan.\n\\[\nd(X,Y) = \\left(\\sum^p_{i=1}|X_i - Y_i|^k\\right)^\\frac{1}{k}\n\\]\nTodas essas distâncias aqui citadas podem ser acessadas pela função dist() do R, alterando o parãmetro method para a distância desejada, da seguinte forma :\n\ndb<- dados[1:10,c('sem_pri','idade_anos','dt_evoluca_2','ano','dt_sint')]\ndb$ano <- db\ndb.dist <- db |> na.omit() |> dist(method = 'euclidean')\ndb.dist\n\n           1         2         3         4         5         6         7\n2  21.633308                                                            \n3  21.633308  8.485281                                                  \n4  31.292172 24.738634 16.321765                                        \n5  50.521283 28.962044 30.886890 37.804762                              \n6  67.242843 47.774470 45.615787 43.266615 24.665766                    \n7  20.435264 11.063453 18.782971 34.985711 35.445733 56.920998          \n8  31.805660 13.813037 10.217632 17.076299 22.126907 35.445733 24.665766\n9  37.994736 28.962044 37.421919 53.699162 40.958516 65.424766 19.809089\n10 44.009090 27.626075 22.768399 18.782971 22.847319 24.738634 38.418745\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  40.249224          \n10 13.813037 52.752251\n\n\nConsguindo a distância euclidiana entre cada uma das 10 primeiras observações para as características selecionadas.\n\n\n6.2.4 Técnicas Hierárquicas\nDentro da estatística multivariada dividimos frequêntemente as técnicas aglomerativas em dois tipos: hierárquicos e não hierárquicos, sendo as hierárquicas classificadas em aglomerativa e divisivas. Métodos hierárquicos são geralmente utilizados na análise exploratória afim de encontrar um número ótimo de clusters para o conjunto de variáveis, para as técnicas não hierárquicas é necessário um valor prévio de grupos.\n\n6.2.4.1 Técnicas Hierárquicas Aglomerativas\nConsidere cada observação como um grupo único, nos métodos aglomerativos vamos anexando cada grupo um ao outro em cada passo, usando suas medidas de similaridade para esse agrupamentos. Em cada instância do processo o par de grupos com a menor medida de dissimilaridade. Suponha a distância euclidiana por exemplo, em cada passo, verificaremos os \\(p\\) grupos e anexamos o par com a menor distância euclidiana, seguindo para o próximo passo realizamos o mesmo com os \\(p-1\\) grupos, até q sobre apenas 1 grupo com todas as observações. Seguindo o processo por \\(p-1\\) passos.\nLigamento Simples: Assumindo que cada observação é um cluster incialmente, suponha as observações X e Y sendo as com menor distância, ou os vizinhos mais próximos, formando o novo cluster {XY}. A distância entre o grupo {XY} e os demais grupos, suponha W, é definida como:\n\\[\nd(\\{XY\\},W) = \\min\\{d_{XW},d_{YW}\\}\n\\]\nConsidere a matriz de distâncias do exemplo anterior das 5 primeiras observações:\n\\[\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nSendo a distância entre a observação 3 e 2 a menor distância dentre todas as observações. Anexaremos as duas observações em um único grupo, assumindo a nova distância desse grupo com as demais observações como sendo o minimo da distância das variaveis do grupo os demais grupos:\n\\[\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 16.32 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=28.96 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nDando prosseguimento com o processo, note que agora a menor distância se da entre os grupos {23} e o grupo {4}, logo os dois serão reagrupados em um único cluster, seguindo com esse mesmo processo até que reste apenas um grupo.\n\\[\n\\begin{bmatrix}\nd_{234,1}=21.63 \\\\\nd_{234,5}= 28.96 & d_{4,5} = 37.80\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=28.96\n\\end{bmatrix}\n\\]\nOs resultados do agrupamento de ligação simples podem ser exibidos graficamente na forma de um dendrograma, ou diagrama de árvore. Os ramos na árvore representam clusters. As ramificações se unem em nós cujas posições ao longo de uma distância (ou similaridade) indicam o nível em que as junções ocorrem. Veja para o exemplo acima considerando agora as 10 observações, passando na função hclust() para ligação dos grupos, o parâmetro method para tipo de ligação, no caso atual method = \"single\".\n\nhc <-  db.dist |> \n  hclust( method = \"single\") \nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\n\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Simples\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\nLigação Completa : Funciona de maneira parecida com a ligação simples, uniremos os grupos com menor distância entre sí até que reste apenas um único grupo. Porém, as distâncias entre as variáveis unidas, digamos X e Y, das demais variáveis W será definida como:\n\\[\nd(\\{XY\\},W) = \\max\\{d_{XW},d_{YW}\\}\n\\]\nMas o procedimento das demais iterações será da mesma forma, fazendo o link entre os grupos de menor distância. Suponha o exemplo anterior:\n\\[\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nUniremos as observações 2 e 3 assim como anteriormente, e a cada passo, a nova distância será a distância máxima entre as variáveis do grupo e os demais grupos:\n\\[\n\\begin{split}\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 24.73 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=30.88 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\\\\\n\\\\\n\\rightarrow\n\\begin{bmatrix}\nd_{123,4}=31.29 \\\\\nd_{123,5}= 50.52 & d_{4,5} = 37.80\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=50.52\n\\end{bmatrix}\n\\end{split}\n\\]\nObserve agora o dendrograma para ligação completa com 10 observações.\n\nhc <-  db.dist |> \n  hclust( method = \"complete\") \nlibrary(factoextra)\n\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Completa\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\nLigação Média: A ligação média trata a distância entre dois clusters como a distância média entre todos os pares de itens onde um membro de um par pertence a cada cluster. Considere o grupo {XY} e o grupo {W}, e \\(N_w\\) como sendo número de elementos em {W}, e \\(N_{XY}\\) número de elementos em {XY}, então:\n\\[\nd(\\{XY\\},W) = \\frac{\\sum^{N_{xy}}_{i=1}\\sum_{j=1}^{N_w}d_{ij}}{N_{xy}N_w}\n\\]\nOnde \\(d_{ij}\\) representa a distância entre a i-ésima observação do grupo {XY} e j-ésima observação do grupo {w}. Seguindo com o exemplo anterior e seu dendrograma obtemos:\n\\[\n\\begin{split}\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\\\\\n\\\\\\rightarrow\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 20.525 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=29.92 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\n\\\\\n\\\\ \\rightarrow\n\\begin{bmatrix}\nd_{234,1}=24.85 \\\\\nd_{234,5}= 32.54 & d_{1,5} = 50.52\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=37.04\n\\end{bmatrix}\n\\end{split}\n\\]\n\nhc <-  db.dist |> \n  hclust( method = \"average\") \nlibrary(factoextra)\n\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Média\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\nMétodo Ward de clusterização : O método de ward se baseia na minimização da “perda de informação” ao juntar dois grupos. É tido como perda de informação o crescimento da soma dos quadrados dos erros, \\(SQE\\). Suponha o grupo {W}, a \\(SQE_W\\) pode ser descrita como a soma dos quadrados das distâncias de cada item do grupo para a média do grupo. Definindo \\(SQE\\) como a soma dos \\(SQE_i\\), onde \\(i\\) representa cada um dos \\(N\\) grupos. Em cada instância do processo é realizado a junção de todos os possiveis pares de grupos, optamos pela união que obtiver o menor incremento da \\(SQE\\). Note que no passo 0 essa soma é equivalente a 0, considerando que para cada \\(SQE_i\\), com apenas uma observação por cluster, a média será a própria observação. Enquanto que ao considerar o grupo final com todas as observações é possível obter a \\(SQE\\) por:\n\\[\nSQE = \\sum^N_{j=1}(X_j - \\bar{X})'(X_j - \\bar{X})\n\\]\nOnde \\(X_j\\) representa a j-ésima observação do grupo.\n\nhc <-  db.dist |> \n  hclust( method = \"ward.D2\") \nlibrary(factoextra)\n\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Ward\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\n\n\n6.2.4.2 Algumas conclusões\nOs métodos hierárquicos são muito utilizados na exploração dos dados, bem como para pré definição do número de clusters, pois como veremos a seguir nos métodos não hierarquicos temos a necessidade de informar um número prévio de grupos. O dendograma é tido como principal forma de definição desses \\(k\\) grupos. Para definir o número ideal de clusters vamos utilizar o exemplo do método Ward. Observe que a distância para união do grupo {5,6} e {2,3,4,8,10} é relativamente grande se comparada as outras junções, uma forma de definir então seria \\(k = 3\\) grupos onde os grupos seriam, {1,7,9},{5,6} e {2,3,4,8,10} olhando o nível de fusão (distância) em que cada grupo precisou para se unir. Podemos então já utilizar \\(k\\) aproximado de 3 para iniciarmos nossos métodos não hieráquicos como veremos a seguir.\n\n\n\n6.2.5 Métodos de Agrupamentos Não Hierárquicos\nDentro desse conjunto de métodos iremos trabalhar com o mais usual e conhecido, k-médias. Bem como dito, os métodos não hierárquicos precisam de um número pré definido de grupos \\(k\\), anexando cada observação a um grupo com base em \\(k\\) centróides que serão definidos pelo algoritmo.\n\n6.2.5.1 K-Médias.\nK-médias é um método simples de particionamento, onde é necessário estabelecer um número \\(k\\) de grupos previamente a separação das variáveis. Definindo um número inicial de centróides, podendo esses ser observações do próprio conjunto de dados ou coordenadas aleatórias, é realizada a divisão do conjunto de dados, sendo cada observação anexada ao centro de menor distância, ou mais próximo. Com base nesse novo grupo criado, é determinado o nomo ponto central, que passa a ser a média do grupo. Baseado nesses novos pontos realizamos os passos anteriores por um número \\(N\\) de vezes até que não se tenha mais alteração na posição dos centróides. O resultado do processo são grupos heterogêneos entre sí com variáveis homogêneas entre sí, tendo a menor variância interna possível e a maior variação externa possível. O núemero de iterações do processo pode também ser pré estabelecido, considerando o custo computacional para bancos de dados grandes, é inviável a realização do processo até a falta de alteração dos clusters.\nExemplo:\nSuponha os seguintes dados para 20 variáveis, e suponha que vamos fazer inicialmente para \\(k=3\\) grupos.\n\n\n\n\n\n\nidade_anos\nsem_pri\n\n\n\n\n1\n24\n17\n\n\n2\n31\n26\n\n\n3\n27\n28\n\n\n4\n20\n33\n\n\n5\n39\n39\n\n\n6\n34\n51\n\n\n7\n34\n21\n\n\n8\n29\n33\n\n\n9\n44\n18\n\n\n10\n27\n40\n\n\n11\n28\n47\n\n\n12\n35\n50\n\n\n13\n37\n29\n\n\n14\n30\n29\n\n\n15\n32\n45\n\n\n16\n27\n32\n\n\n17\n44\n32\n\n\n18\n30\n42\n\n\n19\n16\n21\n\n\n20\n24\n21\n\n\n21\n31\n32\n\n\n22\n24\n36\n\n\n23\n31\n34\n\n\n24\n33\n47\n\n\n25\n25\n50\n\n\n26\n31\n21\n\n\n27\n26\n21\n\n\n28\n49\n19\n\n\n29\n25\n25\n\n\n30\n16\n31\n\n\n31\n20\n32\n\n\n32\n34\n42\n\n\n33\n28\n38\n\n\n34\n33\n16\n\n\n35\n34\n21\n\n\n36\n32\n20\n\n\n37\n38\n32\n\n\n38\n31\n40\n\n\n39\n38\n24\n\n\n40\n41\n27\n\n\n\n\n\nSuponha que os clusters são tidos inicialmente nas cordenadas:\n\n\n\n\n\n\nidade_anos\nsem_pri\n\n\n\n\n1\n23\n27\n\n\n2\n31\n43\n\n\n3\n36\n25\n\n\n\n\n\nAgregando cada variável a um cluster obtemos então.\n\n\n\n\n\n\nidade_anos\nsem_pri\nCentróide\n\n\n\n\n1\n24\n17\n1\n\n\n2\n31\n26\n3\n\n\n3\n27\n28\n3\n\n\n4\n20\n33\n1\n\n\n5\n39\n39\n2\n\n\n6\n34\n51\n2\n\n\n7\n34\n21\n3\n\n\n8\n29\n33\n1\n\n\n9\n44\n18\n3\n\n\n10\n27\n40\n2\n\n\n11\n28\n47\n2\n\n\n12\n35\n50\n2\n\n\n13\n37\n29\n3\n\n\n14\n30\n29\n3\n\n\n15\n32\n45\n2\n\n\n16\n27\n32\n1\n\n\n17\n44\n32\n3\n\n\n18\n30\n42\n2\n\n\n19\n16\n21\n1\n\n\n20\n24\n21\n1\n\n\n21\n31\n32\n3\n\n\n22\n24\n36\n1\n\n\n23\n31\n34\n2\n\n\n24\n33\n47\n2\n\n\n25\n25\n50\n2\n\n\n26\n31\n21\n3\n\n\n27\n26\n21\n1\n\n\n28\n49\n19\n3\n\n\n29\n25\n25\n1\n\n\n30\n16\n31\n1\n\n\n31\n20\n32\n1\n\n\n32\n34\n42\n2\n\n\n33\n28\n38\n2\n\n\n34\n33\n16\n3\n\n\n35\n34\n21\n3\n\n\n36\n32\n20\n3\n\n\n37\n38\n32\n3\n\n\n38\n31\n40\n2\n\n\n39\n38\n24\n3\n\n\n40\n41\n27\n3\n\n\n\n\n\nCom base nesses novos grupos definimos então o novo centróide como sendo a média das variáveis de cada grupo, ou seja:\n\n\n\n\n\ncentroide\nidade_anos\nsem_pri\n\n\n\n\n1\n22.81818\n27.45455\n\n\n2\n31.30769\n43.46154\n\n\n3\n35.87500\n24.68750\n\n\n\n\n\nAgregando cada variável a seu novo grupo e seguindo o processo até o número de iterações pré definidas ou até que não tenha mais alterações nas coordenadas dos centros de cada cluster.\nNo R base já está incluso uma função para o método k-médias, kmeans(), que pode ser implementado de maneira simples. considere o banco já discutido\n\nkmeans.df <- df |>\n  kmeans(centers = 3, iter.max = 300)\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 16, 12\n\nCluster means:\n  idade_anos  sem_pri\n1   37.91667 23.33333\n2   25.06250 28.18750\n3   31.33333 44.25000\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 2  2  2  2  3  3  1  2  1  3  3  3  1  2  3  2  1  3  2  2  2  2  2  3  3  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 2  1  2  2  2  3  3  1  1  1  1  3  1  1 \n\nWithin cluster sum of squares by cluster:\n[1] 669.5833 861.3750 412.9167\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nDe forma simples podemos identificar os centros e em qual cada uma das variáveis foi atribuida após as \\(N\\) iterações. Identificamos também a variância entre clusters, bem como a variância total e entre as variáveis de cada grupo. Essa variância se torna importânte na identificação do valor \\(k\\) estabelecido.\n\ndados_grupos <- kmeans.df |> broom::augment(df)\ncent <- kmeans.df$centers\n\n\ndados_grupos |> \n  ggplot(aes(x = idade_anos, y = sem_pri,col = .cluster)) +\n  geom_point() +\n  geom_point(aes(x = cent[1,1], y = cent[1,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[2,1], y = cent[2,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[3,1], y = cent[3,2]), color = \"black\", size = 3)\n\n\n\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 16, 12\n\nCluster means:\n  idade_anos  sem_pri\n1   37.91667 23.33333\n2   25.06250 28.18750\n3   31.33333 44.25000\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 2  2  2  2  3  3  1  2  1  3  3  3  1  2  3  2  1  3  2  2  2  2  2  3  3  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 2  1  2  2  2  3  3  1  1  1  1  3  1  1 \n\nWithin cluster sum of squares by cluster:\n[1] 669.5833 861.3750 412.9167\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n6.2.5.2 Número Ideal de Grupos\nUma das formas já discutidas aqui sobre seleção do número ideal de \\(k\\) grupos é a pré utilização de um método hierárquico e análise de seu dendrograma. Porém, retomando os assuntos apresentados quando foi discutido PCA, podemos utilizar o scree plot da variação total como metodologia de definição do número ideal de clusters para o algorítmo. A utilização é realizada da mesma maneira, é feita a identificação do número \\(k\\) que sofra grande decréscimo da soma da variação dentro dos clusters para um número \\(k-1\\) e um pequeno em comparação com \\(k+1\\). A variação total é dada como:\n\\[\n\\sum^k_{i=1}\\sum_{j\\in C_i}d^2(x_j,c_i)\n\\] Sendo \\(C_i\\) centro do i-ésimo grupo e \\(x_j\\) a j-ésima variável do i-ésimo grupo. A função de distância mais usual é a euclidiana discutida anteriormente.\n\nvar_totais <- vector()\nfor(i in 1:10){\n  var_totais[i] <- (df |> kmeans(centers = i, iter.max = 400))$tot.withinss\n}\n\n\nqplot(1:10, var_totais, geom = \"line\")\nqplot(c(1:10), var_totais) + \n  geom_line() + \n  xlab(\"Número de clusters\") + \n  ylab(\"Soma das variâncias dentro dos grupos\") +\n  ggtitle(\"Scree Plot\") +\n  scale_x_discrete(limits=c(1:10))\n\n\n\nPodemos definir a partir disso possíveis números ideais como 3, 4 ou até mesmo 6.\n\n\n6.2.5.3 Algumas conclusões\nPara definirmos um modelo como sendo ótimo para aplicação, é necessário defir alguns parâmetros para determinar a qualidade de um modelo ou método. O k-médias por exemplo é um algoritmo muito sucetível a outliers, dados fora do padrão encontrado no banco de dados, que podem acabar por deixar de agrupar uma determinada variável ou simplesmente formar um grupo amais, sem que haja necessidade. Outro fator que é necessário manter a atenção é a determinação do formato do cluster. Muitos dos algoritmos consideram um formato esférico ou circular para as variáveis, veja o exemplo:\n\ndados_circulo <- data.frame(\n X = runif(5000, -1, 1),\n  Y = runif(5000, -1, 1)\n) |>\n  dplyr::filter(X^2 + Y^2 <= 0.2 | (X^2 + Y^2 <= 0.8 & X^2 + Y^2 >= 0.6))\n\n\nqplot(dados_circulo$X, dados_circulo$Y)\n\n\nÉ notável como o agrupamento deve ser realizando simplesmente olhando para o gráfico proposto, porém, algoritmos como k-médias não pensam da mesma forma,\n\ndados_circulo.km <- dados_circulo |> kmeans(centers = 2)\ndados_circulo <- dados_circulo.km |> broom::augment(dados_circulo)\ncent <- dados_circulo.km$centers\n\n\ndados_circulo |> \n  ggplot(aes(x = X, y = Y,col = .cluster)) +\n  geom_point() +\n  geom_point(aes(x = cent[1,1], y = cent[1,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[2,1], y = cent[2,2]), color = \"black\", size = 3)\n\n\n\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 16, 12\n\nCluster means:\n  idade_anos  sem_pri\n1   37.91667 23.33333\n2   25.06250 28.18750\n3   31.33333 44.25000\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 2  2  2  2  3  3  1  2  1  3  3  3  1  2  3  2  1  3  2  2  2  2  2  3  3  1 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 2  1  2  2  2  3  3  1  1  1  1  3  1  1 \n\nWithin cluster sum of squares by cluster:\n[1] 669.5833 861.3750 412.9167\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nÉ perceptível que o método por particionamento em médias não foi eficaz para o banco de dados em questão. Ao utilizar a função hcut() para particionamento utilizando um método hierárquico obtemos uma melhor resposta para o agrupamento das variáveis\n\ndados_circulo.h <- hcut(dados_circulo[,1:2], k = 2, hc_method = \"single\")\nd_circulo.h <- cbind(dados_circulo[,1:2],cluster=dados_circulo.h$cluster)\n\n\nd_circulo.h |> \n  ggplot(aes(x = X, y = Y,col = cluster)) +\n  geom_point()\n\n\nOs métodos hierárquicos apresentados no entanto, por utilizarem da distância de uma variável a outra são limitados a utilização de banco de dados numéricos, o que nem sempre é o encontrado nos problemas reais, é necessário optar nesse caso por métodos e algoritmos que consigam fazer a distinção mesmo na presença de variáveis categóricas. Fator relevante para a escolha do melhor algoritmo é a capacidade de lidar com um grande volume de dados. No k-médias temos a opção por exemplo de pré definirmos o número de iterações. Considere um grupo com milhões de variáveis, ao utilizar um método de ligação simples hierárquico faremos aproximadamente um milhão de ligações para depois identificar o número ideal de grupos, caso esse não seja conhecido (Caso mais comum). O ideal então é a análise exploratória de seus dados para com base nos conhecimentos sobre os diferentes tipos de métodos, saber qual será o de melhor aplicação para o determinado problema. Nada o impede de aplicar mais de um método e após sua aplicação identificar qual foi o modelo ótimo para o problema.\n\n\n\n\nAnton, Howard, e Chris Rorres. 2001. Álgebra linear com aplicações. Vol. 8. Bookman Porto Alegre.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "descritiva.html#medidas-resumo",
    "href": "descritiva.html#medidas-resumo",
    "title": "4  Análise exploratória dos dados",
    "section": "4.1 Medidas-resumo",
    "text": "4.1 Medidas-resumo\nUma medida-resumo é uma construção matemático/estatística que tenta capturar em um único número um comportamento presente nos dados. Quatro grandes grupos de medidas podem ser considerados para resumir variáveis quantitativas, são elas: posição, dispersão, assimetria e curtose. Vejamos agora que medidas são essas e como interpretá-las.\n\n4.1.1 Medidas de posição\nAs medidas de posição, como o nome diz, indicam posições de interesse de valores da variável. Por exemplo, se idade é a variável de interesse investigada em um grupo de pessoas, e se quer trazer um informação resumida dela no grupo, apresentar a menor e a maior idade encontrada, valores típicos da idade no grupo, são exemplos de medidas de posição.\nDe maneira geral, vamos explorar aqui as seguintes medidas posição: valor mínimo, valor máximo, percentis e medidas de tendência central, tais como moda, média e mediana. Os valores mínimo e máximo de uma variável quantitativa estão relacionados, respectivamente, com o menor e o maior valor observado da variável analisada.\nMedidas que buscam descrever um valor típico que a variável apresenta são chamados de medidas de tendência ou posição central. Mas o que seria uma valor típico? Como podemos definir isso? A resposta não é única e, por isso, existem diferentes medidas de tendência central. Por exemplo, se o valor típico considerado for aquele que mais se repete no conjunto de dados para variável, o que temos é a moda. Se o valor típico for aquele que ocupa uma posição central no conjunto de dados, de tal forma que 50% dos dados observados estão abaixo desse valor e os demais 50% estão acima, o que temos é a mediana. Agora, se o valor típico considerado for pensado como um ponto de equilíbrio das observações da variável, então temos a média.\nPor definição, a medida estatística moda corresponde aos(s) valor(es) mais frequente(s) do conjunto de dados observados para uma variável. Conjunto de dados que não apresentam valores repetidos são considerados amodais. Um conjunto de dados é bimodal se tiver duas modas, indicando que não apenas um único valor, mas dois valores do conjunto de dados apresentam frequências igualmente mais altas que os demais valores. Usando de mesmo raciocínio, havendo três ou mais valores modais em um conjunto de dados, dizemos que o conjunto de dados é trimodal ou multimodal, respectivamente. Vale citar que a moda também pode ser obtida para variáveis qualitativas.\nA média é a medida obtida ao somar todos os valores da variável e dividí-la pela quantidade de dados observados. Matematicamente, considere \\(x_1\\), \\(x_2\\), …, \\(x_n\\) as observações de uma variável \\(X\\), assim a média é definida como:\n\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{i = 1}^n x_i}{n}.\n\\end{equation}\\]\nPara entender essa medida como ponto de equilíbrio, vamos representar cada valor observado como pesos de mesma massa e distribuí-los sobre uma reta de massa desprezível nas posições referentes aos valores da variável em questão. Nosso objetivo agora é encontrar um ponto de apoio nessa reta de tal forma que ela e os pesos corretamente posicionados nela fiquem perfeitamente equilibrados, similar a uma balança. A média é o único local em que se pode localizar o ponto de apoio na reta de forma a obter um perfeito equilíbrio da reta e dos pesos. Para ilustrar essa ideia, apresentamos a seguir uma representação gráfica considerando o subconjunto da variável idade {22, 28, 29, 34, 34, 35, 36, 36, 37, 39}, cuja média é 33.\n\n\n\n\n\n\n\n(a) Ponto de equilíbrio na média\n\n\n\n\n\n\n\n\n\n(b) Ponto de equilíbrio fora da média\n\n\n\n\nFigura 4.1: Apresentando a média como ponto de equilíbrio\n\n\nNo início dessa seção, apresentamos a noção intuitiva do que representa o valor mediano, mas não como obtê-lo formalmente. A construção dessa medida passa por organizar os dados de maneira crescente e calcular a posição central dos dados via \\(\\frac{n+1}{2}\\), em que \\(n\\) representa o tamanho do conjunto de dados relacionada a variável de interesse. O valor mediano é o valor na amostra ordenada que ocupa a posição \\(\\frac{n+1}{2}\\).\nQuando \\(n\\) é ímpar, a expressão \\(\\frac{n+1}{2}\\) vai sempre gerar um valor inteiro, facilitando a obtenção da mediana. Por exemplo, o subconjunto a seguir é formado por nove valores retirados da variável idade, sendo eles: 21, 28, 24, 22, 31, 26, 22, 38, 16.\nOrdenando esse subconjunto, obtemos 16, 21, 22, 22, 24, 26, 28, 31, 38.\nComo \\(n=9\\), a posição em que se encontra a mediana será \\(\\frac{9+1}{2}=5\\). Assim, a mediana será 24, pois é o valor que está na quinta posição do subconjunto ordenado.\nSe \\(n\\) é par, a expressão \\(\\frac{n+1}{2}\\) gerará um valor não inteiro que apresenta apenas uma única casa decimal após a vírgula igual a 5. Por exemplo, se a variável idade apresenta apenas 8 valores então \\(n = 8\\) e a posição em que a mediana está localizada é dada por \\(\\frac{n+1}{2} = \\frac{8+1}{2} = 4,5\\). Como inferir um valor para a mediana quando a posição que ela ocupa é decimal? Note que a posição \\(4,5\\) está exatamente no meio das posições 4 e 5, então o valor mediano será definido como a média entre os valores que ocupam as posições 4 e 5.\nO subconjunto abaixo também consiste de valores retirados da variável idade, porém note que nesse exemplo há 8 valores, ou seja, \\(n=8\\).\n\\[ 27, 16, 31, 43, 26, 42, 17, 40. \\]\nAo ordenarmos, temos:\n\\[ 16, 17, 26, 27, 31, 40, 42, 43. \\]\nA mediana será o valor que está na posição \\(\\frac{8+1}{2} = 4,5\\). Logo, visto que a mediana está entre os valores que ocupam a quarta e quinta posição, corresponde à média entre esses valores, sendo \\(\\frac{27+31}{2}=29\\).\nCom ideia correlata a mediana, podemos apresentar medidas de posição não centrais, as quais denominamos quantis ou percentis. O percentil 20, por exemplo, é o valor da variável em que 20% das observações no conjunto de dados apresentam valores menores ou iguais a ele. Por consequência, as restantes 80% das observações possuem valores acima do percentil 20. De maneira geral, podemos definir o percentil de ordem \\(p\\) como o valor da variável em que \\(100p\\%\\) \\((0 < p < 1)\\) das observações estão à sua esquerda, ou seja, são menores ou iguais que ele.\nAlguns percentis destacam-se por serem muito utilizados na análise de dados, não só numericamente como graficamente. Esses percentis são conhecido como quartis e basicamente dividem o conjunto de dados em 4 partes de mesmo tamanho. O primeiro quartil (\\(Q_1\\)) é o percentil 25, o segundo quartil (\\(Q_2\\)) é o percentil 50 e o terceiro quartil (\\(Q_3\\)) é o percentil 75. Vale notar que o segundo quartil é a mediana. De posse desses valores, como veremos mais a frente nesse capítulo, iremos construir o gráfico do tipo \\(boxplot\\), bastante utilizado na análise de dados da saúde.\nAinda com respeito aos percentis, outro termo comum na literatura é o decil que refere-se a divisão em 10 partes de mesmo tamanho do conjunto de dados associado a variável analisada. O primeiro decil, por exemplo, é o percentil 10 e o sexto decil é o percentil 60.\nTodas as medidas de posição aqui apresentadas tem em comum terem a mesma unidade de medida dos valores da variável observada, o que traz bastante interpretabilidade.\n\n\n4.1.2 Medidas de dispersão\nPor mais que as medidas de posição apresentadas sejam muito úteis na análise dados, elas por si só não se bastam como medidas resumo das observações de uma variável em um conjunto de dados. É possível construir diferentes conjuntos de dados para uma mesma variável que apresentam os mesmo valores de medida central (média, mediana e moda), mas tem comportamentos absolutamente diferentes. Por exemplo, veja a figura a seguir.\n\n\n\n\n\n\n\n(a) Dados: 2 ,3, 5 , 5, 7, 8.\n\n\n\n\n\n\n\n(b) Dados: 5,5,5,5.\n\n\n\n\nFigura 4.2: Exemplos de conjuntos de dados com mesma média, moda e mediana.\n\n\nOs dois conjuntos de dados apresentam os mesmos valores de média, mediana e moda. O que diferencia os dois conjuntos? O quão diferentes ou parecidos são as observações entre si em cada conjunto da variável. Na Figura Figura 4.2 (b), notamos que os quatro valores observados são iguais entre si e que, portanto, as observações nesse conjunto não variam, diferente do que ocorre para os dados que geraram o gráfico da Figura Figura 4.2 (a). Medidas de dispersão ou variabilidade são as medidas responsáveis por quantificar o quão diferente são os dados entre si. De forma bastante intuitiva temos que se os dados observados da variável não variam, então a medida de dispersão dela é zero e, caso haja diferenças entre os valores observados, então essa medida vai ser um valor positivo. Quanto maior a medida de variabilidade, mais diferente são os dados observados da variável entre si.\nNão existe uma única medida de dispersão na literatura, aqui vamos considerar as seguintes medidas: amplitude, intervalo interquartil, variância, desvio padrão e coeficiente de variação.\nDe fácil obtenção e interpretabilidade, a amplitude é a diferença entre o valor máximo e o valor mínimo da variável analisada no conjunto de dados e nos dá uma ideia do intervalo de variação dos dados. Uma desvantagem é que essa medida é absolutamente influenciada pela presença de valores discrepantes ou \\(outliers\\). O intervalo interquartil é uma medida mais robusta do que a amplitude intervalar e é calculada como a diferença entre o terceiro e o primeiro quartil, ou seja, é a amplitude entre os 50% dos dados centrais.\nPor mais informativas que sejam as medidas de amplitude e intervalo interquartil, queremos uma medida de dispersão que não considere apenas dois valores da amostra (mínimo e máximo ou primeiro e terceiro quartis) e sim todos os dados. Uma medida bastante intuitiva seria considerar a soma dos desvios de cada uma das observações em torno da média. Mas aí temos um problema: a soma dos desvios da média é sempre zero! Isso acontece porque sempre há desvios positivos e negativos que quando somados se anulam. Uma solução para essa questão é considerar alguma função que considere apenas o valor do desvio e não o seu sinal. Uma função candidata é a função quadrática (lembre que, por exemplo, \\((−2)^2=4\\)). Nessa construção surge a variância: soma dos desvios quadrados dividida pelo total de observações (\\(n\\)), ou seja, a média dos desvios quadrados. Assim, a variância quantifica o quanto os dados estão dispersos da média, em média.\nMatematicamente, considere \\(x_1\\), \\(x_2\\), …, \\(x_n\\) as observações de uma variável \\(X\\) e \\(\\bar{x}\\) a média observada dessa variável. A variância seria calculada como:\n\\[\n\\mbox{Var(X)} = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^2}{n}.\n\\tag{4.1}\\]\nPor mais intuitiva que seja essa construção, programas como o R e similares utilizam em sua análise uma versão modificada do cálculo da variância acima apresentado, em que a soma dos desvios quadrados é dividida por \\(n-1\\), não por \\(n\\). Justificativas para isso se devem a propriedades inferenciais. A maioria dos conjuntos de dados considerados nos estudos referem-se a análise de amostras de uma população e não a análise de todos os elementos de uma população. Ao mesmo tempo, um dos principais objetivos da análise estatística é fazer análises para a população e não apenas para a amostra considerada no estudo. Basicamente, se temos interesse de conhecer o valor médio de uma variável na população (\\(\\mu\\)), na impossibilidade de analisar todos os elementos dela e obter a medida, o fazemos de forma aproximada investigando o valor médio dessa variável na amostra (\\(\\bar{x}\\)). Esse mesmo raciocínio ocorre para a variância, na impossibilidade de obter a variância da variável para todos os elementos da população (\\(\\sigma^2\\)), analisamos essa medida via amostra, o caso é que é possível mostrar que para amostras de tamanho pequeno, a variância apresentada em (Equação 4.1) não aproxima-se bem do valor de \\(\\sigma^2\\). Matematicamente, é possível mostrar que tal dificuldade é contornada fazendo uso do divisor igual a \\(n-1\\) em (Equação 4.1). Na literatura esse cálculo muitas vezes é denominado como variância amostral e representado pelo símbolo \\(S^2\\) de tal forma que\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^2}{n-1}.\n\\tag{4.2}\\]\nVale ressaltar também que a medida que se considera tamanhos de amostra maiores, calcular a variância com divisor \\(n\\) ou \\(n-1\\) torna-se indiferente.\nComo a unidade de medida da variância é o quadrado da unidade de medida da variável correspondente, convém definir outra medida de dispersão que mantenha a unidade de medida original. Uma medida com essa propriedade é a raiz quadrada da variância, conhecida por desvio padrão.\nCaso o interesse seja calcular e comparar a dispersão entre variáveis com unidades dimensionais de natureza diferente, por exemplo, comprimento (em metros) e massa (em kg), não convém utilizar as medidas de dispersão apresentadas anteriormente pois todas as medidas apresentadas carregam consigo a unidade de medida considerada para a variável. Nesse caso, podemos fazer uso do coeficiente de variação (CV) para cada uma das variáveis analisadas, já que o CV é uma medida de dispersão relativa adimensional, calculada via razão entre o desvio-padrão e a média observada para a variável e quanto maior o seu valor, maior a dispersão dos dados em termos relativos a média.\n\n\n4.1.3 Medidas de assimetria e curtose\nAlém das medidas de posição e variabilidade, existe um conjunto de medidas dedicadas a explorar a forma da distribuição de frequências dos dados. Especificamente aqui estudaremos algumas: coeficientes de assimetria e de curtose e variações destas.\nComo boa parte dos estudos na área de saúde é realizado através de amostras da variável de interesse na população, vamos precisar definir os momentos amostrais centrais que serão ferramenta fundamental para a construção dos coeficientes de assimetria, curtose e seus derivados. Por definição, o momento amostral centrado (na média) de ordem \\(r\\) é dado por\n\\[\nm_r = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^r}{n}, \\; r = 1, 2, \\cdots.\n\\]\n\nA versão populacional do momento centrado de ordem \\(r\\) é expressa por \\(\\mu_r = \\frac{\\sum_{i = 1}^n (x_i - \\mu)^r}{N}\\), em que \\(r = 1, 2, \\cdots \\;\\) e \\(\\mu\\) e \\(N\\) referem-se, respectivamente, a média da variável de interesse e a quantidade de elementos investigados na população.\n\nDessa forma, o coeficiente de assimetria amostral é dado por \\(\\frac{m_3}{m_2^{3/2}}\\). Populações cuja a distribuição da variável é simétrica apresentam coeficiente de assimetria igual a zero. Distribuições assimétricas à direita apresentam valores positivos de coeficiente de assimetria para a variável analisada populacionalmente, assim como distribuições assimétricas à esquerda apresentam coeficiente de assimetria negativo.\n\nAnalogamente, \\(\\frac{\\mu_3}{\\mu_2^{3/2}}\\) é o coeficiente de correlação populacional.\nPopulações cuja a distribuição da variável é simétrica apresentam coeficiente de assimetria igual a zero. Distribuições assimétricas à direita apresentam valores positivos de coeficiente de assimetria para a variável analisada populacionalmente, assim como distribuições assimétricas à esquerda apresentam coeficiente de assimetria negativo.\n\n\n\n\n\n\n\n\n(a) Simétrico\n\n\n\n\n\n\n\n(b) Simétrico\n\n\n\n\n\n\n\n\n\n(c) Assimétrico à esquerda\n\n\n\n\n\n\n\n(d) Assimétrico à direita\n\n\n\n\nFigura 4.3: Histogramas e funções de densidade\n\n\nCoeficientes de assimetria amostrais diferentes de zero devem ser interpretados com cautela, uma vez que por se tratar de uma amostra não significa que necessariamente o comportamento da variável na população seja assimétrico. Testes estatísticos devem ser realizados para avaliar a hipotese de simetria da variável na população.\nAinda com respeito a forma da distribuição da variável, podemos avaliar o comportamento em suas caudas através do coeficiente de curtose amostral que se define por \\(\\frac{m_4}{m_2^{2}}\\). Distribuições de variáveis com valor de curtose igual a 3 são denominadas mesocúrticas. Tomada muitas vezes como referência, a distribuição normal apresenta coeficiente de curtose igual a três. Distribuições com coeficiente de curtose menores que 3 são denominadas platicúrticas e apresentam caudas mais leves (“finas”) do que a da distribuição normal. Distribuições com coeficiente de curtose maiores que 3 são denominadas leptocúrticas e apresentam caudas mais pesadas (“grossas”) do que a da distribuição normal. A distribuição t-Student é um exemplo de distribuição leptocúrtica.\nNa literatura é muito comum ser apresentado uma variante do coeficiente de curtose denominada excesso de curtose. Esse excesso é avaliado em relação a curtose do modelo normal por isso seu valor é calculado fazendo o coeficiente de curtose subtraído de 3. Dessa forma, o excesso de curtose em distribuições mesocúrticas é igual a zero, em distribuições leptocúrticas é maior que zero e em distribuições platicúrticas é menor que zero.\n\n\n\nFigura 4.4: Exemplo de funções de densidade com diferentes medidas de curtose.\n\n\nRessalva similar feita ao coeficiente de assimetria deve ser considerado para o coeficiente de curtose ou de excesso de curtose. Um coeficiente de curtose amostral diferente de três ou, de forma equivalente, com excesso de curtose amostral diferente de zero, não implica necessariamente que a distribuição da variável na população possui caudas mais leves ou mais pesadas do que a da distribuição normal. Para que se possa fazer tal afirmação é necessária a realização de testes estatísticos inferenciais. Os coeficientes amostrais de assimetria e curtose tão somente nos dão uma medida da forma da distribuição de frequências e \\(insights\\) do comportamento da variável na população, que devem ser verificados via análise inferencial estatística.\nNo R, para obter essas medidas resumo vamos utilizar a função descr também do pacote summarytools. No comando abaixo pedimos ao R as medidas descritivas da variável quantitativa “idade”.\n\n\n\n\ndescr(dados$idade)\n\nDescriptive Statistics  \ndados$idade  \nN: 11523  \n\n                       idade\n----------------- ----------\n             Mean      30.25\n          Std.Dev       7.04\n              Min      10.00\n               Q1      25.00\n           Median      30.00\n               Q3      35.00\n              Max      55.00\n              MAD       7.41\n              IQR      10.00\n               CV       0.23\n         Skewness       0.17\n      SE.Skewness       0.02\n         Kurtosis      -0.10\n          N.Valid   11514.00\n        Pct.Valid      99.92\n\n\nNote que os nomes de algumas das estatísticas apresentadas pela função descr estão em inglês. \\(Mean\\) refere-se ao valor médio da variável analisada, \\(Std.Dev\\) corresponde ao desvio-padrão, IQR é o símbolo para o intervalo interquartil, MAD é o desvio-médio absoluto, \\(Skewness\\) é o coeficiente de assimetria, \\(SE.Skewness\\) é o erro-padrão do coeficiente de assimetria, \\(Kurtosis\\) é o coeficiente de excesso de curtose e \\(N.Valid\\) e \\(Pct.Valid\\) correspondem, respectivamente, ao número de observações válidas e seu percentual no conjunto de dados considerado para a variável.\nEspecificamente para a variável idade, podemos notar que das 11523 observações, apenas 11514 (99.92 %) foram consideradas válidas. Isso acontece porque nesse conjunto de dados, 9 pessoas não declararam a idade, ficando com a casela vazia (NA). Todas as medidas-resumo foram calculadas considerando apenas as observações válidas. Sendo assim, algumas análises que podem ser realizadas para a variável idade através da função descr são que a idade média dentre as observações válidas foi de 30,25 anos, com desvio-padrão de 7,04 anos. A menor idade observada foi de 10 anos e a máxima foi de 55 anos. 50% das mulheres analisadas tem idade inferior a 30 anos (mediana) e 25% delas tem idade superior a 35 anos (Q3). O coeficiente de assimetria foi 0.17 (com erro-padrão de 0.02), indicando que a distribuição de frequências da variável idade é levemente assimétrica à direita. O coeficiente de excesso de curtose foi -0.10 e o intervalo interquartil (Q3 - Q1) foi 10.\nSe quiser que a tabela apresente apenas algumas medidas-resumo pré-selecionadas, podemos informar ao R por meio do argumento stats. Ainda, se quisermos que na tabela as medidas resumo fiquem na coluna, usamos o argumento transpose = TRUE, como segue:\n\ndescr(dados$idade,stats = c(\"min\", \"mean\", \"med\",\"sd\",\"max\"), transpose = TRUE) #sd é o desvio padrão (standard deviation)\n\nDescriptive Statistics  \ndados$idade  \nN: 11523  \n\n                Min    Mean   Median   Std.Dev     Max\n----------- ------- ------- -------- --------- -------\n      idade   10.00   30.25    30.00      7.04   55.00\n\n\nCaso se tenha interesse em apresentar a natureza da variável e um \\(preview\\) gráfico da distribuição de frequências, podemos fazer uso da função dfSummary. O argumento method = \"render\" para a função print permite uma melhor apresentação visual dos gráficos em documentos do tipo R \\(Markdown\\).\nprint(dfSummary(dados$idade), method = \"render\")\n\n\n\nData Frame Summary\ndados\nDimensions: 11523 x 1\n  Duplicates: 11476\n\n\n  \n    \n      No\n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Valid\n      Missing\n    \n  \n  \n    \n      1\n      idade\n[numeric]\n      Mean (sd) : 30.2 (7)min ≤ med ≤ max:10 ≤ 30 ≤ 55IQR (CV) : 10 (0.2)\n      46 distinct values\n      \n      11514\n(99.9%)\n      9\n(0.1%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2023-01-21\n\n\nAlém do gráfico, uma informação adicional apresentada é que as 11514 observações válidas da variável idade estão distribuídas em 46 distintos valores.\nOutro pacote bastante interessante para medidas descritivas é o modelsummary. Destacamos algumas funções desse pacote:\n\ndatasummary_skim: retorna as medidas descritivas das variáveis do banco de dados a depender do tipo identificado no argumento type= (categorical ou numeric);\ndatasummary: retorna as medidas descritivas das variáveis a depender de como monta os argumentos da função, permitindo retornar as medidas descritivas das variáveis quantitativas de interesse por categorias de outra(s) variável(is).\n\n\n\n\nPara explorar a funcionalidade desse pacote e suas funções, vamos filtrar o banco de dados original considerando apenas as informações das mulheres gestantes e puérperas internadas em UTI.\n\ndados_uti <- dados[!is.na(dados$dias_uti),]\n\nVamos selecionar algumas variáveis do banco de dados dados_uti e organizá-las em um novo \\(data.frame\\).\nlibrary(dplyr)\ndados_uti_res <- select(dados_uti,idade,cardiopati, faixa_et, evolucao, dias_uti)\nPara esses dados, vamos fazer algumas análises via pacote modelsummary. Assim,\nlibrary(modelsummary)\nAo usar a função datasummary_skim, vamos obter as medidas descritivas das variáveis quantitativas (argumento type = \"numeric\") e das variáveis qualitativas (argumento type = \"categorical\"), respectivamente:\ndatasummary_skim(dados_uti_res,\n  type = \"numeric\",\n  histogram = FALSE)\n\n\n\n \n  \n      \n    Unique (#) \n    Missing (%) \n    Mean \n    SD \n    Min \n    Median \n    Max \n  \n \n\n  \n    idade \n    42 \n    0 \n    31.2 \n    6.6 \n    10.0 \n    31.0 \n    55.0 \n  \n  \n    dias_uti \n    75 \n    0 \n    12.0 \n    13.7 \n    0.0 \n    8.0 \n    200.0 \n  \n\n\n\n\ndatasummary_skim(dados_uti_res,\n  type = \"categorical\", na.rm = FALSE)\n\n\n\n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    cardiopati \n    sim \n    182 \n    7.8 \n  \n  \n     \n    nao \n    791 \n    34.0 \n  \n  \n     \n    ignorado \n    17 \n    0.7 \n  \n  \n    faixa_et \n    <20 \n    98 \n    4.2 \n  \n  \n     \n    >=34 \n    908 \n    39.1 \n  \n  \n     \n    20-34 \n    1317 \n    56.7 \n  \n  \n     \n    NA \n    1 \n    0.0 \n  \n  \n    evolucao \n    cura \n    1645 \n    70.8 \n  \n  \n     \n    obito \n    623 \n    26.8 \n  \n  \n     \n    obito por outras causas \n    7 \n    0.3 \n  \n  \n     \n    ignorado \n    31 \n    1.3 \n  \n\n\n\n\nComo a variável faixa etária (faixa_et) foi declarada como fator, a função datasummary_skim apresenta 1 valor NA, indicando que apenas uma mulher que esteve em UTI não teve determinada sua faixa etária/idade. Para que as demais variáveis categóricas apresentem essa informação e não as deixe omitida, como no caso da variável cardiopatia, vamos precisar declarar essas variáveis como caracter e não como fator. Esse procedimento também será adotado para as demais variáveis categóricas.\ndados_uti_res$cardiopati <- as.character(dados_uti_res$cardiopati)\ndados_uti_res$evolucao <- as.character(dados_uti_res$evolucao)\n\ndatasummary_skim(dados_uti_res,\n  type = \"categorical\", na.rm = FALSE)\n\n\n\n \n  \n      \n       \n    N \n    % \n  \n \n\n  \n    cardiopati \n    ignorado \n    17 \n    0.7 \n  \n  \n     \n    nao \n    791 \n    34.0 \n  \n  \n     \n    sim \n    182 \n    7.8 \n  \n  \n     \n    NA \n    1334 \n    57.4 \n  \n  \n    faixa_et \n    <20 \n    98 \n    4.2 \n  \n  \n     \n    >=34 \n    908 \n    39.1 \n  \n  \n     \n    20-34 \n    1317 \n    56.7 \n  \n  \n     \n    NA \n    1 \n    0.0 \n  \n  \n    evolucao \n    cura \n    1645 \n    70.8 \n  \n  \n     \n    ignorado \n    31 \n    1.3 \n  \n  \n     \n    obito \n    623 \n    26.8 \n  \n  \n     \n    obito por outras causas \n    7 \n    0.3 \n  \n  \n     \n    NA \n    18 \n    0.8 \n  \n\n\n\n\nUma das funções mais interessantes do pacote modelsummaryé a datasummary, pois ela nos permite analisar variáveis quantitativas separada pelas categorias (grupos) de uma variável qualitativa. Por exemplo, suponha que tenhamos interesse em analisar o tempo de internação em UTI, estratificado pelos grupos faixa-etária e evolução do caso, fazendo uso das seguintes medidas descritivas: média, mediana, desvio padrão, mínimo, máximo e tamanho da amostra válido (sem considerar observações faltantes para a variável em questão). O primeiro passo é declarar as medidas-resumo de interesse como funções. O argumento na.rm = TRUE indica que o cálculo da função deve ser realizado excluindo os valores faltantes da variável.\nmedia <- function(x)   mean(x, na.rm = TRUE)\nmediana <- function(x) median(x, na.rm = TRUE)\ndp <- function(x) sd(x, na.rm = TRUE)\nminimo <- function(x) min(x, na.rm = TRUE)\nmaximo <- function(x) max(x, na.rm = TRUE)\nn <- function(x) sum(!is.na(x))\ndatasummary( (evolucao + faixa_et) ~\n              dias_uti*(n+media+dp+minimo+mediana+maximo), data = dados_uti_res)\n\n\n\n \n  \n      \n       \n    n \n    media \n    dp \n    minimo \n    mediana \n    maximo \n  \n \n\n  \n    evolucao \n    cura \n    1645 \n    10.78 \n    12.01 \n    0.00 \n    6.00 \n    107.00 \n  \n  \n     \n    ignorado \n    31 \n    7.94 \n    10.19 \n    0.00 \n    4.00 \n    40.00 \n  \n  \n     \n    obito \n    623 \n    15.18 \n    15.46 \n    0.00 \n    12.00 \n    200.00 \n  \n  \n     \n    obito por outras causas \n    7 \n    51.71 \n    64.62 \n    2.00 \n    25.00 \n    183.00 \n  \n  \n    faixa_et \n    <20 \n    98 \n    10.55 \n    11.07 \n    0.00 \n    6.00 \n    58.00 \n  \n  \n     \n    >=34 \n    908 \n    12.91 \n    14.66 \n    0.00 \n    8.00 \n    200.00 \n  \n  \n     \n    20-34 \n    1317 \n    11.55 \n    13.15 \n    0.00 \n    8.00 \n    183.00 \n  \n\n\n\n\nAgora veja como fica se eu considerar as medidas descritivas de mais de uma variável quantitativas por duas variáveis qualitativas, selecionando apenas as medidas descritivas, média, desvio-padrão e número de casos observados:\ndatasummary((evolucao + cardiopati)  ~\n              (dias_uti + idade)*(n+media+dp), data = dados_uti_res)\n\n\n\n \n\n\ndias_uti\nidade\n\n  \n      \n       \n    n \n    media \n    dp \n    n \n    media \n    dp \n  \n \n\n  \n    evolucao \n    cura \n    1645 \n    10.78 \n    12.01 \n    1645 \n    31.05 \n    6.57 \n  \n  \n     \n    ignorado \n    31 \n    7.94 \n    10.19 \n    31 \n    31.29 \n    7.66 \n  \n  \n     \n    obito \n    623 \n    15.18 \n    15.46 \n    622 \n    31.61 \n    6.70 \n  \n  \n     \n    obito por outras causas \n    7 \n    51.71 \n    64.62 \n    7 \n    32.14 \n    7.65 \n  \n  \n    cardiopati \n    ignorado \n    17 \n    8.76 \n    9.62 \n    17 \n    34.00 \n    6.20 \n  \n  \n     \n    nao \n    791 \n    12.74 \n    13.30 \n    791 \n    30.96 \n    6.82 \n  \n  \n     \n    sim \n    182 \n    14.12 \n    14.28 \n    182 \n    33.66 \n    6.86"
  },
  {
    "objectID": "descritiva.html#tabelas-cruzadas---duas-variáveis-qualitativas",
    "href": "descritiva.html#tabelas-cruzadas---duas-variáveis-qualitativas",
    "title": "4  Análise exploratória dos dados",
    "section": "4.2 Tabelas cruzadas - duas variáveis qualitativas",
    "text": "4.2 Tabelas cruzadas - duas variáveis qualitativas\nTabelas cruzadas ou tabelas de contingência são tabelas que apresentam frequências de duas ou mais variáveis qualitativas conjuntamente.\nNo R, para obter tabelas cruzadas, vamos utilizar a função ´ctable´ também do pacote ´summarytools´. No comando abaixo, pedimos ao R uma tabela cruzada entre as variáveis qualitativas evolução (evolucao) e faixa etária (faixa_et) no banco de dados otiginal.\nctable(dados$evolucao,y=dados$obesidade,prop=\"t\")\nCross-Tabulation, Total Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 (4.82%)\n2897 (25.1%)\n95 (0.82%)\n5943 (51.58%)\n9490 ( 82.4%)\n\n\nobito\n\n199 (1.73%)\n446 ( 3.9%)\n14 (0.12%)\n587 ( 5.09%)\n1246 ( 10.8%)\n\n\nobito por outras causas\n\n3 (0.03%)\n13 ( 0.1%)\n0 (0.00%)\n6 ( 0.05%)\n22 ( 0.2%)\n\n\nignorado\n\n10 (0.09%)\n71 ( 0.6%)\n0 (0.00%)\n206 ( 1.79%)\n287 ( 2.5%)\n\n\n\n\n23 (0.20%)\n129 ( 1.1%)\n4 (0.03%)\n322 ( 2.79%)\n478 ( 4.1%)\n\n\nTotal\n\n790 (6.86%)\n3556 (30.9%)\n113 (0.98%)\n7064 (61.30%)\n11523 (100.0%)\n\n\n\nO argumento prop indica a forma como deve ser calculada a proporção. Por padrão, a proporção é sempre calculada tendo-se como referencial o total em linha, ou seja, prop = \"r\". Outras opções são prop = \"t\", indicando que a proporção é em relação ao número total de observações e prop = \"c\" se o referencial for o total por coluna.\nctable(dados$evolucao,y=dados$obesidade,prop=\"r\")\nCross-Tabulation, Row Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 ( 5.8%)\n2897 (30.5%)\n95 (1.0%)\n5943 (62.6%)\n9490 (100.0%)\n\n\nobito\n\n199 (16.0%)\n446 (35.8%)\n14 (1.1%)\n587 (47.1%)\n1246 (100.0%)\n\n\nobito por outras causas\n\n3 (13.6%)\n13 (59.1%)\n0 (0.0%)\n6 (27.3%)\n22 (100.0%)\n\n\nignorado\n\n10 ( 3.5%)\n71 (24.7%)\n0 (0.0%)\n206 (71.8%)\n287 (100.0%)\n\n\n\n\n23 ( 4.8%)\n129 (27.0%)\n4 (0.8%)\n322 (67.4%)\n478 (100.0%)\n\n\nTotal\n\n790 ( 6.9%)\n3556 (30.9%)\n113 (1.0%)\n7064 (61.3%)\n11523 (100.0%)\n\n\n\nctable(dados$evolucao,y=dados$obesidade,prop=\"c\")\nCross-Tabulation, Column Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 ( 70.3%)\n2897 ( 81.5%)\n95 ( 84.1%)\n5943 ( 84.13%)\n9490 ( 82.4%)\n\n\nobito\n\n199 ( 25.2%)\n446 ( 12.5%)\n14 ( 12.4%)\n587 ( 8.31%)\n1246 ( 10.8%)\n\n\nobito por outras causas\n\n3 ( 0.4%)\n13 ( 0.4%)\n0 ( 0.0%)\n6 ( 0.08%)\n22 ( 0.2%)\n\n\nignorado\n\n10 ( 1.3%)\n71 ( 2.0%)\n0 ( 0.0%)\n206 ( 2.92%)\n287 ( 2.5%)\n\n\n\n\n23 ( 2.9%)\n129 ( 3.6%)\n4 ( 3.5%)\n322 ( 4.56%)\n478 ( 4.1%)\n\n\nTotal\n\n790 (100.0%)\n3556 (100.0%)\n113 (100.0%)\n7064 (100.00%)\n11523 (100.0%)\n\n\n\nNote que em todas as tabelas de contingência há a existência de linha e coluna sem nome, isso acontece pois esta linha e/ou coluna está resumindo os valores faltantes (NA). Por exemplo, a distribuição de frequências da variável evolução para os que não preencheram o \\(status\\) de obesidade nos diz que 5943 (84.13%) foram curados, 587 (8.31%) faleceram, 6 (0.08%) viram a óbito por motivos outros que não COVID-19, 206 (2.92%) ignoraram essa informação (preencheram com 9) e 322 (4.56%) deixaram em branco não só a informação da obesidade, mas também o desfecho final da evolução. Para obter a tabela de contingência apenas dos casos válidos simultâneos em ambas as variáveis, insira o argumento useNA = \"no\"\".\nctable(dados$evolucao,y=dados$obesidade, prop=\"c\", useNA = \"no\")\nCross-Tabulation, Column Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\ncura\n\n555 ( 72.4%)\n2897 ( 84.5%)\n95 ( 87.2%)\n3547 ( 82.4%)\n\n\nobito\n\n199 ( 25.9%)\n446 ( 13.0%)\n14 ( 12.8%)\n659 ( 15.3%)\n\n\nobito por outras causas\n\n3 ( 0.4%)\n13 ( 0.4%)\n0 ( 0.0%)\n16 ( 0.4%)\n\n\nignorado\n\n10 ( 1.3%)\n71 ( 2.1%)\n0 ( 0.0%)\n81 ( 1.9%)\n\n\nTotal\n\n767 (100.0%)\n3427 (100.0%)\n109 (100.0%)\n4303 (100.0%)\n\n\n\nCaso não haja interesse em se apresentar as proporções, basta considerar o argumento prop=\"none\", da seguinte forma:\nctable(dados$evolucao,y=dados$obesidade,prop=\"none\")\nCross-Tabulation\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555\n2897\n95\n5943\n9490\n\n\nobito\n\n199\n446\n14\n587\n1246\n\n\nobito por outras causas\n\n3\n13\n0\n6\n22\n\n\nignorado\n\n10\n71\n0\n206\n287\n\n\n\n\n23\n129\n4\n322\n478\n\n\nTotal\n\n790\n3556\n113\n7064\n11523\n\n\n\nPara tabelas de contingência com mais de duas variáveis, podemos adotar o seguinte procedimento:\nwith(dados, stby(data = list(x = evolucao, y = obesidade), \n                   INDICES = faixa_et, FUN = ctable))\nCross-Tabulation, Row Proportions\nevolucao * obesidade\nData Frame: dados\nGroup: faixa_et = <20\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n12 (2.0%)\n214 (35.0%)\n2 (0.3%)\n383 (62.7%)\n611 (100.0%)\n\n\nobito\n\n3 (6.5%)\n22 (47.8%)\n0 (0.0%)\n21 (45.7%)\n46 (100.0%)\n\n\nobito por outras causas\n\n0 (0.0%)\n2 (66.7%)\n0 (0.0%)\n1 (33.3%)\n3 (100.0%)\n\n\nignorado\n\n0 (0.0%)\n7 (20.0%)\n0 (0.0%)\n28 (80.0%)\n35 (100.0%)\n\n\n\n\n0 (0.0%)\n2 (10.5%)\n0 (0.0%)\n17 (89.5%)\n19 (100.0%)\n\n\nTotal\n\n15 (2.1%)\n247 (34.6%)\n2 (0.3%)\n450 (63.0%)\n714 (100.0%)\n\n\n\nGroup: faixa_et = >=34\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n231 ( 7.5%)\n972 (31.5%)\n37 (1.2%)\n1842 (59.8%)\n3082 (100.0%)\n\n\nobito\n\n73 (13.9%)\n182 (34.7%)\n6 (1.1%)\n263 (50.2%)\n524 (100.0%)\n\n\nobito por outras causas\n\n1 (12.5%)\n6 (75.0%)\n0 (0.0%)\n1 (12.5%)\n8 (100.0%)\n\n\nignorado\n\n1 ( 1.2%)\n22 (25.9%)\n0 (0.0%)\n62 (72.9%)\n85 (100.0%)\n\n\n\n\n12 ( 7.4%)\n44 (27.0%)\n0 (0.0%)\n107 (65.6%)\n163 (100.0%)\n\n\nTotal\n\n318 ( 8.2%)\n1226 (31.7%)\n43 (1.1%)\n2275 (58.9%)\n3862 (100.0%)\n\n\n\nGroup: faixa_et = 20-34\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n310 ( 5.4%)\n1710 (29.5%)\n56 (1.0%)\n3715 (64.2%)\n5791 (100.0%)\n\n\nobito\n\n123 (18.2%)\n242 (35.9%)\n8 (1.2%)\n302 (44.7%)\n675 (100.0%)\n\n\nobito por outras causas\n\n2 (18.2%)\n5 (45.5%)\n0 (0.0%)\n4 (36.4%)\n11 (100.0%)\n\n\nignorado\n\n9 ( 5.4%)\n42 (25.1%)\n0 (0.0%)\n116 (69.5%)\n167 (100.0%)\n\n\n\n\n11 ( 3.7%)\n82 (27.9%)\n4 (1.4%)\n197 (67.0%)\n294 (100.0%)\n\n\nTotal\n\n455 ( 6.6%)\n2081 (30.0%)\n68 (1.0%)\n4334 (62.5%)\n6938 (100.0%)"
  },
  {
    "objectID": "descritiva.html#gráficos",
    "href": "descritiva.html#gráficos",
    "title": "4  Análise exploratória dos dados",
    "section": "4.3 Gráficos",
    "text": "4.3 Gráficos\nUm gráfico pode ser a maneira mais adequada para resumir e apresentar um conjunto de dados. Tem a vantagem de facilitar a compreensão de uma determinada situação que queira ser descrita, permitindo uma interpretação rápida e visual das suas principais características.\nA visualização dos dados é uma etapa importantíssima da análise estatística, pois é também a partir dela que criamos a intuição necessária para escolher o teste ou modelo mais adequado para o nosso problema.\n\n4.3.1 Pacote ggplot2\nUm pacote maravilhoso para gráficos no R é o ggplot2. A ideia por trás desse pacote é um gráfico pode ser entendido como um mapeamento dos dados a partir de atributos estéticos (cores, formas, tamanho) de formas geométricas (pontos, linhas, barras).\nlibrary(ggplot2)\n\n4.3.1.1 Atributos estéticos\nA função aes descreve como as variáveis são mapeadas em aspectos visuais. Para isso, vamos precisar indicar qual variável será representada no eixo x, qual será representada no eixo y, a cor e o tamanho dos componentes geométricos, etc. de formas geométricas a serem pré-definidas pelos geoms. A escolha da forma geométrica vai depender da natureza das variáveis a serem analisadas e será discutido na sequencia. Além disso, os aspectos que podem ou devem ser mapeados vão depender do tipo de gráfico que estamos querendo construir. Basicamente, no pacote ´ggplot2´ temos as seguintes formas geométricas:\n\ngeom_point() gera gráficos de dispersão transformando pares (x,y) em pontos.\ngeom_line: para retas definidas por pares (x,y);\ngeom_abline: para retas definidas por um intercepto e uma inclinação;\ngeom_hline: para retas horizontais;\ngeom_bar: para barras;\ngeom_histogram: para histogramas;\ngeom_boxplot: para boxplots;\ngeom_density: para densidades;\ngeom_area: para áreas.\n\nPara cada uma das formas geométricas podemos estabelecer aspectos visuais que podem melhorar a visualização dos dados. Aspectos visuais mais utilizados:\n\ncolor: altera a cor de formas que não têm área (pontos e retas);\nfill: altera a cor de formas com área (barras, caixas, densidades, áreas);\nsize: altera o tamanho de formas;\ntype: altera o tipo da forma, geralmente usada para pontos;\nlinetype: altera o tipo da linha.\n\nPara exemplificar os diferentes tipos de gráficos associando-os a variáveis de diferentes naturezas, vamos considerar o banco de dados de COVID-19 em gestantes e puérperas. De maneira geral, é necessário seguir alguns passos gerais para a construção de qualquer gráfico via ggplot2, a saber:\n\nPasso 1: sempre iniciar a construção chamando a função ggplot.\nPasso 2: especificar na função ggplot o objeto que acomoda o banco de dados e apresenta a variável de interesse para a qual se quer fazer o gráfico. Esse objeto deve ser do tipo \\(dataframe\\).\nPasso 3: informar as variáveis a serem consideradas no eixo horizontal e vertical via função aes e demais funções estéticas dependentes das variáveis.\n\nPasso 4: informar o tipo de gráfico que se quer fazer (barra, histograma, \\(boxplot\\), etc).\n\n\n\n4.3.1.2 Gráficos para variáveis qualitativas e quantitativas discretas com poucos valores diferentes\nUm dos gráficos mais utilizados para a apresentação visual de variáveis qualitativas e quantitativas discretas com poucas observações diferentes é o gráfico de barras. Para construí-lo, é necessário utilizar no Passo 4 a função geom_bar.\nA seguir apresentamos um exemplo de gráfico de barras para a variável qualitativa evolução dos casos relacionado a gestantes e puérperas hospitalizadas por COVID-19.\nggplot(dados, aes(x = evolucao)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\")\n\n\n\nNote que a função labs é responsável não só por inserir os títulos nos eixos, como poder visto no gráfico anterior, mas também títulos e subtítulos. Essa função pode ser sempre utilizada na construção de gráficos via pacote ggplot2, independente do tipo de gráfico a ser apresentado. Por exemplo,\nggplot(dados, aes(x = evolucao)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nNo código a seguir apresentamos como apresentar as barras organizadas de maneira decrescente. Note que há uma mudança na ordem das barras referente a óbitos e óbitos por outras causas. Além disso, independente da disposição escolhida para as barras, a categoria que representa as mulheres que não tiveram sua evolução preenchida na notificação (NA) sempre é apresentada como última barra, ainda que tenha uma alta frequência em relação as outras categorias.\nggplot(dados, aes(x = reorder(evolucao, evolucao, function(x)-length(x)))) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nCaso o interesse seja nas barras dispostas de maneira crescente, com exceção do NA, basta reordenar a variável da seguinte forma:\nggplot(dados, aes(x = reorder(evolucao, evolucao, function(x) length(x)))) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nA variável evolução (evolucao) pode ser classificada como qualitativa nominal e, portanto, estamos livres para escolher a ordem com que as categorias são apresentadas. Vamos ver agora o caso da variável faixa-etária (faixa_et) que, intrinsecamente, é uma variável qualitativa ordinal. Se utilizássemos o mesmo código considerado inicialmente para a variável evolução, obteríamos o seguinte gráfico:\nggplot(dados, aes(x = faixa_et)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Faixa etária\", y = \"Número de casos\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nNote que a barra para as mulheres com pelo menos 34 anos e a barra que representa as mulheres com idade de 20 (incluso) a 34 anos estão em posições trocadas, não respeitando a ordenação natural da variável.\n# Especificando a ordem dos níveis do fator \ndados$faixa_et = factor(dados$faixa_et, levels = c('<20', '20-34', '>=34'))\n\n# Gerando o gráfico\nggplot(dados, aes(x = faixa_et)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Faixa etária\", y = \"Número de casos\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nTodos os gráficos de barra apresentados anteriormente apresentaram no eixo vertical a contagem de indivíduos por categoria. vejamos agora o código para apresentar o gráfico de barras com as frequências relativas.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  labs(x = \"Faixa etária\", y = \"Frequência relativa\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nSe o interesse é apresentar o gráfico em termos percentuais, basta acrescentar a função scale_y_continuous com o argumento labels=scales::percent.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nPara inserir o percentual de cada categoria no gráfico, incluímos no código anterior a função geom_text, com o argumento lab declarando o cálculo da frequência arredondado a duas casas decimais. O argumento vjust indica a que altura se quer que o texto com o percentual apareça no gráfico. Quanto mais negativo for o valor, mais acima da barra estará o texto com o percentual. Se vjust = 0 apresenta o texto encima da barra e, quanto mais positivo for o valor de vjust escolhido, mais interno da barra ficará o texto.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", vjust = -0.1)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nO argumento width da função geom_bar permite controlar a espessura da barra através de valores variando de 0 a 1, sendo que o tamanho 1 representa o maior tamanho.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\", width=0.2) + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", vjust = -0.1)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nPara mudar a disposição das barras de vertical para horizontal, basta acrescentar basta inserir a função coord_flip(). Para diminuir o tamanho da letra, ajustamos na função geom_text alterando os valores do argumento size.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", hjust = 0, size  = 3) +\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")+\n  coord_flip()\n\n\n\nVejamos agora como construir um gráfico de barras com duas variáveis qualitativas conjuntamente. Para exemplificar, vamos considerar as variáveis faixa etária (faixa_et) e cardiopatia (cardiopat). Nosso interesse é analisar a distribuição da faixa-etária estratificada pelas categorias da variável cardiopatia que, como vimos antes, podem assumir os resultados “sim”, “não”, “ignorado” e “NA”.\nggplot(dados, aes(x = faixa_et, group = cardiopati)) +  \n  geom_bar(aes(y = ..prop..), stat = \"count\", fill=\"green4\") + \n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\n\n\nO próximo código apresenta o gráfico anterior com as porcentagens inseridas na figura.\nggplot(dados, aes(x = faixa_et, group = cardiopati)) +  \n  geom_bar(aes(y = ..prop..), stat = \"count\", fill=\"green4\") + \n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat = \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\n\n\nVamos agora colocar cada categoria com a sua própria cor, a fim de facilitar a comparação. Para isso, vamos considerar o argumento fill = factor(..x..) na função geom_bar. Como as categorias da variável faixa-etária são autoexplicativas, o termo theme(legend.position=\"none\") for inserido na construção do gráfico para se evitar a criação de uma nova legenda para as cores relacionadas as faixas de idade, evitando assim repetição de informação.\nggplot(dados, aes(x=faixa_et, group = cardiopati))  + \n  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=\"count\") +\n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat= \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  theme(legend.position=\"none\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\n\n\nA escala de cores utilizada por padrão pela função geom_bar nem sempre atende a todos os públicos. O pacote viridis do R apresenta escalas de cores projetadas para melhorar a legibilidade dos gráficos para leitores com formas comuns de daltonismo e/ou deficiência em visão de cores. Para usá-lo, devemos inserir a função scale_fill_manual(values = c(viridis(4))), com o valor 4 representando as 4 categorias da faixa etária.\nlibrary(viridis)\n\nggplot(dados, aes(x=faixa_et, group = cardiopati))  + \n  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=\"count\") +\n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat= \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  theme(legend.position=\"none\")+\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_manual(values = c(viridis(4))) +\n  facet_grid(~cardiopati)\n\n\n\nVários dos gráficos aqui mencionados podem construídos fazendo usos de outras funções e código do pacote do ggplot2. Especificamente sobre o gráfico de barras, na página (http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization#barplot-of-counts) podemos encontrar outras interessantes customizações gráficas que podem ser implementadas via pacote ggplot2.\n\n\n4.3.1.3 Gráficos para variáveis quantitativas\nTodos os códigos apresentados anteriormente podem ser usados quando a variável de interesse é quantitativa discreta com poucos valores diferentes. No caso de haver muitos diferentes valores para a variável quantitativa discreta, gráficos do tipo histograma costumam ser mais informativos. Vamos apresentar agora ferramentas para a construção de gráficos para esse tipo de variável e para variáveis quantitativas contínuas. Todos os gráficos a serem construídos daqui para frente farão uso da paleta viridis. Vejamos a construção do histograma de densidades para a variável idade.\nA função considerada agora é geom_histogram. O argumento y = ..density.. indica que queremos apresentar o histograma de densidades, bins = 15 refere-se ao número de barras contíguas que queremos que o gráfico apresente, ´fill´ designa a cor a preencher o gráfico e color é a cor da linha das barras.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density..), bins = 15, fill = viridis(1), color = \"black\") +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histograma de densidades das idades de hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nEnquanto medida, densidade corresponde a razão entre o número de casos contabilizados em intervalo e a amplitude do intervalo. Uma das vantagens em considerar o histograma de densidades e não o histograma de frequências é que a área total sob o gráfico corresponde a 1, deixando o gráfico na mesma escala de funções de densidade. Por exemplo, na figura a seguir inserimos uma versão alisada empírica do histograma. O argumento alpha na função geom_density controla o nível de transparência das cores preenchidas no histograma alisado e varia de 0 a 1, com 1 representando a cor sólida.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density..), bins = 15, fill = viridis(1), color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histograma de densidades das idades de hospitalizadas por COVID-19 \", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nNo código a seguir apresentamos uma maneira de construir o histograma de densidades de uma variável quantitativa nas diferentes categorias de uma variável qualitativa. Aqui a variável qualitativa refere-se a informações sobre o \\(status\\) de cardiopatia e a variável quantitativa considerada foi a idade. Para facilitar a comparação das diferentes faixas de valores da variável quantitativa entre as categorias, vamos inserir uma escala inserir uma escala de cores entre as faixas de valores, deixando o gráfico com um aspecto bastante atrativo e informativo. O argumento considerado na função geom_density é fill = ..x.., informando que o preenchimento do histograma será realizado nas faixas de valores da variável quantitativa. Para a escala de cores, inserimos a função scale_fill_gradientn(colours = c(viridis(15))), em que o valor 15 corresponde ao número de barras contíguas (bins = 15) considerado no histograma.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density.., fill=..x..), bins = 15, color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas de densidades das idades fixado status de cardiopatia \", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  scale_fill_gradientn(colours = c(viridis(15)))+\n  theme(legend.position=\"none\")+\n  facet_grid(~cardiopati)\n\n\n\nPara que os histogramas sejam dispostos um abaixo do outro, basta substituir a função facet_grid(~cardiopati) por facet_wrap(~cardiopati, ncol=1).\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density.., fill=..x..), bins = 15, color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas de densidades das idades fixado status de cardiopatia \", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  scale_fill_gradientn(colours = c(viridis(15)))+\n  theme(legend.position=\"none\")+\n  facet_wrap(~cardiopati, ncol=1)\n\n\n\nCaso se queira apresentar os histogramas alisados sobrepostos por categoria, pode-se usar o código proposto na sequência.\nggplot(dados, aes(x = idade, fill = cardiopati)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas alisados das idades pelo status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\") +\n  scale_fill_manual(values = c(viridis(3)), name = \"Cardiopatia\")\n\n\n\nUm outro gráfico muito utilizado na apresentação de uma variável quantitativa é o boxplot que apresenta, visualmente, os valores mínimo, primeiro quartil (Q1), mediana ou segundo quartil (Q2), terceiro quartil (Q3), máximo e possíveis \\(outliers\\). Baseado nessas medidas, temos uma ideia do comportamento da variável quantitativa em termos de posição, dispersão, assimetria e dados discrepantes. A posição central é dada pela mediana e a dispersão pelo intervalo interquartil. As posições relativas entre Q1, Q2 e Q3 nos dão uma ideia da simetria ou assimetria da distribuição.\n\n\n\n\n\nOs valores \\(Q_1\\), \\(Q_2\\) e \\(Q_3\\) já foram apresentados anteriormente. No gráfico, o segundo quartil (ou seja, a mediana) é representada pela linha que corta a caixa do boxplot. Já o primeiro quartil (\\(Q_1\\)) é a base da caixa e o terceiro quartil (\\(Q_3\\)), o topo. O intervalo interquartil está representado na altura da caixa. A escolha do tamanho da base da caixa é arbitrária, devendo-se tão somente garantir que as linhas \\(Q_1\\), \\(Q_2\\) e \\(Q3\\) estão localizadas na altura em que os valores dos quartis foram obtidos.\nAs linhas em azul são linhas imaginárias (normalmente não aparecem graficadas nos gráficos) e representam valores que distinguem valores \\(outliers\\) dos demais. Valores \\(outliers\\) são valores considerados discrepantes dentro do conjunto de dados de uma variável. Para a representação no boxplot, são considerados \\(outliers\\), observações com valores maiores que o Limite Superior (LS) ou com valores menores que o Limite Inferior (LI), tal que \\(LI = Q_1 - 1.5 (Q_3 - Q_1)\\) e \\(LS = Q_3 + 1.5 (Q_3 - Q_1)\\). Ainda na figura, observamos a presença de um \\(\\mbox{Mínimo}^*\\) e \\(\\mbox{Máximo}^*\\) que nem sempre representam o menor e a maior, respectivamente, observações na amostra. Por definição, \\(\\mbox{Mínimo}^*\\) é o menor valor maior que o Limite Inferior (LI), ou seja, é o menor valor observado no conjunto de dados desconsiderado os valores \\(outliers\\). Analogamente, \\(\\mbox{Máximo}^*\\) é o maior valor menor que o Limite Superior (LS), ou seja, é o maior valor observado no conjunto de dados desconsiderado os valores \\(outliers\\).\nNo pacote ggplot2, esse gráfico é construído com a função geom_boxplot que apresenta argumentos similares aos gráficos de barras e histograma.\nggplot(dados, aes(y=idade))  + \n  geom_boxplot(fill = viridis(1), color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades de hospitalizadas por COVID-19 \", subtitle = \"Mulheres gestantes e puérperas\")\n\n\n\nO boxplot, assim como o histograma, também pode ser utilizado para apresentar o comportamento de variáveis quantitativas em função das categorias de variáveis qualitativas.\nggplot(dados, aes(y=idade, x = cardiopati))  + \n  geom_boxplot(fill = viridis(1), color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\n\n\nPara que cada categoria apresente sua própria cor, basta declarar qual a variável que será considerada para colorir os boxplots em suas categorias via argumento fill em ggplot e indicar quais as cores serão utilizadas. Como a variável cardiopatia (cardiopati) tem 4 categorias, informamos as cores na função geom_boxplot usando o termo fill = viridis(4). Além disso, caso se tenha interesse em destacar as observações \\(outliers\\) com outras cores, pode ser usado o argumento outlier.color na função geom_boxplot. Por exemplo, na figura abaixo, vamos destacar os \\(outliers\\) em vermelho.\nggplot(dados, aes(y=idade, x = cardiopati, fill = cardiopati))  + \n  geom_boxplot(fill = viridis(4), color = \"black\", outlier.color = \"red\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\n\n\nNo código a seguir, vamos apresentar o boxplot da variável quantitativa idade pela variável qualitativa \\(status\\) de cardiopatia, estratificado nas diferentes categorias de evolução do caso. Para clareza do texto no gráfico, a legenda da variável cardiopatia foi colocada abaixo dele, assim como o texto no eixo horizontal foi disposto de forma inclinada, através da função theme(legend.position=\"bottom\", axis.text.x=element_text(angle=30, hjust=0.8)).\nggplot(dados, aes(y=idade, x = cardiopati, fill = cardiopati))  + \n  geom_boxplot(color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades pelo status de cardiopatia, fixada a evolução\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  theme(legend.position=\"bottom\", axis.text.x=element_text(angle=30, hjust=0.8)) +\n  scale_fill_manual(values = c(viridis(4)), name = \"Cardiopatia\")+\n  facet_grid(~evolucao)\n\n\n\nNosso objetivo agora é explorar a construção de gráficos quando temos duas variáveis quantitativas através do diagrama de dispersão. Para exemplificar a construção, vamos considerar as variáveis do banco de dados dados_uti_res que contém as variáveis idade e quantidade de dias em uti (dias_uti).\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(colour = viridis(1)) +\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\n\n\nO pacote ggExtra consegue inserir no gráfico de dispersão gerado pelo ggplot2 histogramas, histogramas alisados e boxplot das variáveis marginais. Para isso, basta utilizar a função ggMarginal informando o nome do objeto em que está guardado o gráfico de dispersão e o tipo de gráfico marginal a ser inserido.\np <- ggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(colour = viridis(1)) +\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nlibrary(ggExtra)\n\n# histograma marginal\nggMarginal(p, type=\"histogram\")\n\n\n\n# histograma alisado marginal\nggMarginal(p, type=\"density\")\n\n\n\n# Boxplot marginal\nggMarginal(p, type=\"boxplot\")\n\n\n\nPodemos construir gráficos de dispersão estratificados por categorias de uma variável qualitativa. No código abaixo, vamos utilizar o \\(status\\) de cardiopatia e, para isso, usamos o argumento color = cardiopati na função geom_point.\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(aes(color = cardiopati)) +\n  scale_colour_viridis_d(\"Cardiopatia\", na.value = \"grey50\")+\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI, fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\n\n\nComo pontos de diferentes cores estão sobrepondo, uma visualização possível para facilitar a análise pode ser feita separando os diagramas de dispersão em diferentes planos cartesianos de mesma escala.\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(aes(color = cardiopati)) +\n  scale_colour_viridis_d(\"Cardiopatia\", na.value = \"grey50\")+\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI, fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  facet_wrap(~cardiopati, ncol=1)\n\n\n\n\n\n\n4.3.2 Pacote esquisse\nO pacote esquisse disponibiliza um \\(dashboard\\) interativo para criação de gráficos por meio do pacote ggplot2.\nlibrary(esquisse)\nAo rodar a função esquisser(), um janela é aberta (veja Figura 4.5), em que usuário deve escolher a base de dados a trabalhar. Feito isso, uma outra janela será aberta apresentando todas as variáveis presentes no banco de dados escolhido (veja Figura 4.6), permitindo assim fazer os gráficos. Preparamos um tutorial para a utilização do pacote esquisse que pode ser acessado aqui (https://www.youtube.com/watch?XXXXXXXXXXXXX) .\n\n\n\nFigura 4.5: Primeira tela do esquisser.\n\n\n\n\n\nFigura 4.6: Segunda tela do esquisser.\n\n\n\n\n4.3.3 Materiais complementares\nLivros e Artigos:\n\nMercier F, Consalvo N, Frey N, Phipps A, Ribba B. From waterfall plots to spaghetti plots in early oncology clinical development. Pharm Stat. 2019;18(5):526-532. doi:10.1002/pst.1944\nGillespie TW. Understanding waterfall plots. J Adv Pract Oncol. 2012;3(2):106-111.\nSonnad SS. Describing data: statistical and graphical methods. Radiology. 2002;225(3):622-628. doi:10.1148/radiol.2253012154\nIn J, Lee S. Statistical data presentation. Korean J Anesthesiol. 2017;70(3):267-276. doi:10.4097/kjae.2017.70.3.267\nMorettin P, Singer J. Estatística e Ciência de Dados. 1nd ed. LTC; 2022.\n\n\\(Sites\\):\n\nhttps://r-graph-gallery.com/index.html\n\nhttp://www.sthda.com/english/wiki/data-visualization\nhttps://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/"
  }
]