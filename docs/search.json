[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados Aplicada à Saúde Materno-Infantil",
    "section": "",
    "text": "Prefácio"
  },
  {
    "objectID": "intro.html#bases-de-dados",
    "href": "intro.html#bases-de-dados",
    "title": "1  Introdução",
    "section": "1.1 Bases de dados",
    "text": "1.1 Bases de dados\nOs dados considerados nas aplicações deste livro são provenientes do Sistema de Informação da Vigilância Epidemiológica da Gripe (SIVEP-Gripe), sistema oficial para o registro dos casos e óbitos por Síndrome Respiratória Aguda Grave (SRAG) disponibilizado pelo Ministério da Saúde. Os dados correspondem a registros de gestantes e puérperas de 10 a 55 anos hospitalizadas com SRAG por COVID-19 confirmada por teste de PCR. O conjunto de dados é utilizado para ilustrar e demonstrar diversos aspectos dos conceitos abordados no texto, e pode ser baixado em https://github.com/observatorioobstetrico/dados_livro_cd_saude.\n\n1.1.1 Dados de COVID-19 em gestantes e puérperas\nEssa base consiste em 11.523 registros de gestantes e puérperas diagnosticadas com COVID-19 no período de março de 2020 a dezembro de 2021. Alguns estudos conduzidos pelo OOBr usaram esses dados, dentre os quais podem ser citados: Características demográficas e epidemiológicas sobre mulheres grávidas e puérperas que morreram de Síndrome Respiratória Aguda Grave no Brasil, Mortalidade materna associada à COVID-19 no Brasil em 2020 e 2021: comparação com mulheres não grávidas e homens e Desfechos da COVID-19 em puérperas, gestantes e mulheres não gestantes e nem puérperas hospitalizadas.\nO dicionários das variáveis a ser considerado neste livro está na Tabela 1.1.\n\n\nTabela 1.1: Dicionário das variáveis da base de dados de COVID-19 em gestantes e puérperas.\n\n\n\n\n\n\n\nVariável\nDescrição\nValores\n\n\n\n\nDT_NOTIFIC\nData de preenchimento da ficha de notificação\nDia/Mês/Ano\n\n\nDT_SIN_PRI\nData de primeiros sintomas do caso\nDia/Mês/Ano\n\n\nDT_NASC\nData de nascimento da gestante ou puérpera\nDia/Mês/Ano\n\n\nDT_INTERNA\nData em que gestante ou puérpera foi hospitalizada\nDia/Mês/Ano\n\n\nSEM_PRI\nSemana epidemiológica do início dos sintomas\n1 a 52\n\n\nCS_RACA\nRaça da gestante ou puérpera\n1- branca; 2- preta; 3- amarela; 4- parda; 5-indígena; 9- ignorado\n\n\nCS_ESCOL_N\nNível de escolaridade da gestante ou puérpera\n0- sem escolaridade (analfabeto); 1- fundamental 1° ciclo (1ª a 5ª série); 2- fundamental 2 (6ª a 9ª série); 3- medio (1° ao 3° ano); 4- superior; 5- não se aplica; 9- ignorado\n\n\nidade\nIdade, em anos, da gestante ou puérpera\n10 a 55\n\n\nCS_GESTANT\nMomento gestacional ou puerpério\n1- 1° trimestre; 2- 2° trimestre; 3- 3° trimestre; 4- idade gestacional ignorada; 5- não; 9- ignorado\n\n\nPUERPERA\nSe paciente é puérpera ou parturiente (mulher que pariu recentemente - até 45 dias do parto)\n1- sim; 2- não; 9- ignorado\n\n\nSG_UF\nSigla do estado de residência da gestante ou puérpera\nSigla padronizada pelo IBGE\n\n\nID_MN_RESI\nNome do município de residência da gestante ou puérpera\nNomes padronizados pelo IBGE\n\n\nCO_MUN_RES\nCódigo do município de residência da gestante ou puérpera\nCódigo definido pelo IBGE\n\n\nCS_ZONA\nTipo de zona de residência da gestante ou puérpera\n1- urbana; 2- rural; 3- periurbana; 9- ignorado\n\n\nFEBRE\nSe gestante ou puérpera manifestou sintoma de febre\n1- sim; 2- não; 9- ignorado\n\n\nTOSSE\nSe gestante ou puérpera manifestou sintoma de tosse\n1- sim; 2- não; 9- ignorado\n\n\nGARGANTA\nSe gestante ou puérpera manifestou sintoma de dor de garganta\n1- sim; 2- não; 9- ignorado\n\n\nDISPNEIA\nSe gestante ou puérpera manifestou sintoma de dispneia\n1- sim; 2- não; 9- ignorado\n\n\nDESC_RESP\nSe gestante ou puérpera manifestou sintoma de desconforto respiratório\n1- sim; 2- não; 9- ignorado\n\n\nSATURACAO\nSe gestante ou puérpera manifestou sintoma de saturação\n1- sim; 2- não; 9- ignorado\n\n\nDIARREIA\nSe gestante ou puérpera manifestou sintoma de diarreia\n1- sim; 2- não; 9- ignorado\n\n\nVOMITO\nSe gestante ou puérpera manifestou sintoma de vômito\n1- sim; 2- não; 9- ignorado\n\n\nDOR_ABD\nSe gestante ou puérpera manifestou sintoma de dor abdominal\n1- sim; 2- não; 9- ignorado\n\n\nFADIGA\nSe gestante ou puérpera manifestou sintoma de fadiga\n1- sim; 2- não; 9- ignorado\n\n\nPERD_OLFT\nSe gestante ou puérpera manifestou sintoma de perda de olfato\n1- sim; 2- não; 9- ignorado\n\n\nPERD_PALA\nSe gestante ou puérpera manifestou sintoma de perda de paladar\n1- sim; 2- não; 9- ignorado\n\n\nASMA\nSe gestante ou puérpera tem asma\n1- sim; 2- não; 9- ignorado\n\n\nDIABETES\nSe gestante ou puérpera tem diabetes mellitus\n1- sim; 2- não; 9- ignorado\n\n\nNEUROLOGIC\nSe gestante ou puérpera tem doença neurológica\n1- sim; 2- não; 9- ignorado\n\n\nPNEUMOPATI\nSe gestante ou puérpera tem outra pneumopatia crônica\n1- sim; 2- não; 9- ignorado\n\n\nIMUNODEPRE\nSe gestante ou puérpera tem imunodeficiência ou imunodepressão (diminuição da função do sistema imunológico)\n1- sim; 2- não; 9- ignorado\n\n\nRENAL\nSe gestante ou puérpera tem doença renal crônica\n1- sim; 2- não; 9- ignorado\n\n\nOBESIDADE\nSe gestante ou puérpera tem obesidade\n1- sim; 2- não; 9- ignorado\n\n\nCARDIOPATI\nSe gestante ou puérpera tem doença cardiovascular crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEMATOLOGI\nSe gestante ou puérpera tem doença hematológica crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEPATICA\nSe gestante ou puérpera tem doença hepática crônica\n1- sim; 2- não; 9- ignorado\n\n\nVACINA_COV\nSe gestante ou puérpera recebeu vacina COVID-19\n1- sim; 2- não; 9- ignorado\n\n\nDOSE_1_COV\nData em que gestante ou puérpera recebeu a 1ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nDOSE_2_COV\nData em que gestante ou puérpera recebeu a 2ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nFAB_COV_1\nFabricante da vacina que a gestante ou puérpera recebeu na 1ª dose\n\n\n\nFAB_COV_2\nFabricante da vacina que a gestante ou puérpera recebeu na 2ª dose\n\n\n\nSUPORT_VEN\nSe gestante ou puérpera precisou de ventilação mecânica; se sim, se foi invasiva ou não\n1- sim, invasivo; 2- sim, não invasivo; 3- não; 9- ignorado\n\n\nUTI\nSe gestante ou puérpera foi internada na UTI\n1- sim; 2- não; 9- ignorado\n\n\nDT_ENTUTI\nData de entrada da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nDT_SAIDUTI\nData de saída da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nEVOLUCAO\nEvolução do caso da gestante ou puérpera\n1- cura; 2- óbito; 3- óbito por outras causas; 9- ignorado"
  },
  {
    "objectID": "manipulacao_dados.html",
    "href": "manipulacao_dados.html",
    "title": "2  Manipulação de dados",
    "section": "",
    "text": "Neste capítulo falaremos alguns princípios básicos sobre manipulação de dados. Iremos trabalhar em um cenário mais próximo da realidade possível, ou seja, iremos trabalhar em cima de uma base de dados real. O objetivo é manipular a base e torná-la pronta para ser usada nos capítulos seguintes. Será mostrado desde como importar a base até como criar novas variáveis que poderão ser utilizadas em análises. Não será possível cobrir todo o ramo de manipulação em um só capítulo, mas iremos trabalhar com o máximo de ferramentas possíveis. Pacotes ou funções que não forem utilizadas aqui, mas que são interessantes serão mencionados ao longo do capítulo junto a links que contenham explicações de como utilizá-las. Vale ressaltar que estamos em um cenário mais básico e introdutório. Vamos começar.\n\n2.0.1 Importação de dados\nUm dos caminhos mais simples para importar dados no R é utilizando a função read.table(). Está função é simples pois ja vem instalada com o R, faz parte do pacote base utils, e importa arquivos nos formatos cvs e txt.\nA utilização do pacote é bem simples, não preciso carregá-lo na memória usando library().\n\ndados1 <- read.table(file = \"dados.csv\", sep = \";\")\ndados2 <- read.table(file = \"caminho-para-o-arquivo/dados.csv\", sep = \";\")\n\nObserve que na função temos os argumentos file e sep. O file indica o nome do arquivo que será importado e sep indica qual o símbolo separador de colunas, que neste caso é a virgula. Note também que usamos dois exemplos, o primeiro considera que o seu arquivo está no diretório de trabalho (quando criamos o projeto e colocamos os arquivos de dados na pasta criada pelo projeto), não sendo necessário especificar o caminho até do arquivo. O outro exemplo mostra como especificar o local do seu arquivo. A função possui mais argumentos que você pode explorar usando o help, mas no geral, esses dois são os mais utilizados.\n\n2.0.1.1 Extensão .txt ou .csv\nCaso esteja trabalhando com arquivos do tipo cvs ou txt o pacote readr irá servir muito bem. As funções deste pacote são bem rapidas e algumas delas são focadas em tranformar arquivos simples em data.frame. Aglumas funções do pacote são\n\nread_cvs(): para arquivos delimitados por vírgulas.\nread_cvs2(): para arquivos delimitados por ponto e vírgula.\nread_tsv(): para arquivos delimitados por tabulações.\nread_delim(): para aquivos com qualquer delimitador.\nread_fwf(): para arquivos compactos que devem ter a largura de cada coluna especificada.\nread_table(): para arquivos de texto tabulas com colunas separas por espaço.\n\nCaso esta seja a primeira vez que você ira utilizar este pacote, será necessário instalá-lo em seu computar. Você pode fazer isso utilizando a função install.packages(\"readr\") e é claro, antes de usar qualquer pacote que não faça parte do R base, você deve carregá-lo. Como exemplo, consideramos um arquivo chamado dados1 que queremos importar para o R.\n\nlibrary(readr)\ndados_csv <- read_csv(file = \"caminho-para-o-arquivo/dados1.csv\")\ndados_txt <- read_delim(file = \"caminho-para-o-arquivo/dados1.txt\", delim = \" \")\n\nApesar dos argumentos deste pacote serem semelhantes aos da função read.table(), devemos nos atentar a algumas diferenças. Aqui é o argumento delim que indica qual o separador das colunas no arquivo texto.\nVale ressaltar que para cada função read_, existe umas respectiva função write_ para exportar o arquivo no formato de interesse. Como exemplo, queremos salvar a base de dados mtcars na pasta do meu computador com o nome cars:\n\nwrite_csv(x = mtcars, path = \"cars.csv\")\nwrite_delim(x = mtcars, delim = \" \", path = \"cars.txt\")\n\n\n\n2.0.1.2 Arquivos em Excel\nArquivos em formato xlsx são muito utilizados, porém o R não possui uma função nativa para importar este tipo de arquivo. Existem diversos pacotes para importar dados neste e formato e os principais são redxl, xlsx, XLConnect e tydixl. Apesar destes pacotes terem objetivos semelhantes, cada um tem suas peculiaridades, então aconselhamos estudar cada um desses pacotes e assim decidir qual melhor atende às suas necessidades. Aqui vamos mostrar apenas o pacote readxl, pois é um dos mais facéis e diretos de se utilizar. Este pacote serve para importar e ler planilhas do Excel nos formatos xlsx ou xls. A seguir estão listadas algumas funções para importação e leitura de dados:\n\nread_excel(): esta função detecta automaticamente a extensão do arquivo, e importa arquivos do tipo xsl e xlsx.\nread_xsl(): importa arquivos no formato xsl.\nread_xlsx(): importa arquivos no formato xlsx.\n\nNovamente, é necessário à instalação e carregamento do pacote caso não o tenha em seu computador. Para exemplicar consideramos um arquivo chamado dados2 que queremos importar para o R.\n\nlibrary(readxl)\ndados_excel1 <- read_excel(path = \"dados2.xls\")\ndados_excelx1 <- read_excel(path = \"dados2.xlsx\")\n\nPor meio da função read_excel conseguimos importar tanto um arquivo no formato xls quanto no formato xlsx.\nPodemos também exportar um arquivo em excel (.xls e .xlsx) ao considerar a função write_xlsx do pacote writexl. Suponha que temos o interesse em salvar a base de dados dados em excel na pasta do computador (exportar) com o nome de dados_correto:\n\nlibrary(writexl)\nwrite_xlsx(dados, \"dados_correto.xlsx\")\n\n\n\n\n2.0.2 Análise de consistência e tratamento de dados\nO tratamento dos dados toma muitas vezes a maior parte do tempo de uma análise estatística.\nA análise de consistência consiste em realizar uma primeira análise dos dados com o intuito de encontrar inconsistências. São exemplos de inconsistências:\n\nboas práticas para nome das variáveis.\ncomo erros de digitação;\nindivíduos imputados mais de uma vez na planilha de dados de maneira errada;\nidentificar casos missings e avaliar se a observação está ausente de maneira correta ou não;\nidentificar as categorias de variáveis qualitativas.\n\nA partir daqui iremos trabalhar com a nossa base de dados de COVID-19 em gestantes e puérperas.\nImportando os dados\nComo já aprendemos a importar os dados, vamos direto ao ponto. Nos dados estão no forma rds que não foi mencionado anteriormente, mas o pacote readr tem uma função para importar este tipo de arquivo.\n\ndados <- readr::read_rds(\"dados/dados_covid[SUJO].rds\")\nknitr::kable(head(dados))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDT_NOTIFIC\nDT_SIN_PRI\nDT_NASC\nDT_INTERNA\nSEM_PRI\nSG_UF\nID_MN_RESI\nCO_MUN_RES\nCS_ZONA\nCS_RACA\nCS_ESCOL_N\nidade\nCS_GESTANT\nPUERPERA\nFEBRE\nTOSSE\nGARGANTA\nDISPNEIA\nDESC_RESP\nSATURACAO\nDIARREIA\nVOMITO\nFADIGA\nPERD_OLFT\nPERD_PALA\nDOR_ABD\nCARDIOPATI\nHEMATOLOGI\nHEPATICA\nASMA\nDIABETES\nNEUROLOGIC\nPNEUMOPATI\nIMUNODEPRE\nRENAL\nOBESIDADE\nVACINA_COV\nDOSE_1_COV\nDOSE_2_COV\nFAB_COV_1\nFAB_COV_2\nDT_ENTUTI\nDT_SAIDUTI\nUTI\nSUPORT_VEN\nEVOLUCAO\n\n\n\n\n15/05/2020\n06/05/2020\n03/06/2003\n15/05/2020\n19\nCE\nMORRINHOS\n230890\nNA\n4\nNA\n16\n2\nNA\n1\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n18/05/2020\n10/05/2020\n07/07/1996\n15/05/2020\n20\nPR\nCURITIBA\n410690\n1\n1\n2\n23\n2\n2\n2\n2\n2\n1\n2\n2\n1\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n30/04/2020\n20/04/2020\n26/03/1996\n24/04/2020\n17\nSP\nSAO CAETANO DO SUL\n354880\n1\n9\n9\n24\n9\n1\n1\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n11/05/2020\n04/05/2020\n02/06/1986\n09/05/2020\n19\nPA\nMARABA\n150420\n1\n4\n4\n33\n5\n1\n1\n1\n2\n2\n1\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n01/07/2020\n12/06/2020\n11/12/1996\n30/06/2020\n24\nDF\nSANTA MARIA\n530150\n1\n9\nNA\n23\n5\n1\n2\n2\n2\n2\n2\n2\n1\n2\nNA\n1\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n09/06/2020\n09/06/2020\n09/12/1984\n09/06/2020\n24\nRO\nPORTO VELHO\n110020\n1\n4\n2\n35\n3\n2\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n\n\n\n\n2.0.2.1 Tratamento da base de dados\nInicialmente, vamos verificar os nomes das variáveis na base de dados por meio da função names. Note que os nomes estão, de certa forma, padronizados. Todos maíusculos (com exceção de “idade”), separados por “_”. Este ainda não é o cenário ideal para trabalharmos, mas poderia ser pior, contendo maiúsculas, espaços e acentos. Utilizar os dados com essas características não impossibilita as futuras análises, mas pode atrapalhar quando precisamos selecionar algumas dessas variáveis.\n\nnames(dados)\n\n [1] \"DT_NOTIFIC\" \"DT_SIN_PRI\" \"DT_NASC\"    \"DT_INTERNA\" \"SEM_PRI\"   \n [6] \"SG_UF\"      \"ID_MN_RESI\" \"CO_MUN_RES\" \"CS_ZONA\"    \"CS_RACA\"   \n[11] \"CS_ESCOL_N\" \"idade\"      \"CS_GESTANT\" \"PUERPERA\"   \"FEBRE\"     \n[16] \"TOSSE\"      \"GARGANTA\"   \"DISPNEIA\"   \"DESC_RESP\"  \"SATURACAO\" \n[21] \"DIARREIA\"   \"VOMITO\"     \"FADIGA\"     \"PERD_OLFT\"  \"PERD_PALA\" \n[26] \"DOR_ABD\"    \"CARDIOPATI\" \"HEMATOLOGI\" \"HEPATICA\"   \"ASMA\"      \n[31] \"DIABETES\"   \"NEUROLOGIC\" \"PNEUMOPATI\" \"IMUNODEPRE\" \"RENAL\"     \n[36] \"OBESIDADE\"  \"VACINA_COV\" \"DOSE_1_COV\" \"DOSE_2_COV\" \"FAB_COV_1\" \n[41] \"FAB_COV_2\"  \"DT_ENTUTI\"  \"DT_SAIDUTI\" \"UTI\"        \"SUPORT_VEN\"\n[46] \"EVOLUCAO\"  \n\n\numa boa prática consiste em padronizar os nomes das variáveis, até para facilitar a lembrança deles. Para isso, utilizaremos o pacote janitor para a arrumação da base de dados. Usamos a função clean_names() para primeiro ajuste dos nomes das variáveis.\n\ndados <- janitor::clean_names(dados) \nnames(dados)\n\n [1] \"dt_notific\" \"dt_sin_pri\" \"dt_nasc\"    \"dt_interna\" \"sem_pri\"   \n [6] \"sg_uf\"      \"id_mn_resi\" \"co_mun_res\" \"cs_zona\"    \"cs_raca\"   \n[11] \"cs_escol_n\" \"idade\"      \"cs_gestant\" \"puerpera\"   \"febre\"     \n[16] \"tosse\"      \"garganta\"   \"dispneia\"   \"desc_resp\"  \"saturacao\" \n[21] \"diarreia\"   \"vomito\"     \"fadiga\"     \"perd_olft\"  \"perd_pala\" \n[26] \"dor_abd\"    \"cardiopati\" \"hematologi\" \"hepatica\"   \"asma\"      \n[31] \"diabetes\"   \"neurologic\" \"pneumopati\" \"imunodepre\" \"renal\"     \n[36] \"obesidade\"  \"vacina_cov\" \"dose_1_cov\" \"dose_2_cov\" \"fab_cov_1\" \n[41] \"fab_cov_2\"  \"dt_entuti\"  \"dt_saiduti\" \"uti\"        \"suport_ven\"\n[46] \"evolucao\"  \n\n\nVeja que ele deixou todos os nomes minúsculos. Neste caso não foi feito, mas a função substitui o espaço por “_” e tira acentos. Isso ajuda a evitar problemas futuros em algumas análises que não lidam muito bem com acentos e espaços nos nomes das variáveis.\nOutro problema comum é a presença de linhas e colunas vazias. Na base de dados em questão, não há linhas nem colunas em branco, como pode ser visto na saída abaixo.\n\njanitor::remove_empty(dados,\"rows\")\njanitor::remove_empty(dados,\"cols\")\n\n\n\n2.0.2.2 \nIdentificando casos duplicados\nOutra boa prática é identificar casos duplicados, isto é, identificar se há casos erroneamente repetidos. O ideal é utilizar variável chave do seu banco de dados, ou seja, aquela em que cada observação é única. Por exemplo, em uma base de dados de funcionários de uma empresa, uma variável chave poderia ser o CPF. Uma variável chave também pode ser a combinação de variáveis, gerando assim observações únicas. Para identificar casos duplicados, usamos a função get_dupes do pacote janitor. Em nosso banco de dados não tempos uma varíavel chave, então não vamos especificá-la na função, assim a função irá procurar observações repetidas considerando todas as variáveis, ou seja, linhas repetidas.\n\njanitor::get_dupes(dados)\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: dt_notific, dt_sin_pri, dt_nasc, dt_interna, sem_pri, sg_uf, id_mn_resi, co_mun_res, cs_zona, ... and 37 other variables\n\n\n [1] dt_notific dt_sin_pri dt_nasc    dt_interna sem_pri    sg_uf     \n [7] id_mn_resi co_mun_res cs_zona    cs_raca    cs_escol_n idade     \n[13] cs_gestant puerpera   febre      tosse      garganta   dispneia  \n[19] desc_resp  saturacao  diarreia   vomito     fadiga     perd_olft \n[25] perd_pala  dor_abd    cardiopati hematologi hepatica   asma      \n[31] diabetes   neurologic pneumopati imunodepre renal      obesidade \n[37] vacina_cov dose_1_cov dose_2_cov fab_cov_1  fab_cov_2  dt_entuti \n[43] dt_saiduti uti        suport_ven evolucao   dupe_count\n<0 linhas> (ou row.names de comprimento 0)\n\n\nEm nosso caso, não temos casos duplicados. Caso tivesse, seria necessário remover as linhas duplicadas. Isto pode ser feito com o uso da função distinct do pacote dplyr.\n\n\n\n2.0.3 Identificar problemas nas variáveis da base de dados\nOutra etapa importante na análise de consistência é identificar o tipo de variável e ver se o R está interpretando corretamente o tipo de cada variável.\nTemos na nossa base de dados variáveis de data, além de variáveis qualitativas e quantitativas (veja o dicionário das variáveis na em: refenciar parte). Assim, precisamos entender se o R realmente entendeu todas as variáveis da maneira correta. Uma maneira de identificar isso e também de ver algumas descritivas das variáveis que nos auxiliam a ver possíveis inconsistências na base de dados é a a função glimpse do pacote dplyr. A função skim do pacote skimr também pode ajudar nisso.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific <chr> \"15/05/2020\", \"18/05/2020\", \"30/04/2020\", \"11/05/2020\", \"01…\n$ dt_sin_pri <chr> \"06/05/2020\", \"10/05/2020\", \"20/04/2020\", \"04/05/2020\", \"12…\n$ dt_nasc    <chr> \"03/06/2003\", \"07/07/1996\", \"26/03/1996\", \"02/06/1986\", \"11…\n$ dt_interna <chr> \"15/05/2020\", \"15/05/2020\", \"24/04/2020\", \"09/05/2020\", \"30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <int> NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    <int> 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n <int> NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <int> 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   <int> NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      <int> 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      <int> 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   <int> NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   <int> NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  <int> NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  <int> NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   <int> NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     <int> NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  <int> NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"30/06/2020\", \"\", \"12/07/2020\",…\n$ dt_saiduti <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"29/07/2020\", \"\", \"\", \"\", \"\", \"…\n$ uti        <int> 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven <int> 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   <int> 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nNo R, as variáveis qualititativas são nomeadas “factor”, as variáveis quantitativas são nomeadas “numeric” e as variáveis de data são “date”. Note que na importação dos dados o R não entendeu corretamente os tipos de variáveis. Mas vamos corrigir isso no que segue.\nComeçando pela data, vamos rodar o seguinte código:\n\ndados$dt_notific  <- as.Date(dados$dt_notific, format = \"%d/%m/%Y\")\ndados$dt_sin_pri  <- as.Date(dados$dt_sin_pri, format = \"%d/%m/%Y\")\ndados$dt_nasc  <- as.Date(dados$dt_nasc, format = \"%d/%m/%Y\")\ndados$dt_interna  <- as.Date(dados$dt_interna, format = \"%d/%m/%Y\")\ndados$dt_entuti  <- as.Date(dados$dt_entuti, format = \"%d/%m/%Y\")\ndados$dt_saiduti  <- as.Date(dados$dt_saiduti, format = \"%d/%m/%Y\")\n\nA função as.Date informa para o R que a variável indicada é de data. O argumento format indica o formato que está a data, nesse caso, “dia/mês/ano”. Aqui é possível verificar todos os formatos de datas da função. Vamos ver como ficou:\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific <date> 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri <date> 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    <date> 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna <date> 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <int> NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    <int> 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n <int> NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <int> 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   <int> NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      <int> 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      <int> 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   <int> NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   <int> NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  <int> NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  <int> NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   <int> NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     <int> NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  <int> NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    <int> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  <int> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov <int> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <date> NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti <date> NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        <int> 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven <int> 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   <int> 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nAgora vamos lidar com as variáveis qualitativas. Veja que “cs_zona” foi identificada como int. Isso acontece porque ela foi tabulada como número, como posteriormente variáveis deste tipo serão recodificadas de acordo com o dicionário, precisamos tratá-la como fator. Já as demais variáveis qualitativas estão identificadas como numeric, dbl ou chacacter pois na tabulação suas categorias estão codificadas com números ou textos. Para então dizer ao R o verdadeiro tipo dessas variáveis, vamos utilizar os seguintes comandos:\n\ndados$cs_raca <- as.factor(dados$cs_raca)\ndados$cs_escol_n <- as.factor(dados$cs_escol_n)\ndados$cs_gestant <- as.factor(dados$cs_gestant)\ndados$puerpera <- as.factor(dados$puerpera)\ndados$cs_zona <- as.factor(dados$cs_zona)\ndados$febre <- as.factor(dados$febre)\ndados$tosse <- as.factor(dados$tosse)\ndados$suport_ven <- as.factor(dados$suport_ven)\ndados$uti <- as.factor(dados$uti)\ndados$evolucao <- as.factor(dados$evolucao)\n\nUma forma um pouco mais eficiente de fazer isso é selecionar as variáveis por meio de um vetor, por exemplo, quero que as variáveis da coluna 10 até a coluna 20 sejam fatores. Podemos fazer isso com a ajuda a função lapply. Essa função, em resumo, nos possibilita aplicar uma função em uma lista de elementos e retorna uma lista de mesmo tamanho em que o resultado é a aplicação desta função a cada elemento da lista. Neste caso, aplicamos a função as.factor nas colunas selecionadas (lista de elementos). Veja como é feito.\n\n\ndados[,c(17:37)] <- lapply(dados[,c(17:37)], as.factor)\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific <date> 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri <date> 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    <date> 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna <date> 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <fct> NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    <fct> 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n <fct> NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <fct> 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   <fct> NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      <fct> 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      <fct> 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   <fct> NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   <fct> NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  <fct> NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  <fct> NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   <fct> NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     <fct> NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     <fct> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  <fct> NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  <fct> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    <fct> NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  <fct> NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <date> NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti <date> NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        <fct> 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven <fct> 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   <fct> 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nÓtimo! Corrigimos as inconsistências das variáveis qualitativas. Mas outra questão surge: como faço para usar um rótulo nos números codificados nas categorias das variáveis qualitativas? Para o grupo, por exemplo, ao invés de aparecer 1 quero que apareça “sim”. Para isso, vamos utilizar o pacote forcats que lida com variáveis qualitativas (categóricas). Para renomear as categorias das variáveis, vamos usar a função fct_recode desse pacote:\n\ndados$cs_raca <- forcats::fct_recode(dados$cs_raca,\n                                   branca = \"1\",\n                                   preta = \"2\",\n                                   amarela = \"3\",\n                                   parda = \"4\",\n                                   indigena = \"5\",\n                                   ignorado = \"9\")\n\ndados$cs_escol_n <- forcats::fct_recode(dados$cs_escol_n,\n                                     \"sem escola\"  = \"0\",\n                                     fund1 = \"1\",\n                                     fund2 = \"2\",\n                                     medio = \"3\",\n                                     superior = \"4\",\n                                     ignorado = \"9\")\n\ndados$cs_gestant <- forcats::fct_recode(dados$cs_gestant,\n                                     \"1tri\" = \"1\",\n                                     \"2tri\" = \"2\",\n                                     \"3tri\" = \"3\",\n                                     IG_ig = \"4\",\n                                     nao = \"5\",\n                                     ignorado = \"9\")\n\ndados$puerpera <- forcats::fct_recode(dados$puerpera,\n                                      sim = \"1\",\n                                      nao = \"2\",\n                                      ignorado = \"9\")\n\ndados$cs_zona <- forcats::fct_recode(dados$cs_zona,\n                                  urbana = \"1\",\n                                  rural = \"2\",\n                                  periurbana = \"3\",\n                                  ignorado = \"9\")\n\ndados$febre <- forcats::fct_recode(dados$febre,\n                                   sim = \"1\",\n                                   nao = \"2\",\n                                   ignorado = \"9\")\n\ndados$suport_ven <-forcats::fct_recode(dados$suport_ven,\n                                       \"sim, invasivo\" = \"1\",\n                                       \"sim, nao invasivo\" = \"2\",\n                                       nao = \"3\",\n                                       ignorado = \"9\")\n\ndados$uti <-forcats::fct_recode(dados$uti,\n                                       sim = \"1\",\n                                       nao = \"2\",\n                                       ignorado = \"9\")\n\ndados$evolucao <-forcats::fct_recode(dados$evolucao,\n                                       cura = \"1\",\n                                       obito = \"2\",\n                                       \"obito por outras causas\" = \"3\",\n                                       ignorado = \"9\")\n\nEste tramanto foi feito para todas as variáveis qualitativas da base, mas por conta do tamanho do código, omitimos algumas da saída.\nFinalmente chegamos nas variáveis quantitativas. Uma forma de identificar problemas em variáveis quantitativas é avaliar os valores mínimo e máximo de cada variável e ver se tem algum valor impossível para a mesma. Em nosso caso podemos verificar a variável idade. Seria meio estranho encontrar alguém com valores extremamente altos ou negativos, concorda?! A função summary pode ser uma opção boa aqui, ela nos formece algumas medidas descritivas como, media, mínimo, máximo, entre outros.\n\nsummary(dados$idade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   25.00   30.00   30.25   35.00   55.00       9 \n\n\nAparentemente nossa variável esta dentro do esperado, sem valores inesperados.\n\n2.0.3.1 Transformação de dados\nÉ possível modificar ou criar novas variáveis na base de dados por meio da função mutate do pacote dplyr, você pode veificar melhor essa função clicando aqui. Também podemos criar categorias com base em alguma condição por meio da função case_when também do pacote dplyr, veja melhor aqui. Para ficar mais claro, vamos a um exemplo combinando as duas funções. Vamos criar a variável “faixa_et”, onde as observações serão as faixas etárias. São essas: “<20”, “20-34” e “>=”. Veja como faz:\n\ndados <- dados |> \n  mutate(faixa_et = case_when(\n    idade < 20 ~ \"<20\",\n    idade >= 20 & idade < 34 ~ \"20-34\",\n    idade >= 34 ~ \">=34\"\n  ))\n\ntable(dados$faixa_et)#table nos mostra as observações da quela variável e a sua frequência. \n\n\n  <20  >=34 20-34 \n  714  3862  6938 \n\n\nAqui fizemos a utilização da função “pipe” |> que agora está no pacote base do R, mas que antes era necessário carregá-la por meio de pacotes. Essa função é de extrema importância, facilita a programção no R de uma forma inimaginável. É válido dedicar um pouco de seu tempo para entender melhor essa função. Separamos alguns links que pode te ajudar a entender melhor e você pode acessá-los clickando aqui, aqui ou aqui. Como foi mencionado acima, a função foi adicionada ao R base há pouco tempo, então esses links se referem ao pipe “antigo”, mas fique tranquilo, a função é a mesma. Para resumir, o pipe pega a saída de uma função e a passa para outra função como um argumento. Isso nos permite vincular uma sequência de etapas de análise.\n\n\n2.0.3.2 Manipulação de datas\nAlgo interessante também é trabalhar com a varíavel de datas. Podemos calulcar a diferença entre duas datas no R de forma bem simples por meio da função difftime do pacote base do R. Para exemplificar vamos criar a variável “dias_uti” que vai ser ser quantos dias a pessoa ficou internada na uti. Vamos fazer isso calculando a diferença entre a data de saída e a data de entrada na uti e queremos o resultado em dias.\n\ndados$dias_uti <- difftime(dados$dt_saiduti, dados$dt_entuti, units = \"days\")\nglimpse(dados)\n\nRows: 11,523\nColumns: 48\n$ dt_notific <date> 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri <date> 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    <date> 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna <date> 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <fct> NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    <fct> parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n <fct> NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <fct> 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   <fct> NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      <fct> sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      <fct> sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   <fct> NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   <fct> NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  <fct> NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  <fct> NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   <fct> NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     <fct> NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  <fct> NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <date> NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti <date> NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        <fct> nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven <fct> \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   <fct> cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   <chr> \"<20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \">=34\", \"20-34\",…\n$ dias_uti   <drtn> NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nNote que não utilizamos a função mutate para criar está nova variável, utilizamos apenas o $ para representar a variável e atribuímos a função. Assim, o R já entende como uma variável.\n\n\n2.0.3.3 Manipulação de dados\nJá temos a nossa base de dados devidamente tratada para prosseguir com a análise descritiva, mas quando falamos de manipulação de dados, um leque de possibilidades aparece. Em diversos cenários precisamos filtrar observações, reordená-las, selecionar variáveis específicas, entre outras coisas. Não poderíamos deixar de mencionar o poderoso tidyverse. O tidyverse é um pacote que contém um coleção de outros pacotes que são utilizados para manipulação, exploração e visualização de dados e que compartilham uma filosofia de design bem parecida, por isso de forma combinada permitem que você consiga fazer inúmeros trabalhos. Os pacotes que fazem parte desse universo são: dplyr, tidyr, ggplot2, forcats, purrr, stringr, tibble e readr. Anteriormente já trabalhamos com alguns destes pacotes, mas agora é válido aprofundarmos um pouco mais em alguns deles. Aqui você irá acessar o site do tidyverse onde podera navegar por cada pacote e aprender mais sobre suas utilidades e aqui você irá acessar um post escrito pelo Laboratório de Data Scinence - UFES (daslab) que contem diversos exemplos práticos de uso de todos os pacotes do universo tidyverse. Neste capítulo iremos trabalhar com algumas funções específicas.\n\n2.0.3.3.1 Pacote dplyr\nO dplyr é extremamente útil e nos ajuda a resolver os desafios mais comuns de manipulação de dados.\nSuas principais funções são:\n\nfilter() - filtra linhas;\ngroup_by() - agrupa pela(s) variável(is) no argumento. Função muito útil quando usada a funçaõ summurise.\nsummarise() - reduz vários valores a um único resumo.\nselect() - seleciona colunas;\narrange() - ordena a base;\nmutate() - cria/modifica colunas.\n\nJá utilizamos algumas funções do pacote, vamos falar sobre outras. Como já avançamos um pouco sobre a utilização de funções, vamos combinar algumas funções, o que geralmente é feito no dia a dia.\n\n#criando um novo banco de dados selecionando 3 variáveis\ndados_tratamento <- dados |> \n  select(sg_uf, cs_zona, idade)\n\nglimpse(dados_tratamento)\n\nRows: 11,523\nColumns: 3\n$ sg_uf   <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\", \"M…\n$ cs_zona <fct> NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana, ur…\n$ idade   <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26, 20…\n\n\nAqui criamos a base “dados_tratamento” onde apenas selecionamos algumas colunas da base de dados inicial com a função select.\n\ndados_tratamento2 <- dados_tratamento |> \n  filter(cs_zona == \"urbana\") |> \n  group_by(sg_uf) |> \n  summarise(media = mean(idade, na.rm = TRUE)) |> \n  arrange(desc(media))\n\nknitr::kable(head(dados_tratamento2))\n\n\n\n\nsg_uf\nmedia\n\n\n\n\nAP\n36.66667\n\n\nBA\n31.04152\n\n\nRR\n31.00000\n\n\nSP\n30.94237\n\n\nRJ\n30.93570\n\n\nMG\n30.93257\n\n\n\n\n\nVamos entender o código acima. Primeiro acessamos a base “dados_tratamento” e com função filter selecionamos apenas as observações “urbana”. Após isso utilizamos a função group_by para agrupar nossas observações pela variável “sg_uf” e por últimos, combinamos com a função summarise para criar a variável “media” que será a media da variável idade. Note que nesta função utilizamos o argumento na.rm - TRUE. Este argumento serve para indicar para a função se ela deve ou não remover valores NA's do cálculo, o default é FALSE. Como não é possível calcular a média de valores ausentes e temos variáveis ausentes, foi necessário utilizar este argumento. Caso contrário, Estados com valores faltantes ficariam com NA. Por último, utilizamos a função arrange para ordernar os dados em em ordem descrente pela variavel media. Uma dica para tentar entender melhor o funcionamento das funções é tentar refazer o código utilizando uma função de cada vez e ir vendo como fica. Então, em poucas linhas de códigos conseguimos criar uma base com a idade média dos Estados considerando apenas zonas urbanas, legal né?\n\n\n2.0.3.3.2 Pacote stringr\nUm desafio muito grande na manipulação de dados é extrair informações de caracteres. Em resumo caracteres são letras, símbolos, sinais, números que representem algo escrito, etc.. Essa sequência de caracteres formam o que chamamos de string. Diversas vezes encontramos variáveis com categorias não padronizadas, como, por exemplo, uma variável contendo “São Paulo”, “sao paulo” e “sp”. Apesar de representarem o mesmo estado, elas são diferentes. Nesse sentido, uma parte muito importante no tratamento de dados é “lapidar” esse conjunto de caracteres para que seja possível usá-los nas análises. Essa é a introdução do post do daslab onde é passado de uma maneira muito prática como trabalhar com strings utilizando o pacote stringr, la você vai aprender também sobre expressões regulares, que com certeza serão úteis em vários momentos da sua carreira. Link do post. Como as variáveis de texto do nosso banco de dados já estão bem padronizadas não será necessário realizar nenhum tratamento, mas por ser um pacote de extrema importância e que não havia sido mencionado ainda, deixamos ele aqui para que você possa se aprofundar mais. Como em nossa base dados as variáveis de texto estão padrozinados, não será necessário realizar nenhum tratamento.\n\n\n\n2.0.3.4 Manipulando o formato da base de dados\nEm certos casos é necessário mudar o formato das bases de dados, fazer com que colunas se tornem linhas vice-versa. Vamos utilizar a base “dados_tratamento”. Veja que ela está no formato long, em que as avaliações do mesmo indivíduo (variável de identificação de indivíduo é “registro”) estão nas linhas. Queremos que as zonas fiquem nas colunas, com as três colunas (vamos tirar valores ignorados): urbana, rural e periurbana, ou seja, queremos o formato wide. Um pacote do R que pode nos auxiliar a transformar formato long em wide e vice-versa é o tidyr. A função que usaremos é spread, como segue:\n\nlibrary(tidyr)\n\ndados_formato <- dados_tratamento |> \n  filter(!is.na(cs_zona) & cs_zona != \"ignorado\") |> \n  mutate(id = row_number())\n\nknitr::kable(head(dados_formato))\n\n\n\n\nsg_uf\ncs_zona\nidade\nid\n\n\n\n\nPR\nurbana\n23\n1\n\n\nSP\nurbana\n24\n2\n\n\nPA\nurbana\n33\n3\n\n\nDF\nurbana\n23\n4\n\n\nRO\nurbana\n35\n5\n\n\nPI\nurbana\n31\n6\n\n\n\n\n\nFizemos pequenas alterações na base de dados. Primeiro realizamos um filtro para retirarmos valores faltantes da variável “cs_zona”, pois essa passará a\n\ndados_formato2 <- dados_formato |> \n  pivot_wider(names_from = cs_zona, values_from = idade)  \n\nknitr::kable(head(dados_formato2))\n\n\n\n\nsg_uf\nid\nurbana\nrural\nperiurbana\n\n\n\n\nPR\n1\n23\nNA\nNA\n\n\nSP\n2\n24\nNA\nNA\n\n\nPA\n3\n33\nNA\nNA\n\n\nDF\n4\n23\nNA\nNA\n\n\nRO\n5\n35\nNA\nNA\n\n\nPI\n6\n31\nNA\nNA\n\n\n\n\n\n\ndados_formato3 <- dados_formato2 |> \n  pivot_longer(cols = c(\"urbana\",   \"rural\",    \"periurbana\"), names_to = \"cs_zona\", values_to = \"idade\")\n\nknitr::kable(head(dados_formato3))\n\n\n\n\nsg_uf\nid\ncs_zona\nidade\n\n\n\n\nPR\n1\nurbana\n23\n\n\nPR\n1\nrural\nNA\n\n\nPR\n1\nperiurbana\nNA\n\n\nSP\n2\nurbana\n24\n\n\nSP\n2\nrural\nNA\n\n\nSP\n2\nperiurbana\nNA\n\n\n\n\n\n\n\n2.0.3.5 Combinando bases de dados\nQuando estamos trabalhando com dados, nem sempre uma única base irá conter todas as informações que precisamos, na verdade, isso é mais comum do que se possa imaginar. Assim, saber juntar duas bases de dados é indispensável. Vamos começar então falando sobre chave primária. Em resumo, chave primária se refere a um ou mais campos, onde combinados (no caso de mais de uma chave primária), não se repete na mesma tabela. Em outras palavras, uma chave primária no meu banco dados seria uma variável onde as observações não se repetem ou a combinação de variáveis que tornam as observações únicas. Para exemplicar vamos pegar nossa base de dados e separar em duas, para que posteriormente possamos juntalas. Como em nossa base de dados não temos naturalmente nenhuma chave primária, vamos utilizar a função mutate(id = row_number()) para criarmos um identificar único para este exemplo. Após isso, vamos dividir a nossa base de dados em duas, mantendo em comum entre elas apenas a nossa chave primária, neste caso, a variável “id”\n\ndados <- dados |> \n  mutate(id = row_number()) |> \n  select(id, everything())#selecionar variavel id e todas as outras \n\ndados1 <- dados[, c(1:24)]\n\nglimpse(dados1)\n\nRows: 11,523\nColumns: 24\n$ id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific <date> 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri <date> 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    <date> 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna <date> 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <fct> NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    <fct> parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n <fct> NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <fct> 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   <fct> NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      <fct> sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      <fct> sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   <fct> NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   <fct> NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  <fct> NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  <fct> NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   <fct> NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     <fct> NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n\ndados2 <- dados[, c(1, 25:49)]\n\nglimpse(dados2)\n\nRows: 11,523\nColumns: 26\n$ id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ perd_olft  <fct> NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <date> NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti <date> NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        <fct> nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven <fct> \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   <fct> cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   <chr> \"<20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \">=34\", \"20-34\",…\n$ dias_uti   <drtn> NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nEm “dados1” selecionamos as colunas de 1 até a 24, onde a coluna 1 é a variável “id”. Em dados 2 selecionamos a coluna depois e as colunas de 25 até a 49. Agora temos dois banco de dados e precisamos juntá-los.\nHá algumas funções de combinação de duas bases de dados no pacote dplyr. Elas recebem três argumentos: a primeira base a ser declarada (x=), a segunda base a ser declarada (y=) e a variável de identificação informada no argumento by=. Aqui estão as funções mais úteis:\n\nleft_join() - retorna todas as linhas da base de dados no argumento x e todas as colunas das duas bases de dados. Linhas da base de dados de x sem correspondentes em y receberão NA na base de dados combinada.\nright_join() - retorna todas as linhas da base de dados no argumento y e todas as colunas das duas bases de dados. Linhas da base de dados de y sem correspondentes em x receberão NA na base de dados combinada.\nfull_join() - retorna todas as linhas e todas as colunas de x e de y. As linhas sem correspondência entre as bases receberão NA na base de dados combinada.\ninner_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y e todas as colunas das duas bases de dados.\nsemi_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y, mantendo apenas as colunas da base de dados de x.\nanti_join() - filtra a base de dados no argumento x para incluir apenas valores que não possuem correspondências na base de dados no argumento y.\n\nAssim sendo, no nosso exemplo, tanto as funções left_join(), right_join(), full_join() e inner_join() retornarão a mesma combinação, pois “dados1” e “dados2” possuem exatamente os mesmos indivíduos, ou seja, não há nenhuma linha que esteja em uma das bases de dados e que não está na outra. Este cenário foi um pouco mais simples, mas pense que no dia a dia você irá encontrar bases onde você precisará encontrar chaves primarias entre elas. Além disso, varios problemas podem vir acompanhados, por exemplo, imagine que para juntar duas bases você utilizará uma chave formada pela combinação de duas variáveis: UF e Município. Em uma base a sua UF está no formato de sigla e na outra está sendo representada pelo código da UF atribuido pelo IBGE. Já na variável de Município, Em uma base os dados estão todos padronizados, maiúsculos e sem acentuação, já na outra base está no formato “padrão” com a primeira letra maiúscula e acentuação. Veja que será necessário um bom tratamento de dados para pode juntar essas bases. Voltando para o nosso exemplo, vamos a prática.\n\ndados_todos <- full_join(dados1, dados2, by=c(\"id\")) \n\nglimpse(dados_todos)\n\nRows: 11,523\nColumns: 49\n$ id         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific <date> 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri <date> 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    <date> 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna <date> 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    <int> 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      <chr> \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi <chr> \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res <int> 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    <fct> NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    <fct> parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n <fct> NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      <dbl> 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant <fct> 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   <fct> NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      <fct> sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      <fct> sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   <fct> NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   <fct> NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  <fct> NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  <fct> NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   <fct> NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     <fct> NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  <fct> NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    <fct> NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  <fct> NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov <fct> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  <date> NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti <date> NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        <fct> nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven <fct> \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   <fct> cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   <chr> \"<20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \">=34\", \"20-34\",…\n$ dias_uti   <drtn> NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nPronto, temos nossa base completa e aprendemos um pouco sobre manipular dados. O pacote tidyverse será um grande aliado seu no R de forma geral. Como mencionado anteriormente, não cobrimos tudo o que é necessário saber para trabalhar com manipulação de dados, é necessário entender a demanda e pesquisar soluções. Saber traduzir o seu problema para que consiga pesquisar com mais facilidade é uma habilidade muito importante. Recomendamos que treine, trabalhe com diferentes tipos de dados, pesquise pacotes, funções, etc. Com o tempo fará com tranquilidade coisas que hoje considera difícil. Lembre-se, a pratica leva a perfeição."
  },
  {
    "objectID": "tabulacao.html#variáveis",
    "href": "tabulacao.html#variáveis",
    "title": "3  Tabulação de dados",
    "section": "3.1 Variáveis",
    "text": "3.1 Variáveis\nOs objetos apresentados, ou variáveis, podem ser denotados como o armazenamento de informações sobre a característica de interesse a respeito de cada unidade amostral, variáveis socioeconômicas como raça, renda e escolaridade são um ótimo exemplo. As variáveis podem ser divididas em dois tipos:\n\nVariáveis Qualitativas: cujos valores podem ser separados por categorias não numéricas. Sendo chamadas de variáveis qualitativas ordinais quando há presença de uma ordenação entre as categorias (Ex.: Escolaridade), e variáveis qualitativas nominais caso contrário (Ex.: Raça, Sexo)\nVariáveis Quantitativas: onde os valores são expressos em números resultantes de uma contagem ou mensuração. Podendo ser quantitativas discretas, quando resultam de um conjunto finito ou enumerável de possíveis valores (Ex.: Número de vitórias ou de filhos), ou ainda variáveis quantitativas continuas quando assumem valores em uma escala continua (Ex.: Peso, Altura).\n\nObserve as 10 unidades amostrais para as variáveis da base de dados COVID-19 para melhor compreensão, onde a idade representa variável quantitativa discreta, a raça represeta qualitativa nominal e a escolaridade é relativa a qualitativa ordinal.\n\n\n\n\n\n\nidade_anos\nraca\nescol\n\n\n\n\n5\n39\nparda\nsuperior\n\n\n6\n34\nbranca\nsuperior\n\n\n8\n29\nbranca\nmedio\n\n\n11\n28\nbranca\nmedio\n\n\n13\n37\nparda\nfund2\n\n\n16\n27\nbranca\nmedio\n\n\n17\n44\nbranca\nmedio\n\n\n23\n31\nbranca\nmedio\n\n\n24\n33\namarela\nmedio\n\n\n25\n25\nparda\nmedio\n\n\n\n\n\nPodemos olhar uma variável por outra perspectiva, assumindo um outro tipo de classificação. Isso pode soar um pouco estranho a princípio, mas olher o exemplo a seguir para melhor compreensão, considere a variável idade, podemos transformar em faixas de idade para classificação em criança, jovem, adulto e idoso. Observe:\n\n#criacao da variavel classificacao\nclassificacao <- idade_anos |>\n  lapply(function(x) ifelse(x < 12, 'crianca',\n                            ifelse(x < 25, 'jovem',\n                                   ifelse( x < 60 ,'adulto','idoso'))))\n#tabela concatenando idade e classificacao\nclassificacao |> \n  unlist() |> \n  cbind(idade_anos) |> \n  head(10) |> knitr::kable()\n\n\n\n\n\nidade_anos\n\n\n\n\njovem\n24\n\n\nadulto\n31\n\n\nadulto\n27\n\n\njovem\n20\n\n\nadulto\n39\n\n\nadulto\n34\n\n\nadulto\n34\n\n\nadulto\n29\n\n\nadulto\n44\n\n\nadulto\n27\n\n\n\n\n\nAgora, temos uma variável categórica ordinal."
  },
  {
    "objectID": "tabulacao.html#como-tabular",
    "href": "tabulacao.html#como-tabular",
    "title": "3  Tabulação de dados",
    "section": "3.2 Como tabular",
    "text": "3.2 Como tabular\nÉ perceptível, até mesmo quando trabalhamos com DataFrames e matrizes, a forma proposta de visualização e armazenamento dessas variáveis. Por colunas onde cada coluna representa uma das características (no nosso exemplo, idade, raça e escolaridade).\nFazemos isso de forma a facilitar nossa análise, sendo cada linha um indíviduo e, cada uma das observações dentro dessa linha, suas características.\nAssim como discutido, podemos obter nossas bases de dados de diversas fontes, como planilhas excel, arquivos .csv, bases SQL, ou até mesmo criá-las no nosso próprio R script com a função data.frame() como já apresentado. Por ser mais intuitivo e mais utilizado no dia a dia, vamos tomar o excel para exemplificar todo o processo. Você irá notar que o processo é realizado de forma bem simples.\n\n\n\n\n\nTabulação das variáveis no excel\n\n\n\n\nCada uma das células receberá um valor x referente a alguma característica indicada pela coluna e um indivíduo representado pela linha, em nosso caso temos 3 características para cada uma das 4 observações."
  },
  {
    "objectID": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "href": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "title": "3  Tabulação de dados",
    "section": "3.3 Alguns problemas no meio do caminho",
    "text": "3.3 Alguns problemas no meio do caminho\nÉ valido ressaltar que é possível se deparar com alguns problemas que talvez possam vir a ser solucionados da maneira errada.\nA forma como tabulamos nossos dados pode vir a ser um facilitar ou empecilho em nossas análises, um belo exemplo é a forma citada anteriormente de classificação dos dados ou transformação para que sejam salvos em alguma outra categoria, como faixa etária ou idade.\nOutro problema é quando trabalhamos com dados que possas vir a ter mais de uma resposta. Por exemplo: Quais sintomas estava sentindo? O melhor a se fazer nesse caso é criar uma coluna para cada um dos possíveis sintomas.\n\n\n\n\n\nMais de uma opção de escolha na variável\n\n\n\n\nUma outra forma seria:\n\n\n\n\n\nMais de uma opção de escolha na variável, outra forma\n\n\n\n\nDevemos lembrar sempre de anexar um ID ou forma de identificação única para cada uma das observações. É possível criar uma ou trabalhar com alguma já existente, um exemplo de uma já existente é o próprio CPF ou RG quando trabalhamos com pessoas.\nVale ressaltar outras boas práticas ao realizar a tabulação:\n\nSe trabalhando com Excel ou Softwares parecidos, deixe a planilha apenas com a tabela de dados, evite armazenar na mesma planilha várias informações avulssas que não façam parte da sua tabela;\nNo R conseguimos especificar qual planilha de um arquivo .xlsx queremos transferir, porém pode vir a ser um pouco confuso as vezes, então é sugerido deixar todas as suas informações em uma única tabela em uma única planilha;\nPadronização é extremamente importante, salve todos os dados para cada coluna em apenas um determinado formato (Ex.: Coluna Idade - Integer, Coluna Raça - Character), lembrando sempre de manter um padrão de medida (cm, L), variáveis do tipo categórico tambem precisam de padronização (Evite coisas como: Não, nao, n, N, não);\nCuidado ao classificar dados faltantes, uma prática errada é preencher esses dados com 0, isso pode vir a atrapalhar toda sua análise\nFoi citado CPF como forma de identificação, mas pode haver casos em que teremos mais de uma linha contendo um mesmo indivíduo dependendo do nosso tipo de dados. Ou seja, esteja atento para que não haja duplicidade de variável identificadora ou ID."
  },
  {
    "objectID": "estimacao.html#conceitos-básicos-de-probabilidade",
    "href": "estimacao.html#conceitos-básicos-de-probabilidade",
    "title": "5  Estimação",
    "section": "5.1 Conceitos básicos de probabilidade",
    "text": "5.1 Conceitos básicos de probabilidade\nA fazer"
  },
  {
    "objectID": "estimacao.html#estimadores-e-estimativas",
    "href": "estimacao.html#estimadores-e-estimativas",
    "title": "5  Estimação",
    "section": "5.2 Estimadores e estimativas",
    "text": "5.2 Estimadores e estimativas\nAgora que estamos a parte de algumas definições que serão importantes para o entendimento do capítulo, podemos iniciar com o seu conteúdo propriamente dito. Em primeiro lugar, chamamos de estimador qualquer estatística cujos valores são utilizados para se estimar um parâmetro ou uma função de um parâmetro. Dessa forma, temos, portanto, que todo estimador será, também, uma variável aleatória, uma vez que eles são funções das variáveis aleatórias que compõem nossa amostra. Quando coletamos a amostra, observamos os valores das variáveis aleatórias que a compõem e os substituímos na expressão do estimador, obtemos o que chamamos de estimativa. Estimativas não são valores aleatórios, mas sim realizações de variáveis aleatórias (dos estimadores).\nDiversos métodos foram desenvolvidos ao longo dos anos para se encontrar estatísticas que possam ser utilizadas como estimadores. Entre os mais conhecidos, podemos citar: o método dos momentos, que encontra estimadores relacionando os momentos amostrais e populacionais; o método dos mínimos quadrados, a partir do qual encontramos o estimador que minimiza a chamada soma de quadrado dos erros; e o método da máxima verossimilhança, provavelmente o mais conhecido e utilizado, por meio do qual encontramos o estimador que maximiza a probabilidade de a amostra coletada ter sido observada, através da maximização da função de verossimilhança. Não entraremos em maiores detalhes sobre nenhum desses métodos, uma vez que nosso objetivo aqui é introduzir o conceito de estimação de forma mais intuitiva. Entretanto, caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o método da máxima verossimilhança e tudo aquilo que está por trás dele. O post está disponível  neste link .\nO problema da estimação não se resume somente a encontrar estimadores. De fato, existem infinitos estimadores para qualquer que seja o parâmetro que tenhamos interesse. A questão agora é, então, estabelecer critérios que nos permitam determinar o melhor estimador em um certo conjunto. Nesse contexto, podemos definir uma série de propriedades que os estimadores possuem ou podem possuir. São elas:\n\nVício: dizemos que um estimador é não viciado (ou não viesado) se o seu valor esperado coincide com o verdadeiro valor do parâmetro em questão. Em outras palavras, estimadores não viciados acertam, em média, o valor do parâmetro que estão estimando. Caso essa afirmação não seja verdadeira, dizemos que o estimador apresenta vício. O vício é a diferença entre o verdadeiro valor do parâmetro e o valor esperado de seu estimador.\nConsistência: dizemos que um estimador é consistente se, à medida que o tamanho da amostra aumenta, o seu valor esperado converge para o verdadeiro valor do parâmetro em questão e sua variância converge para zero. Dessa forma, estimadores consistentes não necessariamente são não viciados para tamanhos pequenos de amostra: eles só precisam ser não viciados quando esse tamanho é muito grande.\nErro quadrático médio: definimos o erro quadrático médio como sendo o valor esperado da diferença quadrática entre o estimador e o verdadeiro valor do parâmetro estimado. Tal como a variância de uma variável aleatória é uma medida da dispersão de seus valores em torno de sua média, o erro quadrático médio é uma medida da dispersão dos valores do estimador em torno do verdadeiro valor do parâmetro. Dessa forma, estimadores com erros quadráticos médios pequenos são preferíveis. Algo a se notar é que o erro quadrático médio pode ser reescrito como sendo a soma entre o quadrado do vício do estimador e sua variância. Com isso, para estimadores não viciados, o erro quadrático médio se reduz à variância do estimador.\n\nEm um mundo ideal, parece ser intuitivo que nossa busca pelo melhor estimador se dê através do erro quadrático médio, tentando encontrar aquele para o qual essa medida seja a menor possível. Essa tarefa é, entretanto, raramente possível. Em geral, o erro quadrático médio de um estimador é uma função do valor desconhecido do parâmetro a ser estimado, e é muito comum que, para dois estimadores de um parâmetro, seus erros quadráticos médios se entrelassem: para certos valores do parâmetro, o primeiro estimador pode ter o menor erro quadrático médio, enquanto para outros valores o segundo estimador é o que o tem. Com isso, em nossa busca pelo melhor estimador, é comum restringirmos o conjunto de todos os estimadores possíveis à classe dos estimadores não viciados, fazendo com que o erro quadrático médio seja apenas uma função da variância do estimador. Essa restrição é feita porque existem técnicas que nos permitem encontrar, entre os estimadores não viciados, aqueles que possuem a menor variância possível. Não trataremos dessas técnicas ao longo deste livro, mas essa é uma importante consideração para se entender o motivo de, em geral, trabalharmos com estimadores não viciados. De toda forma, passemos, então, para a próxima seção, na qual veremos alguns exemplos de estimadores pontuais utilizando os dados apresentados e tratados em capítulos anteriores."
  },
  {
    "objectID": "estimacao.html#estimação-pontual",
    "href": "estimacao.html#estimação-pontual",
    "title": "5  Estimação",
    "section": "5.3 Estimação pontual",
    "text": "5.3 Estimação pontual\nPara iniciar esta seção, comecemos com uma definição. Chamamos de estimação pontual a técnica de estimação na qual utilizamos um único valor de uma estatística para representarmos, ou estimarmos, o valor desconhecido de um parâmetro de interesse. Chamamos essa estatística de estimador pontual, enquanto ao seu valor observado damos o nome de estimativa pontual. [completar]\n\n5.3.1 Estimação pontual da média de uma população\nVoltando aos exemplos apresentados na introdução do capítulo, suponha, primeiramente, que nosso parâmetro de interesse seja a idade média das gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período de março de 2020 a dezembro de 2021, a qual denotaremos por \\(\\mu\\). Como temos acesso a todos os registros dessa população, o valor desse parâmetro não é desconhecido, mas isso servirá de auxílio para exemplificar os métodos que aqui serão empregados. Criando um vetor contendo todos os elementos da população, temos:\n\ndados <- readr::read_rds(\"dados/dados_covid[LIMPO].rds\")\npopulacao1 <- dados$idade_anos[which(dados$evolucao == \"óbito\")]\nlength(populacao1)\n\n[1] 1266\n\n\nO processo envolvido na criação do vetor populacao1 é o seguinte: dentro do data frame dados, que contém todos os registros de nossa população, estamos selecionando o valor da variável idade_anos de todas as pacientes para as quais o valor da variável evolucao é “óbito”. O tamanho desse vetor, obtido por meio da função length(), do pacote básico {base}, é de 1266. Ou seja, a população de gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período considerado é composta por 1266 elementos. Para calcular o valor da idade média dessas mulheres, podemos utilizar a função mean(), também do pacote básico {base}, que calcula a média aritmética de um dado vetor.\n\nmean(populacao1)\n\n[1] 31.80806\n\n\nA saída do código nos revela o valor de \\(31.81\\) anos. Note que esse valor não representa uma estimativa; ele é, de fato, o verdadeiro valor do parâmetro \\(\\mu\\). O que aconteceria, entretanto, se tivéssemos acesso apenas a uma amostra da população em questão? Poderíamos garantir que os resultados obtidos seriam válidos para todas as gestantes e puérperas desse grupo? É o que começaremos a ver na subseção seguinte.\n\n5.3.1.1 Trabalhando com amostras da população\nPara exemplificar os conceitos de estimação definidos anteriormente, simularemos a retirada de amostras da população de gestantes e puérperas com a qual estamos trabalhando. Dentre as várias maneiras de se obter amostras de uma população, utilizaremos, aqui, a amostragem aleatória simples (AAS) com reposição, uma técnica de amostragem probabilística (ou seja, que atribui a cada elemento da população uma probabilidade, conhecida a priori, de pertencer à amostra), na qual todos os elementos da população possuem a mesma probabilidade de serem sorteados. Utilizaremos a AAS com reposição, que admite a possibilidade de um elemento ser selecionado mais de uma vez, por sua maior simplicidade teórica e por algumas implicações matemáticas e estatísticas que ela carrega, como a independência entre as unidades sorteadas.\nAntes da realização da amostragem, denotamos as variáveis a serem selecionadas por \\(X_1, X_2, ..., X_{n}\\), sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera hospitalizada pela COVID-19 e que faleceu em decorrência dessa doença, com \\(i = 1, 2, ..., n\\). Dizemos que essa sequência de variáveis aleatórias forma uma amostra aleatória de tamanho \\(n\\). Sendo, novamente, \\(\\mu\\) o parâmetro que representa a idade média da população em questão, e denotando por \\(\\sigma^2\\) o parâmetro que representa a variância populacional das idades dessas gestantes e puérperas, temos, ainda, que \\(E(X_i) = \\mu\\) e que \\(Var(X_i) = \\sigma^2\\).\nDentro do R, podemos obter uma amostra aleatória de tamanho, digamos, \\(n = 30\\), a partir da função sample(), do pacote básico {base}. Utilizaremos três argumentos dessa função: o primeiro, x, recebe o vetor de elementos do qual a amostra será retirada; o segundo, size, recebe o número de itens a serem sorteados; por fim, o terceiro argumento, replace, receberá o valor TRUE, indicando que a amostragem deve ser realizada com reposição. O código utilizado para a realização desse processo, bem como a amostra obtida, podem ser vistos abaixo.\n\nset.seed(43)\namostra1 <- sample(x = populacao1, size = 30, replace = TRUE)\namostra1\n\n [1] 32 27 37 38 39 32 30 24 29 30 34 40 33 17 34 35 32 38 23 24 33 38 18 25 26\n[26] 32 25 36 29 32\n\n\nÉ importante ressaltar que, enquanto \\(X_1, X_2, ..., X_{30}\\) são variáveis aleatórias, os valores guardados no vetor amostra1 representam realizações das mesmas. Dessa forma, temos, para a amostra sorteada, que \\(x_1 = 32\\), \\(x_2 = 27\\), \\(x_3 = 37\\) e assim por diante. Essas realizações seriam, muito provavalmente, diferentes caso desempenhássemos o procedimento de retirada da amostra novamente. No presente caso, o código acima sempre resultará nos mesmos elementos, uma vez que estamos fixando a semente inicial do gerador de números pseudo-aleatórios do R por meio da função set.seed(), do pacote básico {base}. Desfixando a semente inicial do sorteio, entretanto, o resultado obtido através da função sample() seria diferente a cada vez que rodássemos o bloco de código. Observe.\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 33 27 17 27 24 28 29 30 32 32 31 41 25 30 26 41 26 27 33 27 33 24 39 36\n[26] 30 40 32 28 23\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 42 32 29 32 30 30 39 35 48 32 26 27 42 23 29 31 37 27 32 25 41 43 22 21\n[26] 30 34 22 30 37\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 22 33 32 30 30 31 36 37 37 23 28 33 38 40 21 23 38 33 23 37 27 31 43 39 41\n[26] 36 35 39 39 45\n\n\nCom a distinção entre variáveis aleatórias e suas realizações em mente, precisamos, nesse próximo caso, definir qual estimador utilizaremos para estimarmos o parâmetro em questão. Como temos interesse na idade média da população, uma escolha muito intuitiva seria utilizar a média aritmética dos valores amostrados como uma estimativa do valor desse parâmetro. Assim, considerando que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória de tamanho \\(n\\) dessa população, definimos o estimador da média amostral como sendo a estatística dada por\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{n}}{n} = \\frac{\\sum_{i = 1}^{n} X_i}{n}.\n\\]\nAlém de muito intuitivo, esse estimador é, também, não viciado e consistente. Podemos facilmente demonstrar essas propriedades de forma matemática, utilizando para isso propriedades de valores esperados. Quanto a ser não viciado, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da esperança o multiplicando:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nSendo a esperança da soma de variáveis aleatórias equivalente à soma das esperanças marginais de cada variável, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right)\n\\]\nPor fim, como \\(E(X_i) = \\mu\\),\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mu = \\frac{1}{n} (n\\mu) = \\mu.\n\\]\nLogo, como o valor esperado do estimador é igual ao parâmetro que ele estima, concluímos que \\(\\bar{X}\\) é um estimador não viciado. Para demonstrarmos que ele é, também, consistente, precisamos calcular sua variância. Assim,\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da variância elevando-o ao quadrado:\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nComo utilizamos a AAS com reposição para a retirada da amostra, garantimos que as variáveis aleatórias que a compõem são independentes. Assim, sendo a variância da soma de variáveis aleatórias independentes dada pela soma das variâncias marginais de cada variável, e como \\(Var(X_i) = \\sigma^2\\), temos:\n\\[\n\\begin{align}\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(X_i) \\\\ & = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\sigma^2 \\\\ & = \\frac{1}{n^2} (n \\sigma^2) \\\\ & = \\frac{\\sigma^2}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\bar{X}\\). Essa informação, aliada ao fato de \\(\\bar{X}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Voltando ao exemplo em que estávamos, como definimos que \\(n = 30\\), a expressão do estimador da média amostral se torna\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{30}}{30} = \\frac{\\sum_{i = 1}^{30} X_i}{30}.\n\\]\nSubstituindo os valores da amostra coletada e calculando sua média aritmética, obtemos:\n\nmean(amostra1)\n\n[1] 30.73333\n\n\nCom isso, concluímos que uma estimativa pontual para a idade média das gestantes e puérperas que faleceram em decorrência da COVID-19 é de \\(30.73\\). Esse valor é relativamente próximo do verdadeiro valor do parâmetro, o qual sabemos ser 31.81. Lembre-se, entretanto, que a estimativa obtida depende diretamente da amostra que foi coletada, uma vez que nosso estimador é uma função da amostra e, portanto, é uma variável aleatória. A cada vez que realizássemos um novo sorteio, o valor de nossa estimativa seria, muito provavelmente, diferente do anterior. Como nosso objetivo é fazer uma afirmação sobre o parâmetro \\(\\mu\\) a partir da amostra coletada, é interessante considerar que a validade dessa afirmação seria melhor compreendida se soubéssemos o que acontece com nosso estimador quando retiramos todas as amostras de mesmo tamanho possíveis de nossa população. Retomaremos essa discussão posteriormente. Buscaremos, agora, estimar um outro tipo de parâmetro: a proporção populacional.\n\n\n\n5.3.2 Estimação pontual da proporção populacional\nPara o segundo exemplo, suponha que o parâmetro no qual temos interesse seja a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas. Representaremos esse parâmetro por \\(p\\). Assim como no exemplo anterior, podemos calcular seu valor, uma vez que temos acesso a todos os registros dessa população. Note que, como estamos tratando da proporção válida, precisamos que nossa população seja composta apenas pelas mulheres para as quais o valor da variável diarreia foi preenchido de forma válida (ou seja, com sim ou não). Assim, temos:\n\npopulacao2 <- dados$diarreia[which(!is.na(dados$diarreia) & dados$diarreia != \"ignorado\")]\nlength(populacao2)\n\n[1] 8472\n\nhead(populacao2, 20)\n\n [1] \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n[13] \"não\" \"sim\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n\n\nObservando as saídas acima, podemos notar que nossa população é formada por 8.472 elementos, que assumem valor sim, quando a gestante ou puérpera apresentou diarreia como um dos sintomas da COVID-19, e não, quando esse sintoma não foi apresentado. Para facilitar nosso trabalho a partir daqui, transformaremos os valores sim em 1 e os valores não em 0, utilizando para isso a função ifelse(), do pacote básico {base}. Essa função recebe três argumentos: o primeiro, test, recebe um vetor lógico; o segundo, yes, recebe o valor que a função deve retornar quando o dado elemento desse vetor lógico for verdadeiro; por fim, o terceiro argumento, no, recebe o valor que a função deve retornar quando o dado elemento do vetor lógico for falso. Observe o código e a saída abaixo.\n\npopulacao2_transformada <- ifelse(populacao2 == \"sim\", yes = 1, no = 0)\nhead(populacao2_transformada, 20)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n\n\nCalculando, por fim, a proporção desejada, que nada mais será do que a média aritmética do vetor populacao2_transformada, uma vez que ele é formado por zeros e uns, temos:\n\nmean(populacao2_transformada)\n\n[1] 0.128305\n\n\nLogo, o valor do parâmetro \\(p\\) - a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas - é de 0,128, ou 12,8%. Vamos, agora, fingir que não tínhamos acesso a métodos de se calcular o valor desse parâmetro, tentando novamente estimá-lo por meio da coleta de amostras da população.\n\n5.3.2.1 Trabalhando com amostras de uma população com distribuição Bernoulli\nDiferentemente do que ocorria com o exemplo anterior, a amostra aleatória que agora coletaremos será composta por variáveis aleatórias para as quais sabemos a “forma” de sua distribuição de probabilidade. Podemos definir \\(Y_1, Y_2, ..., Y_{n}\\) como sendo uma amostra aleatória da distribuição Bernoulli com parâmetro \\(p\\), na qual \\(Y_i\\) recebe o valor 1, caso a \\(i\\)-ésima gestante ou puérpera sorteada tenha apresentado diarreia como um dos sintomas da COVID-19 (sucesso), e 0, caso contrário (fracasso), com \\(i = 1, 2, ..., n\\). O parâmetro \\(p\\) representa a probabilidade de sucesso (que sabemos ser de 0,128, apesar de estarmos fingindo que não temos essa informação). Como queremos estimar uma proporção, é intuitivo considerarmos como estimador a proporção das gestantes ou puérperas da amostra que apresentaram diarreia como sintoma. Assim, definimos o estimador da proporção amostral, denotado por \\(\\hat{p}\\), como sendo\n\\[\n\\hat{p} = \\frac{Y_1 + Y_2 + ... + Y_n}{n} = \\frac{\\sum_{i = 1}^{n} Y_i}{n}.\n\\]\nDe maneira similar ao que fizemos com o estimador da média amostral, \\(\\bar{X}\\), podemos demonstrar que o estimador da proporção amostral é, também, não viciado e consistente. Quanto à primeira propriedade, sabendo que \\(E(Y_i) = p\\), para \\(i = 1, 2, ..., n\\), temos:\n\\[\nE\\left(\\hat{p}\\right) = E \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n} \\sum_{i = 1}^{n} E\\left(Y_i \\right) =  \\frac{1}{n}  \\sum_{i = 1}^{n} p = \\frac{1}{n} (np) = p.\n\\]\nAssim, como \\(E(\\hat{p}) = p\\), podemos concluir que o estimador da proporção amostral é não viesado. Em outras palavras, esse estimador “acerta”, em média, o verdadeiro valor do parâmetro \\(p\\). Para a segunda propriedade, precisamos, primeiramente, calcular a variância de \\(\\hat{p}\\). Como \\(Y_i\\) segue distribuição \\(Bernoulli(p)\\), sabemos que \\(Var(Y_i) = p(1 - p)\\). Dessa forma, temos que\n\\[\n\\begin{align}\nVar(\\hat{p}) = Var \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(Y_i) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} p(1 - p) \\\\ &  = \\frac{1}{n^2} \\left[np(1 - p)\\right] \\\\ &  = \\frac{p(1 - p)}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\hat{p}\\). Essa informação, aliada ao fato de \\(\\hat{p}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Investigadas as propriedades do estimador, podemos partir para a retirada da amostra, utilizando novamente a função sample() para simular uma amostra de tamanho \\(n = 30\\) obtida por meio da AAS com reposição. A amostra coletada pode ser vista abaixo.\n\nset.seed(43)\namostra2 <- sample(x = populacao2_transformada, size = 30, replace = TRUE)\namostra2\n\n [1] 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n\n\nAplicando os valores obtidos no estimador da proporção amostral, que nada mais é do que a média aritmética da amostra, temos:\n\nmean(amostra2)\n\n[1] 0.1333333\n\n\nCom isso, concluímos que uma estimativa pontual para a proporção válida de gestantes e puérperas hospitalizadas pela COVID-19 no período em estudo e que apresentaram diarreia como um dos sintomas da doença é de 0,133, ou de 13,3%. Novamente, essa estimativa depende diretamente da amostra obtida; novas amostragens quase certamente resultariam em estimativas diferentes para o parâmetro. Com isso, volta à tona a reflexão levantada no final do exemplo anterior, de que a validade de nossa afirmação sobre o verdadeiro valor de \\(p\\) seria melhor compreendida caso levássemos em consideração a distribuição de nosso estimador, \\(\\hat{p}\\). Conseguiríamos estudar o comportamento probabilístico de \\(\\hat{p}\\) caso aumentássemos o tamanho da amostra? A resposta, já adiantando, é sim. O que utilizamos para realizar esse estudo, entretanto, será visto na próxima seção.\n\n\n\n5.3.3 A distribuição amostral de estimadores\nComo vimos ao longo das seções anteriores, o problema da Inferência Estatística que queremos resolver consiste em fazer uma afirmação sobre um certo parâmetro de uma determinada população por meio de uma amostra. Para encará-lo, decidimos que nossa afirmação será baseada em uma certa estatística \\(T\\), para a qual demos o nome de estimador, que será uma função da amostra (\\(X_1, X_2, ..., X_n\\)). Quando coletamos a amostra, podemos obter um valor particular de \\(T\\), digamos \\(t_0\\), para o qual demos o nome de estimativa. E é com base nesse valor \\(t_0\\) que faremos a afirmação sobre o parâmetro de interesse. Para entendermos melhor a incerteza por trás de nossa afirmação, entretanto, seria de nosso interesse determinar qual é a distribuição de \\(T\\) quando a amostra, \\(X_1, X_2, ..., X_n\\), assume todos os valores possíveis. Chamamos essa distribuição de distribuição amostral da estatística T. Bussab e Moretin (referência) esquematizam o procedimento para a obtenção da distribuição amostral da seguinte maneira:\n\nA partir de uma determinada população \\(X\\), com certo parâmetro de interesse \\(\\theta\\), obtemos todas as amostras possíveis com um mesmo tamanho amostral \\(n\\), de acordo com uma certa técnica de amostragem;\nPara cada amostra obtida, calculamos o valor \\(t\\) da estatística \\(T\\);\nOs valores \\(t\\) formam uma nova população, cuja distribuição recebe o nome de distribuição amostral de \\(T\\).\n\nÉ muito comum, no entanto, que não sejamos capazes de coletar todas as amostras possíveis de uma população. Com isso, acabamos tendo que nos contentar em simular um número grande de amostras, para assim termos uma ideia do que acontece com a estatística de interesse. Para melhor entendermos as ideias apresentadas, consideremos os estimadores \\(\\bar{X}\\), a média amostral, e \\(\\hat{p}\\), a proporção amostral. Nos exemplos antecedentes, acabamos determinando, talvez sem perceber, a média e a variância das distribuições amostrais de ambos os estimadores quando estávamos demonstrando duas de suas propriedades - a falta de vício e a consistência. Retomando os resultados obtidos, encontramos que\n\n\\(E(\\bar{X} = \\mu)\\) e \\(Var(\\bar{X}) = \\displaystyle \\frac{\\sigma^2}{n}\\);\n\\(E(\\hat{p} = p)\\) e \\(Var(\\hat{p}) = \\displaystyle \\frac{p(1 - p)}{n}\\).\n\nMédias e variâncias não são, todavia, tudo aquilo que precisamos para determinar a distribuição amostral de estimadores. Precisamos, também, determinar sua “forma”. Para isso, coletaremos várias amostras e construiremos histogramas das distribuições de \\(\\bar{X}\\) e \\(\\hat{p}\\) para diferentes tamanhos de amostra. Comecemos simulando \\(M\\) = 100 amostras, cada uma com tamanho \\(n\\) = 15, da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo, a qual chamamos de populacao1. Utilizaremos para isso a função replicate(), do pacote básico {base}. Essa função recebe dois argumentos: o primeiro, n, recebe o número de replicações a serem feitas, enquanto o segundo, expr, recebe a expressão que será replicada. O resultado, guardado no objeto amostras_pop1, é uma matriz na qual o elemento \\([a_{ij}]\\) representa o \\(i\\)-ésimo elemento da \\(j\\)-ésima amostra, com \\(i = 1, 2, ..., 15\\) e \\(j = 1, 2, ..., 100\\). As cinco primeiras colunas dessa matriz podem ser vistas abaixo.\n\nset.seed(43)\nM <- 100\nn <- 15\namostras_pop1 <- replicate(M, expr = sample(x = populacao1, size = n, replace = TRUE))\namostras_pop1[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]   32   35   33   26   33\n [2,]   27   32   33   41   42\n [3,]   37   38   27   26   32\n [4,]   38   23   17   27   29\n [5,]   39   24   27   33   32\n [6,]   32   33   24   27   30\n [7,]   30   38   28   33   30\n [8,]   24   18   29   24   39\n [9,]   29   25   30   39   35\n[10,]   30   26   32   36   48\n[11,]   34   32   32   30   32\n[12,]   40   25   31   40   26\n[13,]   33   36   41   32   27\n[14,]   17   29   25   28   42\n[15,]   34   32   30   23   23\n\n\nRepetimos o mesmo processo para a população de gestantes e puérperas com preenchimento válido da variável diarreia, que chamamos de populacao2_transformada. Novamente, as cinco primeiras colunas da matriz de amostras, que agora denominamos amostras_pop2, podem ser vistas abaixo.\n\nset.seed(43)\namostras_pop2 <- replicate(M, expr = sample(x = populacao2_transformada, size = n, replace = TRUE))\namostras_pop2[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    0    0\n [2,]    0    0    0    0    0\n [3,]    0    1    0    0    0\n [4,]    0    0    0    0    0\n [5,]    0    0    0    1    1\n [6,]    0    0    0    0    0\n [7,]    0    0    0    0    0\n [8,]    1    0    0    0    0\n [9,]    1    0    0    0    0\n[10,]    0    0    0    1    0\n[11,]    0    0    0    0    0\n[12,]    0    0    0    1    1\n[13,]    0    0    0    0    0\n[14,]    0    1    0    0    0\n[15,]    0    0    0    1    0\n\n\nCom as amostras em mãos, o próximo passo é calcular o valor do respectivo estimador em cada uma delas. Realizaremos esse processo com a função apply(), também do pacote básico {base}, a qual permite que apliquemos qualquer função em todas as linhas ou colunas de uma matriz. Utilizaremos três de seus argumentos: o primeiro, X, recebe a matriz na qual queremos aplicar a função; o segundo, MARGIN, recebe a direção em que a função será aplicada (1 caso queiramos que a função seja aplicada nas linhas da matriz, ou 2 caso queiramos a aplicar em suas colunas); por fim, o terceiro, FUN, recebe a função que queremos aplicar. O código utilizado nesse processo, bem como parte dos vetores obtidos, podem ser vistos abaixo.\n\nx_barras <- apply(X = amostras_pop1, MARGIN = 2, FUN = mean)\nhead(x_barras)\n\n[1] 31.73333 29.73333 29.26667 31.00000 33.33333 30.73333\n\np_chapeus <- apply(X = amostras_pop2, MARGIN = 2, FUN = mean)\nhead(p_chapeus)\n\n[1] 0.1333333 0.1333333 0.0000000 0.2666667 0.1333333 0.2000000\n\n\nPor fim, criemos os histogramas da distribuição de cada estimador. Como já discutimos sobre a criação de histogramas no capítulo de Estatística Descritiva, o código abaixo deve ser familiar.\n\nlibrary(ggplot2)\n\nggplot(data.frame(x_barra = x_barras), aes(x = x_barra))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"lightblue\",\n    bins = 15,\n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(26, 36)) +\n  labs(\n    x = \"Idade média das gestantes e puérperas\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral da médias amostrais para n = 15\"\n    ) +\n  geom_vline(xintercept = mean(x_barras), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 34, \n    y = 0.3,\n    label = paste(\"Valor médio das estimativas:\", round(mean(x_barras), 3))\n    )\n\n\n\nggplot(data.frame(p_chapeu = p_chapeus), aes(x = p_chapeu))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"steelblue\", \n    bins = 15, \n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(0, 0.5)) +\n  labs(\n    x = \"Proporção de gestantes ou puérperas com diarreia como sintoma\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral das proporções amostrais para n = 15\"\n    ) + \n  geom_vline(xintercept = mean(p_chapeus), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 0.23, \n    y = 9,\n    label = paste(\"Valor médio das estimativas:\", round(mean(p_chapeus), 3))\n    )\n\n\n\n\nObservando os histogramas, podemos notar que, mesmo para um tamanho de amostra pequeno como \\(n\\) = 15, a distribuição de \\(\\bar{X}\\) se assemelha à distribuição normal, visto que apresenta o característico formato aproximado de sino e uma quase simetria em torno de sua média. Essa combinação de fatores nos sugere que \\(X_1, X_2, ..., X_{15}\\), as variáveis aleatórias que compõem as amostras da população de idades, seguem, também, uma distribuição simétrica em torno da média. Além disso, o valor médio das estimativas \\(\\bar{x}\\), de 31,843, está muito próximo do verdadeiro valor do parâmetro populacional \\(\\mu\\), que sabemos ser de 31,81 anos. Esse resultado já era esperado, uma vez que a distribuição de \\(\\bar{X}\\) está centrada em \\(\\mu\\). Pouco podemos dizer, entretanto, do histograma da distribuição amostral de \\(\\hat{p}\\) até o momento; apenas que sua média está muito próxima do verdadeiro valor de \\(p\\), que sabemos ser de 0,128, como já era esperado pelo mesmo motivo. Aumentemos, então, o tamanho das amostras, e observemos os resultados obtidos. Como a única modificação será o valor da variável n, ocultaremos os códigos utilizados para evitar uma maior poluição visual. Dessa forma, para \\(n\\) = 30, temos:\n\n\n\n\n\n\n\n\nPara \\(n = 50\\),\n\n\n\n\n\n\n\n\nPara \\(n = 100\\),\n\n\n\n\n\n\n\n\nPor fim, para \\(n = 200\\),\n\n\n\n\n\n\n\n\nObserve que, conforme aumentamos o tamanho das amostras, os histogramas de ambos os estimadores tendem a se concentrar cada vez mais em torno de suas respectivas médias, uma vez que as variância das estimativas se torna cada vez menor. Dessa forma, podemos concluir que estimativas obtidas a partir de tamanhos de amostra maiores têm uma maior probabilidade de “acertarem” o verdadeiro valor do parâmetro que estão estimando. É também notável que mesmo os histogramas das proporções amostrais aparentam convergir para o formato da distribuição normal conforme o valor de \\(n\\) aumenta. Esse fato, por incrível que pareça, não é coincidência: ele é decorrência direta do Teorema Central do Limite (TCL), o qual afirma que, independente da distribuição da população, quanto maior o tamanho amostral, mais próxima será a distribuição amostral da média de uma distribuição normal. Vale lembrar que a proporção amostral nada mais é do que um caso particular da média amostral em que os valores observados na amostra contém apenas zeros e uns, o que explica a aplicação do TCL nesse caso. Para sermos mais precisos, podemos dizer de forma aproximada que, para tamanhos suficientemente grandes de amostra,\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right) \\text{ e } \\hat{p}\\sim N\\left(p, \\frac{p(1-p)}{n} \\right).\n\\]\nCom isso, aqui terminamos o conteúdo referente à estimação pontual."
  },
  {
    "objectID": "estimacao.html#estimação-intervalar",
    "href": "estimacao.html#estimação-intervalar",
    "title": "5  Estimação",
    "section": "5.4 Estimação intervalar",
    "text": "5.4 Estimação intervalar\nAté este ponto, todos os estimadores que apresentamos e discutimos são pontuais, uma vez que fornecem um único valor como estimativa para o parâmetro de interesse. Estimativas pontuais, por mais úteis que sejam, acabam fornecendo uma informação incompleta sobre o valor estimado do parâmetro em questão. Como estimadores são variáveis aleatórias e, portanto, possuem uma distribuição de probabilidade, seria de nosso interesse que a estimativa a ser apresentada levasse em consideração uma medida de seu possível erro. Essa essa medida pode ser, por exemplo, um intervalo relacionado à dimensão da confiança que temos de que o verdadeiro valor do parâmetro está sendo captado. Dessa forma, a partir daqui, entramos no campo da estimação intervalar. Dentro da Inferência Clássica, que estamos estudando neste capítulo, estimativas intervalares se dão a partir dos chamados intervalos de confiança. Intervalos de confiança incorporam, à estimativa pontual do parâmetro, informações a respeito da variabilidade do estimador. Além disso, eles são obtidos através da distribuição amostral de seus estimadores, o que justifica ainda mais o conteúdo que vimos na última subseção de estimação pontual.\nComo o intuito deste livro não é conter uma porção pesada de teoria, introduziremos o conceito de intervalos de confiança a partir de exemplos, realizando explicações sobre os elementos envolvidos em sua construção conforme seja necessário. Caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o melhor a teoria por trás dos intervalos de confiança, que conta também com o detalhamento de um dos principais métodos utilizados para a construção desses intervalos: o método da quantidade pivotal. O post está disponível  neste link . Com isso em mente, prossigamos para nosso primeiro exemplo: a criação de intervalos de confiança para a média amostral.\n\n5.4.1 Intervalos de confiança para a média amostral quando a variância populacional é conhecida\nUtilizando o exemplo já apresentado na seção anterior, considere que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Denotando, novamente, por \\(\\mu\\) a média populacional das idades dessas mulheres, e por \\(\\sigma^2\\) a variância populacional dessas idades, temos, ainda que \\(E(X_i) = \\mu\\) e \\(Var(X_i) = \\sigma^2\\). Suponha que queiramos estimar o valor de \\(\\mu\\), utilizando para isso o estimador \\(\\bar{X}\\). Suponha também, neste primeiro exemplo, que o valor de \\(\\sigma^2\\) é conhecido. Note que não estamos fazendo nenhuma suposição sobre a distribuição de probabilidade dessas variáveis. Dessa forma, precisaremos, a partir deste ponto, impor uma restrição: o tamanho da amostra deve ser grande o suficiente para que possamos aplicar o Teorema Central do Limite. Caso essa restrição seja cumprida, sabemos, por meio do TCL e de forma aproximada, que\n\\[\n\\bar{X} \\sim N \\left(\\mu, \\frac{\\sigma^2}{n} \\right).\n\\]\nSubtraindo de uma variável aleatória a sua média e dividindo o resultado por seu desvio padrão, obtemos o que chamamos de variável aleatória padronizadas. Uma variável aleatória padronizada tem média igual a zero e variância igual a um. Aplicando esse resultado em nosso estimador, \\(\\bar{X}\\), obtemos uma nova variável, a qual chamaremos de \\(Z\\), cuja distribuição estará totalmente definida, o que será de grande utilidade na construção de nosso intervalo. Observe.\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\] Como conhecemos a distribuição de probabilidade de \\(Z\\), podemos, para um certo valor \\(\\alpha\\), com \\(0 < \\alpha < 1\\), encontrar valores \\(z_1\\) e \\(z_2\\), com \\(z_1 < z_2\\), tais que\n\\[\nP(z_1 < Z < z_2) = 1 - \\alpha.  \\qquad \\qquad \\text{(I)}\n\\]\nChamamos o valor \\(1 - \\alpha\\) de coeficiente de confiança. Sua interpretação será feita posteriormente. Quanto à probabilidade acima, note que existem infinitos valores de \\(z_1\\) e \\(z_2\\) que a satisfazem. Como queremos encontrar um intervalo que contenha os valores mais plausíveis do parâmetro em estudo, é de nosso interesse que a amplitude desse intervalo seja a menor possível, sendo a amplitude de um intervalo definida como a diferença entre seus extremos superior e inferior. E, para que esse interesse seja cumprido, é necessário que os valores de \\(z_1\\) e \\(z_2\\) sejam os mais próximos possíveis. Para distribuições simétricas em torno do zero, como é o caso da distribuição normal padrão, podemos mostrar que a amplitude do intervalo será mínima se os valores de \\(z_1\\) e \\(z_2\\) forem opostos, ou seja, se \\(z_1 = -z_2\\). Com isso, precisamos apenas encontrar um valor \\(z\\) tal que\n\\[\nP(Z \\leqslant z) = 1 - \\frac{\\alpha}{2}\n\\]\nA este valor, o qual denotamos por \\(z_{1 - \\alpha/2}\\), damos o nome de quantil de ordem \\(1 - \\alpha/2\\). Um quantil de ordem \\(k\\) de uma variável aleatória, com \\(0 < k < 1\\), nada mais é que o ponto tal que, quando nele aplicada a função de distribuição acumulada da variável, a probabilidade obtida é igual a \\(k\\) (a ordem que o quantil representa). Em uma situação prática, na qual teríamos um valor definido de \\(\\alpha\\), poderíamos utilizar uma tabela da distribuição normal padrão para encontrar o valor de \\(z_{1 - \\alpha/2}\\), ou mesmo utilizar a função qnorm(), do pacote básico {stats}, para realizar esse processo. A função qnorm(), bem como a família de funções do R que seguem a estrutura “qnome_da_distribuição()”, representa a função quantílica: para uma dada probabilidade e para dados valores dos parâmetros da distribuição, a função retorna o quantil cuja ordem é a probabilidade estipulada em seus arguementos. Com isso em mente, podemos reescrever \\(z_1\\) e \\(z_2\\) como sendo\n\\[\nz_1 = -z_{1 - \\alpha/2} \\text{ e } z_2 = z_{1 - \\alpha/2}.\n\\]\nPara que a explicação acima seja melhor absorvida, observe o gráfico a seguir, que representa a curva da densidade de probabilidade da distribuição normal padrão.\n\n\n\n\n\nFigura 1: Para uma confiança de (100 - \\(\\alpha\\))%, a área em cada cauda da distribuição deverá ser de \\(\\alpha\\)/2 para que o intervalo seja o menor possível.\n\n\n\n\nVoltando à probabilidade definida em \\(\\text{(I)}\\), a atualizando com os resultados obtidos e reescrevendo \\(Z\\), temos:\n\\[\nP(z_1 < Z < z_2) = P \\left(-z_{1 - \\alpha/2} < \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma}  < z_{1 - \\alpha/2} \\right)  = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-z_{1 - \\alpha/2} < \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} < z_{1 - \\alpha/2} \\right) \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\sigma < \\sqrt{n}\\left(\\bar{X} - \\mu\\right) < z_{1 - \\alpha/2}\\sigma \\right)  \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} < \\bar{X} - \\mu < z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\  &\n= P\\left(-\\bar{X} + -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} < - \\mu < -\\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\ & = P\\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar{X} - -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\] Portanto, quando a variância populacional é conhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\; \\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nA interpretação do resultado acima deve ser feita com cuidado. É preciso entender que a expressão \\(IC(\\mu,\\ 1 - \\alpha)\\) envolve uma variável aleatória, \\(\\bar{X}\\), fazendo com que o intervalo obtido também seja aleatório. Para o intervalo aleatório encontrado acima, podemos dizer que a probabilidade aproximada de ele conter o verdadeiro valor do parâmetro \\(\\mu\\) é de \\(1 - \\alpha\\). Aproximada, nesse caso, porque estamos utilizando o TCL para fazer uma aproximação da distribuição de probabilidade de \\(\\bar{X}\\); caso a população seguisse distribuição normal, essa probabilidade seria exata. De qualquer forma, quando coletamos a amostra e observamos uma estimativa \\(\\bar{x}\\), obtemos um intervalo numérico, que chamamos de intervalo de confiança observado. A partir desse ponto, não existem mais quantidades aleatórias na expressão do intervalo, uma vez que, na Inferência Clássica, os parâmetros, por mais que possam ser desconhecidos, são quantidades fixas. Dessa forma, não podemos mais afirmar que um intervalo de confiança observado possui probabilidade \\(1 - \\alpha\\) de conter o verdadeiro valor do parâmetro. Podemos apenas dizer que temos uma confiança considerável de que esse intervalo contém o verdadeiro valor do parâmetro. A medida da nossa confiança é de \\(1 - \\alpha\\) porque, antes de colhermos a amostra, \\(1 - \\alpha\\) era a probabilidade aproximada de que o intervalo aleatório contivesse o verdadeiro valor de \\(\\mu\\).\nComo a distinção entre confiança e probabilidade pode ser difícil de se entender, uma interpretação conveniente para intervalos de confiança é a seguinte: se obtivéssemos várias amostras de mesmo tamanho e, para cada uma delas, calculássemos os correspondentes intervalos de confiança com coeficiente de confiança \\(1 - \\alpha\\), esperaríamos que a proporção de intervalos que contivessem o verdadeiro valor do parâmetro fosse igual a \\(1 - \\alpha\\).\nPor fim, antes de realizarmos um exemplo numérico, podemos fazer algumas considerações a respeito da escolha do valor de \\(\\alpha\\). É possível mostrar que, conforme aumentamos o coeficiente de confiança, a amplitude do intervalo aumenta. Isso, claro, é algo que deveríamos esperar, visto que intervalos maiores possuem naturalmente uma maior chance de conterem um valor desconhecido. Com isso, para que os intervalos sejam o mais informativo possível, mantendo uma confiança elevada, é necessário que selecionemos \\(\\alpha\\) de forma balanceada, sendo uma escolha muito comum o valor 0,05 (nesse caso, temos que o coeficiente de confiança é de 0,95). É muito mais interessante, por exemplo, um intervalo de confiança que diz que o verdadeiro valor do salário médio de um estatístico está entre 3,5 a 6 salários mínimos do que um intervalo que diz que esse valor está entre 2 a 7,5 salários mínimos. Apesar de, com o segundo intervalo, termos uma maior confiança de que o verdeiro valor do salário médio está sendo captado, a qualidade da informação que extraímos dele é consideravelmente pior do que aquela obtida com o primeiro intervalo.\n\n5.4.1.1 Um exemplo numérico\nSubstituindo as letras por números, e continuando o exemplo em que estávamos, vamos, agora, obter, através do R, um intervalo de 95% confiança para \\(\\mu\\). Lembre-se que aqui \\(\\mu\\) é a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021. Como precisamos que nossa amostra seja suficientemente grande para que possamos aplicar o TCL, utilizaremos \\(n = 50\\), uma vez que, observando os histogramas criados na seção de Distribuição Amostral, a distribuição de \\(bar{X}\\) já se aproxima satisfatóriamente bem da distribuição normal a partir desse ponto. Além disso, como a expressão do intervalo de confiança obtido acima leva em consideração que a variância populacional é conhecida, precisaremos, também, dessa informação. Para calculá-la, teremos de multiplicar o resultado da função var() por \\(\\frac{N - 1}{N}\\), sendo \\(N\\) o tamanho da população, uma vez que essa função utiliza \\(N - 1\\) como denominador para o cálculo da variância. Observe o código abaixo.\n\nsigma2 <- var(populacao1) * (length(populacao1) - 1)/length(populacao1)\nsigma2\n\n[1] 45.97659\n\n\nObtendo uma amostra de tamanho \\(n = 50\\) por meio da função sample(), e a armazenando no vetor amostra_ic_media, temos:\n\nset.seed(34)\namostra_ic_media <- sample(x = populacao1, size = 50, replace = TRUE)\nhead(amostra_ic_media, 20)\n\n [1] 26 39 32 35 35 35 31 21 25 40 33 32 19 22 19 30 40 31 38 36\n\n\nNesse próximo passo, criaremos uma função, a qual chamaremos de ic_media_caso1(), que calculará intervalos de confiança para o parâmetro \\(\\mu\\) quando a variância populacional é conhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função possuirá três argumentos: dados, que receberá o vetor de valores observados na amostra; sigma, que receberá o valor do desvio padrão populacional; e alpha, que receberá o valor necessário para se obter o coeficiente de confiança associado ao intervalo, que nesse caso será de 0,05. Utilizaremos a já explicada função qnorm() para encontrar o valor do quantil de ordem \\(1 - \\alpha\\) da normal padrão (não utilizaremos os argumentos mean e sigma dessa função, uma vez que seus valores padrões são, respectivamente, 1 e 0). O limite inferior do intervalo será guardado no vetor limite_inferior, enquanto o superior será guardado em limite_superior. A função retornará um data frame contendo algumas medidas referentes à amostra e o intervalo de confiança propriamente dito. O resultado do processo pode ser visto abaixo.\n\nic_media_caso1 <- function(amostra, sigma, alfa) {\n  media <- mean(amostra)\n  n <- length(amostra)\n  z <- round(qnorm(p = 1 - alfa/2), 3)\n  limite_inferior <- round(media - z * sigma/sqrt(n), 3)\n  limite_superior <- round(media + z * sigma/sqrt(n), 3)\n  amplitude <- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n, \n      estimativa_pontual = media,\n      limite_inferior, \n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso1(amostra = amostra_ic_media, sigma = sqrt(sigma2), alfa = 0.05)\n\n   n estimativa_pontual limite_inferior limite_superior amplitude\n1 50              32.16          30.281          34.039     3.758\n\n\nPortanto, um intervalo de 95% de confiança para a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo é de (28,401; 32,159). Intervalos de confiança podem, inclusive, ser utilizados como substitutos para testes de hipóteses, dos quais falaremos no próximo capítulo. Caso quiséssemos, por exemplo, testar a hipótese de que o valor de \\(\\mu\\) é igual a 27, rejeitaríamos a hipótese nula sob um nível de significância de 0,05, uma vez que 27 não está contido no intervalo de 95% de confiança para \\(\\mu\\) obtido acima.\n\n\n\n5.4.2 Intervalos de confiança para a média amostral quando a variância populacional é desconhecida\nConsidere, assim como no exemplo anterior, que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Suponha que queiramos, novamente, estimar o valor de \\(\\mu\\), mas que agora o valor da variância populacional \\(\\sigma^2\\) é desconhecido. Ainda com a restrição de que o tamanho da amostra deve ser suficientemente grande para que possamos aplicar o Teorema Central do Limite, sabemos, como visto anteriormente, que\n\\[\nZ  = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\]\nLembre-se, entretanto, que estamos no caso em que \\(\\sigma^2\\) é desconhecido, e por isso não podemos utilizar a variável acima para construirmos o intervalo de confiança, uma vez que o intervalo encontrado seria função de \\(\\sigma\\). Dessa forma, é intuitivo que em algum momento utilizemos um estimador da variância populacional na expressão do intervalo. Por sorte, existe um conhecido resultado dentro da Estatística que fornecerá uma função que nos seja conveniente. Definindo, primeiramente, o estimador da variância amostral, denotado por \\(S^2\\), como sendo\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})}{n - 1},\n\\] sabemos ser válido que\n\\[\nT = \\frac{\\sqrt{n} \\left(\\bar{X} - \\mu \\right)}{S} \\sim t_{n - 1},\n\\] onde \\(S\\) é a raiz quadrada de \\(S^2\\) e \\(t_{n - 1}\\) representa a distribuição de probabilidade T de Student com \\(n - 1\\) graus de liberdade. A demonstração desse resultado pode ser consultada no post do Observatório que referenciamos no início da seção. A partir daqui, o processo para obtermos a expressão do intervalo de confiança será muito semelhante ao que realizamos no exemplo anterior. Como a distribuição da variável aleatória \\(T\\) é totalmente conhecida, uma vez que, em uma situação prática, o valor do tamanho da amostra \\(n\\) estaria definido, podemos encontrar valores \\(t_{1_{(n-1)}}\\) e \\(t_{2_{(n-1)}}\\), com \\(t_{1_{(n-1)}} < t_{2_{(n-1)}}\\), tais que\n\\[\nP\\left(t_{1_{(n-1)}} < T < t_{2_{(n-1)}} \\right) = 1 - \\alpha. \\qquad \\qquad \\text{(II)}\n\\]\nComo a distribuição T de Student, assim como a distribuição normal padrão, é simétrica em torno de zero, os valores de \\(t_1\\) e \\(t_2\\) que geram a menor amplitude possível para o intervalo de confiança serão dados por\n\\[\nt_{1_{(n-1)}} = -t_{(n - 1;\\;1 -\\alpha/2)} \\text{ e } t_{2_{(n-1)}} = t_{(n - 1;\\;1 -\\alpha/2)}.\n\\]\nEm outras palavras, para encontrarmos os valores desses pontos, basta que calculemos o quantil de ordem \\(1 - \\alpha/2\\) da distribuição T de Student com \\(n - 1\\) graus de liberdade. Voltando à probabilidade definida em \\(\\text{(II)}\\), a atualizando com os resultados obtidos e reescrevendo \\(T\\), temos:\n\\[\nP\\left(t_{1_{(n-1)}} < T < t_{2_{(n-1)}} \\right) = P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} < \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} < -t_{(n - 1;\\;1 -\\alpha/2)} \\right) = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} < \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} < t_{(n - 1;\\;1 -\\alpha/2)} \\right) \\\\ &\n= P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} S < \\sqrt{n}\\left(\\bar{X} - \\mu\\right) < t_{(n - 1;\\;1 -\\alpha/2)} S\\right) \\\\ & =\nP\\left(-t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} < \\bar{X} - \\mu < t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ & =\nP\\left(-\\bar{X} + -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} < - \\mu < -\\bar{X} + t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ &\n= P\\left(\\bar{X} - t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} < \\mu < \\bar{X} - -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\]\nPortanto, quando a variância populacional é desconhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança de \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}}; \\bar{X} + t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}} \\right).\n\\]\nAlgo interessante a se observar é que, tanto o intervalo aleatório acima, quanto o intervalo aleatório encontrado no exemplo anterior, possuem probabilidade 1 - \\(\\alpha\\) de conterem o verdadeiro valor do parâmetro \\(\\mu\\), mesmo que suas estruturas sejam diferentes. Entretanto, como na construção do intervalo apresentado no presente exemplo foi utilizado um estimador de \\(\\sigma\\), a amplitude média dos intervalos de confiança obtidos por esse método será maior do que a amplitude média dos intervalos obtidos quando utilizamos o método apresentado no exemplo anterior. Com isso, dizemos que o intervalo de confiança acima é menos informativo, uma vez que o intervalo de valores plausíveis para \\(\\mu\\), nesse caso, será maior que o do intervalo de confiança construído anteriormente. Esse problema é mais evidente para tamanhos pequenos de amostra, e torna-se menos relevante conforme o valor de \\(n\\) aumenta.\n\n5.4.2.1 Um exemplo numérico\nPara demonstrar a diferença entre o intervalo de confiança derivado acima e aquele derivado no exemplo anterior, podemos aproveitar o mesmo exemplo sobre a média das idades das gestantes e puérperas previamente discutido. Novamente, criaremos uma função, a qual chamaremos de ic_media_caso2(), que retornará o intervalo de confiança para a média populacional quando a variância populacional é desconhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função se dará de forma similar àquela criada anteriormente, sendo as únicas diferenças o cálculo do desvio padrão amostral, feito com a função sd(), do pacote básico {stats} e a obtenção do quantil da distribuição T de Student através da função qt(), do mesmo pacote. O resultado do processo pode ser visto abaixo. Lembre-se que o vetor amostra_ic_media foi criado no exemplo anterior.\n\nic_media_caso2 <- function(dados, alfa) {\n  media <- mean(dados)\n  n <- length(dados)\n  S <- round(sd(dados), 3)\n  t <- qt(1 - alfa/2, n - 1)\n  limite_inferior <- round(media - t * S/sqrt(n), 3)\n  limite_superior <- round(media + t * S/sqrt(n), 3)\n  amplitude <- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n,\n      S, \n      estimativa_pontual = media,\n      limite_inferior,\n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso2(dados = amostra_ic_media, alfa = 0.05)\n\n   n     S estimativa_pontual limite_inferior limite_superior amplitude\n1 50 6.634              32.16          30.275          34.045      3.77\n\n\nComo podemos perceber, o intervalo de confiança obtido acima apresenta uma amplitude levemente maior do que o intervalo obtido anteriormente (3,77 contra 3,758). Dessa forma, apesar de a diferença não ser gritante, podemos perceber que a utilização de uma estimativa para o valor da variância populacional acarretou, como já explicado anteriormente, em um intervalo levemente menos informativo a respeito do verdadeiro valor da média populacional. Essa diferença na amplitude, entretanto, não é regra: poderíamos ter coletado uma amostra em que a amplitude do intervalo de confiança construído pelo presente método fosse menor. Além disso, como o tamanho da amostra é grande o suficiente, a diferença nas amplitudes dos dois intervalos passa a ser praticamente desconsiderável. Com isso, aqui encerramos a construção de intervalos de confiança para a média de uma população. Partamos, agora, para a última seção desse capítulo, na qual trataremos da obtenção de intervalos de confiança para a proporção populacional.\n\n\n\n5.4.3 Intervalos de confiança para a proporção populacional\nA fazer"
  },
  {
    "objectID": "naosupervisionado.html#alguns-conceitos-básicos-de-algebra",
    "href": "naosupervisionado.html#alguns-conceitos-básicos-de-algebra",
    "title": "7  Aprendizado não supervisionado",
    "section": "7.1 Alguns conceitos básicos de algebra",
    "text": "7.1 Alguns conceitos básicos de algebra\nPara melhor introduzir o campo do aprendizado não supervisionado, alguns conceitos de álgebra são necessários para compreender o que se passa por trás de cada algoritmo da análise de dados multivariada. Vamos introduzir com vetores e matriz, seguindo com decomposição espectral para então darmos inicio a área da estatística multivariada ou aprendizado não supervisionado. Não é objetivo desse livro demonstrar conceitos algébricos e nem se aprofundar demais no assunto Johnson, Wichern, et al. (2002).\n\n7.1.1 Definições importantes\nVetor Aleatório : Seja X um vetor contendo p componentes, onde cada componente é uma variável aleatória, isto é, \\(X_i\\) é uma variável aleatória, \\(\\forall\\quad i =1,2,...,p\\). Então X é chamado de vetor aleatório e é denotado por:\n\\[\n\\begin{align}\n  X &= \\begin{bmatrix}\n           X_{1} \\\\\n           X_{2} \\\\\n           \\vdots \\\\\n           X_{p}\n         \\end{bmatrix}\n  \\end{align}\n\\]\nO vetor transposto do vetor aleatório X é denotadopor \\(X' = [X_1 X_2 X_3 ...X_p]\\)\nVetor de Médias : O vetor \\(\\mu\\) é chamado vetor de médias quando \\(E(X) = \\mu\\) onde X é um vetor aleatório. Dessa forma\n\\[\n\\begin{align}\n  E(X) &= \\begin{bmatrix}\n           E(X_{1}) \\\\\n           E(X_{2}) \\\\\n           \\vdots \\\\\n           E(X_{p})\n         \\end{bmatrix}\n  \\end{align} = \\mu = \\begin{bmatrix}\n           \\mu_1 \\\\\n           \\mu_2 \\\\\n           \\vdots \\\\\n           \\mu_p\n         \\end{bmatrix}\n\\]\nMatriz de covariâncias : A matriz de variâncias e covariâncias do vetor X é denotada por,\n\\[\nCov(X) = V(X) = Var(X) = \\Sigma_{p\\times p} = \\begin{bmatrix}\n           \\sigma_{11} & \\sigma_{12} & ... & \\sigma_{1p}  \\\\\n          \\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n         \\end{bmatrix}\n\\]\nOnde \\(\\sigma_{ii}\\) representa a variância do elemento \\(X_i\\) do vetor aleatório e \\(\\sigma_{ij} = E[(X_i- \\mu_i)(X_j - \\mu_j)]\\) \\(\\forall\\quad i,j = 1,\\dots,p\\). A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja \\(\\Sigma = '\\Sigma\\). Sendo tambem não negativa definida, \\(a'\\Sigma a \\geq 0\\) para todo vetor de constantes pertencentes aos reais.\nMatriz de correlação : A matriz de correlação do vetor X é denotada por,\n\\[\nP_{p\\times p} = \\begin{bmatrix}\n           1 & \\rho_{12} & \\rho_{13}& ... & \\rho_{1p}  \\\\\n          \\rho_{21} & 1 & \\rho_{23}&... & \\rho_{2p}  \\\\\n          \\rho_{31} & \\rho_{32} & 1 &... & \\rho_{3p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\rho_{p1} & \\rho_{p2} &\\rho_{p3}& ... & 1\n         \\end{bmatrix}\n\\]\nEm que\n\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}} = \\frac{\\sigma_{ij}}{\\sigma_i\\sigma_j}\n\\]\nAuto Valores e Auto Vetores : Se \\(\\Sigma\\) for uma matriz quadrada, ou seja \\(\\Sigma_{p\\times p}\\), então um vetore não nulo \\(e\\) em \\(R^n\\) é denominado autovetor de \\(\\Sigma\\) se \\(\\Sigma e\\) for um múltiplo escalar de \\(e\\), isto é,\n\\[\n\\Sigma e = \\lambda e\n\\]\ncom algum escalar \\(\\lambda\\). O escalar \\(\\lambda\\) é denominado de autovalor de \\(\\Sigma\\), e dizemos que \\(e\\) é um autovetor associado a \\(\\lambda\\). Por \\(\\Sigma\\) ser uma matriz não negativa definida seus autovalores \\(\\lambda_i\\) associados tambem serão não negativos. Os autovetores e autovalores serão necessários para a análise de componentes principais mais a frente abordada.\n\n7.1.1.1 Equação característica\nAinda é necessário uma forma de encontrar os autovetores e autovalores associados a uma matriz \\(\\Sigma\\). Se \\(\\Sigma\\) for uma matriz quadrada, então \\(\\lambda\\) se, e somente se, \\(\\lambda\\) satisfaz a equação\n\\[\ndet(\\lambda I - \\Sigma) = 0\n\\]\nOnde det é o determinante e \\(I\\) a matriz identidade. Para esclarecimento, suponha como exemplo que,\n\\[\n\\Sigma = \\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\]\nEntão,\n\\[\n\\begin{split}\ndet\\left(\\begin{bmatrix}\n\\lambda& 0\\\\\n0 & \\lambda\n\\end{bmatrix}\n-  \n\\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\right) = 0\\\\\ndet\\left(\\begin{bmatrix}\n\\lambda - 8 & 2 \\\\\n2 & \\lambda-5\n\\end{bmatrix}\n\\right) = 0 \\\\\n(\\lambda - 8)\\times(\\lambda-5) - (2)\\times(2) = 0\n\\end{split}\n\\]\nResolvendo a equação obtemos os valores de \\(\\lambda_1 = 9\\) e \\(lambda_2 = 4\\), podemos encontrar os autovetores \\(v\\) associados seguindo a definição:\n\\[\n\\begin{bmatrix}\n8&-2\\\\\n-2 & 5\n\\end{bmatrix}\n\\begin{bmatrix}\nv_{11}\\\\\nv_{12}\n\\end{bmatrix} =\n9\\begin{bmatrix}\nv_{11}\\\\\nv_{12}\n\\end{bmatrix} \\rightarrow v_{11} =- 2v_{12}\n\\]\nNote que para cada autovalor temos infinitos possíveis autovetores dentro dos reais. Nos restringiremos aos autovetores normalizados.Dizemos que um vetor \\(e_i\\) é normalizado quando:\n\\[\ne_i = \\begin{bmatrix}\ne_{i1}\\\\\ne_{i2}\\\\\n\\vdots\\\\\ne_{ip}\n\\end{bmatrix}\n\\]\nEm que\n\\[\n||e_i|| = \\sqrt{e^2_{i1} + e^2_{i2} + \\dots + e^2_{ip}} = 1\n\\]\n\n\n\n7.1.2 Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.\nO teorema da decomposição espectral é de extrema importância em álgebra matricial e estatística multivariada, ele relaciona a matriz com seus autovalores e autovetores normalizados.\nSuponha \\(\\Sigma\\) a matriz de covariâncias. Então existe uma matriz ortogonal \\(O\\)(matriz no qual sua transposta é igual a sua inversa) tal que,\n\\[\nO'\\Sigma O = \\begin{bmatrix}\n\\lambda_1 & 0 & 0 &\\dots & 0\\\\\n0&\\lambda_2& 0 & \\dots & 0 \\\\\n0 & 0 &\\lambda_3 &\\dots & 0\\\\\n\\vdots& \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\dots& \\lambda_p\n\\end{bmatrix} = \\Lambda\n\\]\nOnde \\(\\lambda\\_1 \\geq \\lambda\\_2 \\geq \\dots \\lambda\\_p\\geq0\\) são os autovalores ordenados em ordem decrescente da matriz \\(\\Sigma\\). Nesse caso, dizemos que a matriz \\(\\Sigma\\) é similar à matriz \\(\\Lambda\\), que implica em:\n\n\\(det(\\Sigma) = det(\\Lambda) = \\prod^p_{i=1} \\lambda_i\\)\ntraço\\((\\Sigma) =\\) traço\\((\\Lambda) = \\lambda_1 +\\dots+\\lambda_p\\)\n\nTem-se que a i-ésima coluna da matriz \\(O\\) é o autovetor normalizado \\(e_i\\) relacionado ao autovalor \\(\\lambda_i\\). Então a matriz \\(O\\) é dada por \\(O = [e_1,e_2,\\dots,e_p]\\) e pelo teorema da decomposição espectral, podemos ver que:\n\\[\n\\Sigma = O \\Lambda O' = \\sum_{i=1}^p \\lambda_i e_i e_i'\n\\]\nDentro do R é possível realizar a decomposição espectral usando a função eigen(),\n\nsigma <- matrix(c(8,-2,-2,5),nrow = 2)\nsigma\n\n     [,1] [,2]\n[1,]    8   -2\n[2,]   -2    5\n\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 9 4\n\n$vectors\n           [,1]       [,2]\n[1,] -0.8944272 -0.4472136\n[2,]  0.4472136 -0.8944272"
  },
  {
    "objectID": "naosupervisionado.html#análise-de-componentes-principais-pca",
    "href": "naosupervisionado.html#análise-de-componentes-principais-pca",
    "title": "7  Aprendizado não supervisionado",
    "section": "7.2 Análise de Componentes Principais (PCA)",
    "text": "7.2 Análise de Componentes Principais (PCA)\nA análise de componentes principais se preocupa em conseguir explicar a variância e covariância de uma estrutura de variáveis através de algumas poucas combinações lineares. Tendo como principal objetivo dessa análise a redução de dimensionalidade e interpretação das relações. Essas combinações lineares são os componentes principais e são não correlacionadas entre sí. Quando assumimos que as variáveis originiais possuem distribuição normal, as componentes, além de não correlacionadas são normalmente distribuidas e idependentes. Os componentes principais são extraidos através da decomposição da matriz de covariância do vetor aleatório. Caso alguma trasnformação seja realizada nesse vetor, a decomposição será realizada na matriz de covariância do vetor transformado. Um caso muito utilizado, suponha que nossas variáveis estão em escalas muito diferentes, o PCA pode acabar por dar mais variabilidade a essa variável com escala superior, para isso então padronizamos o vetor. Utilizar a matriz de covariância do vetor transformado e a matriz de correlação do vetor originais são ações equivalentes nessa situação.\nDefinição: Seja X um vetor aleatório com \\(\\mu = E(X)\\) e \\(\\Sigma = Var(X)\\) e \\((\\lambda_i,e_i), i = 1,\\dots,p\\) os pares de autovalores e autovetores normalizados associados de \\(\\Sigma\\). Então,\n\\[\n\\begin{split}\nY = O'X,\\quad \\textrm{com}\\quad O = [e_1,e_2,\\dots,e_p],\\textrm{ os componentes principais de X}\\\\\n\\textrm{ou seja}\\\\\nY =\n\\begin{bmatrix}\nY_1\\\\\n\\vdots\\\\\nY_d\n\\end{bmatrix} \\textrm{ com  } \\quad Y_1 = e_1'X = e_{11}X_1 + e_{12}X_2 +  \\dots + e_{1p}X_p\n\\end{split}\n\\]\nO primeiro componente principal. Os componentes principais de X, Y, são tais que,\n\\[\n\\begin{split}\n\\mu_y = E(Y) = E(O'X) = O'E(X) = O'\\mu_x\\\\\n\\Sigma_y = Var(Y) = Var(O'X) = O'Var(X)O = O'\\Sigma_xO = \\Lambda\n\\end{split}\n\\]\nou seja\n\\[\ncov(Y_i,Y_j) = 0, \\forall i \\neq j \\textrm{ e } Var(Y_i) = \\lambda_i\n\\]\nA prova desse resultado pode ser vista em (Johnson, Wichern, et al. 2002, 5:432).\nDescrevemos a variância total da população como sendo o somatório de todos os autovalores \\(\\lambda\\). A partir disso, podemos descrever a proporção da variância total explicada pela j-ésima componente como sendo:\n\\[\n\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i} \\qquad \\forall j =1,\\dots,p\n\\]\nPara algum \\(p\\) significativamente grande, podemos utilizar \\(d<p\\) componentes ao invés das \\(p\\) variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas \\(d\\) componentes.\nSe \\(Y_i = e'_iX, i =1\\dots,p\\) são as componentes principais obtidas da matriz de covariância, então\n\\[\n\\rho_{Y_i,X_j} = \\frac{e_{ij}\\sqrt{\\lambda_i}}{\\sigma_{jj}}, \\quad \\forall i,j=1,\\dots p\n\\]\nSão os coeficientes de correlação entre a componente \\(Y_i\\) e a variável \\(X_j\\)\n\n7.2.1 Exemplo.\nPara realmente entender a aplicabilidade da análise de componentes, vamos pegar um subconjunto do banco de dados mtcars, conjunto de dados no R base, consiste nas características de modelos de carros. Selecionaremos um subconjunto de colunas numéricas para conseguirmos trabalhar, considerando que PCA funciona melhor com variáveis numéricas. Há possibilidade de transformação de variáveis categoricas em variáveis dummy, porem o algoritmo não será tão preciso, também não sendo possível trabalhar com variáveis categóricas ordinais nesse caso.\n(deixei um exemplo com mtcars pois o banco de dados trabalhado no livro tem poucas variaveis numericas)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n3\n1\n\n\n\n\n\nPodemos obter de forma simples no R as componentes, bem como a proporção da variância explicada, com a função prcomp(). Bem como citado tambem é comum padronização das variáveis devido a escala de cada característica, para isso basta informar o parâmetro scale. como TRUE dentro da função.\n\ndados.pca <- dados |> \n  prcomp()\npaste('dados não padronizados: ',sep = \"\\n\")\n\n[1] \"dados não padronizados: \"\n\ndados.pca |> summary()\n\nImportance of components:\n                           PC1      PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     136.532 38.14735 3.06642 1.27492 0.90474 0.64734 0.3054\nProportion of Variance   0.927  0.07237 0.00047 0.00008 0.00004 0.00002 0.0000\nCumulative Proportion    0.927  0.99938 0.99985 0.99993 0.99997 0.99999 1.0000\n                          PC8    PC9\nStandard deviation     0.2859 0.2159\nProportion of Variance 0.0000 0.0000\nCumulative Proportion  1.0000 1.0000\n\n#padronizando as variaveis devido a diferenca de escalas\n\ndados.pca.padr <- dados |>\n  prcomp(scale. = T)\npaste('dados padronizados:',sep = \"\\n\")\n\n[1] \"dados padronizados:\"\n\ndados.pca.padr |> summary()\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.3782 1.4429 0.71008 0.51481 0.42797 0.35184 0.32413\nProportion of Variance 0.6284 0.2313 0.05602 0.02945 0.02035 0.01375 0.01167\nCumulative Proportion  0.6284 0.8598 0.91581 0.94525 0.96560 0.97936 0.99103\n                          PC8     PC9\nStandard deviation     0.2419 0.14896\nProportion of Variance 0.0065 0.00247\nCumulative Proportion  0.9975 1.00000\n\n#outras informacoes\n\ndados.pca.padr |> print()\n\nStandard deviations (1, .., p=9):\n[1] 2.3782219 1.4429485 0.7100809 0.5148082 0.4279704 0.3518426 0.3241326\n[8] 0.2418962 0.1489644\n\nRotation (n x k) = (9 x 9):\n            PC1         PC2         PC3          PC4        PC5         PC6\nmpg  -0.3931477  0.02753861 -0.22119309 -0.006126378 -0.3207620  0.72015586\ncyl   0.4025537  0.01570975 -0.25231615  0.040700251  0.1171397  0.22432550\ndisp  0.3973528 -0.08888469 -0.07825139  0.339493732 -0.4867849 -0.01967516\nhp    0.3670814  0.26941371 -0.01721159  0.068300993 -0.2947317  0.35394225\ndrat -0.3118165  0.34165268  0.14995507  0.845658485  0.1619259 -0.01536794\nwt    0.3734771 -0.17194306  0.45373418  0.191260029 -0.1874822 -0.08377237\nqsec -0.2243508 -0.48404435  0.62812782 -0.030329127 -0.1482495  0.25752940\ngear -0.2094749  0.55078264  0.20658376 -0.282381831 -0.5624860 -0.32298239\ncarb  0.2445807  0.48431310  0.46412069 -0.214492216  0.3997820  0.35706914\n             PC7         PC8         PC9\nmpg  -0.38138068 -0.12465987  0.11492862\ncyl  -0.15893251  0.81032177  0.16266295\ndisp -0.18233095 -0.06416707 -0.66190812\nhp    0.69620751 -0.16573993  0.25177306\ndrat  0.04767957  0.13505066  0.03809096\nwt   -0.42777608 -0.19839375  0.56918844\nqsec  0.27622581  0.35613350 -0.16873731\ngear -0.08555707  0.31636479  0.04719694\ncarb -0.20604210 -0.10832772 -0.32045892\n\n\nDa informação obtida por print(dados.pca.padr), podemos identificar os loadings da análise. Os loadings podem ser definidos como os coeficientes da combinação linear das variáveis originais de onde as componentes principais são construidas. De um ponto de vista matemático os loadings são iguais às coordenadas das variáveis divididas pela raiz quadrada do autovalor associado ao componente. são úteis quando você deseja entender os resultados. Lembre-se de que cada nova variável Y é uma combinação linear de todas as variáveis. A matriz de loadings representa verticalmente quanto da variância de cada componente é explicada por cada variável original. Vemos por exemplo que, conforme mpg aumenta, a PC1 tem um descrécimo. Os loadings são muito úteis na hora de nomear nossas componentes por essa relação que faz com cada uma das variáveis.\n\n\n7.2.2 Número de Componentes Principais\nAté agora foi descrito que podemos utilizar um número \\(d < p\\) de componentes principais que contenha uma explicabilidade aproximada dos dados originais, mas qual seria esse valor \\(d\\)? Há um conjunto de técnicas para essa tomada de decisão, sendo uma delas por exemplo a proporção de variância acumulada total explicada pelas componentes \\(Y_1,\\dots,Y_p\\):\n\\[\n\\frac{\\sum^d_{j=1}\\lambda_j}{\\sum_{i=1}^p \\lambda_i}\n\\]\nEsse valor é observado na função prcomp() já citada, como cumulative Proportion no resultado do summary() da função.\nPodemos utilizar como apoio gráfico e auxílio na tomada de decisão para o número de componentes é o scree plot, conhecido também como gráfico do cotovelo. Consiste na ordenação dos autovalores do maior para o menor, procurando por uma espécie de cotovelo dentro do gráfico. Selecionamos o número $i $ de componentes em que há um grande valor para observação \\(\\lambda_{i-1}\\) em comparação a observação \\(\\lambda_i\\) e uma pequena alteração da observação \\(\\lambda_i\\) para a observação \\(\\lambda_{i+1}\\). Observe a seguir\n\n#variancia explicada por cada componente\nvar_explicada = dados.pca.padr$sdev^2 / sum(dados.pca.padr$sdev^2)\nlibrary(ggplot2)\n\nqplot(c(1:9), var_explicada) + \n  geom_line() + \n  xlab(\"Principal Componente\") + \n  ylab(\"variancia explicada\") +\n  ggtitle(\"Scree Plot\") +\n  ylim(0, 1) + \n  scale_x_discrete(limits=c(1:9))\n\n\n\n\nPodemos por meio, tanto do scree plot, quanto pelo valor da variância explicada acumulada, selecionar \\(d= 3\\) componentes para reter, pela queda de 2 para 3 ser significante, enquanto a de 3 para 4 nem tanto. Reduzindo número de variáveis a 3.\n##Métodos de Agrupamentos\nA análise de agrupamentos ou clusterização, tem como objetivo, agrupar indivíduos da população usando como base medidas de similaridade entre eles, formando grupos heterogêneos entre sí com homogenuidade entre indivíduos de mesmo cluster. Muito utilizado na classificação de tipos de clientes de mercado, usuários de aplicativos, ou até mesmo em áres como psicologia, para agrupamentos de perfis de personalidade. Outro exemplo pode ser visto no trabalho (colocar link mariana).\n\n\n7.2.3 Medidas de dissimilaridade\nDe forma mais intutitiva, essas medidas de dissimilaridade seriam formas de numerar o quão próximo ou distânte a característica de um indivíduo (Idade por exemplo), se aproxima da mesma característica de outro indivíduo da mesma população. Não possuimos uma única forma de medida. Aqui apresentaremos as mais conhecidas e mais trabalhadas. Não existe uma métrica melhor, a eficácia de uma medida dependerá do caso em que a mesma será aplicada. Suponha que para cada elemento amostral será obtido o vetor \\(X = [X_{1},X_{2},\\dots,X_{p}]'\\) de medidas, onde \\(X_{i}\\) representa a medida da i-ésima característica para a unidade amostral.\nDistância Euclidiana: Essa é provavelmente a mais conhecida e usada medida de distância. Ela simplesmente é a distância geométrica no espaço multidimensional. Considere o i-ésimo e o j-ésimo indivíduo:\n\\[\nd(X,Y) = \\sqrt{\\sum^p_{i=1}(X_i - Y_i)^2}\n\\]\nDistância de Canberra: A distância de Camberra examina a soma das séries de diferenças fracionárias entre as coordenadas do par de observações.\n\\[\nd(X,Y) = \\sum^p_{i=1}\\frac{|X_i - Y_i|}{|X_i| + |Y_i|}\n\\]\nDistância de Manhattan: A distância de Manhattan (“City Block” ou “Geometria do Táxi”) é uma forma de geometria em que a distância entre dois pontos é a soma das diferenças absolutas de suas coordenadas.\n\\[\nd(X,Y) = \\sum^p_{i=1}|X_i- Y_i|\n\\]\nDistância de Chebyshev: Em matemática, distância de Chebyshev (ou distância de Tchebychev), métrica máxima ou \\(L_{\\infty}\\) métrica, é uma métrica definida em um espaço vetorial onde a distância entre dois vetores é a maior de suas diferenças ao longo de qualquer característica.\n\\[\nd(X,Y) = \\max_i(|X_i - Y_i|)\n\\]\nDistância de Minkowski: A distância de Minkowski de ordem \\(k\\), sendo \\(k\\) inteiro, pode ser considerada uma generalização tanto da distância euclidiana quanto da distância de manhattan.\n\\[\nd(X,Y) = \\left(\\sum^p_{i=1}|X_i - Y_i|^k\\right)^\\frac{1}{k}\n\\]\nTodas essas distâncias aqui citadas podem ser acessadas pela função dist() do R, alterando o parãmetro method para a distância desejada, da seguinte forma :\n\ndb<- dados[1:10,c('sem_pri','idade_anos','dt_evoluca_2','ano','dt_sint')]\ndb$ano <- db\ndb.dist <- db |> na.omit() |> dist(method = 'euclidean')\ndb.dist\n\n           1         2         3         4         5         6         7\n2  21.633308                                                            \n3  21.633308  8.485281                                                  \n4  31.292172 24.738634 16.321765                                        \n5  50.521283 28.962044 30.886890 37.804762                              \n6  67.242843 47.774470 45.615787 43.266615 24.665766                    \n7  20.435264 11.063453 18.782971 34.985711 35.445733 56.920998          \n8  31.805660 13.813037 10.217632 17.076299 22.126907 35.445733 24.665766\n9  37.994736 28.962044 37.421919 53.699162 40.958516 65.424766 19.809089\n10 44.009090 27.626075 22.768399 18.782971 22.847319 24.738634 38.418745\n           8         9\n2                     \n3                     \n4                     \n5                     \n6                     \n7                     \n8                     \n9  40.249224          \n10 13.813037 52.752251\n\n\nConsguindo a distância euclidiana entre cada uma das 10 primeiras observações para as características selecionadas.\n\n\n7.2.4 Técnicas Hierárquicas\nDentro da estatística multivariada dividimos frequêntemente as técnicas aglomerativas em dois tipos: hierárquicos e não hierárquicos, sendo as hierárquicas classificadas em aglomerativa e divisivas. Métodos hierárquicos são geralmente utilizados na análise exploratória afim de encontrar um número ótimo de clusters para o conjunto de variáveis, para as técnicas não hierárquicas é necessário um valor prévio de grupos.\n\n7.2.4.1 Técnicas Hierárquicas Aglomerativas\nConsidere cada observação como um grupo único, nos métodos aglomerativos vamos anexando cada grupo um ao outro em cada passo, usando suas medidas de similaridade para esse agrupamentos. Em cada instância do processo o par de grupos com a menor medida de dissimilaridade. Suponha a distância euclidiana por exemplo, em cada passo, verificaremos os \\(p\\) grupos e anexamos o par com a menor distância euclidiana, seguindo para o próximo passo realizamos o mesmo com os \\(p-1\\) grupos, até q sobre apenas 1 grupo com todas as observações. Seguindo o processo por \\(p-1\\) passos.\nLigamento Simples: Assumindo que cada observação é um cluster incialmente, suponha as observações X e Y sendo as com menor distância, ou os vizinhos mais próximos, formando o novo cluster {XY}. A distância entre o grupo {XY} e os demais grupos, suponha W, é definida como:\n\\[\nd(\\{XY\\},W) = \\min\\{d_{XW},d_{YW}\\}\n\\]\nConsidere a matriz de distâncias do exemplo anterior das 5 primeiras observações:\n\\[\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nSendo a distância entre a observação 3 e 2 a menor distância dentre todas as observações. Anexaremos as duas observações em um único grupo, assumindo a nova distância desse grupo com as demais observações como sendo o minimo da distância das variaveis do grupo os demais grupos:\n\\[\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 16.32 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=28.96 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nDando prosseguimento com o processo, note que agora a menor distância se da entre os grupos {23} e o grupo {4}, logo os dois serão reagrupados em um único cluster, seguindo com esse mesmo processo até que reste apenas um grupo.\n\\[\n\\begin{bmatrix}\nd_{234,1}=21.63 \\\\\nd_{234,5}= 28.96 & d_{4,5} = 37.80\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=28.96\n\\end{bmatrix}\n\\]\nOs resultados do agrupamento de ligação simples podem ser exibidos graficamente na forma de um dendrograma, ou diagrama de árvore. Os ramos na árvore representam clusters. As ramificações se unem em nós cujas posições ao longo de uma distância (ou similaridade) indicam o nível em que as junções ocorrem. Veja para o exemplo acima considerando agora as 10 observações, passando na função hclust() para ligação dos grupos, o parâmetro method para tipo de ligação, no caso atual method = \"single\".\n\nhc <-  db.dist |> \n  hclust( method = \"single\") \nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Simples\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\n\n\nLigação Completa : Funciona de maneira parecida com a ligação simples, uniremos os grupos com menor distância entre sí até que reste apenas um único grupo. Porém, as distâncias entre as variáveis unidas, digamos X e Y, das demais variáveis W será definida como:\n\\[\nd(\\{XY\\},W) = \\max\\{d_{XW},d_{YW}\\}\n\\]\nMas o procedimento das demais iterações será da mesma forma, fazendo o link entre os grupos de menor distância. Suponha o exemplo anterior:\n\\[\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\n\\]\nUniremos as observações 2 e 3 assim como anteriormente, e a cada passo, a nova distância será a distância máxima entre as variáveis do grupo e os demais grupos:\n\\[\n\\begin{split}\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 24.73 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=30.88 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\\\\\n\\\\\n\\rightarrow\n\\begin{bmatrix}\nd_{123,4}=31.29 \\\\\nd_{123,5}= 50.52 & d_{4,5} = 37.80\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=50.52\n\\end{bmatrix}\n\\end{split}\n\\]\nObserve agora o dendrograma para ligação completa com 10 observações.\n\nhc <-  db.dist |> \n  hclust( method = \"complete\") \nlibrary(factoextra)\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Completa\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\n\n\nLigação Média: A ligação média trata a distância entre dois clusters como a distância média entre todos os pares de itens onde um membro de um par pertence a cada cluster. Considere o grupo {XY} e o grupo {W}, e \\(N_w\\) como sendo número de elementos em {W}, e \\(N_{XY}\\) número de elementos em {XY}, então:\n\\[\nd(\\{XY\\},W) = \\frac{\\sum^{N_{xy}}_{i=1}\\sum_{j=1}^{N_w}d_{ij}}{N_{xy}N_w}\n\\]\nOnde \\(d_{ij}\\) representa a distância entre a i-ésima observação do grupo {XY} e j-ésima observação do grupo {w}. Seguindo com o exemplo anterior e seu dendrograma obtemos:\n\\[\n\\begin{split}\n\\begin{bmatrix}\nd_{1,2}=21.63 &  & & \\\\\nd_{1,3}= 21.63 & d_{2,3}=8.48 & & \\\\\nd_{1,4}=31.29 & d_{2,4}=24.73 & d_{3,4}=16.32 \\\\\nd_{1,5}=50.52 & d_{2,5}=28.96 & d_{3,5}=30.88 & d_{4,5}=37.80\n\\end{bmatrix}\\\\\n\\\\\\rightarrow\n\\begin{bmatrix}\nd_{23,1}=21.63 &  & & \\\\\nd_{23,4}= 20.525 & d_{1,4}=31.29 & & \\\\\nd_{23,5}=29.92 & d_{1,5}=50.52 & d_{4,5}=37.80\n\\end{bmatrix}\n\\\\\n\\\\ \\rightarrow\n\\begin{bmatrix}\nd_{234,1}=24.85 \\\\\nd_{234,5}= 32.54 & d_{1,5} = 50.52\n\\end{bmatrix} \\rightarrow\n\\begin{bmatrix}\nd_{1234,5}=37.04\n\\end{bmatrix}\n\\end{split}\n\\]\n\nhc <-  db.dist |> \n  hclust( method = \"average\") \nlibrary(factoextra)\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Média\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\n\n\nMétodo Ward de clusterização : O método de ward se baseia na minimização da “perda de informação” ao juntar dois grupos. É tido como perda de informação o crescimento da soma dos quadrados dos erros, \\(SQE\\). Suponha o grupo {W}, a \\(SQE_W\\) pode ser descrita como a soma dos quadrados das distâncias de cada item do grupo para a média do grupo. Definindo \\(SQE\\) como a soma dos \\(SQE_i\\), onde \\(i\\) representa cada um dos \\(N\\) grupos. Em cada instância do processo é realizado a junção de todos os possiveis pares de grupos, optamos pela união que obtiver o menor incremento da \\(SQE\\). Note que no passo 0 essa soma é equivalente a 0, considerando que para cada \\(SQE_i\\), com apenas uma observação por cluster, a média será a própria observação. Enquanto que ao considerar o grupo final com todas as observações é possível obter a \\(SQE\\) por:\n\\[\nSQE = \\sum^N_{j=1}(X_j - \\bar{X})'(X_j - \\bar{X})\n\\]\nOnde \\(X_j\\) representa a j-ésima observação do grupo.\n\nhc <-  db.dist |> \n  hclust( method = \"ward.D2\") \nlibrary(factoextra)\n\nfviz_dend(hc, cex = 0.5,\n          main = \"Dendrogram - Ward\",\n          xlab = \"observacoes\", ylab = \"distancia\", sub = \"\")\n\n\n\n\n\n\n7.2.4.2 Algumas conclusões\nOs métodos hierárquicos são muito utilizados na exploração dos dados, bem como para pré definição do número de clusters, pois como veremos a seguir nos métodos não hierarquicos temos a necessidade de informar um número prévio de grupos. O dendograma é tido como principal forma de definição desses \\(k\\) grupos. Para definir o número ideal de clusters vamos utilizar o exemplo do método Ward. Observe que a distância para união do grupo {5,6} e {2,3,4,8,10} é relativamente grande se comparada as outras junções, uma forma de definir então seria \\(k = 3\\) grupos onde os grupos seriam, {1,7,9},{5,6} e {2,3,4,8,10} olhando o nível de fusão (distância) em que cada grupo precisou para se unir. Podemos então já utilizar \\(k\\) aproximado de 3 para iniciarmos nossos métodos não hieráquicos como veremos a seguir.\n\n\n\n7.2.5 Métodos de Agrupamentos Não Hierárquicos\nDentro desse conjunto de métodos iremos trabalhar com o mais usual e conhecido, k-médias. Bem como dito, os métodos não hierárquicos precisam de um número pré definido de grupos \\(k\\), anexando cada observação a um grupo com base em \\(k\\) centróides que serão definidos pelo algoritmo.\n\n7.2.5.1 K-Médias.\nK-médias é um método simples de particionamento, onde é necessário estabelecer um número \\(k\\) de grupos previamente a separação das variáveis. Definindo um número inicial de centróides, podendo esses ser observações do próprio conjunto de dados ou coordenadas aleatórias, é realizada a divisão do conjunto de dados, sendo cada observação anexada ao centro de menor distância, ou mais próximo. Com base nesse novo grupo criado, é determinado o nomo ponto central, que passa a ser a média do grupo. Baseado nesses novos pontos realizamos os passos anteriores por um número \\(N\\) de vezes até que não se tenha mais alteração na posição dos centróides. O resultado do processo são grupos heterogêneos entre sí com variáveis homogêneas entre sí, tendo a menor variância interna possível e a maior variação externa possível. O núemero de iterações do processo pode também ser pré estabelecido, considerando o custo computacional para bancos de dados grandes, é inviável a realização do processo até a falta de alteração dos clusters.\nExemplo:\nSuponha os seguintes dados para 20 variáveis, e suponha que vamos fazer inicialmente para \\(k=3\\) grupos.\n\n\n\n\n\n\nidade_anos\nsem_pri\n\n\n\n\n1\n24\n17\n\n\n2\n31\n26\n\n\n3\n27\n28\n\n\n4\n20\n33\n\n\n5\n39\n39\n\n\n6\n34\n51\n\n\n7\n34\n21\n\n\n8\n29\n33\n\n\n9\n44\n18\n\n\n10\n27\n40\n\n\n11\n28\n47\n\n\n12\n35\n50\n\n\n13\n37\n29\n\n\n14\n30\n29\n\n\n15\n32\n45\n\n\n16\n27\n32\n\n\n17\n44\n32\n\n\n18\n30\n42\n\n\n19\n16\n21\n\n\n20\n24\n21\n\n\n21\n31\n32\n\n\n22\n24\n36\n\n\n23\n31\n34\n\n\n24\n33\n47\n\n\n25\n25\n50\n\n\n26\n31\n21\n\n\n27\n26\n21\n\n\n28\n49\n19\n\n\n29\n25\n25\n\n\n30\n16\n31\n\n\n31\n20\n32\n\n\n32\n34\n42\n\n\n33\n28\n38\n\n\n34\n33\n16\n\n\n35\n34\n21\n\n\n36\n32\n20\n\n\n37\n38\n32\n\n\n38\n31\n40\n\n\n39\n38\n24\n\n\n40\n41\n27\n\n\n\n\n\nSuponha que os clusters são tidos inicialmente nas cordenadas:\n\n\n\n\n\n\nidade_anos\nsem_pri\n\n\n\n\n1\n37\n23\n\n\n2\n32\n43\n\n\n3\n25\n28\n\n\n\n\n\nAgregando cada variável a um cluster obtemos então.\n\n\n\n\n\n\nidade_anos\nsem_pri\nCentróide\n\n\n\n\n1\n24\n17\n3\n\n\n2\n31\n26\n3\n\n\n3\n27\n28\n3\n\n\n4\n20\n33\n3\n\n\n5\n39\n39\n2\n\n\n6\n34\n51\n2\n\n\n7\n34\n21\n1\n\n\n8\n29\n33\n3\n\n\n9\n44\n18\n1\n\n\n10\n27\n40\n2\n\n\n11\n28\n47\n2\n\n\n12\n35\n50\n2\n\n\n13\n37\n29\n1\n\n\n14\n30\n29\n3\n\n\n15\n32\n45\n2\n\n\n16\n27\n32\n3\n\n\n17\n44\n32\n2\n\n\n18\n30\n42\n2\n\n\n19\n16\n21\n3\n\n\n20\n24\n21\n3\n\n\n21\n31\n32\n3\n\n\n22\n24\n36\n3\n\n\n23\n31\n34\n3\n\n\n24\n33\n47\n2\n\n\n25\n25\n50\n2\n\n\n26\n31\n21\n1\n\n\n27\n26\n21\n3\n\n\n28\n49\n19\n1\n\n\n29\n25\n25\n3\n\n\n30\n16\n31\n3\n\n\n31\n20\n32\n3\n\n\n32\n34\n42\n2\n\n\n33\n28\n38\n2\n\n\n34\n33\n16\n1\n\n\n35\n34\n21\n1\n\n\n36\n32\n20\n1\n\n\n37\n38\n32\n1\n\n\n38\n31\n40\n2\n\n\n39\n38\n24\n1\n\n\n40\n41\n27\n1\n\n\n\n\n\nCom base nesses novos grupos definimos então o novo centróide como sendo a média das variáveis de cada grupo, ou seja:\n\n\n\n\n\ncentroide\nidade_anos\nsem_pri\n\n\n\n\n1\n37.36364\n22.54545\n\n\n2\n32.30769\n43.30769\n\n\n3\n25.06250\n28.18750\n\n\n\n\n\nAgregando cada variável a seu novo grupo e seguindo o processo até o número de iterações pré definidas ou até que não tenha mais alterações nas coordenadas dos centros de cada cluster.\nNo R base já está incluso uma função para o método k-médias, kmeans(), que pode ser implementado de maneira simples. considere o banco já discutido\n\nkmeans.df <- df |>\n  kmeans(centers = 3, iter.max = 300)\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 12, 16\n\nCluster means:\n  idade_anos  sem_pri\n1   31.33333 44.25000\n2   37.91667 23.33333\n3   25.06250 28.18750\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 3  3  3  3  1  1  2  3  2  1  1  1  2  3  1  3  2  1  3  3  3  3  3  1  1  2 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 3  2  3  3  3  1  1  2  2  2  2  1  2  2 \n\nWithin cluster sum of squares by cluster:\n[1] 412.9167 669.5833 861.3750\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nDe forma simples podemos identificar os centros e em qual cada uma das variáveis foi atribuida após as \\(N\\) iterações. Identificamos também a variância entre clusters, bem como a variância total e entre as variáveis de cada grupo. Essa variância se torna importânte na identificação do valor \\(k\\) estabelecido.\n\ndados_grupos <- kmeans.df |> broom::augment(df)\ncent <- kmeans.df$centers\n\ndados_grupos |> \n  ggplot(aes(x = idade_anos, y = sem_pri,col = .cluster)) +\n  geom_point() +\n  geom_point(aes(x = cent[1,1], y = cent[1,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[2,1], y = cent[2,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[3,1], y = cent[3,2]), color = \"black\", size = 3)\n\n\n\n\n\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 12, 16\n\nCluster means:\n  idade_anos  sem_pri\n1   31.33333 44.25000\n2   37.91667 23.33333\n3   25.06250 28.18750\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 3  3  3  3  1  1  2  3  2  1  1  1  2  3  1  3  2  1  3  3  3  3  3  1  1  2 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 3  2  3  3  3  1  1  2  2  2  2  1  2  2 \n\nWithin cluster sum of squares by cluster:\n[1] 412.9167 669.5833 861.3750\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\n\n7.2.5.2 Número Ideal de Grupos\nUma das formas já discutidas aqui sobre seleção do número ideal de \\(k\\) grupos é a pré utilização de um método hierárquico e análise de seu dendrograma. Porém, retomando os assuntos apresentados quando foi discutido PCA, podemos utilizar o scree plot da variação total como metodologia de definição do número ideal de clusters para o algorítmo. A utilização é realizada da mesma maneira, é feita a identificação do número \\(k\\) que sofra grande decréscimo da soma da variação dentro dos clusters para um número \\(k-1\\) e um pequeno em comparação com \\(k+1\\). A variação total é dada como:\n\\[\n\\sum^k_{i=1}\\sum_{j\\in C_i}d^2(x_j,c_i)\n\\] Sendo \\(C_i\\) centro do i-ésimo grupo e \\(x_j\\) a j-ésima variável do i-ésimo grupo. A função de distância mais usual é a euclidiana discutida anteriormente.\n\nvar_totais <- vector()\nfor(i in 1:10){\n  var_totais[i] <- (df |> kmeans(centers = i, iter.max = 400))$tot.withinss\n}\n\nqplot(1:10, var_totais, geom = \"line\")\n\n\n\nqplot(c(1:10), var_totais) + \n  geom_line() + \n  xlab(\"Número de clusters\") + \n  ylab(\"Soma das variâncias dentro dos grupos\") +\n  ggtitle(\"Scree Plot\") +\n  scale_x_discrete(limits=c(1:10))\n\nWarning: Continuous limits supplied to discrete scale.\nℹ Did you mean `limits = factor(...)` or `scale_*_continuous()`?\n\n\n\n\n\nPodemos definir a partir disso possíveis números ideais como 3, 4 ou até mesmo 6.\n\n\n7.2.5.3 Algumas conclusões\nPara definirmos um modelo como sendo ótimo para aplicação, é necessário defir alguns parâmetros para determinar a qualidade de um modelo ou método. O k-médias por exemplo é um algoritmo muito sucetível a outliers, dados fora do padrão encontrado no banco de dados, que podem acabar por deixar de agrupar uma determinada variável ou simplesmente formar um grupo amais, sem que haja necessidade. Outro fator que é necessário manter a atenção é a determinação do formato do cluster. Muitos dos algoritmos consideram um formato esférico ou circular para as variáveis, veja o exemplo:\n\ndados_circulo <- data.frame(\n X = runif(5000, -1, 1),\n  Y = runif(5000, -1, 1)\n) |>\n  dplyr::filter(X^2 + Y^2 <= 0.2 | (X^2 + Y^2 <= 0.8 & X^2 + Y^2 >= 0.6))\n\nqplot(dados_circulo$X, dados_circulo$Y)\n\n\n\n\nÉ notável como o agrupamento deve ser realizando simplesmente olhando para o gráfico proposto, porém, algoritmos como k-médias não pensam da mesma forma,\n\ndados_circulo.km <- dados_circulo |> kmeans(centers = 2)\ndados_circulo <- dados_circulo.km |> broom::augment(dados_circulo)\ncent <- dados_circulo.km$centers\n\ndados_circulo |> \n  ggplot(aes(x = X, y = Y,col = .cluster)) +\n  geom_point() +\n  geom_point(aes(x = cent[1,1], y = cent[1,2]), color = \"black\", size = 3)+\n  geom_point(aes(x = cent[2,1], y = cent[2,2]), color = \"black\", size = 3)\n\n\n\n\n\nkmeans.df\n\nK-means clustering with 3 clusters of sizes 12, 12, 16\n\nCluster means:\n  idade_anos  sem_pri\n1   31.33333 44.25000\n2   37.91667 23.33333\n3   25.06250 28.18750\n\nClustering vector:\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 3  3  3  3  1  1  2  3  2  1  1  1  2  3  1  3  2  1  3  3  3  3  3  1  1  2 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 \n 3  2  3  3  3  1  1  2  2  2  2  1  2  2 \n\nWithin cluster sum of squares by cluster:\n[1] 412.9167 669.5833 861.3750\n (between_SS / total_SS =  67.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nÉ perceptível que o método por particionamento em médias não foi eficaz para o banco de dados em questão. Ao utilizar a função hcut() para particionamento utilizando um método hierárquico obtemos uma melhor resposta para o agrupamento das variáveis\n\ndados_circulo.h <- hcut(dados_circulo[,1:2], k = 2, hc_method = \"single\")\nd_circulo.h <- cbind(dados_circulo[,1:2],cluster=dados_circulo.h$cluster)\n\nd_circulo.h |> \n  ggplot(aes(x = X, y = Y,col = cluster)) +\n  geom_point()\n\n\n\n\nOs métodos hierárquicos apresentados no entanto, por utilizarem da distância de uma variável a outra são limitados a utilização de banco de dados numéricos, o que nem sempre é o encontrado nos problemas reais, é necessário optar nesse caso por métodos e algoritmos que consigam fazer a distinção mesmo na presença de variáveis categóricas. Fator relevante para a escolha do melhor algoritmo é a capacidade de lidar com um grande volume de dados. No k-médias temos a opção por exemplo de pré definirmos o número de iterações. Considere um grupo com milhões de variáveis, ao utilizar um método de ligação simples hierárquico faremos aproximadamente um milhão de ligações para depois identificar o número ideal de grupos, caso esse não seja conhecido (Caso mais comum). O ideal então é a análise exploratória de seus dados para com base nos conhecimentos sobre os diferentes tipos de métodos, saber qual será o de melhor aplicação para o determinado problema. Nada o impede de aplicar mais de um método e após sua aplicação identificar qual foi o modelo ótimo para o problema.\n\n\n\n\nAnton, Howard, e Chris Rorres. 2001. Álgebra linear com aplicações. Vol. 8. Bookman Porto Alegre.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "tutorialr.html#sobre-o-software-r",
    "href": "tutorialr.html#sobre-o-software-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.1 Sobre o software R",
    "text": "A.1 Sobre o software R\nR é um ambiente computacional e uma linguagem de programação para manipulação, análise e visualização de dados. Para essas finalidades, ele é considerado um dos melhores e um dos mais utilizados dentre os ambientes computacionais disponíveis. O R é mantido pela R Development Core Team e está disponível para diferentes sistemas operacionais: Linux, Mac e Windows.\nO software é livre, ou seja, gratuito, com código aberto em uma linguagem acessível. Nele, estão implementadas muitas metodologias estatísticas. Muitas dessas fazem parte do ambiente base do R e outras acompanham o ambiente sob a forma de pacotes, o que o torna altamente flexível. Os pacotes são bibliotecas com funções extras devidamente documentadas criadas para ajudar a resolver problemas de diferentes áreas do conhecimento.\nO R possui uma comunidade extremamente ativa, engajada desde o aprimoramento de ferramentas e desenvolvimento de novas bibliotecas, até o suporte aos usuários. Sobre o desenvolvimento de novas bibliotecas, um pesquisador em Estatística que desenvolve um novo modelo estatístico pode disponibilizá-lo em um pacote acessível aos usuários que se interessem pelo modelo, por exemplo. Além disso, a disponibilidade e compartilhamento da pesquisa em um pacote no R é uma boa prática quando falamos de reprodutibilidade na ciência. Ainda nesse ponto, realizar as análises de uma pesquisa aplicada em um programa livre e acessível a todos é um dos principais pontos para permitir reprodutibilidade.\nOptar por programar em R também implica na escolha de uma IDE (Integrated Development Environment). Uma IDE é um ambiente de desenvolvimento integrado onde podem ser combinadas ferramentas utilizadas no desenvolvimento de aplicações, como um editor de código ou uma ferramenta de preenchimento inteligente de código. Para o R, a IDE mais popular entre os usuários é o RStudio. O RStudio é um conjunto de ferramentas integradas projetadas para editar e executar os códigos em R. Assim, quando for o interesse utilizar o R, basta abrir o RStudio (R é automaticamente carregado)."
  },
  {
    "objectID": "tutorialr.html#instalação-do-r",
    "href": "tutorialr.html#instalação-do-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.2 Instalação do R",
    "text": "A.2 Instalação do R\nA seguir, será apresentado o passo a passo de como instalar o R e o RStudio para os três sistemas operacionais: Windows, MAC e Linux, respectivamente.\n\nA.2.1 R no Windows\nA forma mais simples de instalar o R consiste em primeiramente acessar a página do software pelo endereço https://cloud.r-project.org/. Ao acessar a página haverão três opções para download, sendo cada uma referente a um sistema operacional em específico. Assim, para conseguir instalar o software em um sistema operacional Windows basta primeiramente clicar no link Download R for Windows.\n\n\n\n\n\nPasso 1\n\n\n\n\nQuatro subdiretórios irão surgir, dentre eles é necessário clicar na base, pois este contém a distribuição base do R para instalação.\n\n\n\n\n\nPasso 2\n\n\n\n\nO subdiretório base irá redirecionar para uma página que contém o link de download do arquivo de instalação do software. Este por sua vez, pode ser identificado como Download + versão atual do R + for Windows.\n\n\n\n\n\nPasso 3\n\n\n\n\nFeito isso, um arquivo executável será baixado no computador, o qual, ao abri-lo, deverá escolher o idioma (português brasileiro) e simplesmente clicar em Avançar toda vez que o cliente de instalação requerer.\n\n\n\n\n\nPasso 4\n\n\n\n\n\n\n\n\n\nPasso 5\n\n\n\n\nAssim, uma instalação padrão do software será instalada no computador.\n\n\nA.2.2 R no MAC\nDa mesma forma a qual iniciamos a instalação do R no Windows também iniciaremos no MAC, onde é necessário acessar o endereço https://cloud.r-project.org/ e clicar no link Download R for macOS.\n\n\n\n\n\nPasso 1\n\n\n\n\nO link irá redirecionar para uma página com arquivos de extensão .pkg típicos de macOS. É importante verificar qual versão disponível é a ideal para seu sistema. A versão do tipo arm64.pkg é referente a versão mais recente do macOS na data deste material.\n\n\n\n\n\nPasso 2\n\n\n\n\nTendo feito o download do arquivo, basta abri-lo para um cliente de instalação ficar disponível, e então, para efetuar uma instalação padrão deve-se seguir as instruções do cliente sem customizações aditivas assim como foi feito para o Windows.\n\n\nA.2.3 R no Linux\nA instalação do R no Linux depende da distribuição sendo utilizada. Basta acessar o mesmo endereço https://cloud.r-project.org/ utilizado na instalação dos outros sistemas, e clicar no link Download R for Linux.\n\n\n\n\n\nPasso 1\n\n\n\n\nFeito isso irá aparecer as opções de distribuições para Linux em que o software está disponível para download, basta selecionar a distribuição compatível. Caso sua distribuição for Ubuntu por exemplo, clicamos nela no respectivo link.\n\n\n\n\n\nPasso 2\n\n\n\n\nAssim, irá ser redirecionado para uma página com as devidas instruções de instalação do R para a distribuição escolhida. Basta seguir as instruções para efetuar uma instalação padrão do software."
  },
  {
    "objectID": "tutorialr.html#instalação-do-rstudio",
    "href": "tutorialr.html#instalação-do-rstudio",
    "title": "Appendix A — Tutorial de R",
    "section": "A.3 Instalação do RStudio",
    "text": "A.3 Instalação do RStudio\nO RStudio é um conjunto de ferramentas integradas projetadas (IDE - Integrated Development Environment) da linguagem R para auxiliar na produtividade ao utilizar o R. Embora não seja obrigatório o seu uso, é um consenso na comunidade de que o uso do RStudio facilita o aprendizado enquanto acelera a produtividade do usuário, tornando-o indispensável principalmente para iniciantes.\nNo ano de 2022, RStudio iniciou um processo de transição de nome onde passou a se chamar Posit. O objetivo por de trás desse processo se dá na inclusão da comunidade de Python ao R, dado o crescimento notório do Python na área de análise de dados nos últimos anos e que ambas as linguagens se complementam.\nO primeiro passo para instalar o RStudio é acessar o site da Posit e ir até a página de download que pode ser acessada pelo endereço https://posit.co/download/rstudio-desktop/. Feito isso, a página irá apresentar algumas opções, dentre elas uma breve tabela com arquivos executáveis mais recentes disponíveis de instalação do RStudio.\n\n\n\n\n\nArquivos executáveis de instalação\n\n\n\n\nDentre os arquivos executáveis está a versão mais recente para Windows (retângulo vermelho), macOS (retângulo azul) e para diferentes distribuições do Linux (retângulo verde). É preciso fazer o download conforme o seu sistema operacional.\nApós o download basta abrir o arquivo executável baixado e seguir as instruções do cliente para que a instalação seja feita.\n\n\n\n\n\nRStudio aberto pela primeira vez"
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-rstudio",
    "href": "tutorialr.html#primeiros-passos-no-rstudio",
    "title": "Appendix A — Tutorial de R",
    "section": "A.4 Primeiros passos no RStudio",
    "text": "A.4 Primeiros passos no RStudio\nO RStudio é uma ferramenta que por padrão é dividida em quatro painéis, sendo que cada um deles contêm abas com diferentes utilidades.\n\n\n\n\n\nPainéis do RStudio\n\n\n\n\nA seguir descrevemos melhor os painéis e algumas abas comumente utilizadas do RStudio:\n Editor/Scripts: local para escrever códigos (principalmente arquivos em formato .R).\n Console: onde se executa os códigos e visualiza resultados.\n Aqui, é possível acessar todos os objetos criados em Environment e o histórico de códigos executados em History e conectar fonte de dados em Connections.\n Nessa área, temos diversas utilidades frequentemente utilizadas:\n\npodemos acessar arquivos e pastas do computador pela aba Files;\nna aba Plots, visualizamos resultados em que são gerados figuras (como gráficos e tabelas), caso um comando desse tipo tenha sido executado;\nem Packages, podemos manusear pacotes (instalar, atualizar ou deletar);\nna aba Help temos acesso à documentação de uma determinada função quando utilizado o comando help() ou ?. Uma função nada mais é do que uma estrutura de código pronta com a forma de acesso nome(argumento) que recebe argumentos de entrada e retorna uma resposta. O próprio comando help() é uma função.\n\nO usuário pode alterar as configurações padrões do RStudio ao acessar as opções globais.\n\n\n\n\n\nOpções globais\n\n\n\n\nPara usuários iniciantes, é recomendável configurar a aparência e estrutura (layout) dos painéis conforme a própria preferência para tornar a experiência de uso mais confortável.\n\n\n\n\n\nMenu de aparência\n\n\n\n\nPodemos alterar o layout pelo menu Panel Layout. Usualmente, os painéis são estruturados de forma que o painel Console fique ao lado do painel de Script (Source/Editor), facilitando a visualização dos comandos rodados.\n\n\n\n\n\nMenu de estruturação dos painéis\n\n\n\n\n\nA.4.1 Projetos\nUma funcionalidade importante é a criação de projetos, permitindo dividir o trabalho em múltiplos ambientes, cada um com o seu diretório, documentos e workspace.\nPara criar um projeto, os seguintes passos podem ser seguidos:\n\nClique na opção File do menu, e então em New Project.\nClique em New Directory.\nClique em New Project.\nEscreva o nome do diretório (pasta) onde deseja manter seu projeto, exemplo: “my_project”.\nClique no botão Create Project.\n\nPara criar um novo script para escrever os códigos, vá em File -> New File -> R Script.\n\n\nA.4.2 Boas práticas\nComente bem o seu código: é possível fazer comentários usando o símbolo #. É sempre bom explicar o que uma variável armazena, o que uma função faz, por que alguns parâmetros são passados para uma determinada função, qual é o objetivo de um trecho de código, etc.\nEvite linhas de código muito longas: usar linhas de código mais curtas ajuda na leitura do código.\nEscreva um código organizado. Por exemplo, adote um padrão no uso de minúsculas e maiúsculas, uma lógica única na organização de pastas e arquivos, pode ser adotada uma breve descrição (como comentário) indicando o que um determinado script faz.\nCarregue todos os pacotes que irá usar sempre no início do arquivo: quando alguém abrir o seu código será fácil identificar quais são os pacotes que devem ser instalados e quais dependências podem existir."
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-r",
    "href": "tutorialr.html#primeiros-passos-no-r",
    "title": "Appendix A — Tutorial de R",
    "section": "A.5 Primeiros passos no R",
    "text": "A.5 Primeiros passos no R\nO código pode ser escrito no Script e então ser executado ao apertar o botão Run (localizado no painel de Script) ou com o atalho no teclado Ctrl + Enter. É importante salientar que, apenas a linha em que o símbolo de inserção de código (barra vertical do cursor) estiver é que será executada. Para executar múltiplas linhas simultaneamente, é necessário selecionar as linhas desejadas e então utilizar o comando de execução mencionado.\nOutra forma de escrever e executar códigos é através do painel Console. Normalmente, o Console é utilizado para executar códigos sem muitas linhas de estruturação ou para fazer testes rápidos (ex: uso do R como calculadora). Para rodar o código diretamente pelo painel Console, basta escrevê-lo na linha em que contém o símbolo >, o qual indica que o R está pronto para receber comandos, e então pressionar a tecla Enter.\n\nA.5.1 R como calculadora\nUma das utilidades do R é utilizá-lo como uma calculadora, onde podemos realizar contas matemáticas simples até as mais complexas.\nPor padrão, o R entende as linhas de códigos da esquerda para a direita e de cima para baixo. No entanto, ao se deparar com operações matemáticas, ele respeita algumas prioridades. A operação com maior para a menor prioridade é: potenciação > multiplicação ou divisão > adição ou subtração. Caso haja a necessidade de alterar essa ordem, isso pode ser feito utilizando parênteses.\n\n# Adição.\n10 + 15\n\n[1] 25\n\n# Subtração.\n10 - 2\n\n[1] 8\n\n# Multiplicação.\n2 * 10\n\n[1] 20\n\n# Divisão.\n30/2\n\n[1] 15\n\n# Raiz quadrada.\nsqrt(4)\n\n[1] 2\n\n# Potência.\n2^2\n\n[1] 4\n\n# Potência > Multiplicação > Soma.\n2^2 + 5 * 2\n\n[1] 14\n\n# Multiplicação > Potência > Soma.\n2^2 + (5 * 2)\n\n[1] 14\n\n# Potência > Soma > Multiplicação.\n2 * (2^2 + 5) \n\n[1] 18\n\n\nCaso um comando incompleto seja dado, como 10 ^, o R mostrará um +. Isso não tem a ver com a soma e apenas que o R está esperando que o comando que estava sendo escrito seja finalizado. Para recomeçar, basta terminar a escrita do comando ou apenas pressionar Esc.\nVale também ressaltar que se um comando que o R não reconhece for dado, ele retornará uma mensagem de erro.\n\n\nA.5.2 Atribuição\nOs objetos (também chamados de variáveis) são “locais” onde são guardadas informações (números, textos etc). O ato de “guardar” informações dentro de objetos é chamado de atribuição, e pode ser feito com <- ou =. Embora ambas as formas funcionem, na prática, o sinal <- é usualmente utilizado para atribuições enquanto que o sinal = é utilizado para configurar argumentos de funções.\n\n# Variável x recebe o número 5 de diferentes formas.\nx <- 5 \n\nx = 5\n\ny = (2^2 + 6) - 4\nx <- y - 1\n\nUm ponto importante a se atentar é que o R é case sensitive, isto é, faz a diferenciação entre as letras minúsculas e maiúsculas. Portanto, x é diferente de X.\n\n# Dica: Podemos obter o output do comando ao colocá-lo em volta de ().\n(x <- 10/2)\n\n[1] 5\n\n# Ao chamar X obteremos um erro, pois a variável criada era minúscula.\nX\n\nError in eval(expr, envir, enclos): objeto 'X' não encontrado\n\n\n\n\nA.5.3 Objetos em R\nExistem cinco classes básicas de objetos no R:\n\nCharacter: “UAH!”\nNumeric: 0.95 (números reais)\nInteger: 100515 (inteiros)\nComplex: 2 + 5i (números complexos, a + bi)\nLogical: TRUE (booleanos, TRUE/FALSE)\n\nApós realizar a atribuição, podemos verificar a classe do objeto com a função class().\n\n# Character/texto, deve estar entre aspas \"\".\nx <- \"gestante\"; \nclass(x) \n\n[1] \"character\"\n\n# Numeric/números reais.\nx <- 0.9 \nclass(x) \n\n[1] \"numeric\"\n\n# Integer/números inteiros, tem que ser atribuído com o valor acompanhado de um ‘L’.\nx <- 5L\nclass(x)\n\n[1] \"integer\"\n\n# Complex/números complexos.\nx <- 2 + 5i\nclass(x)\n\n[1] \"complex\"\n\n# logical/valores lógicos.\nx <- TRUE\nclass(x)\n\n[1] \"logical\"\n\n\nOs valores lógicos são apresentados em letra maiúscula. Isso é muito importante, pois o R diferencia letras maiúsculas de minúsculas. Então, valores lógicos só são reconhecidos se escritos como TRUE ou FALSE. Além disso, cada valor lógico assume um valor numérico, sendo TRUE referente ao valor 1 e FALSE referente ao valor 0.\n\n# Operações matemáticas com valores lógicos.\n(TRUE*2)^2 + TRUE + FALSE + 2*TRUE\n\n[1] 7\n\n\nMuitas vezes é do interesse do usuário apagar objetos que foram criados, principalmente se for rodar códigos prontos em um ambiente que outra pessoa estava trabalhando, pois pode haver objetos já criados com os mesmos nomes dos que se encontram no código/script de interesse, o que poderá levar a erros e dificuldades de execução. A remoção de objetos pode ser feito com a função rm() ou remove().\n\n# Criando o objeto x.\nx <- 20\nx\n\n[1] 20\n\n# Removendo o objeto x.\nrm(x)\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\n# Removendo todos os objetos criados.\n(x <- 1)\n\n[1] 1\n\n(y <- 2)\n\n[1] 2\n\nrm(list=ls())\n\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\ny\n\nError in eval(expr, envir, enclos): objeto 'y' não encontrado\n\n\nVale notar que ao utilizar a função rm() ou a função remove() para remover todos os objetos criados, é necessário incluir um argumento chamado list onde utilizamos o sinal de = para especificar os objetos a serem deletados. A função ls() lista todos os objetos criados até o momento.\n\n\nA.5.4 Vetores\nNo R a estrutura mais básica de dados é chamada de Vector (vetor), podendo aparecer no formado Atomic (atômico) ou no formado de list (lista). Dentre os vetores atômicos existem quatro tipos, sendo eles: Character, Integer, Double e Logical.\n\nCom vetores podemos atribuir vários valores a um mesmo objeto. Para entrar com vários números (ou nomes, ou qualquer outro grupo de coisas), precisamos usar uma função para dizer ao programa que os valores serão combinados em um único vetor. Para criar vetores atômicos a função c() é a mais usual por podermos criar vetores atômicos de todos os tipos diretamente. Também podemos utilizar a função seq() e o símbolo : para criar vetores do tipo Integer, e a função rep() que é capaz de criar vetores Double, por exemplo. Além disso, podemos verificar o tipo do vetor com a função typeof().\n\n# Vetor Double com a função c().\n(vetor1 <- c(2.5, 3, 4/5))\n\n[1] 2.5 3.0 0.8\n\ntypeof(vetor1)\n\n[1] \"double\"\n\n# Vetor Integer com a função c().\n(vetor2 <- c(5L, 7L, 9L))\n\n[1] 5 7 9\n\ntypeof(vetor2)\n\n[1] \"integer\"\n\n# Vetor Character com a função c().\n(vetor3 <- c(\"hospital1\", \"hospital2\"))\n\n[1] \"hospital1\" \"hospital2\"\n\ntypeof(vetor3)\n\n[1] \"character\"\n\n# Vetor Logical com a função c().\n(vetor4 <- c(TRUE, FALSE, FALSE, TRUE))\n\n[1]  TRUE FALSE FALSE  TRUE\n\ntypeof(vetor4)\n\n[1] \"logical\"\n\n# Vetor Integer com a função seq().\n(vetor5 <- seq(1, 5))\n\n[1] 1 2 3 4 5\n\ntypeof(vetor5)\n\n[1] \"integer\"\n\n# Vetor Integer com o símbolo :.\n(vetor6 <- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ntypeof(vetor6)\n\n[1] \"integer\"\n\n# Vetor Double com a função rep(). \n(vetor7 <- rep(1,10))\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\ntypeof(vetor7)\n\n[1] \"double\"\n\n\nÉ comum o usuário querer saber o tamanho do vetor que ele está trabalhando, isso pode ser feito com a função length(). Além disso, é importante ter certeza de que estamos trabalhando com um vetor atômico, o que pode ser verificado com a função is.vector().\n\n# Podemos construir um vetor com vetores dentro da função c().\n(vetor <- c(c(1, 2), rep(1, 2), seq(1, 2), 1:2))\n\n[1] 1 2 1 1 1 2 1 2\n\nis.vector(vetor)\n\n[1] TRUE\n\ntypeof(vetor)\n\n[1] \"double\"\n\nlength(vetor)\n\n[1] 8\n\n\nObserve que é possível criar um vetor com elementos de diferentes tipos. Sabemos que a função rep() gera um vetor de tipo Double e a seq() gera um vetor de tipo Integer, e ao criar um vetor utilizando a função c() em conjunto com estas obtemos um vetor de tipo Double, de forma que o R priorizou este tipo ao invés do Integer. No R isso é chamado de coerção, onde o vetor sendo criado irá manter o tipo de maior prioridade dentre os seus elementos, e os elementos de tipos com menor prioridade serão convertidos para o tipo prioritário. Isso ocorre, pois todos os elementos de um vetor atômico devem ter o mesmo tipo. Para os tipos apresentados temos como o de menor prioridade para o maior: Logical < Integer < Double < Character. Além disso, se considerarmos Complex e List, teremos List com maior prioridade seguido de Character e Complex.\nPode ser do interesse do usuário visualizar elementos específicos que existem dentro de um vetor, isso pode ser feito ao especificar a posição do elemento dentro do vetor entre os símbolos [].\n\n# vetor com varios elementos.\nvet <- c(TRUE, 5, 7L, \"hospital\")\ntypeof(vet)\n\n[1] \"character\"\n\n# elemento de posição 3.\nvet[3]\n\n[1] \"7\"\n\n# elementos das posições 2, 3 e 4.\nvet[2:4]\n\n[1] \"5\"        \"7\"        \"hospital\"\n\n\nAs operações vetoriais podem ser realizadas de maneira bastante intuitiva, pois em vetores atômicos as operações são realizadas elemento a elemento.\n\n# Operações com vetores.\nvetor1 <- c(4, 9, 16)\n(vetor1_menos1 <- vetor1 - 1)\n\n[1]  3  8 15\n\n(vetor1_vezes2 <- vetor1 * 2)\n\n[1]  8 18 32\n\n(vetor1_dividido2 <- vetor1/2)\n\n[1] 2.0 4.5 8.0\n\n(vetor1_raiz <- sqrt(vetor1))\n\n[1] 2 3 4\n\nvetor2 <- c(1, 2, 3)\n(vetor1_mais_vetor2 <- vetor1 + vetor2)\n\n[1]  5 11 19\n\n\nVamos agora considerar vetores de pesos (quilos) e alturas (metros) de 6 pessoas.\n\n# Vetores de peso e de quilo.\n(peso <- c(62, 70, 52, 98, 90, 70))\n\n[1] 62 70 52 98 90 70\n\n(altura <- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61))\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Obs: note que o separador decimal do R é um . (ponto).\n\nPodemos a partir dessas informações calcular o IMC. Vale lembrar que o IMC é dado pelo peso (em kg) dividido pela altura (em metros) ao quadrado.\n\n(imc <- peso/(altura^2))\n\n[1] 21.45329 21.13271 16.97959 26.03890 26.58318 27.00513\n\n\nÉ importante saber que, no R, vetores são a base dos demais objetos. Objetos com apenas um elemento, por exemplo, não são considerados escalares, mas vetores de tamanho um. Em outras palavras, os próprios elementos de um vetor são também vetores.\n\nelemento1 <- \"\"\nis.vector(elemento1)\n\n[1] TRUE\n\nlength(elemento1)\n\n[1] 1\n\nelemento2 <- 5\nis.vector(elemento2)\n\n[1] TRUE\n\nlength(elemento2)\n\n[1] 1\n\nelemento3 <- TRUE\nis.vector(elemento3)\n\n[1] TRUE\n\nlength(elemento3)\n\n[1] 1\n\n\nAlém dos vetores de formato atômico também existem os de formado lista, que diferente dos atômicos, as listas podem ter elementos de tipos diferentes de forma que não há necessidade do R efetuar coerções. Para criar listas no R podemos utilizar a função list().\n\n# Lista com vários tipos de elementos (inclusive listas).\n(lista <- list(5, \"hospital\", list(1:5), c(rep(1, 2)), seq(1, 2)))\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] \"hospital\"\n\n[[3]]\n[[3]][[1]]\n[1] 1 2 3 4 5\n\n\n[[4]]\n[1] 1 1\n\n[[5]]\n[1] 1 2\n\nis.vector(lista)\n\n[1] TRUE\n\ntypeof(lista)\n\n[1] \"list\"\n\nlength(lista)\n\n[1] 5\n\n# Dica: podemos verificar a estrutura de qualquer objeto com a função str().\nstr(lista)\n\nList of 5\n $ : num 5\n $ : chr \"hospital\"\n $ :List of 1\n  ..$ : int [1:5] 1 2 3 4 5\n $ : num [1:2] 1 1\n $ : int [1:2] 1 2\n\n# Dica: podemos retornar uma lista para vetor atômico com a função unlist().\nunlist(lista)\n\n [1] \"5\"        \"hospital\" \"1\"        \"2\"        \"3\"        \"4\"       \n [7] \"5\"        \"1\"        \"1\"        \"1\"        \"2\"       \n\n\n\n\nA.5.5 Matrizes\nMatrizes são vetores numéricos com duas dimensões, sendo estas a linha e a coluna às quais o elemento pertence. No R podemos criar matrizes com a função matrix().\n\n# Criando uma matriz de 16 elementos com 4 linhas e 4 colunas.\n(matri <- matrix(seq(1,16), nrow = 4, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nstr(matri)\n\n int [1:4, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Podemos verificar se é uma matriz com a função is.matrix().\nis.matrix(matri)\n\n[1] TRUE\n\n\nNote que os números de 1 a 16 foram dispostos na matriz coluna por coluna, ou seja, preenchendo de cima para baixo e depois da esquerda para a direita. Isso ocorre por padrão, pois a função matrix() possui um argumento chamado byrow = FALSE em que, para criar uma matriz em que é preenchida de elementos por linha, basta alterar o argumento para byrow = TRUE. Além disso, a função seq() está gerando os elementos da matriz enquanto o argumento nrow indica o número de linhas e ncol o número de colunas.\nPara visualizar um elemento específico de uma matriz podemos utilizar o mesmo método que usamos com vetores. Lembrando que matrizes ainda são vetores, porém, com uma dimensão a mais. Então, para visualizar um elemento específico devemos indicar a posição do elemento para todas as dimensões existentes, no caso das matrizes, para linha e coluna.\n\n# Obtendo linhas, colunas e elementos específicos.\nmatri[3,  ]   # seleciona a 3ª linha.\n\n[1]  3  7 11 15\n\nmatri[ , 2]   # seleciona a 2ª coluna.\n\n[1] 5 6 7 8\n\nmatri[1, 2]   # seleciona o elemento da primeira linha e segunda coluna.\n\n[1] 5\n\n\nPerceba que cada linha e cada coluna de uma matriz é um vetor (uma dimensão). Assim, podemos alterar uma linha ou uma coluna atribuindo um vetor de interesse, por exemplo.\n\n# substituindo a primeira linha e quarta coluna da matriz.\nmatri[1, ] <- c(9, 9, 9, 9)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    9\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nmatri[, 4] <- rep(1, 4)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    1\n[2,]    2    6   10    1\n[3,]    3    7   11    1\n[4,]    4    8   12    1\n\n\nÉ de importância para o usuário verificar o tamanho (número de elementos) quando se trata de vetores. Porém, quando se trata de matrizes, é importante conhecer as dimensões além do número de elementos. Para verificar as dimensões de uma matriz podemos utilizar a função dim(), enquanto para o tamanho (número de elementos) ainda podemos utilizar a função length().\n\n# Verificando o tamanho e dimensões da matriz.\nlength(matri)\n\n[1] 16\n\ndim(matri)\n\n[1] 4 4\n\n\nComo sabemos que as linhas e colunas de uma matriz são vetores, podemos adicionar mais linhas e colunas a ela com os elementos que queremos. Para concatenar linhas e colunas em uma matriz podemos utilizar as funções rbind() e cbind() respectivamente.\n\nvet1 <- c(99, 98, 97, 95)\nvet2 <- c(0, 5, 7, 9, 99) \n(matri <- rbind(matri, vet1))\n\n     [,1] [,2] [,3] [,4]\n        9    9    9    1\n        2    6   10    1\n        3    7   11    1\n        4    8   12    1\nvet1   99   98   97   95\n\n(matri <- cbind(matri, vet2))\n\n                 vet2\n      9  9  9  1    0\n      2  6 10  1    5\n      3  7 11  1    7\n      4  8 12  1    9\nvet1 99 98 97 95   99\n\n\nOperações matemáticas entre matrizes e elementos são realizadas elemento a elemento assim como vetores. Porém, quando se trata de matrizes, é de interesse efetuar a multiplicação matricial clássica, o que pode ser feito com a operação %*% respeitando a equidade do número de colunas da matriz que pré-multiplica e o número de linhas da matriz que pós-multiplica.\n\n# Criando duas matrizes 2x2 (duas linhas e duas colunas).\n(matriz1 <- matrix(c(rep(1, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    2\n\n(matriz2 <- matrix(c(rep(2, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n# Soma duas matrizes (elemento a elemento).\nmatriz1 + matriz2\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    3    4\n\n# Subtrai duas matrizes (elemento a elemento).\nmatriz1 - matriz2\n\n     [,1] [,2]\n[1,]   -1    0\n[2,]   -1    0\n\n# Divide duas matrizes (elemento a elemento).\nmatriz1/matriz2\n\n     [,1] [,2]\n[1,]  0.5    1\n[2,]  0.5    1\n\n# Multiplica duas matrizes (elemento a elemento).\nmatriz1 * matriz2\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    2    4\n\n# Multiplicação matricial clássica.\nmatriz1 %*% matriz2\n\n     [,1] [,2]\n[1,]    6    6\n[2,]    6    6\n\n# Potência de uma matriz (elemento a elemento).\n(matriz3 <- matriz2^2)\n\n     [,1] [,2]\n[1,]    4    4\n[2,]    4    4\n\n# Raiz quadrada de uma matriz (elemento a elemento).\nsqrt(matriz3)\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nA.5.6 Fatores\nÉ muito comum termos que lidar com variáveis categóricas, ou seja, variáveis que possuem categorias intrínsecas em sua natureza. No R, existe uma classe de objetos chamada Fatores especificamente para representar esse tipo de variável (nominal e ordinal). Os fatores podem ser vistos como vetores de elementos numéricos inteiros (pois são assim internamente representados no R) e possuem rótulos (labels). Consequentemente, são vetores do tipo Double.\n\n# Criando um vetor/variável com a informação do sexo de 7 pessoas. \n(sexo1 <- c(\"Mulher\", \"Homem\", \"Homem\", \"Mulher\", \"Mulher\", \"Mulher\", \"Homem\"))\n\n[1] \"Mulher\" \"Homem\"  \"Homem\"  \"Mulher\" \"Mulher\" \"Mulher\" \"Homem\" \n\n# Verificando a classe da variável sexo1.\nclass(sexo1)\n\n[1] \"character\"\n\n# Transformando em fator.\n(sexo2 <- as.factor(sexo1))\n\n[1] Mulher Homem  Homem  Mulher Mulher Mulher Homem \nLevels: Homem Mulher\n\nclass(sexo2)\n\n[1] \"factor\"\n\n# Verificando os levels da variável de classe factor (sexo2).\nlevels(sexo2)\n\n[1] \"Homem\"  \"Mulher\"\n\n\nPodemos verificar que a variável é representada internamente por elementos numéricos inteiros ao tentar transformá-la em um vetor numérico com a função as.numeric().\n\n# Ao transformar sexo1 obteremos um vetor de dados faltantes (NA) por coerção.\nas.numeric(sexo1)\n\nWarning: NAs introduzidos por coerção\n\n\n[1] NA NA NA NA NA NA NA\n\n# Ao transformar sexo2 obteremos um vetor double com valores inteiros.\n(sexo2_num <- as.numeric(sexo2))\n\n[1] 2 1 1 2 2 2 1\n\ntypeof(sexo2_num)\n\n[1] \"double\"\n\n\nFatores possuem levels em ordem alfabética, e isso pode influenciar diretamente na hora de construir gráficos e realizar aplicações de modelos.\n\n\nA.5.7 Data Frame\nTrata-se de uma “tabela de dados” onde as colunas são as variáveis e as linhas são os registros, e as colunas podem ser de classes diferentes. Logo, a principal diferença entre data frame e matriz é que matrizes só podem conter elementos da mesma classe.\nPara criar data frame no R é utilizado a função data.frame().\n\n# Colunas/variáveis para o data frame.\nID <- seq(1,6)\npes <- c(62, 70, 52, 98, 90, 70)\nalt <- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61)\nimc <- pes/(alt^2)\n\n# Criando o data frame.\n(dados <- data.frame(ID = ID, peso = pes, altura = alt, imc = imc))\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nPodemos pensar na estrutura de um data frame da mesma forma que de uma matriz. Se por acaso for do interesse olhar os dados de altura, por exemplo, basta acessar a coluna três do data frame.\n\n# Selecionando a variável \"altura\".\ndados[, 3]\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n\nEmbora possamos usar os mesmos métodos discutidos na seção de matrizes, quando se trata de data frames, usualmente selecionamos as variáveis de interesse sem ter que saber em qual coluna ela está. Isso pode ser feito ao utilizar o símbolo $, dessa forma a coluna será selecionada em forma de vetor.\n\n# Selecionando a variável \"altura\".\ndados$altura\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Dica: também é possível fazer a seleção de colunas da seguinte forma:\ndados[, c(\"altura\", \"peso\")]\n\n  altura peso\n1   1.70   62\n2   1.82   70\n3   1.75   52\n4   1.94   98\n5   1.84   90\n6   1.61   70\n\n\nUtilizando o mesmo símbolo podemos adicionar ou deletar colunas.\n\n# Adicionando a variável \"grupo\".\ngr <- c(rep(1,3),rep(2,3))\ndados$grupo <- gr\ndados\n\n  ID peso altura      imc grupo\n1  1   62   1.70 21.45329     1\n2  2   70   1.82 21.13271     1\n3  3   52   1.75 16.97959     1\n4  4   98   1.94 26.03890     2\n5  5   90   1.84 26.58318     2\n6  6   70   1.61 27.00513     2\n\n# Deletando a variável \"grupo\".\ndados$grupo <- NULL\ndados\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nNote que ao adicionar variáveis a um data frame essa variável tem que ter o mesmo número de elementos que as demais variáveis, caso isso não seja respeitado o R ira retornar um erro.\nA estrutura de data frame é provavelmente a mais utilizada no dia a dia de quem analisa dados. Sabendo disso, existem algumas funções que são importantes de um usuário de R ter em mente.\n\nhead() - Mostra as primeiras 6 linhas.\ntail() - Mostra as últimas 6 linhas.\ndim() - Número de linhas e de colunas.\nnames() - Os nomes das colunas (variáveis).\nstr() - Estrutura do data frame. Mostra, entre outras coisas, a classe de cada coluna.\n\nAlgumas dessas funções já foram abordadas ao longo do texto. As funções de visualização head() e tail() possuem um argumento chamado n o qual podemos customizar o número de linhas que queremos visualizar.\n\nhead(dados, n = 4)\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n\ntail(dados, n = 4)\n\n  ID peso altura      imc\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\ndim(dados)\n\n[1] 6 4\n\nnames(dados)\n\n[1] \"ID\"     \"peso\"   \"altura\" \"imc\"   \n\nstr(dados)\n\n'data.frame':   6 obs. of  4 variables:\n $ ID    : int  1 2 3 4 5 6\n $ peso  : num  62 70 52 98 90 70\n $ altura: num  1.7 1.82 1.75 1.94 1.84 1.61\n $ imc   : num  21.5 21.1 17 26 26.6 ...\n\n\nCada coluna do data frame pode ser interpretada como um vetor. Dessa forma, as operações de vetores discutidas anteriormente são válidas.\n\n# Cria uma coluna do produto de peso por altura.\ndados$pesovezesaltura <- dados$peso * dados$altura\ndados\n\n  ID peso altura      imc pesovezesaltura\n1  1   62   1.70 21.45329          105.40\n2  2   70   1.82 21.13271          127.40\n3  3   52   1.75 16.97959           91.00\n4  4   98   1.94 26.03890          190.12\n5  5   90   1.84 26.58318          165.60\n6  6   70   1.61 27.00513          112.70\n\n# Cria uma coluna de peso + cinco.\ndados$peso5 <- dados$peso + 5\ndados\n\n  ID peso altura      imc pesovezesaltura peso5\n1  1   62   1.70 21.45329          105.40    67\n2  2   70   1.82 21.13271          127.40    75\n3  3   52   1.75 16.97959           91.00    57\n4  4   98   1.94 26.03890          190.12   103\n5  5   90   1.84 26.58318          165.60    95\n6  6   70   1.61 27.00513          112.70    75\n\n# Cria uma coluna da metade do peso original.\ndados$pesometade <-  dados$peso/2\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n\n\n\nA.5.8 Operadores lógicos\nSabemos que TRUE e FALSE são objetos que pertencem à classe logical, além de terem representação numérica de 1 e 0 respectivamente.\nA operação lógica nada mais é do que um teste que retorna verdadeiro (TRUE) ou falso (FALSE). Assim, podemos realizar comparações entre valores utilizando alguns operadores específicos.\n\n# Verifica se 9 é igual a 12.\n9 == 12\n\n[1] FALSE\n\n# Verifica se 12 é igual a 12.\n12 == 12\n\n[1] TRUE\n\n# Verifica se 9 é diferente de 12.\n9 != 12\n\n[1] TRUE\n\n# Verifica se 9 é maior que 5.\n9 > 5\n\n[1] TRUE\n\n# Verifica se 9 é maior ou igual a 9.\n9 >= 9\n\n[1] TRUE\n\n# Verifica se 4 é menor que 4.\n4 < 4\n\n[1] FALSE\n\n# Verifica se 4 é menor ou igual que 4.\n4 <= 4\n\n[1] TRUE\n\n\nPodemos notar que estes operadores funcionam bem com números, mas isso não é verdade quando se trata de objetos do tipo character (texto). Dentre esses, o operador == apenas funciona com números e o != funciona normalmente tanto com números quanto para textos. Os operadores >, >=, < e <= funcionam com textos pensando na ordem alfabética destes.\nPodemos utilizar operadores de comparação múltipla mais usuais em conjunto com estes discutidos para tornar as comparações ainda mais dinâmicas.\n\nE: & - será verdadeiro se todas operações forem TRUE.\n\n\nx <- 17\n\n# Verifica se x > 9 é verdadeiro E x < 50 é verdadeiro.\n(x > 9) & (x < 50)\n\n[1] TRUE\n\n# Verifica se x < 9 é verdadeiro E x < 50 é verdadeiro E x > 17 é verdadeiro.\n(x > 9) & (x < 50) & (x > 17)\n\n[1] FALSE\n\n\n\nOU: | - será verdadeiro se pelomenos uma operação for TRUE.\n\n\nx <- 17\n\n# Verifica se x < 9 é verdadeiro OU x < 50 é verdadeiro.\n(x < 9) | (x < 50)\n\n[1] TRUE\n\n# Verifica se x < 9 é verdadeiro OU x > 50 é verdadeiro OU x <= 17 é verdadeiro.\n(x < 9) | (x > 50) | (x <= 17)\n\n[1] TRUE\n\n\n\nNegação: ! - nega a resposta lógica da comparação.\n\n\nx <- 17\n\n# Retorna TRUE se x < 50 for FALSE, e FALSE caso contrário. \n!(x < 50)\n\n[1] FALSE\n\n\nPodemos verificar se um valor (ou conjunto de valores) está contido em um vetor utilizando o operador %in%.\n\nex <- 1:15\n\n# Verifica se os valores 3 e 5 fazem parte dos elementos do vetor ex.\nc(3, 5) %in% ex\n\n[1] TRUE TRUE\n\n# Dica: o operador %in% também funciona com character:\ntexto <- c(\"hospital1\", \"hospital2\", \"hospital3\", \"hospital4\", \"hospital5\")\nc(\"hospital5\", \"UTI\") %in% texto\n\n[1]  TRUE FALSE\n\n\nTodos esses operadores podem ser utilizados ao manipular data frames. Iremos aproveitar o data frame criado anteriormente e adicionar mais duas colunas de textos para realizar alguns testes.\n\n# Visualizando o data frame criado anteriormente\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n# Adicionando a coluna sexo.\ndados$sexo <- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"M\")\n\n# Adicionando a coluna olhos (preenchimento impreciso = F).\ndados$olhos <- c(\"preto\", \"castanho\", \"F\", \"preto\", \"azul\", \"F\")\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Utilizando o operador %in% para obter as linhas com a cor dos olhos imprecisa.\ndados[dados$olhos %in% dados$sexo, ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo olhos\n3  3   52   1.75 16.97959            91.0    57         26    M     F\n6  6   70   1.61 27.00513           112.7    75         35    M     F\n\n# %in% com ! para obter as linhas com a cor dos olhos correta.\ndados[!(dados$olhos %in% dados$sexo), ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n\n# Linhas onde o peso é menor que o imc + 40. Retorna apenas colunas peso e imc.\ndados[(dados$peso < (dados$imc + 40)), c(\"peso\", \"imc\")]\n\n  peso      imc\n3   52 16.97959\n\n\n\n\nA.5.9 Dados faltantes, infinitos e indefinições matemáticas\nDados faltantes é uma das coisas mais comuns em bases de dados, podendo surgir por diferentes fatores. No R, dados faltantes são representados por NA e é um símbolo que todo usuário deve conhecer e saber lidar. Além do NA, símbolos como NaN e Inf também são muito comuns no dia a dia.\n\nNA (Not Available): dado faltante/indisponível.\nNaN (Not a Number): indefinições matemáticas. Como 0/0 e log(-1).\nInf (Infinito): número muito grande ou o limite matemático. Aceita sinal negativo (-Inf).\n\n\nx <- c(1, 6, 9)\n\n# Retorna NA\nx[4]\n\n[1] NA\n\n# Retorna NaN\nlog(-10)\n\nWarning in log(-10): NaNs produzidos\n\n\n[1] NaN\n\n# Retorna Inf\n10^14321\n\n[1] Inf\n\n\nAo lidar com bases de dados é necessário saber verificar se ela apresenta dados faltantes.\n\n# Base de dados que estamos usando.\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Adiciona linhas com dados faltantes.\ndados <- rbind(dados, c(6, NA, 1.75, NA, 125, 99, 50, \"M\", \"castanho\"))\ndados <- rbind(dados, c(9, 50, NA, 50, 127, 97, 55, \"F\", \"azul\"))\n\n# Deleta colunas que não iremos mais usar.\ndados[, c(\"pesovezesaltura\", \"peso5\", \"pesometade\")] <- NULL\ndados\n\n  ID peso altura              imc sexo    olhos\n1  1   62    1.7 21.4532871972318    M    preto\n2  2   70   1.82 21.1327134404057    F castanho\n3  3   52   1.75 16.9795918367347    M        F\n4  4   98   1.94 26.0388989265597    F    preto\n5  5   90   1.84 26.5831758034026    F     azul\n6  6   70   1.61 27.0051309748852    M        F\n7  6 <NA>   1.75             <NA>    M castanho\n8  9   50   <NA>               50    F     azul\n\n# Ao incluir NA a variável imc passou a apresentar mais casas decimais.\n# Dica: podemos arredondar os valores do vetor alterados com a função round().\ndados[1:6, \"imc\"] <- round(as.numeric(dados[1:6, \"imc\"]), digits = 2) \ndados\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n7  6 <NA>   1.75  <NA>    M castanho\n8  9   50   <NA>    50    F     azul\n\n# Avalia se os elementos da coluna peso são NA ou não.\nis.na(dados$peso)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n# Verifica se existe pelomenos 1 dado faltante no data frame.\nany(is.na(dados))\n\n[1] TRUE\n\n# Filtra apenas as linhas com NA na variável peso.\ndados[is.na(dados$peso),]\n\n  ID peso altura  imc sexo    olhos\n7  6 <NA>   1.75 <NA>    M castanho\n\n# Dica: as funções na.omit() e complete.cases() podem remover linhas com NA.\nna.omit(dados)\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\ndados[complete.cases(dados), ]\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\n\nPara lidar com dados faltantes é importante ter pelo menos uma ideia do motivo para eles existirem na base de dados sendo analisada. Muitas vezes não temos ideia desse motivo, e a melhor estratégia acaba sendo analisar os dados, incluindo e reportando com transparência os dados faltantes. Ao analisar dados sem excluir os casos faltantes, muitas vezes nos deparamos com erros inesperados que ocorrem por tentarmos usar funções que não estão considerando esses casos. Situações como essas exigem uma atenção a mais do usuário, tendo que pesquisar e ler documentações de funções para ter certeza do que a função sendo usada está fazendo.\n\n# Criando um vetor com dados faltante.\nvetor1 <- c(NA, 1, 1, 1, 5)\n\n# mean() calcula a média do vetor.\nmean(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nmean(vetor1, na.rm = TRUE)\n\n[1] 2\n\n# sum() calcula a soma dos elementos do vetor.\nsum(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nsum(vetor1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\nA.5.10 Condicionamento: If e else\nAs estruturas if e else, também chamadas de condicionais, servem para executar códigos apenas se uma condição (teste lógico) for satisfeita.\n\nvalor1 <- 224\nvalor2 <- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta <- 10 # resposta é 10.\n} else { # caso contrário.\n  resposta <- 15 # respota é 15.\n  }\nresposta\n\n[1] 15\n\n\nVeja que o R só executa o conteúdo das chaves {} se a expressão dentro dos parênteses () retornar TRUE. Além disso, note que a condição de igualdade é representada por dois iguais (==). Como dito anteriormente, apenas um igual (=) é símbolo de atribuição (preferível <-), em argumentos de estruturas condicionais queremos realizar comparações.\nPara utilizar mais condições podemos utilizar o else if ().\n\nvalor1 <- 224\nvalor2 <- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta <- 10 # resposta é 10.\n} else if (valor1 > valor2) { # Se não, então valor1 é maior que valor2 ?\n  resposta <- 15 # então a resposta é 15.\n  } else { # caso contrário.\n    resposta <- 25 # respota é 25.\n    }\nresposta\n\n[1] 25\n\n\n\n\nA.5.11 Iterador for\nO for serve para repetir uma mesma tarefa para um conjunto de valores diferentes (realiza um loop). Cada repetição é chamada de iteração.\nComo exemplo, considere o vetor atribuído ao objeto vetor1 como segue:\n\nvetor1 <- c(1,20,50,60,100)\n\nPodemos criar um novo vetor que seja formado por cada elemento do vetor1 dividido por sua posição.\n\nvetor2 <- NULL\nfor (i in 1: length(vetor1)){\n  vetor2[i] <- vetor1[i]/i\n}\nvetor2\n\n[1]  1.00000 10.00000 16.66667 15.00000 20.00000\n\n\nNote que primeiro definimos o objeto vetor2, recebendo NULL. O NULL representa a ausência de um objeto e serve para já declarar algum objeto que receberá valor na sequência. Ao rodar o for, o vetor2 passa a ser um vetor de tamanho 5 (tamanho do vetor1).\nNo exemplo, temos 5 iterações e para cada valor de i, correndo de 1 até 5 (tamanho do vetor1), pegamos o valor do vetor1 na posição i e dividimos por i. Assim, formamos o vetor2.\n\n\nA.5.12 Funções\nFunções no R são nomes que guardam um código de R. A ideia é que sempre que rodar a função com os seus argumentos, o código que ela guarda será executado e o resultado será retornado.\nJá usamos anteriormente algumas funções que estão na base do R. Por exemplo, quando usamos class() para entender a classe do objeto que o R está entendendo. Colocamos um argumento dentro do parêntese e o R retornou qual a classe do objeto em questão.\nImportantes:\n\nSe a função tiver mais de um argumento, eles são sempre separados por vírgulas;\nCada função tem os seus próprios argumentos. Para saber quais são e como usar os argumentos de uma função, basta acessar a sua documentação. Uma forma de fazer isso é pela função help(), cujo argumento é o nome da função em questão.\n\n\nhelp(mean)\n\nVeja que abrirá a documentação sobre a função mean no menu “Help” do RStudio. Assim, é possível ver os argumentos e exemplos de uso da função.\nAinda sobre funções já presentes no R, vamos considerar agora a função sample. Veja a documentação dessa função para ver o que ela faz.\n\nhelp(sample)\n\nA função sample retorna uma amostra de um vetor com tamanho especificado em um de seus argumentos com ou sem reposição. Ela apresenta quatro argumentos na forma sample(x, size, replace = FALSE, prob = NULL), em que: x é o vetor do qual será amostrado o número de elementos especificado no argumento size, replace indica se é com ou sem reposição e prob é para especificar probabilidades de seleção.\nPodemos usar essa função para amostrar de um objeto dois elementos (size = 2) em uma seleção com reposição (replace = TRUE) e que a probabilidade de seleção seja a mesma para todos os elementos do vetor. No caso da probabilidade, como podemos ver na documentação da função sample, o default (padrão se o usuário não mudar o argumento) é ser a mesma probabilidade de seleção para todos os elementos. Assim, se o usuário nada especificar para esse argumento, o R entenderá o seu default. O mesmo vale para o argumento replace: caso fosse o interesse fazer a seleção sem reposição, não precisaríamos colocar esse argumento por seu default ser FALSE.\n\n\n[1] 20  1\n\n\nTambém poderíamos usar a mesma função sem colocar o nome dos argumentos, desde que o usuário entenda o que ela está fazendo.\n\nsample(vetor_am, 2 , TRUE) \n\n[1] 1 2\n\n\nNesse caso, é importante que se respeite a ordem dos argumentos: o vetor tem que ser o primeiro, o segundo argumento é size e assim por diante.\nVale ressaltar que as duas últimas saídas não necessariamente serão as mesmas, porque é feito um sorteio aleatório de dois elementos de vetor_am em cada uma delas.\nAlém de usar funções já prontas, podemos criar novas funções. Suponha que queremos criar uma função de dois argumentos que retorna o primeiro mais três vezes o segundo argumento.\n\nf_conta <- function(x, y) {\n  out <- x + 3 * y\n  return(out)\n}\n\nA função acima possui:\n\nnome: f_conta.\nargumentos: x e y.\no corpo out: <- x + 3 * y.\no que retorna: return(out).\n\nPara chamar a função e utilizá-la basta chamar pelo nome com os devidos argumentos, assim como temos feito até então.\n\nf_conta(x = 10, y = 20)\n\n[1] 70\n\n\nVeja que o cálculo acima retorna exatamente o mesmo que o seguinte:\n\nf_conta(y = 20, x = 10)\n\n[1] 70\n\n\nIsso acontece porque a ordem dos argumentos foi alterada, porém, mantendo seus devidos nomes. Se não quiser colocar os nomes dos argumentos, precisa tomar cuidado para não errar a ordem deles. Isso porque:\n\nf_conta(10,20)\n\n[1] 70\n\n\né diferente de:\n\nf_conta(20,10)\n\n[1] 50\n\n\n\n\nA.5.13 Como obter ajuda no R\nListamos aqui 3 maneiras para buscar ajuda no R:\n\nHelp/documentação do R (comandos help(nome_da_funcao) ou ?nome_da_funcao). Como exemplo:\n\n\nhelp(mean) \n?mean\n\n\nGoogle: especificar a linguagem é de suma importância na pesquisa, além de deixar o problema ou a função bem claro.\n\n\n\n\n\n\nPesquisa no Google\n\n\n\n\n\nComunidade: O Stack Overflow e o Stack Overflow em Português são sites de perguntas e respostas amplamente utilizados por todas as linguagens de programação.\n\n\n\nA.5.14 Pacotes\nComo dito quando falamos “Sobre o R”, o R apresenta funções na sua base e também em forma de pacotes (conjunto de funções bem documentado), que precisam ser instalados (uma vez no seu computador) e carregados na sessão de utilização do R (carregado em toda sessão aberta).\nDificilmente uma análise será feita apenas com as funções básicas do R e dificilmente não vai existir um pacote com as funções que você precisa. Por esse motivo, falamos a seguir em como instalar e carregar pacotes.\n\nA.5.14.1 Instalação de pacotes\n\nVia CRAN:\n\n\ninstall.packages(\"nome-do-pacote\")\n\nExemplo: Instalação do pacote dplyr.\n\ninstall.packages(\"dplyr\")\n\nNote que o nome do pacote está entre aspas.\n\nVia Github: Para instalar via Github precisa primeiramente instalar o pacote devtools.\n\n\ndevtools::install_github(\"nome-do-repo/nome-do-pacote\")\n\nExemplo:\n\ndevtools::install_github(\"tidyverse/dplyr\")\n\n\n\nA.5.14.2 Carregar pacotes\nUma vez que um pacote de interesse está instalado em sua máquina, para carregá-lo na sessão atual do R é só rodar a seguinte linha de comando:\n\nlibrary(nome-do-pacote)\n\nVeja que para carregar o pacote não se usa aspas.\nComo exemplo, o carregamento do pacote dplyr:\n\nlibrary(dplyr)\n\nSó é necessário instalar o pacote uma vez, mas é necessário carregá-lo toda vez que começar uma nova sessão.\nDado que o pacote está carregado ao rodar a função library(), todas as funções desse pacote podem ser usadas sem problemas.\nCaso você não queira carregar o pacote e apenas usar uma função específica do pacote, você pode usar nome-do-pacote::nome-da-funcao. Por exemplo:\n\ndplyr::distinct(...)\n\nTendo carregado o pacote dplyr anteriormente (pela função library()), não seria necessário colocar dplyr:: antes da função distinct do pacote.\n\n\n\nA.5.15 Materiais complementares\nLivros e Artigos:\n\nCritical Thinking in Clinical Research. Felipe Fregni & Ben M. W. Illigens. 2018.\nCHAPTER 3: Selecting the Study Population. In: Critical Thinking in Clinical Research by Felipe Fregni and Ben Illigens. Oxford University Press 2018.\nFandino W. Formulating a good research question: Pearls and pitfalls. Indian J Anaesth. 2019;63(8):611–616. doi:10.4103/ija.IJA_198_19\nRiva JJ, Malik KM, Burnie SJ, Endicott AR, Busse JW. What is your research question? An introduction to the PICOT format for clinicians. J Can Chiropr Assoc. 2012;56(3):167–171.\nExternal validity, generalizability, and knowledge utilization. Ferguson L1. J Nurs Scholarsh. 2004;36(1):16-22.\nPeter M Rothwell; Commentary: External validity of results of randomized trials: disentangling a complex concept, International Journal of Epidemiology, Volume 39, Issue 1, 1 February 2010, Pages 94–96, https://doi.org/10.1093/ije/dyp305\n\nSites:\n\nhttps://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/1-data-display-and-summary\nhttp://www.sthda.com/english/wiki/statistical-tests-and-assumptions"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Anton, Howard, and Chris Rorres. 2001. Álgebra Linear\nCom Aplicações. Vol. 8. Bookman Porto\nAlegre.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied\nMultivariate Statistical Analysis. Vol. 5. 8. Prentice hall Upper\nSaddle River, NJ."
  }
]