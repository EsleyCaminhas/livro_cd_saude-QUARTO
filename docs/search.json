[
  {
    "objectID": "testes_np.html#grupos-não-pareados",
    "href": "testes_np.html#grupos-não-pareados",
    "title": "6  Testes não-paramétricos",
    "section": "6.1 Grupos não-pareados",
    "text": "6.1 Grupos não-pareados\nPara grupos não-pareados ou independentes, dentre os testes mais conhecidos e utilizados temos os testes de Mann-Whitney e Kolmogorov-Smirnov para dois grupos, e o de Kruskal-Wallis para três ou mais grupos.\n\n6.1.1 Teste de Mann-Whitney\nO teste de Mann-Whitney é um teste estatístico não paramétrico usado para comparar dois grupos independentes, sendo o grau de medida pelo menos ordinal para variável de interesse e nominal para variável independente. Além disso, o teste é geralmente usado quando a variável resposta ou de interesse não é normalmente distribuída, sendo assim, uma alternativa não-paramétrica ao teste t para amostras independentes.\nQuando o resultado do teste for significativo, significa que ambos os grupos representam populações com distribuições diferentes. Caso a suposição de que ambas as distribuições são iguais seja feita, então podemos interpretar o teste como uma comparação de medianas, que ao ser significativo dizemos que as medianas são significativamente diferentes. Assim, considerando dois grupos independentes, as hipóteses podem ser definidas como:\n\\(H_0:\\) distribuição\\(_1=\\) distribuição\\(_2\\)\n\\(H_{\\mathrm{a}}:\\) distribuição\\(_1 \\neq\\) distribuição\\({ }_2\\)\nSuponha que queremos comparar as idades (anos) de gestantes hospitalizadas com Covid-19 de dois hospitais diferentes. Considere as idades coletadas nos seguintes grupos:\nHospital A: \\(\\quad 46,48,34,36,22,44,50\\)\nHospital B: \\(\\quad 25,21,39,14,37\\)\nConfirmado que o teste de Mann-Whitney é apropriado, o primeiro passo que devemos tomar, é ordenar os valores de ambos os grupos de forma crescente em um único grupo, e então, identificar o posto de cada valor desse grupo. Para o exemplo abordado, teremos o seguinte:\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & 14 & 21 & \\color{red}{22} & 25 & \\color{red}{34} & \\color{red}{36} & 37 & 39 & \\color{red}{44} & \\color{red}{46} & \\color{red}{48} & \\color{red}{50}\\\\\n\\hline \\text { Posto } & 1 & 2 & \\color{red}{3} & 4 & \\color{red}{5} & \\color{red}{6} & 7 & 8 & \\color{red}{9} & \\color{red}{10} & \\color{red}{11} & \\color{red}{12}\\\\\n\\hline\n\\end{array}\\]\nOnde está destacado em vermelho as idades das gestantes hospitalizadas no Hospital A e seus respectivos postos. Além disso, destacado em preto essa mesma informação para as gestantes hospitalizadas no Hospital B.\n\n6.1.1.1 Estatística \\(U\\)\nO teste de Mann-Whitney também é chamado de teste \\(U\\) de Mann-Whitney, pois calculamos o que chamamos de estatística \\(U\\), a qual é uma quantidade baseada na soma dos postos identificados de cada grupo calculada para fazer a avaliação das hipóteses.\nAgora, considere \\(n_1\\) e \\(n_2\\) como o número de observações do grupo 1 e grupo 2 respectivamente, em que, o grupo 1 será aquele com o menor número de observações (caso ambos os grupos tenham a mesma quantidade de observações, então \\(n_1 = n_2\\)). Além disso, considere \\(R_1\\) e \\(R_2\\) como a soma dos postos identificados nos grupos 1 e 2 respectivamente, e \\(N = n_1 + n_2\\) como o total de observações considerando ambos os grupos.\nDefinido os termos necessários e considerando o exemplo apresentado, calculamos as seguintes quantidades:\n\\(U_1=n_1n_2+\\displaystyle \\frac{n_1\\left(n_1+1\\right)}{2}-R_1=5\\cdot7+\\displaystyle \\frac{5 \\cdot\\left(5+1\\right)}{2}-22 = 28\\),\n\\(U_2=n_1 n_2+\\displaystyle \\frac{n_2\\left(n_2+1\\right)}{2}-R_2=5 \\cdot 7+\\displaystyle \\frac{7 \\cdot\\left(7+1\\right)}{2}-56 = 7\\).\nTendo calculado \\(U_1\\) e \\(U_2\\) a estatística \\(U\\) será o menor valor entre ambas, neste caso, \\(U = U_2\\). Podemos confirmar se todo procedimento foi feito corretamente ao verificar a relação \\(n_1n_2 = U_1 + U_2\\), a qual será \\(5\\cdot 7 = 28 + 7 = 35\\) no exemplo considerado.\n\n\n6.1.1.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos calcular a quantidade \\(U_c\\), o qual é o valor crítico de \\(U\\) necessário para decidir em relação às hipóteses. Este valor pode ser encontrado em tabelas de valores críticos da estatística U bilateral (ou unilateral a depender das hipóteses), ou podemos utilizar a função qwilcox() do R e subtrair o resultado por 1, para obtê-lo no caso bilateral. Para o exemplo dos hospitais temos \\(n_1 = 5\\) e \\(n_2 = 7\\), considerando nível de 5% de significância, podemos utilizar a função qwilcox() para encontrar \\(U_c\\) da seguinte forma:\n\nqwilcox(0.05/2,5,7) - 1\n\n[1] 5\n\n\nOnde \\(0.05/2\\) é o nível de significância dividido por dois devido ao teste ser bilateral, e subtraímos por \\(1\\) como uma correção matemática por \\(U\\) ser discreto e qwilcox() calcular quantis, e não valores críticos de \\(U\\). Assim, \\(U_c = 5\\) no exemplo considerado.\nAssim, rejeitamos a hipótese nula quando o valor da estatística \\(U\\) calculada for menor ou igual ao valor crítico de \\(U\\), ou seja, quando \\(U \\leq U_c\\). Com os valores calculados de \\(U = 7\\) e \\(U_c = 5\\), podemos concluir ao nível de 5% de significância que não há diferença significativa entre as distribuições das idades das gestantes hospitalizadas em ambos hospitais.\n\n\n6.1.1.3 Empates\nDependendo do problema, pode existir valores repetidos observados, e ao ordená-los para identificar os postos, esses valores são considerados empates.\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & 14 & 21 & \\color{red}{25} & \\color{red}{25} & \\color{red}{34} & \\color{red}{36} & 37 & 39 & \\color{red}{44} & \\color{red}{46} & \\color{red}{48} & \\color{red}{50}\\\\\n\\hline \\text { Posto } & 1 & 2 & \\color{red}{3,5} & \\color{red}{3,5} & \\color{red}{5} & \\color{red}{6} & 7 & 8 & \\color{red}{9} & \\color{red}{10} & \\color{red}{11} & \\color{red}{12}\\\\\n\\hline\n\\end{array}\\]\nObserve nesse exemplo que há um empate para idade de 25 anos. Nesse caso, ao ordenar os valores teríamos os postos 3 e 4 para esses valores repetidos, mas devemos atribuir a média desses postos para dar continuidade ao teste. Assim, temos \\(\\displaystyle \\frac{3 + 4}{2}= 3,5\\).\n\n\n6.1.1.4 Estatística \\(U\\) normalizada\nÉ tido como um consenso a utilização dos procedimentos apresentados até então sobre o teste Mann-Whitney no cenário em que \\(n_1\\) e \\(n_2\\) são menores ou iguais que 20. Por outro lado, no cenário em que \\(n_1\\) e \\(n_2\\) são grandes (\\(&gt;20\\)), podemos aproximar a distribuição de \\(U\\) para uma normal. Assim, calculamos a estatística \\(Z_u\\) para avaliar as hipóteses da seguinte forma:\n\\[Z_u = \\displaystyle \\frac{U-\\displaystyle\\frac{n_1  n_2}{2}}{\\sqrt{\\displaystyle\\frac{n_1 n_2\\left(n_1+n_2+1\\right)}{12}}}.\\]\nEsse procedimento nada mais é do que subtrair a média de \\(U\\) e dividir pelo seu desvio padrão.\nAssim, para avaliar as hipóteses, é necessário obter o valor crítico de \\(Z_u\\), o qual pode ser obtido a partir de tabelas de valores críticos da distribuição normal padrão, ou podemos recorrer à função qnorm() no R com os seguintes argumentos: qnorm(0.05/2, lower.tail = FALSE) para teste bilateral e nível de 5% de significância; qnorm(0.05, lower.tail = FALSE) para teste unilateral e nível de 5% de significância. Os valores críticos de \\(Z_u\\) para o teste bilateral e unilateral ao nível de 5% de significância são 1,96 e 1,65 respectivamente. Assim, rejeitamos a hipótese nula quando \\(|Z_u|\\geq1,96\\) e \\(|Z_u|\\geq1,65\\) no teste bilateral e unilateral respectivamente.\nCaso haver empates, a aproximação deve ser feita da seguinte forma:\n\\[Z_u = \\displaystyle \\frac{U-\\displaystyle\\frac{n_1  n_2}{2}}{\\sqrt{\\displaystyle\\frac{n_1n_2\\left(n_1+n_2+1\\right)}{12}- \\displaystyle\\frac{n_1n_2\\left[\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\right]}{12  \\left(n_1+n_2\\right) \\left(n_1+n_2-1\\right)}}},\\]\nOnde, na quantidade \\(\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) o \\(s\\) indica quantos grupos de empates ocorreram, ex: idade 25 se repete duas vezes e idade 45 se repete cinco vezes, totalizando 2 grupos de empates ou de repetições. Além disso, o termo \\(t\\) é o número total de empates no grupo de repetições \\(i\\). Assim, para as repetições hipotéticas da idade 25 e 45 teremos:\n\\[\\sum_{i=1}^2\\left(t_i^3-t_i\\right) = (2^3 - 2) + (5^3 - 5) = 126.\\]\nDessa forma, podemos calcular \\(Z_u\\) e então avaliar as hipóteses da mesma forma que avaliamos no cenário sem empates.\n\n\n6.1.1.5 Como aplicar o teste no R\nNa prática, tendo os dados em um formato de planilha, podemos carregá-los no software R e então aplicar o teste recorrendo a uma função específica, a qual irá efetuar os procedimentos formalmente explicados ao longo do texto e retornar um valor-p, o qual podemos utilizar para avaliar as hipóteses, onde, rejeitamos a hipótese nula caso valor-p \\(\\leq 0,05\\), sendo 5% de nível de significância escolhido. Podemos usar a seguinte estrutura:\n\nstats::wilcox.test(x, y, correct = FALSE,  alternative = \"two.sided\")\n\nOs argumentos x e y são os vetores dos valores dos grupos a serem comparados. O argumento correct indica se o usuário quer que seja feito uma correção de continuidade do valor-p no caso de aproximação normal da estatística \\(U\\), o que geralmente não é feito, a menos que a estatística \\(Z_u\\) esteja muito próxima do valor crítico utilizado. E por fim, o argumento alternative permite escolher o formato do teste de hipóteses desejado.\n\n\n\n6.1.2 Teste de Kolmogorov-Smirnov\nAssim como o teste de Mann-Whitney, o teste de Kolmogorov-Smirnov é uma alternativa não paramétrica ao teste t para grupos independentes quando há a suspeita de não normalidade da variável de interesse. O teste possui grau de medida pelo menos ordinal, e é sensível a qualquer diferença de tendência central, dispersão, simetria ou curtose das distribuições.\nVisa verificar se ambos os grupos originaram-se de populações com a mesma distribuição. Para isso, é calculado a estatística de teste \\(D\\) a partir das distribuições de frequências acumuladas dos grupos, e então, utiliza-se essa estatística \\(D\\) para avaliar as hipóteses. Quanto maior a estatística, maior será a evidência para rejeitar a hipótese nula.\nRetornando ao exemplo das idades de gestantes hospitalizadas com Covid-19 em dois diferentes hospitais, as hipóteses para o teste bilateral terão a seguinte forma:\n\\(H_0:\\) distribuição\\(_1=\\) distribuição\\(_2\\)\n\\(H_{\\mathrm{a}}:\\) distribuição\\(_1 \\neq\\) distribuição\\({ }_2\\)\nSão hipóteses com formas idênticas as do teste de Mann-Whitney. Porém, embora tenham o mesmo objetivo de comparar as distribuições para ambos os grupos, o método difere consideravelmente. Além disso, o teste de Mann-Whitney é sensível em relação à mediana, enquanto o teste de Kolmogorov-Smirnov é sensível em relação aos aspectos gerais das distribuições sendo testadas como foi discutido anteriormente.\nEmbora seja o mesmo exemplo, serão considerados valores diferentes por motivos didáticos:\nHospital A: \\(\\quad 22,22,25,27,29\\)\nHospital B: \\(\\quad 27,31,34,37,44\\)\nConsiderando as notações introduzidas no desenvolvimento do teste de Mann-Whitney, os tamanhos dos grupos são \\(n_1 = 5\\) e \\(n_2 = 5\\). Vale lembrar que, usualmente, denotamos \\(n_1\\) como sendo o tamanho do menor grupo, chamado de grupo 1. Como neste exemplo ambos possuem o mesmo tamanho, o grupo 1 sera composto dos valores das idades referentes as gestantes hospitalizadas no Hospital A.\n\n6.1.2.1 Estatística \\(D\\)\nPrecisamos calcular a estatística \\(D\\) para avaliar as hipóteses. Essa estatística nada mais é do que o maior valor que encontrarmos ao realizar a diferença entre as proporções de frequências relativas acumuladas dos dois grupos. Para isso, construímos a seguinte tabela:\n\\[\\begin{array}{ccccc}\n\\hline\n\\text{Idades}_1 & S_1 & \\text{Idades}_2 & S_2 & S_1-S_2 \\\\\n\\hline 22;22 & 2 / 5=0,40 & - & 0 & 0,40-0=0,40 \\\\\n25 & 3 / 5=0,60 & - & 0 & 0,60-0=0,60 \\\\\n27 & 4 / 5=0,80 & 27 & 1 / 5=0,20 & 0,80-0,20=0,60 \\\\\n29 & 5 / 5=1,00 & - & 1 / 5=0,20 & 1,00-0,20=\\color{red}{0,80} \\\\\n- & 5 / 5=1,00 & 31 & 2 / 5=0,40 & 1,00-0,40=0,60 \\\\\n- & 5 / 5=1,00 & 34 & 3 / 5=0,60 & 1,00-0,60=0,40 \\\\\n- & 5 / 5=1,00 & 37 & 4 / 5=0,80 & 1,00-0,80=0,20 \\\\\n- & 5 / 5=1,00 & 44 & 5 / 5=1.00 & 1,00-1,00=0,00 \\\\\n\\hline\n\\end{array}\\]\nAs colunas Idades1 e Idades2 são referentes aos valores do grupo 1 (usualmente o menor grupo, o que não é o caso, pois \\(n_1 = n_2\\)) e grupo 2 respectivamente. Porém, note que existe um valor repetido (Idades1 \\(= 22\\)) na primeira coluna, quando isso ocorre todas as repetições desse mesmo valor permanecem na mesma linha. Além disso, observe que na primeira linha e coluna Idades2 foi atribuído um símbolo de \\(-\\), isso ocorre, pois não há um valor de Idades2 \\(= 22\\) assim como na coluna Idades1. O mesmo ocorre na quinta linha e coluna Idades1, pois nessa coluna não há um valor de Idades1 \\(= 31\\) assim como ocorre na coluna Idades2, por exemplo. Para mais, a tabela é construída com os valores de ambos os grupos ordenados de forma crescente no decorrer das linhas.\nAs colunas \\(S_1\\) e \\(S_2\\) são referentes as proporções das frequências relativas acumuladas das colunas Idades1 e Idades2 respectivamente. Note que, na primeira linha e coluna Idades1 da tabela, temos uma frequência relativa de \\(\\displaystyle \\frac{2}{5}\\) para Idades1 \\(= 22\\) devido a esse valor repetir uma vez. Na segunda linha temos uma frequência relativa de \\(\\displaystyle \\frac{1}{5}\\) para Idades1 \\(= 25\\), pois esse valor aparece uma única vez, que ao acumular com a frequência da linha anterior obtemos \\(\\displaystyle \\frac{2}{5} + \\displaystyle \\frac{1}{5} = \\displaystyle \\frac{3}{5}\\). Na terceira e quarta linha da tabela os valores da coluna Idades1 também não se repetem, o que faz com que tenham uma frequência relativa de \\(\\displaystyle \\frac{1}{5}\\) que ao acumular com as frequências anteriores resultam em \\(\\displaystyle \\frac{4}{5}\\) e \\(\\displaystyle \\frac{5}{5}\\). Após a quarta linha não há mais valores para a coluna Idades1, portanto, a proporção das frequências relativas se mantêm em \\(\\displaystyle \\frac{5}{5} = 1\\). Além disso, na primeira e segunda linha as frequências relativas referentes a coluna Idades2 são zeradas, pois não há valores para essa coluna nessas linhas, e por serem as primeiras linhas não há nenhuma frequência a ser acumulada. Isso muda a partir da terceira linha, onde a mesma lógica utilizada para construir as frequências relativas \\(S_1\\) pode ser usada na construção de \\(S_2\\).\nPor fim, basta calcular os valores de \\(S_1 - S_2\\) para todas as linhas da tabela. A estatística \\(D\\) será o maior dentre esses valores, sendo \\(D = 0,80\\) no exemplo abordado.\n\n\n6.1.2.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos obter o valor crítico \\(D_c\\), o qual é um valor tabelado que pode ser verificado em Sheskin (2003) fazendo uso apenas das quantidades \\(n_1\\), \\(n_2\\) e o nível de significância escolhido. Para \\(n_1 = 5\\), \\(n_2 = 5\\) e o nível de significância de 5%, temos que \\(D_c = 0,80\\). Assim, em um teste de hipóteses bilateral rejeitamos a hipótese nula quando o valor absoluto da estatística \\(D\\) for maior ou igual que o valor crítico, ou seja, quando \\(|D| \\geq D_c\\). No exemplo, como \\(D = 0,80\\) e \\(D_c = 0,80\\), poderíamos concluir que há evidências de que ambos os grupos se originaram de populações com distribuições diferentes.\nEmbora não seja muito comum, testes de hipóteses unilaterais podem ser empregados:\n\nCaso a hipótese alternativa seja \\(H_{\\mathrm{a}}:\\) distribuição\\(_1 &gt;\\) distribuição\\({ }_2\\), então rejeita-se a hipótese nula quando \\(|D| \\geq D_c\\), onde \\(D_c\\) é o valor crítico tabelado para o teste de Kolmogorov-Smirnov unilateral, que para o exemplo temos \\(D_c = 0,60\\) ao nível de 5% de significância. Além disso, para os dados serem consistentes com o teste, a proporção da frequência acumulada referente ao grupo 1 deve ser maior que a do grupo 2 no ponto em que a estatística de teste \\(D\\) foi identificada, ou seja, precisamos ter \\(S_1 &gt; S_2\\) na quarta linha da tabela do exemplo considerado para este teste unilateral ser utilizado. Assim, esse formato de hipótese é aplicável ao problema dado que as condições são satisfeitas, e poderíamos chegar nas mesmas conclusões que no teste bilateral.\nCaso a hipótese alternativa seja \\(H_{\\mathrm{a}}:\\) distribuição\\(_1 &lt;\\) distribuição\\({ }_2\\), o procedimento de avaliação da hipótese é o mesmo dos casos anteriores. Porém, para ser utilizado temos que respeitar a condição \\(S_1 &lt; S_2\\) na linha da tabela referente a estatística \\(D\\), o que não ocorre no exemplo considerado, o que faz com que esse formato de hipótese seja inapropriado.\n\n\n\n6.1.2.3 Como aplicar o teste no R\nÉ fácil notar que com valores grandes de \\(n_1\\) e \\(n_2\\) a construção da tabela para encontrar a estatística \\(D\\) se torna trabalhosa sem o auxílio computacional. Agora que a ideia de como o teste é construído foi formalmente explicada, podemos verificar como utilizá-lo no software R através de uma função específica. Segue abaixo a estrutura para efetuar o teste de Kolmogorov-Smirnov no R:\n\nstats::ks.test(x, y, alternative = \"two.sided\")\n\nOs argumentos x e y são os vetores dos valores dos grupos a serem comparados, enquanto o argumento alternative permite escolher o formato do teste de hipóteses desejado. Ao executar a função, ela irá retornar a estatística de teste e o valor-p, os quais podemos utilizar para avaliar as hipóteses, onde, rejeitamos a hipótese nula caso valor-p \\(\\leq 0,05\\), sendo 5% de nível de significância escolhido.\nAlém disso, podemos utilizar a mesma função para efetuar o teste de Kolmogorov-Smirnov para um grupo. A lógica de construção do teste para um grupo é similar a construção do teste para dois, a principal diferença é de que em vez de comparar as distribuições para dois grupos, estaremos comparando a distribuição de um único grupo com uma distribuição hipotética ou teórica. A estrutura de aplicação no R será a seguinte:\n\nstats::ks.test(x, \"pnorm\", alternative = \"two.sided\")\n\nOs argumentos x e alternative são os mesmos, a diferença é que em vez de utilizar o argumento y utilizamos o argumento \"pnorm\", o qual estamos especificando que queremos comparar a distribuição da população que deu origem ao grupo com a distribuição empírica normal.\n\n\n\n6.1.3 Teste de Kruskal-Wallis\nO teste de Kruskal-Wallis é uma extensão do teste de Mann-Whitney em que é possível contruí-lo utilizando a lógica de postos dos valores ordenados dos grupos do estudo, e comparar a distribuição desses grupos, os quais, nesse caso, podem ser mais do que dois. Além disso, é um teste sensível à mediana como o teste de Mann-Whitney, e requer que a variável de interesse seja no mínimo ordinal. Como os grupos precisam ser independentes e não é necessário assumir normalidade, o teste de Kruskal-Wallis é uma alternativa não-paramétrica a análise de variância de um fator (One-way ANOVA).\nConsidere \\(k\\) como sendo o número de grupos no estudo. Quando \\(k = 2\\), o teste de Kruskal-Wallis terá resultados equivalentes aos do teste de Mann-Whitney. Além disso, o teste verifica se as distribuições das populações das quais os grupos foram amostrados são iguais, mas caso a suposição de que a forma dessas distribuições é a mesma for feita, então, o teste pode ser interpretado com base na mediana. Assim, considerando \\(k\\) grupos independentes, as hipóteses podem ser definidas como:\n\\(H_0:\\) Todos os \\(k\\) grupos originam-se de populações com distribuições idênticas.\n\\(H_{\\mathrm{a}}:\\) Pelo menos 2 grupos originam-se de populações com distribuições diferentes.\nRetornaremos ao exemplo que considera a idade(anos) das gestantes hospitalizadas com Covid-19. Considere \\(n_1 = 5\\), \\(n_2 = 5\\) e \\(n_3 = 5\\) como a quantidade de gestantes de três hospitais diferentes dos quais foram registradas as idades para o estudo. Os grupos podem ser vistos abaixo:\nHospital A: \\(\\quad 27,24,18,18,25\\)\nHospital B: \\(\\quad 43,39,31,36,41\\)\nHospital C: \\(\\quad 25,27,29,24,24\\)\nAssim como feito na construção do teste de Mann-Whitney, deve-se considerar todas as idades registradas como se fossem de um único grupo e então ordená-las para que os postos de cada registro seja identificado. No caso de empates, o posto atribuído a cada valor repetido será a média dos postos desses valores inicialmente identificados, assim como feito para o teste de Mann-Whitney.\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & \\color{red}{18} & \\color{red}{18} & \\color{red}{24} & 24 & 24 & \\color{red}{25} & 25 & \\color{red}{27} & 27 & 29 & \\color{blue}{31} & \\color{blue}{36} & \\color{blue}{39} & \\color{blue}{41} & \\color{blue}{43}\\\\\n\\hline \\text { Posição} & \\color{red}{1} & \\color{red}{2} & \\color{red}{3} & 4 & 5 & \\color{red}{6} & 7 & \\color{red}{8} & 9 & 10 & \\color{blue}{11} & \\color{blue}{12} & \\color{blue}{13} & \\color{blue}{14} & \\color{blue}{15}\\\\\n\\hline \\text { Posto } & \\color{red}{1,5} & \\color{red}{1,5} & \\color{red}{4} & 4 & 4 & \\color{red}{6,5} & 6,5 & \\color{red}{8,5} & 8,5 & 10 & \\color{blue}{11} & \\color{blue}{12} & \\color{blue}{13} & \\color{blue}{14} & \\color{blue}{15}\\\\\n\\hline\n\\end{array}\\]\nIdentificado os postos de cada observação, é possível obter algumas quantidades de interesse para a construção do teste. Considere \\(R_1 = 22\\), \\(R_2 = 65\\) e \\(R_3 = 33\\) como sendo a soma dos postos do grupo 1 (verde), grupo 2 (vermelho) e grupo 3 (preto) respectivamente. Além disso, considere \\(N = n_1 + n_2 + n_3\\) como o total de observações registradas, que para o exemplo abordado é de \\(N = 15\\).\n\n6.1.3.1 Estatística \\(H\\)\nPara avaliar as hipóteses é necessário calcular a estatística de teste \\(H\\), a qual é obtida a partir de uma aproximação pela distribuição qui-quadrado e possui a seguinte forma:\n\\[H=\\displaystyle \\frac{12}{N(N+1)} \\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right]-3(N+1).\\]\nNote que a quantidade \\(\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right]\\) indica, para cada grupo, a soma da razão entre o quadrado da soma dos postos e o número de observações, ou seja:\n\\[\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right] = \\displaystyle \\frac{\\left(R_1\\right)^2}{n_1} + \\displaystyle \\frac{\\left(R_2\\right)^2}{n_2} + \\displaystyle \\frac{\\left(R_3\\right)^2}{n_3} = \\displaystyle \\frac{\\left(22\\right)^2}{5} + \\displaystyle \\frac{\\left(65\\right)^2}{5} + \\displaystyle \\frac{\\left(33\\right)^2}{5} = 1159,6.\\]\nAssim, sabendo que \\(N = 15\\) e \\(\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right] = 1159,6\\) obtemos a estatística \\(H = 9,98\\).\n\n\n6.1.3.2 Avaliação das hipóteses\nTendo calculado a estatística \\(H\\), basta obtermos o valor crítico \\(H_c\\) para avaliar as hipóteses. O valor crítico \\(H_c\\) pode ser obtido a partir da distribuição qui-quadrado com \\(df = K-1\\) graus de liberdade quando há pelo menos cinco observações em cada grupo. Os valores críticos de uma qui-quadrado são quantidades tabeladas, assim, podemos acessá-los diretamente de uma tabela de pontos críticos dessa distribuição, ou, a partir da função qchisq() no r, sendo necessário especificar o grau de liberdade df e o nível de significância p com a estrutura qchisq(p = 0.05, df = 2, lower.tail = FALSE), considerando o exemplo abordado. Quando existem menos que cinco observações nos grupos, o valor de \\(H_c\\) pode ser obtido a partir de tabelas de valores exatos. Esses valores tabelados podem ser verificados em Sheskin (2003) ou pelo endereço www.dataanalytics.org.uk/critical-values.\nConsiderando o nível de 5% de significância e grau de liberdade \\(df = 2\\), o valor crítico para o exemplo é de \\(H_c = 5,99\\). Além disso, rejeitamos a hipótese nula quando \\(H \\geq H_c\\), como o valor da estatística de teste \\(H\\) é de fato maior que o valor crítico \\(H_c\\), podemos concluir que há evidências de que pelo menos dois dos três grupos originam-se de populações com distribuições diferentes.\nCaso haja um número excessivo de empates, é recomendado utilizar uma correção para a estatística \\(H\\) que considere essas repetições. A seguinte quantidade deve ser calculada:\n\\[C=1-\\displaystyle \\frac{\\sum_{i=1}^s\\left(t_i^3-t_i\\right)}{N^3-N}.\\]\nNote que \\(\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) é a mesma quantidade utilizada para incluir a informação de empates ao calcular a estatística \\(U\\) normalizada no teste de Mann-Whitney, em que \\(s\\) indica quantos grupos de empates ocorreram e \\(t\\) o número total de empates no grupo. No exemplo sendo considerado temos \\(s = 4\\) grupos de empates, sendo duas repetições do valor 18, três do valor 24, duas do valor 25 e duas do valor 27, ou seja, \\(t_1 = 2\\), \\(t_2 = 3\\), \\(t_3 = 2\\) e \\(t_4 = 2\\) respectivamente. Assim, para calcular \\(C\\) teremos:\n\\[\\sum_{i=1}^s\\left(t_i^3-t_i\\right) = (2^3-2) + (3^3-3) + (2^3-2) + (2^3-2) = 42.\\]\nSabendo que \\(N = 15\\), o valor do fator de correção será \\(C = 0.987\\). Para obter a estatística \\(H\\) corrigida, basta dividir pelo valor de \\(C\\), onde teremos \\(\\displaystyle \\frac{H}{C} = 10,11\\). Assim, para o exemplo abordado, as conclusões serão as mesmas feitas anteriormente ao avaliar as hipóteses com a estatística sem correção.\n\n\n6.1.3.3 Como aplicar o teste no R\nO teste pode ser facilmente aplicado no R ao utilizar a função kruskal.test(), a qual irá retornar o valor da estatística \\(H\\) aproximada pela qui-quadrado, os graus de liberdade considerados e o valor-p. Assim, rejeita-se a hipótese nula quando valor-p \\(\\leq 0,05\\) considerando 5% de nível de significância. A função possui a seguinte estrutura:\n\nstats::kruskal.test(resposta ~ grupos, data = dados)\n\nO argumento resposta ~ grupos é a fórmula de entrada da função para aplicação do teste, em que resposta é a variável de interesse e grupos é a variável independente onde os grupos são especificados, sendo grupos uma variável que precisa ser tratada como factor dentro do R (ver apêndice). No exemplo abordado, teríamos idade ~ hospitais, por exemplo. O argumento data é onde precisamos especificar o nome da base de dados utilizada que possui as variáveis sendo incluídas na fórmula.\nCaso o teste de Kruskal-Wallis seja realizado, e como resultado, a hipótese nula seja rejeitada, podemos identificar quais pares de grupos apresentam diferentes distribuições de suas populações. O teste de Dunn, uma alternativa não-paramétrica ao teste de Tukey, pode ser usado para realizar comparações múltiplas entre os grupos. Para aplicá-lo no R, podemos instalar o pacote FSA e recorrer à função dunnTest(), a qual possui a seguinte estrutura:\n\nFSA::dunnTest(resposta ~ grupos, data = dados, method = \"bonferroni\")\n\nOs argumentos resposta ~ grupos e data são os mesmos utilizados na função kruskal.test(). O argumento method é utilizado para especificar o método de controle da taxa de erro experimental, sendo o bonferroni o mais usual. A função retorna uma matriz com valores-p ajustados pelo método bonferroni organizados em uma coluna chamada P.adj para o usuário avaliar cada par de grupos, sendo cada linha equivalente a cada par sendo comparado. Caso o valor-p ajustado for menor ou igual ao nível de significância (geralmente 5%), então, as distribuições das populações dos grupos sendo comparados são significativamente diferentes."
  },
  {
    "objectID": "testes_np.html#grupos-pareados",
    "href": "testes_np.html#grupos-pareados",
    "title": "6  Testes não-paramétricos",
    "section": "6.2 Grupos pareados",
    "text": "6.2 Grupos pareados\nGrupos ou amostras são considerados pareados quando as observações em um grupo estão relacionadas ou emparelhadas com as observações de outro grupo. Isso significa que cada observação em um grupo é emparelhada ou relacionada com uma observação correspondente no outro grupo.\nUm exemplo comum de grupos pareados é quando o mesmo grupo de indivíduos é medido duas ou mais vezes em diferentes momentos, o que chamamos de amostras antes e depois. Outro cenário, é quando temos grupos não necessariamente com os mesmos indivíduos avaliados em diferentes momentos, mas com indivíduos diferentes em que conseguimos pareá-los de acordo com características em comum. Um exemplo de pareamento entre indivíduos de dois grupos é quando há o interesse em estudar a eficácia de um novo tratamento médico, onde cada paciente no grupo de tratamento é emparelhado com um paciente no grupo de controle com características semelhantes, formando assim, pares de observações pareadas.\nDentre os testes não-paramétricos mais conhecidos e utilizados nestes cenários, temos os testes de Wilcoxon e McNemar para dois grupos, e o de Friedman para três ou mais grupos.\n\n6.2.1 Teste de Wilcoxon\nO teste de Wilcoxon é utilizado em problemas onde a variável de interesse pode ser ordenada (medida pelo menos ordinal) e há dois grupos dependentes a serem comparados. A hipótese a ser testada é se a mediana das diferenças dos valores pareados das populações que deram origem aos grupos é zero. O teste é usualmente aplicado quando as suposições sobre o teste t para dois grupos dependentes são violadas, principalmente em relação à normalidade dos dados, pois, pelo teste de Wilcoxon ser uma alternativa não-paramétrica ao teste t para duas amostras dependentes, as suposições requeridas são mais leves.\nO teste consiste em identificar as diferenças entre os valores pareados dos grupos, identificar os postos das diferenças absolutas, e então, atribuir os sinais originais das diferenças em cada posto (sinalização dos postos). Além disso, diferenças que resultaram em zero, não são consideradas na construção do teste.\nSejam as seguintes hipóteses:\n\\(H_0:\\) A mediana das diferenças dos valores pareados é igual a zero.\n\\(H_{\\mathrm{a}}:\\) A mediana das diferenças dos valores pareados difere de zero.\nConsidere que em um determinado hospital estava sendo reportado altos níveis de ansiedade em gestantes no terceiro trimestre de gravidez. Foi medido o nível de ansiedade de 10 gestantes antes e após participarem de um programa de atividades em grupo que durou duas semanas, onde, 0 indica sem ansiedade e 10 ansiedade severa. Suponha que as 10 gestantes foram selecionadas de forma aleatória, e que as diferenças dos pares de níveis de ansiedade (antes e depois) seguem uma distribuição simétrica em torno da mediana. Considere os seguintes dados:\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Indivíduo } & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\\\n\\hline \\text { Antes} & 8 & 9 & 8 & 7 & 6 & 4 & 6 & 6 & 7 & 3 \\\\\n\\hline \\text { Depois } & 2 & 6 & 7 & 5 & 4 & 5 & 6 & 3 & 2 & 5 \\\\\n\\hline\n\\end{array}\\]\nCada indivíduo possui um nível de ansiedade registrado antes e depois do programa de atividades em grupos. O grupo 1 é composto pelos níveis de ansiedade registrados antes das atividades, e o grupo 2 composto pelos níveis de ansiedade registrados depois das atividades.\nConsidere \\(D\\) como uma representação das diferenças. Assim, calculamos \\(D\\), o valor absoluto das diferenças \\(|D|\\), os postos de \\(|D|\\), e então, os postos de \\(|D|\\) sinalizados:\n\\[\\begin{array}{cccccc}\n\\hline \\text { Indivíduo } & \\text{Grupo} 1 & \\text{Grupo} 2 & D & |D| & \\text { Postos de }|D| & \\text { Postos sinalizados de }|D|\\\\\n\\hline\n\\mathbf{7} & 6 & 6 & 0 & - & - & -\\\\\n\\mathbf{6} & 4 & 5 & -1\\phantom{-} & 1 & 1,5 & -1,5\\phantom{-}\\\\\n\\mathbf{3} & 8 & 7 & 1 & 1 & 1,5 & 1,5\\\\\n\\mathbf{10} & 3 & 5 & -2\\phantom{-} & 2 & 4,0 & -4,0\\phantom{-}\\\\\n\\mathbf{4} & 7 & 5 & 2 & 2 & 4,0 & 4,0\\\\\n\\mathbf{5} & 6 & 4 & 2 & 2 & 4,0 & 4,0\\\\\n\\mathbf{2} & 9 & 6 & 3 & 3 & 6,5 & 6,5\\\\\n\\mathbf{8} & 6 & 3 & 3 & 3 & 6,5 & 6,5\\\\\n\\mathbf{9} & 7 & 2 & 5 & 5 & 8,0 & 8,0\\\\\n\\mathbf{1} & 8 & 2 & 6 & 6 & 9,0 & 9,0\\\\\n\\hline\n\\end{array}\\]\nNote que a tabela é construída ordenando a coluna \\(D\\) de forma crescente desconsiderando o sinal dos valores. Além disso, a primeira linha gerou um valor de \\(D = 0\\), o qual deve ser desconsiderado no teste, e então, os postos são identificados a partir da segunda linha. Para mais, note que houve empates nos valores de \\(|D|\\), para os quais os postos são identificados como a média dos postos caso não fossem empates, assim como é feito para os testes de Mann-Whitney e Kruskal-Wallis.\nO próximo passo é calcular a soma dos postos em que a sinalização de \\(|D|\\) foi negativa, o qual denotaremos de \\(R_{neg}\\). Além disso, devemos calcular a soma dos postos em que a sinalização de \\(|D|\\) foi positiva, denotada por \\(R_{pos}\\).\nPara o exemplo, temos \\(R_{neg} = 5,5\\) e \\(R_{pos} = 39,5\\). Além disso, podemos verificar a seguinte relação:\n\\[R_{neg} + R_{pos} =\\displaystyle \\frac{n(n+1)}{2}.\\]\nCaso essa relação não for verdadeira, é um indicativo de que houve erro nos cálculos. Como houve um par de valores entre os grupos cuja diferença foi zero, o número de pares considerados no teste para o exemplo abordado será de \\(n = 9\\). Dessa forma, teremos \\(\\displaystyle \\frac{n(n+1)}{2} = \\displaystyle \\frac{9(9+1)}{2} = 45\\) que corresponde ao valor de \\(R_{neg} + R_{pos} = 5,5 + 39,5 = 45.\\)\n\n6.2.1.1 Avaliação das hipóteses\nCaso os valores das diferenças originaram-se de uma população cuja mediana é zero, espera-se que os valores de \\(R_{neg}\\) e \\(R_{pos}\\) sejam equivalentes ao valor esperado da estatística de teste de Wilcoxon, a qual denotaremos de estatística \\(V\\). O valor esperado possui a seguinte forma:\n\\[\\displaystyle \\frac{n(n+1)}{4}.\\]\nAssim, para o exemplo, o valor esperado da estatística de teste \\(V\\) é de \\(\\displaystyle \\frac{9(9+1)}{4} = 22,5\\). Note que o valor esperado da estatística de teste \\(V\\) parece ser razoavelmente diferente de \\(R_{neg}\\) e \\(R_{pos}.\\)\nCaso \\(R_{pos}\\) for significativamente maior que \\(R_{neg}\\), significa haver uma alta probabilidade de que o grupo 1 originou-se de uma população com valores maiores do que os da população da qual o grupo 2 originou-se, o contrário também é verdadeiro caso \\(R_{neg}\\) for significativamente maior. Como \\(R_{pos} &gt; R_{neg}\\) no exemplo considerado, podemos interpretar a hipótese alternativa como sendo:\n\\(H_{\\mathrm{a}}:\\) A mediana das diferenças dos valores pareados é maior que zero.\nAssim, resta identificar se \\(R_{pos}\\) é de fato significativamente maior que \\(R_{neg}\\).\n\n\n6.2.1.2 Estatística \\(V\\)\nA estatística \\(V\\) de Wilcoxon nada mais é do que o menor dos valores entre \\(R_{neg}\\) e \\(R_{pos}\\). Assim, para o exemplo em questão, a estatística de teste será \\(V = R_{neg} = 5,5\\).\nPara avaliar as hipóteses precisamos obter o valor crítico de \\(V\\), denotado por \\(V_c\\). Valores críticos de \\(V\\) são quantidades tabeladas identificadas de acordo com a quantidade de postos utilizados no teste ou pares válidos, esses valores podem ser encontrados em Sheskin (2003). Assim, para \\(n = 9\\) e nível de 5% de significância, o valor crítico de \\(V\\) é de \\(V_c = 8\\) para o teste unilateral.\nRejeitamos a hipótese nula quando \\(V \\leq V_c\\). Como \\(R_{pos} &gt; R_{neg}\\), \\(V = 5,5\\) e \\(V_c = 8\\), podemos concluir, ao nível de 5% de significância, que os níveis de ansiedade registrados das gestantes antes do programa de atividades em grupo foram maiores do que após as atividades, ou seja, houve uma redução significativa nos níveis de ansiedade das gestantes em decorrência das atividades em grupo.\n\n\n6.2.1.3 Estatística \\(V\\) normalizada\nNa prática, quando \\(n &gt; 30\\) podemos normalizar a estatística de teste \\(V\\), e então, avaliar as hipóteses baseado na distribuição normal padrão. A estatística de teste \\(V\\) normalizada será denotada por \\(Z_V\\), e possui a seguinte forma:\n\\[Z_T=\\displaystyle \\frac{T-\\displaystyle\\frac{n(n+1)}{4}}{\\sqrt{\\displaystyle\\frac{n(n+1)(2 n+1)}{24}}}.\\]\nComo \\(n = 9\\), temos que o valor da estatística \\(V\\) normalizada é de \\(Z_V = -2,01\\). O valor crítico da estatística de teste normalizada é um valor tabelado o qual pode ser verificado em tabelas de valores críticos da distribuição normal padrão. Pode ser encontrada em Sheskin (2003), ou, obtido computacionalmente no R através da função qnorm(). Para o nível de 5% de significância, os valores críticos para normal padrão são muito conhecidos, sendo 1,96 e 1,65 para os testes bilaterais e unilaterais respectivamente.\nRejeita-se a hipótese nula quando \\(|Z_V|\\geq Z_{crítico}\\). Como \\(R_{pos} &gt; R_{neg}\\), \\(Z_V = -2,01\\) e \\(Z_{crítico} = 1,65\\), então, podemos concluir que há evidências para a rejeição da hipótese nula, e a mesma interpretação feita anteriormente para o teste sem normalização permanece.\n\n\n6.2.1.4 Como aplicar o teste no R\nPara aplicar o teste de Wilcoxon no R podemos utilizar a mesma função e estrutura utilizada para o teste de Mann-Whitney. Utilizamos a seguinte estrutura:\n\nstats::wilcox.test(x, y, correct = FALSE,  alternative = \"two.sided\", paired = TRUE)\n\nOs argumentos x e y são referentes aos grupos 1 e 2 respectivamente. O argumento correct indica se o usuário quer que seja feito uma correção de continuidade do valor-p no caso de aproximação normal da estatística \\(V\\). O argumento alternative especifica a forma do teste a ser aplicado (unilateral ou bilateral). E por fim, o mais importante no caso do teste de Wilcoxon, o argumento paired que indica que o teste considere grupos pareados.\nA função wilcox.test() neste caso irá retornar o valor da estatística de teste \\(V\\) e o valor-p. A hipótese nula pode ser rejeitada quando valor-p \\(\\leq 0,05\\), quando o nível de significância desejado for de 5%.\n\n\n\n6.2.2 Teste de McNemar\nO teste de McNemar é usualmente utilizado em cenários onde a variável de interesse é categórica, dicotômica (duas categorias) e os grupos são dependentes (geralmente experimentos do tipo “antes-depois” de um evento de intervenção). O objetivo do teste é verificar a existência de diferenças significativas entre os grupos em relação à variável de interesse, e sua construção é feita tendo como base tabelas de contingência 2x2.\nConsidere o exemplo abordado na construção do teste de Wilcoxon, onde, foi coletado os dados do nível de ansiedade de gestantes no terceiro trimestre gestacional, antes e depois, de um programa de atividades em grupo, sendo 0 indicando sem ansiedade e 10 indicando ansiedade severa. Porém, considere que o número de gestantes que participaram do programa foi de 100, e que o nível de ansiedade foi classificado como baixo ou alto (baixo: 0 a 5; alto: 6 a 10). Além disso, considere que as gestantes foram selecionadas de forma aleatória e que as categorias baixa e alta são mutualmente exclusivas. Os dados são apresentados na seguinte forma:\n\\[\\begin{array}{c|cc|c}\n\\hline \\text { Antes/Depois } & \\text { Baixo } & \\text { Alto } & {\\text { Soma das linhas }} \\\\\n\\hline \\text { Baixo } & \\mathrm{a = 17} & \\mathrm{b = 10} & \\mathrm{n}_1=\\mathrm{27} \\\\\n\\text { Alto } & \\mathrm{c = 59} & \\mathrm{d = 14} & \\mathrm{n}_2=\\mathrm{73} \\\\\n\\hline \\text { Soma das colunas } & \\mathrm{76} & \\mathrm{24} & \\mathrm{n} =\\mathrm{100} \\\\\n\\hline\n\\end{array}\\]\nPara a construção do teste, as caselas de interesse são aquelas onde houve mudança na avaliação, ou seja, a casela \\(c\\), em que indica que a gestante apresentava alto nível de ansiedade, e então, passou a apresentar baixo nível de ansiedade após as atividades em grupo, e a casela \\(b\\), em que indica que a gestante apresentava baixo nível de ansiedade, e passou a apresentar alto nível após as atividades em grupo. As caselas \\(a\\) e \\(d\\) não são consideradas na construção do teste, pois não houve alterações em decorrência da intervenção (programa de atividades em grupo). Assim, considerando as populações as quais os grupos (antes; depois) representam, as hipóteses terão a seguinte forma bilateral:\n\\(H_0:\\) A proporção das observações da casela \\(b\\) é igual a proporção das observações da casela \\(c\\), ou seja, \\(\\pi_b = \\pi_c\\).\n\\(H_{\\mathrm{a}}:\\) A proporção das observações da casela \\(b\\) difere da proporção das observações da casela \\(c\\), ou seja, \\(\\pi_b \\neq \\pi_c\\).\nOs valores \\(\\pi_b\\) e \\(\\pi_c\\) podem ser estimados por \\(p_b = \\displaystyle \\frac{b}{b + c}\\) e \\(p_c = \\displaystyle \\frac{c}{b + c}\\) respectivamente. Para o exemplo considerado, temos \\(p_b = 0,14\\) e \\(p_c = 0,85\\). Assim, como \\(p_c &gt; p_b\\), a hipótese unilateral \\(H_{\\mathrm{a}}: \\pi_b &lt; \\pi_c\\) é consistente com os dados e pode ser considerada.\n\n6.2.2.1 Estatística \\(M\\)\nA estatística de teste utilizada no teste de McNemar é baseada na distribuição qui-quadrado e será denotada por \\(M\\). O cálculo da estatística \\(M\\) pode ser feito da seguinte maneira:\n\\[M = \\displaystyle \\frac{(b-c)^2}{b+c}.\\]\nNote que essa estatística sempre será não-negativa, e caso o número de observações na casela \\(b\\) e \\(c\\) seja o mesmo, então, a estatística \\(M\\) será zero. Para o exemplo considerado, teremos que o valor da estatística de teste será \\(M = 34,79\\).\n\n\n6.2.2.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos obter o valor crítico da estatística \\(M\\), que será denotado por \\(M_c\\). Valores críticos de \\(M\\) são os mesmos da distribuição qui-quadrado, os quais são valores tabelados podendo ser encontrados em Sheskin (2003), ou, podemos utilizar a função qchisq() no R para encontrá-los. A função qchisq() terá a seguinte forma:\n\nstats::qchisq(p = 0.10, df = 1, lower.tail = FALSE)\n\n[1] 2.705543\n\n\nO argumento p indica o nível de significância considerado a depender do formato das hipóteses, como estamos considerando um teste unilateral, para o teste ter o nível de significância de 5% devemos especificar \\(p = 0,10\\). Caso o teste fosse bilateral, então, teríamos \\(p = 0,05\\). O argumento df indica o grau de liberdade considerado, que será o número de caselas avaliadas menos 1, como por construção o teste avalia apenas duas caselas, especificamos \\(df = 1\\). O argumento lower.tail deve ser especificado como FALSE para o formato de p apresentado, caso esse argumento for especificado como TRUE, o valor utilizado no argumento p precisa ser indicado como seu complementar. No exemplo, caso lower.tail = TRUE, então, teríamos p = 1 - 0,10 = 0,90.\nAssim, rejeitamos a hipótese nula quando \\(M \\geq M_c\\). Como \\(M = 34,79\\) e \\(M_c = 2,71\\), então, ao nível de 5% de significância, há evidências de uma mudança significativa nos níveis de ansiedade das gestantes, onde a direção dessa mudança é de níveis altos de ansiedade para níveis baixos. É importante notar, que essa hipótese apenas é aceita, pois a relação \\(p_c &gt; p_b\\) é consistente com os dados.\nEmbora o exemplo utilizado seja de um experimento em que os grupos são formados a partir de dados obtidos das mesmas pessoas duas vezes (antes e depois da intervenção), em experimentos em que os grupos são formados de indivíduos diferentes, mas pareados por características em comum, o teste de McNemar pode ser aplicado normalmente.\n\n\n6.2.2.3 Como aplicar o teste no R\nPodemos aplicar o teste de McNemar facilmente no R ao utilizar a função mcnemar.test(), a qual irá retornar a estatística de teste, os graus de liberdades considerados e o valor-p calculado. Essa função possui a seguinte estrutura:\n\nstats::mcnemar.test(x, correct = TRUE)\n\nO argumento x é a tabela de contingência 2x2 considerada, e deve ser passada como um objeto do tipo matriz (ver apêndice). O argumento correct indica se o usuário quer que seja aplicado uma correção de continuidade na estatística de teste, o que é sempre recomendado na prática dado que o teste de McNemar utiliza uma distribuição contínua para aproximar uma distribuição de probabilidade discreta. Assim, aplicando o teste com a função mcnemar.test(), podemos rejeitar a hipótese nula quando valor-p \\(\\leq 0,05\\) considerando nível de 5% de significância, ou podemos obter o valor crítico da estatística de teste e avaliar \\(M \\geq M_c\\).\n\n\n\n6.2.3 Teste de Friedman\nO teste de Friedman é utilizado para comparar mais de dois grupos dependentes, onde, sua construção de baseia em postos. Assim, a variável de interesse precisar ser pelo menos ordinal para ser possível identificar os postos. Além disso, é usado quando os pressupostos de normalidade não são atendidos, sendo uma alternativa não-paramétrica a análise de variância de um fator (One-way ANOVA) de medidas repetidas. O teste tem por objetivo verificar se as distribuições das populações as quais os grupos representam são iguais em relação à mediana, logo, teremos a seguinte estrutura de hipóteses:\n\\(H_0:\\) Todos os \\(k\\) grupos originam-se de populações com medianas idênticas.\n\\(H_{\\mathrm{a}}:\\) Pelo menos 2 grupos originam-se de populações com medianas diferentes.\nConsidere o exemplo abordado na construção dos testes de Wilcoxon e de McNemar, onde, foi coletado os dados do nível de ansiedade de gestantes no terceiro trimestre gestacional, antes e depois, de um programa de atividades em grupo, sendo 0 indicando sem ansiedade e 10 indicando ansiedade severa. Porém, agora considere que os níveis de ansiedade foram medidos antes do início do programa e, posteriormente, após duas atividades em grupo diferentes (atividades A e B), onde, foram escolhidas baseado em estudos anteriores que indicaram que eram atividades capazes de reduzir a ansiedade. Além disso, considere que foram selecionadas de forma aleatória 5 gestantes no terceiro trimestre gestacional para participarem do experimento, e que os tempos entre as atividades foram determinados apropriadamente. Os dados e os postos foram identificados da seguinte forma:\n\\[\\begin{array}{ccccccc}\n\\hline \\text { Indivíduos } & \\text{Grupo}_1 & \\text{Postos}_1 & \\text{Grupo}_2 & \\text{Postos}_2 & \\text{Grupo}_3 & \\text{Postos}_3 \\\\\n\\hline 1 & 5 & 3 & 3 & 1 & 4 & 2 \\\\\n2 & 7 & 3 & 3 & 1 & 5 & 2 \\\\\n3 & 6 & 3 & 5 & 2 & 4 & 1 \\\\\n4 & 9 & 3 & 2 & 1 & 5 & 2 \\\\\n5 & 6 & 2,5 & 6 & 2,5 & 3 & 1 \\\\\n\\hline\n\\end{array}\\]\nA coluna Grupo1 indica os níveis de ansiedade das gestantes (identificadas na coluna Indivíduos) registrados antes das atividades, sendo os valores na coluna Postos1 seus respectivos postos. Além disso, a coluna Grupo2 indica os níveis de ansiedade registrados após a atividade A, cujos postos são identificados na coluna Postos2. Para mais, a coluna Grupo3 indica os níveis de ansiedade registrados das gestantes após a atividade B, sendo seus postos identificados na coluna Postos3. Por fim, detonamos a soma dos postos dos grupos por \\(R_1\\), \\(R_2\\) e \\(R_3\\), onde aplicamos a soma de todos os valores para cada coluna Postos1, Postos2 e Postos3. Assim, teremos \\(R_1 = 14,5\\), \\(R_2 = 7,5\\) e \\(R_3 = 8,0\\).\nNote que o procedimento de identificação dos postos é feito para cada indivíduo em específico, ou seja, é identificado os postos dos valores (níveis de ansiedade) dos grupos por linha separadamente, de forma que os postos de cada linha não interferem os postos das outras. Caso houver empates dos valores, então, a média dos postos identificados caso não fossem empates será aplicado, da mesma forma em que é feito nos demais testes não-paramétricos abordados nesta seção.\nEmbora o exemplo utilizado seja de um experimento onde os mesmos indivíduos passam por condições diferentes, e então, para cada condição é formado um novo grupo, cenários onde os grupos são formados por indivíduos diferentes, porém, pareados por características em comum, também são válidos para aplicação do teste de Friedman.\n\n6.2.3.1 Estatística \\(Q\\)\nPara avaliar as hipóteses devemos calcular a estatística de teste, a qual denotaremos de estatística \\(Q\\). A estatística \\(Q\\) é aproximada a partir da distribuição qui-quadrado da seguinte forma:\n\\[Q=\\displaystyle \\frac{12}{n k(k+1)}\\left[\\sum_{i = 1}^k\\left(R_i\\right)^2\\right]-3 n(k+1).\\]\nExperimentos do tipo “antes-depois” possuem a característica de serem geralmente avaliados os mesmos indivíduos mais de uma vez, assim, os grupos formados serão do mesmo tamanho, onde, teremos \\(n_1 = n_2 = n_3 = n\\), sendo \\(n = 5\\) no exemplo considerado. O número de grupos é denotado por \\(k\\), assim, podemos calcular a seguinte quantidade:\n\\[\\displaystyle \\sum_{i = 1}^k(R_i)^2 = (R_1)^2 + (R_2)^2 + (R_3)^2 = (14,5)^2 + (7,5)^2 + (8,0)^2 = 330,5.\\]\nCom todos os termos identificados, a estatística para o teste de Friedman será \\(Q = 6,1\\).\n\n\n6.2.3.2 Avaliação das hipóteses\nTendo calculado a estatística de teste, precisamos obter o valor crítico de \\(Q\\) para avaliar as hipóteses, o qual será denotado por \\(Q_c\\). Como a estatística \\(Q\\) é aproximada pela qui-quadrado, então, os valores críticos da distribuição qui-quadrado podem ser utilizados. Tais valores podem ser obtidos de forma tabelada, onde, tais tabelas podem ser encontradas em Sheskin (2003). Outra forma de obtê-los é através da função qchisq(), da mesma forma em que foi explicado na construção do teste de McNemar. Considerando que os graus de liberdade são \\(k - 1\\), então, ao nível de 5% de significância, teremos o seguinte valor crítico para o exemplo abordado:\n\nstats::qchisq(p = 0.05, df = 2, lower.tail = FALSE)\n\n[1] 5.991465\n\n\nRejeitamos a hipótese nula quando \\(Q \\geq Q_c\\), logo, dado que \\(Q = 6,1\\) e \\(Q_c = 5,99\\), há evidências de que pelo menos dois grupos (condições as quais o nível de ansiedade foi registrado) diferem significativamente.\n\n\n6.2.3.3 Correção de empates\nCaso houver um número excessivo de indivíduos onde ocorreu empates (muitas linhas da tabela em que tenham empates ou muitos empates em uma mesma linha), é recomendado utilizar um fator de correção para aplicar na estatística de teste, o qual, terá a seguinte forma:\n\\[C=1- \\displaystyle\\frac{\\sum_{i=1}^s\\left(t_i^3-t_i\\right)}{n\\left(k^3-k\\right)}.\\]\nNote que a quantidade \\(\\displaystyle \\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) é a mesma utilizada no fator de correção de empates para o teste de Kruskal-Wallis, onde, \\(s\\) indica quantos conjuntos de empates ocorreram e \\(t\\) o número total de empates no conjunto. Logo, para o exemplo considerado teremos:\n\\[\\displaystyle \\sum_{i=1}^s\\left(t_i^3-t_i\\right) = \\left(2^3-2\\right) = 6.\\]\nAssim, sabendo que \\(n = 5\\) e \\(k = 3\\), o valor do fator de correção será \\(C = 0,95\\). Então, a estatística de teste \\(Q\\) corrigida será \\(Q_{corrigida} = \\displaystyle \\frac{Q}{C} = 6,42\\).\nA avaliação das hipóteses é feita da mesma forma, onde, rejeitamos a hipótese nula quando \\(Q_{corrigida} \\geq Q_c\\).\n\n\n6.2.3.4 Como aplicar o teste no R\nPodemos aplicar o teste de Friedman no R utilizando a função friedman.test(), a qual, possui a seguinte estrutura:\n\nstats::friedman.test(y, groups, blocks)\n\nÉ recomendável criar um objeto do tipo data.frame (ver apêndice) com cada coluna necessária para realização do teste, sendo: uma coluna com as observações da variável de interesse para o argumento y da função; uma coluna com os grupos (variável independente) para o argumento groups; uma coluna com a identificação numérica dos indivíduos para o argumento blocks. Tendo criado o objeto, digamos, objeto dados, basta passar as colunas para os argumentos, sendo y = dados$variavel, groups = dados$grupos e blocks = dados$individuos. O teste irá retornar a estatística de teste, os graus de liberdade utilizados e o valor-p, em que rejeitamos a hipótese nula quando \\(Q \\geq Q_c\\) ou quando valor-p \\(\\leq 0,05\\) ao nível de 5% de significância.\nPodemos realizar comparações múltiplas após a confirmação da rejeição da hipótese nula, para isso, devemos utilizar métodos específicos para o teste de Friedman. Um dos métodos mais utilizados é o teste de Conover, que pode ser utilizado para comparações múltiplas tanto para o teste de Friedman quanto para o teste de Kruskal-Wallis, como também, em outros cenários. Podemos aplicar o teste de Conover no R com a função frdAllPairsConoverTest(), a qual, pertence ao pacote PMCMRplus e possui a seguinte estrutura:\n\nPMCMRplus::frdAllPairsConoverTest(y, groups, blocks, p.adjust.method = \"bonf\")\n\nNote que a estrutura é a mesma da função friedman.test(), a única diferença é o argumento p.adjust.method, o qual, indica o método de ajuste do valor-p a ser utilizado, onde, usualmente especificamos o método de Bonferroni por “bonf”. A função irá retornar uma pequena matriz de valores-p para cada comparação feita, onde, identificamos os pares de grupos que diferem de forma significativa ao nível de 5% quando valor-p \\(\\leq 0,05\\)."
  },
  {
    "objectID": "testes_np.html#materiais-complementares",
    "href": "testes_np.html#materiais-complementares",
    "title": "6  Testes não-paramétricos",
    "section": "6.3 Materiais complementares",
    "text": "6.3 Materiais complementares\nLivros e Artigos:\n\nCorder, Gregory W., and Dale I. Foreman. “Nonparametric statistics for non‐statisticians.” (2011).\nPett, Marjorie A. Nonparametric statistics for health care research: Statistics for small samples and unusual distributions. Sage Publications, 2015.\nSheskin, David J. Handbook of parametric and nonparametric statistical procedures. Chapman and hall/CRC, 2003.\nTomkins, C. C., and C. Hall. “An introduction to non-parametric statistics for health scientists.” University of Alberta Health Sciences Journal 3.1 (2006): 20-26.\n\n\\(Sites\\):\n\nhttps://www.graphpad.com/guides/prism/latest/statistics/index.htm\nhttps://www.dataanalytics.org.uk/critical-values-for-the-kruskal-wallis-test/"
  },
  {
    "objectID": "supervisionada.html",
    "href": "supervisionada.html",
    "title": "7  Aprendizado supervisionado",
    "section": "",
    "text": "7.0.1 Aprendizado supervisionado (conceitos iniciais)\nAprendizado supervisionado pode ser definido como a tarefa de aprender uma função que mapeia uma entrada em uma saída e isso é feito com base em exemplos e treinos. Em outras palavras, uma máquina é treinada para encontrar soluções chamadas rótulos, onde esses rótulos identificam alguma característica. Apesar de também poder ser usada para regressão, o aprendizado supervisionado tem como tarefa típica a classificação. Um exemplo bem simples de classificação é: suponha que eu queira classificar imagens de animais, nesse caso possuo um banco de dados com imagens de cachorros e gatos. Quero que meu algoritmo classifique as imagens identificando o tipo do animal na imagem. Para isso o algoritmo é treinado utilizando vários exemplos para que ele consiga classificar novas imagens posteriormente. Outra tarefa é predizer um valor com base em características, por exemplo, prever o valor de um carro dado um conjunto de características (quilometragem, idade, marca, etc.) chamadas preditores. Este tipo de tarefa é chamada regressão. Para treinar o sistema é preciso incluir diversos exemplos, assim o banco de dados é separado em treino e teste, onde o é feito o treinamento na base treino para posteriormente serem feitos os testes de predição e avaliação da qualidade do ajuste na base teste.\n\n7.0.1.0.1 Dificuldades gerais do machine learnig\nComo dito anteriormente, a idéia geral do aprendizado de máquina é construir um algoritmo para solucionar os meus problemas, onde esse algoritmo será treinado com dados. Mas, o que acontece se o meu algoritmo for ruim ou os dados estiverem ruins?\nQuantidade insuficiente de dados\nFalando sobre dados ruins, o primeiro problema é a quantidade de dados. Já parou pra pensar em quão difícil é treinar uma máquina? Voltando ao exemplo anterior, para você aprender a diferenciar um cachorro de um gato quando era criança, bastou alguém lhe apontar qual era qual algumas vezes você se tornou capaz de diferenciar cães de gatos independente das características. Uma máquina não consegue fazer isso facilmente, é necessário uma quantidade grande de dados para a maioria dos algoritmos, até mesmo para problemas simples como o do exemplo citado e para problemas complexos, como reconhecimento de imagem ou fala você pode precisar de milhões de exemplos.\n\n\n7.0.1.0.2 Dados de treino não representativos\nComo mencionado anteriormente, o treinamento de um algoritmo é feito por meio de uma base de dados, onde está é separada em dados de treinamento e de teste, para que eu possa usá-lo e generalizá-lo em dados futuros. Dados de treinamento que não representem bem os dados que serão usados no futuro podem ser um modelo que não funcionará bem. Utilizando o exemplo do algoritmo de regressão onde o objetivo era prever os valores dos carros com base em suas características. Digamos que meu algoritmo foi treinado com uma base de dados de carros apenas do estado de São Paulo, mas meu algoritmo será utilizado para prever carros de todo o país, pode ser que não funcione tão bem. Os estados podem alterar significativamente os preços dos carros por meio de impostos, por exemplo. É de extrema importância utilizar um conjunto de dados de treino que represente bem os dados que você deseja generalizar. Isso pode não ser uma tarefa fácil, pode encontrar problemas com amostras, principalmente se ela for muito pequena e até mesmo uma amostra grande pode não ser representativa.\n\n\n7.0.1.0.3 Qualidade dos dados\nComo pode ter imaginado, a qualidade dos dados também é de extrema importância. Dados com discrepâncias, vários erros, e gerados a partir de medições de baixa qualidade fará com que fique mais difícil o seu algoritmo identificar padrões e tomar decisões. Se você convive com pessoas do ramo da ciência de dados em geral, é bem provável que já tenha ouvido alguém dizer algo do tipo: “gastamos a maior parte do nosso tempo para limpar os dados”. Isso não é em vão. Na maioria dos casos, principalmente no ramo de aprendizagem de máquinas é gasto um enorme tempo para limpar os dados pois pode influenciar muito na qualidade do modelo. Por exemplo, se algumas informações forem muito discrepantes, é preciso decidir entre tentar corrigir ou excluí-las. Se uma variável tiver uma quantidade significativa de valores faltantes, deverá ser decidido se essas observações serão excluídas ou se será possível utilizar métodos de imputação de dados. Treinar mais de um modelo com diferentes decisões tomadas sobre os dados também pode ser efetivo.\nSobreajustamento dos dados (Overfitting)\nO sobreajustamento é um conceito que ocorre quando nosso modelo (não só um modelo de aprendizado de máquinas), se ajusta exatamente aos nossos dados de treinamento. Ouvir isso uma primeira vez pode parecer excelente, ou até mesmo o cenário ideal, afinal, queremos que o nosso modelo se ajuste o máximo possível, certo? bom.. não exatamente. O que acontece neste caso, é que o modelo mostra-se adequado apenas para os dados de treino, como se o modelo tivesse apenas decorado os dados de treino e não fosse capaz de generalizar para outros dados nunca vistos antes. Assim, o desempenho do nosso modelo quando usado em novos dados cai drasticamente. Algumas razões que podem levar a um sobreajustamento: base de treino muito pequena, não contendo dados suficientes para representar bem todos valores de entrada possíveis; grande quantidade de informações irrelevantes (dados ruidosos); treinamento excessivo em um único conjunto de amostra; modelo muito complexo, fazendo com que ele aprenda os ruídos nos dados de treinamento. Agora que sabemos o problema que é um sobreajustamento e as razões que podem levar a isso, precisamos falar sobre como evitar que isso aconteça. Existem algumas técnicas comumente utilizadas.\n\nRegularização: Foi dito anteriormente que uma razão para o sobreajustamento é a complexidade do modelo, então, faz sentido diminuirmos sua complexidade. Isso pode ser feito removendo ou diminuindo o número de parâmetros.\nParada antecipada: Quando um modelo está sendo treinado por rodadas de repetição, é possível avaliar cara uma dessa repetição. Normalmente o desempenho de um modelo melhora a cada repetição, mas chega um momento em que começa a acontecer o sobreajustamento. A ideia da parada antecipada é pausar o treinamento antes que chegue a esse ponto.\nAumento de dados: Essa técnica consiste em aumentar ligeiramente os dados da amostra toda vez que o modelo os processa, ou seja, injetar dados limpos e relevantes nos dados de treino. Isso faz com que os conjuntos de treino pareçam “exclusivos” do modelo, impedindo que ele aprenda suas características. Mas isso deve ser feito com moderação, pode injetar dados que não estão limpos pode fazer mais mal do que bem. Além disso, não é um método garantido.\n\n\n\n7.0.1.0.4 Existem outras técnicas que podem ser utilizadas para evitar o sobreajustamento. Mas precisamos falar também sobre como detectá-los.\nUma forma “não técnica” e que não deve ser a sua única forma de tentar identificar o sobreajustamento é por meio da visualização gráfica. A visualização gráfica pode ser usada apenas para levantar hipóteses, nunca para tomar uma decisão final. Até mesmo porque nem sempre é possível verificar esse problema visualmente. Talvez a técnica mais eficiente para isso é a Validação Cruzada k-fold (k-fold Cross Validation). Vamos falar sobre posteriormente.\n\n\n7.0.1.0.5 Subajustamento dos dados (Underfitting)\nComo pode ter imaginado, subajustamento é o oposto do sobreajustamento. Ocorre quando seu modelo é muito simples para aprender a estrutura dos dados. O subajustamento leva a um erro elevado tanto nos dados de treino quanto nos dados de teste. Pode ocorrer quando o modelo não foi treinado por tempo suficiente ou as variáveis ​​de entrada não são significativas o suficiente para determinar uma relação significativa entre as variáveis ​​de entrada e saída. Aqui também estamos em um cenário a ser evitado e apesar de ser contrário ao sobreajustamento, as técnicas tanto para identificar quanto para evitar o problema são semelhantes. Um adendo, geralmente, identificar um subajustamento é mais fácil que identificar um sobreajustamento.\n\n\n7.0.1.1 Modelo de Regressão Linear\nJá temos uma breve noção sobre o que é aprendizado supervisionado, agora vamos introduzir o funcionamento básico de um modelo. Como foi mencionado, aprendizado supervisionado é usado principalmente para métodos de classificação e regressão. Um modelo de regressão linear, como o próprio nome já diz, se enquadra nos métodos de regressão. A regressão consiste em modelar um valor de previsão com base em variáveis independentes. De forma mais geral, o modelo consiste em fazer uma previsão “simples” calculando uma ponderação entre as somas dos recursos de entrada e uma constante chamada intercepto. Assim, obtemos uma relação linear entre a variável de saída e as variáveis de entrada. A linha de regressão é a linha de melhor ajuste para o modelo.\n\n\n\n\n\nFonte: https://is.gd/Z9yiHp\n\n\n\n\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nonde:\n\n\\(\\hat y\\) é o valor predito\n\\(n\\) o número de características\n\\(x_i\\) é a \\(i^{th}\\) característica\n\\(\\beta_j\\) é o \\(j^{th}\\) parâmetro do modelo\n\nCerto, temos uma definição matemática do nosso modelo, mas como posso treiná-lo? Treinar um modelo significa também definir os parâmetros para que o modelo se ajuste melhor aos meus dados. Em outras palavras, um modelo treinado irá se ajustar à melhor linha para prever o valor de \\(y\\) para um dado valor de \\(x\\). Assim, ao encontrar os melhores valores de \\(\\beta 's\\) obtemos a melhor linha de ajuste. Para isso, primeiro precisamos de uma medida de quão bem (ou mal) o modelo se ajusta aos meus dados. Posteriormente será discutido quais as medidas mais comuns para avaliação de modelos de regressão.\nExistem algumas suposições importantes que devem ser feitas para utilizar um modelo de regressão linear. Estas são algumas verificações formais durante a construção de um modelo de regressão linear, o que garante a obtenção do melhor resultado possível do conjunto de dados fornecido.\n\nSuposição de linearidade: A regressão linear assume que a relação entre a entrada e saída é linear. Pode parecer um pouco óbvio, mas em alguns casos onde, em um primeiro olhar, faça sentido usar uma regressão linear, nossos dados não permitam isso. Pode ser necessário transformar os dados.\nHomocedasticidade: Homocedasticidade é uma situação em que o termo de erro é o mesmo para todos os valores de variáveis ​​independentes. Com homocedasticidade, não deve haver uma distribuição padrão clara de dados no gráfico de dispersão.\nErros normalmente distribuídos: A regressão linear assume que o termo de erro deve seguir o padrão de distribuição normal. Se os termos de erro não forem normalmente distribuídos, os intervalos de confiança se tornarão muito amplos ou muito estreitos, o que pode causar dificuldades em encontrar coeficientes. Você pode obter algum benefício usando transformações (por exemplo, log ou BoxCox) em suas variáveis ​​para tornar sua distribuição mais gaussiana.\nMulticolinearidade: O modelo de regressão linear não assume nenhuma autocorrelação em termos de erro. Se houver alguma correlação no termo de erro, isso reduzirá drasticamente a precisão do modelo. A autocorrelação geralmente ocorre se houver uma dependência entre os erros residuais. Considere calcular correlações pareadas para seus dados de entrada e remover os mais correlacionados.\n\n\n\n7.0.1.2 Modelo de Regressão logística\nAlguns algoritmos de regressão podem ser usados para classificação (o contrário também é válido). A regressão logística é um dos algoritmos mais populares do machine learning e geralmente é usada para estimar a probabilidade de que uma instância pertença a uma classe. Por exemplo, qual a probabilidade de que o objeto de uma imagem seja um cachorro? ou um gato? Neste caso, se a probabilidade estimada for maior que 50%, então o modelo pode prever que naquela imagem tem um cachorro (classe rotulada como “1”), se for menor, prevê que é um gato (classe rotulada como “0”). Este tipo de regressão pode retornar valores categóricos ou discretos, como: Sim ou Não, 0 ou 1, verdadeiro ou falso, entre outros. Mas aqui, ela fornece os valores probabilísticos que estão entre 0 e 1. Apesar de ser semelhante a regressão linear, aqui não ajustamos uma linha de regressão, mas sim uma função logística em forma de “S” que prevê os dois valores máximos (0 ou 1).\n\n\n\n\n\nFonte: https://is.gd/87NSTU\n\n\n\n\nA equação de regressão Logística pode ser obtida a partir da equação de Regressão Linear.\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nO problema de usar essa abordagem é que podemos prever probabilidades negativas em alguns casos e valores maiores que 1 em outros. Essas previsões não são sensatas, pois sabemos que a verdadeira probabilidade deve ser um número entre 0 e 1. Para resolver esse problema, devemos modelar \\(\\hat y\\) usando uma função que forneça saídas entre 0 e 1 para todos os valores de \\(\\hat y\\). Na regressão logística usamos a função logística como sendo:\n\\[\n\\hat y = \\frac{e^{\\beta_0+\\beta_1X}}{1 + e^{\\beta_0+\\beta_1X}}\n\\]\nDepois de algumas manipulações, chegamos que\n\\[\n\\frac{\\hat y}{1- \\hat y} = e^{\\beta_0+\\beta_1X}\n\\]\nMas precisamos variar de \\(-\\infty\\) até \\(\\infty\\), então pegue o logaritmo da equação e temos:\n\\[\n\\log\\bigg[\\frac{\\hat y}{1- \\hat y} \\bigg ] = {\\beta_0+\\beta_1X}\n\\]\nExistem alguns tipos de regressão logística:\n\nBinomial: Aqui deve haver apenas dois tipos de possíveis variáveis, como 0 ou 1, Falso ou Verdadeiro, etc.\nMultinomial: Pode também haver 3 ou mais tipos não ordenados possíveis da variável dependente, como, cachorro, gato ou tigre.\nOrdinal: Na regressão logística ordinal, pode haver 3 ou mais tipos ordenados possíveis de variáveis ​​dependentes, como “baixo”, “médio” ou “alto”.\n\n\n\n7.0.1.3 Medidas de desempenho\nAo desenvolver projetos de Aprendizado de Máquinas no geral, é preciso sempre verificar a qualidade do modelo. Assim como não existe um modelo padrão para resolver todos os problemas, não existe uma única métrica para avaliar a qualidade do modelo. Saber qual métrica é mais apropriada para determinado cenário é crucial, pois escolhas erradas podem gerar modelos problemáticos.\n\n7.0.1.3.1 Modelos de regressão\nExistem algumas métricas importantes para medirmos a qualidade de um modelo de regressão.\n\nRoot Mean Squared Error (RMSE): Uma medida muito comum usada em um modelo de regressão é a Root Mean Squared Error (REQM). Em resumo, o REQM é uma medida que mostra o quão espalhados estão esses resíduos. A métrica possui valor mínimo 0 e sem valor máximo. Quanto maior esse número, pior o modelo.\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}  (\\hat y_i - y_i)^2}\n\\]\nUma vantagem dessa métrica é que predições muito distantes do real aumentam o valor da medida com facilidade, o que torna a métrica bem vinda em problemas onde erros grandes não são tolerados.\nMean Absolute Error (MAE): Esta medida é bem simples de entender. Nada mais é que a média do erro que cada ponto tem em relação a linha de regressão. É um pouco parecido com o que RMSE faz, a diferença é que aqui, erros grandes não afetam tanto a medida. A sua interpretação é: quanto maior o MAE, maior é o erro do modelo. Apesar de ser simples, não deve ser usado em todos os modelos. Esta medida é bastante sensível a valores discrepantes (outliers). Portanto, deve-se avaliar os dados antes de utilizar esta métrica.\n\\[\nMAE = \\frac{1}{N}\\sum|y_i - \\hat{y_i}|\n\\]\n\\(R^2\\): R-quadrado é uma medida que visa explicar qual a porcentagem de variância que pôde ser prevista pelo modelo, ou seja, o qua “próximo” as medidas reais estão dos nossos dados. Segue a fórmula:\n\\[\nR^2 = 1 - \\frac{\\sum(y_i - {\\hat{y_i}})^2} {\\sum(y_i - {\\bar{y}})^2}\n\\]\nonde, \\(\\hat{y_i}\\) é o valor predito, \\(\\bar{y}\\) é o valor médio das amostras e \\(y_i\\) é o valor observado. O valor resultante varia de 0 a 1. Quanto maior o valor, melhor o modelo. Por exemplo, caso tivéssemos avaliando um modelo e tivéssemos \\(R^2 = 0.87\\), entende-se então que 87% da variância de nossos dados podem ser explicadas pelo modelo. Esta medida possui algumas limitações importantes: só pode ser aplicada perfeitamente para modelos univariados; é uma medida enviesada, pode definição e em casos de overfitting, o valor da métrica ainda continua alto.\n\n\n\n7.0.1.3.2 Modelos de classificação\n\nAcurácia: A acurácia nos diz quantos de nossos exemplos foram classificados corretamente e é dada pela seguinte fórmula:\n\\[\n\\mbox{acurácia}   =  \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nOnde TP = True Positive (Verdadeiro Positivo), TN = True Negative (Verdadeiro Negativo), FP = False Negative (Falso Negativo) e FN = False Negative (Falso Negativo).\nA métrica então é definida pela razão entre os acertos e o total (erros + acertos). É uma métrica extremamente fácil de ser usada e interpretada. Por exemplo, se tenho 100 observações e meu modelo classificou 80 corretamente, então a acurácia do meu modelo é 80%. Apesar de simples, essa métrica pode não ser adequada em alguns casos.\nUma desvantagem é que podemos obter uma acurácia alta, mas o nosso modelo pode ter uma performance inadequada. Por exemplo, pense que temos um conjunto de dados de animais com 100 observações, sendo 90 cachorros e 10 gatos. Se tivermos um modelo que sempre classificará todas as observações como cachorro, ainda teríamos um modelo com acurácia de 90%. É uma métrica boa, mas não estamos avaliando o nosso modelo de uma boa forma. Se introduzirmos novos dados que venham de uma forma mais equilibrada, ou seja, com quantidade de cachorros e gatos parecidas, o modelo se comportaria de forma ruim. Portanto, para conjuntos de dados desbalanceados, outras métricas são mais eficientes.\nOutra desvantagem é que esta medida atribui o mesmo peso para ambos os erros. Por exemplo, em um modelo que classifica exames de câncer entre positivo e negativo para a doença. Um modelo com acurácia 90% aparentemente é um bom modelo onde os 10% de erro podem ter sido falsos negativos ou falsos positivos. Porém, o erro por falso negativo aqui é bem mais grave.\nPrecisão: Essa métrica é definida pela razão entre a quantidade de observações classificadas corretamente como positivos e o total de classificados como positivos, segue a fórmula:\n\n\\[\n\\mbox{precisão}  =  \\frac{TP}{TP + FP}\n\\]\nEsta medida atribui um peso maior para os erros por falso positivo. Pode ser entendida como a resposta para a pergunta: das observações classificadas como positivos, quantos são verdadeiramente positivos? então, neste caso, se a precisão fosse de 90%, é esperado que a cada 100 observações classificadas como positivos, apenas 90 são de fato positivos.\n\nSensibilidade: Essa métrica avalia então a capacidade do modelo detectar com sucesso resultados classificados como positivos em relação a todos os pontos de dados positivos.\n\\[\n\\mbox{sensibilidade} = \\frac{TP}{TP + FN}\n\\]\nEspecificidade: Ao contrário da sensibilidade, a especificidade avalia a capacidade do modelo detectar resultados negativos. Segue a fórmula:\n\n\\[\n\\mbox{especificidade} = \\frac{TN}{TN + FP}\n\\]\n\n\n7.0.1.3.3 Matriz de confusão\nMatriz de confusão é uma matriz usada para avaliar o desempenho de um modelo de classificação. A matriz compara os valores de destino reais com os previstos pelo modelo. A matriz é N x N onde é N é o número de classes. Para um problema de classificação binária teríamos uma matriz 2 x 2. As matrizes de confusão revelam quando um modelo confunde consistentemente duas classes, simplificando a determinação da probabilidade de os resultados de um modelo serem confiáveis.\n\n\n\n\n\n\n\n\n\n\n\n\nObservado\n\n\n\nPredito\nNegativo\nPositivo\n\n\n\n\nNegativo\nVerdadeiro Negativo (TN)\nFalso Negativo (FN)\n\n\nPositivo\nFalso Positivo (FP)\nVerdadeiro Positivo (TP)\n\n\n\n\n\n\n\nNa matriz de confusão colocamos os valores reais nas colunas e os valores preditos nas linhas. Assim o cruzamento das linhas e das colunas passam a ser nossas métricas. (não é incomum vermos versões invertidas)\n\n\n7.0.1.3.4 Curva AUC-ROC\nQuando queremos verificar ou visualizar o desempenho de um modelo de classificação binária (0, 1), podemos utilzar a AUC (Area Under The Curve) ROC (Receiver Operating Characteristics). Está é uma das métricas de avaliação de desempenho de modelos de classificação mais importantes.\n\n\n7.0.1.3.5 O que é a curva AUC-ROC\nA curva ROC é uma métrica de avaliação para problemas de classificação binária. É uma curva de probabilidade que plota o TPR (sensibilidade) contra o FPR (1 - especificidade). A AUC (Area Under the Curve) é a medida da capacidade de um classificador para distinguir entre classes e é usada com um resumo da curva ROC. Quanto maior a AUC, melhor o modelo está em distinguir entre classes positivas e negativas. Por analogia, quanto maior a AUC, melhor o modelo está em distinguir entre pacientes com a doença e sem a doença.\nQuando AUC = 1, o modelo é capaz de distinguir perfeitamente os pontos positivos e negativos corretamente. Porém, se AUC = 0, o modelo estaria predizendo todos valores negativos como positivos e vice e versa. Quando AUC= 0,5, o modelo não é capaz de distinguir os valores. Neste caso o modelo está prevendo cada classe de forma aleatória ou de forma constante para todos os dados. Já quando 0,5 &lt; AUC &lt; 1, há uma grande chance de que o modelo consiga distinguir os valores negativos das classes positivas. Aqui o modelo é capaz de detectar mais números de verdadeiros positivos e verdadeiros negativos do que falsos negativos e falsos positivos.\nA curva ROC é produzida calculando e plotando a taxa de verdadeiros positivos em relação à taxa de falsos positivos para um único classificador em vários limites . Por exemplo, na regressão logística, o limiar seria a probabilidade prevista de uma observação pertencente à classe positiva. Normalmente, na regressão logística, se uma observação é prevista como positiva com probabilidade &gt; 0,5, ela é rotulada como positiva. No entanto, poderíamos realmente escolher qualquer limite entre 0 e 1 (0,1, 0,3, 0,6, 0,99, etc.) — e as curvas ROC nos ajudam a visualizar como essas escolhas afetam o desempenho do classificador.\nA figura abaixo demonstra como alguns modelos teóricos podem plotar a curva ROC. A linha cinza pontilhada representa um classificador que não é melhor do que a adivinhação aleatória, seria a linha diagonal. Um modelo com uma taxa de verdadeiros positivos de 100% e falsos negativos de 0% seria plotado sobre a linha da esquerda e a de cima. Quase todos os exemplos do mundo real cairão em algum lugar sobre essas duas linhas tendo o cenário 0,5 &lt; AUC &lt; 1, como exemplo, a linha azul da figura. Normalmente procuramos um classificador que mantenha uma alta taxa de verdadeiros positivos e, ao mesmo tempo, uma baixa taxa de falsos positivos. Embora seja útil visualizar a curva ROC, em muitos casos podemos reduzir essas informações a uma única métrica, a AUC.\n\n\n\n\n\n\n\nFonte: https://is.gd/iyWHe8\n\n\n\n\nUm grande ponto positivo da curva ROC é que ela permita que encontremos um limite de classificação adequado ao nosso problema específico\nPor exemplo, se estivéssemos avaliando um classificador de spam de e-mail, gostaríamos que a taxa de falsos positivos fosse muito, muito baixa. Não queremos que alguém perca um e-mail importante para o filtro de spam só porque nosso algoritmo foi muito agressivo. Provavelmente até permitiríamos uma boa quantidade de e-mails de spam reais (verdadeiros positivos) através do filtro apenas para garantir que nenhum e-mail importante fosse perdido.\nPor outro lado, se nosso classificador está prevendo se alguém tem uma doença terminal, podemos aceitar um número maior de falsos positivos (diagnosticado incorretamente a doença), apenas para garantir que não perderemos nenhum verdadeiro positivo (pessoas que realmente têm a doença).\nAlém disso, também podemos comparar o desempenho de diferentes classificadores para o mesmo problema.\nExistem outras métricas de desempenho que não foram apresentadas aqui, mas devemos ressaltar alguns pontos. Não existe uma métrica certa ou errada, devemos apenas nos atentar e buscar a que melhor atende o nosso problema. É possível também utilizar mais de uma métrica para o mesmo modelo.\n\n\n\n7.0.1.4 Validação Cruzada (Cross-Validation)\nAté aqui falamos um pouco sobre alguns problemas que podem ser encontrados no aprendizado de máquinas e superficialmente sobre dois modelos de regressão. Vamos falar agora sobre um método que é bem utilizado para validar a estabilidade do seu modelo. Como mencionamos anteriormente, não podemos simplesmente ajustar um modelo aos meus dados de treino e esperar que ele funcione perfeitamente, ou até mesmo esperar que aquele seja o melhor modelo possível para fazer alguma validação. Falamos um pouco sobre isso quando discutimos sobre sobreajustamento e subajustamento. Então, vamos nos aprofundar sobre um método que nos garanta que o nosso modelo obteve a maioria dos padrões dos dados corretos sem captar muitos ruídos.\nO que é validação cruzada?\nValidação cruzada é uma técnica para avaliar um modelo de aprendizado de máquina e testar o seu desempenho. Pode ajudar a comparar e selecionar um modelo mais apropriado para o nosso problema. É bem fácil de entender, de implementar e tende a ter um viés menor do que outros métodos usados para o mesmo objetivo. Por isso é uma ferramenta tão utilizada. Tanto a validação cruzada quanto outros algoritmos funcionam de maneira semelhante, consiste em: dividir o conjunto de dados em treino e teste; treinar o modelo no conjunto treino; validar o modelo no conjunto teste e repetir as etapas anteriores algumas vezes. Dentro da validação cruzada existem diversas técnicas onde umas são mais utilizadas. Já mencionamos anteriormente o método\nk-fold, mas exite também os métodos, hold-out, leave-p-out, k-fold stratified, entre outros. Vamos falar sobre alguns deles.\n\nHold-Out Cross Validation: Esta é a técnica mais simples e comum. Ele consiste em remover uma parte dos dados de treinamento e usá-la para obter previsões do modelo treinado no restante dos dados. A estimativa de erro informa como nosso modelo está se saindo em dados não vistos ou no conjunto de validação. A implementação é extremamente fácil e existem pacotes que podem ajudar nisso. Mas apesar disto, esse método tem um grande desvantagem. Se estivermos trabalhando com um conjunto de dados que não é completamente uniforme, podemos acabar em uma situação difícil após a separação. O conjunto de treino pode não representar muito bem o conjunto de teste, ou seja, os conjuntos podem ser bem diferentes, onde um é mais fácil do que o outro.\nK-Fold Cross Validation: O K-Fold pode se apresentar como um técnica que minimiza as desvantagens do método Hold-Out apresentando uma nova maneira de dividir o banco de dados. Neste método os dados são divididos em k subconjuntos (daí o nome). O método de validação é repetido k vezes, onde, a cada vez, um dos k subconjuntos é usado como conjunto de teste e os outros k-1 conjuntos são unidos para formar o conjunto de treinamento. A estimativa de erro é a média de todas as k tentativas. Como cada ponto de dados chega a um conjunto de validação exatamente uma vez e a um conjunto de treinamento k-1 vezes, isso reduz significativamente o viés. Como “regra geral”, k=5 ou k=10 é escolhido, mas não existe nada fixo. Comparando diretamente ao método Hold-Out, o método K-Fold tende a ser melhor, mas também possui uma desvantagem. Aumentar o k resultado no treinamento de mais modelos e o processo de treinamento pode ser custoso e demorado.\nLeave-P-Out Cross Validation: Este método consiste em criar todos os conjuntos de treinamento e testes possíveis usando p amostras como conjunto de teste. Em outras palavras, deixa p pontos de dados fora dos dados de treino, ou seja, se houver n pontos de dados na amostra original, np amostras são usadas para treinar o modelo p pontos são usadas como conjunto teste. Como se pode imaginar, este método é extremamente exaustivo, tendo em vista que é preciso validar o modelo para todas as combinações possíveis e para um p demasiadamente grande, pode ser computacionalmente inviável.\n\nO método de validação cruzada também pode nos ajudar a ajustar hiperparâmetros, falaremos sobre isso posteriormente.\n\n\n7.0.1.5 Hiperparâmetros\nJá discutimos anteriormente sobre treinamento de um modelo e mencionamos os Hiperparâmetros, vamos agora discutir de um modo geral o que eles são e qual a sua importância. Hiperparâmetros são atributos que controlam o treinamento de um modelo. Eles ajudam a direcionar um modelo, melhorando seu desempenho e evitando que ele aprenda somente com os dados de treino, ou seja, evitar o overfitting e underfitting. Por exemplo, no processo de configuração de uma rede neural, precisamos decidir quantas camadas ocultas de nós precisam ser usadas entre a camada de entrada e a camada de saída, assim como quantos nós cada camada vai usar. Esses parâmetros não estão diretamente ligados aos dados de treino e não são aprendidos diretamente pelos modelos. Parâmetros são as variáveis ​​que o algoritmo de Machine Learning usa para prever resultados com base na entrada de dados históricos. Já os hiperparâmetros são variáveis ​​que são especificadas ao longo do processo de construção do modelo. Com isso os hiperparâmetros são fornecidos antes dos parâmetros, ou podemos dizer que os hiperparâmetros são utilizados para avaliar os parâmetros ideais do modelo. Encontrar a melhor combinação de hiperparâmetros pode fazer a diferença no seu modelo.\nExemplos comuns de hiperparâmetros\n\nNúmero de árvores em um algoritmo de Random Forest.\nA escolha da função custo ou perda que o modelo usará.\nNúmero de cadamadas ocultas em redes neurais.\nQuantidade mínima de observações dentro de um nó.\nTaxa de divisão de treino e teste.\nProporção de linhas para sortear por árvore.\n\n\n\n7.0.1.6 Ajuste de hiperparâmetro\nO ajuste de hiperparâmetro consiste em encontrar a configuração de hiperparâmetro que irá resultar no melhor desempenho do modelo. A busca manual pode ser utilizada para localizar os hiperparâmetros ótimos, utilizando uma abordagem de acerto e erro, mas obviamente isso levaria muito tempo. As técnicas mais utilizadas para isso são o Grid Search ou Random Search.\n\nGrid Search: Essa é uma técnica de ajuste que tenta calcular os valores ótimos dos hiperparâmetros. É calculado exaustivamente o desempenho do modelo para cada combinação de todos os hiperparâmetros fornecidos previamente e escolhe, a partir dai, o valor ideal para os hiperparâmetros. Apesar de ser uma técnica que foge da abordagem manual, é um processo demorado e computacionalmente caro.\nRandom Search: Random Search é um método que usa combinações aleatórias de hiperparâmetros para treinar o modelo. As melhores combinações são usadas. A diferença para o Grid Search é que aqui não é especificado um conjunto de valores para os hiperparâmetros. Em vez disso, os valores de cada hiperparâmetro são amostrados a partir de uma distribuição. Essa técnica permite controlar o número de tentativas de combinações de hiperparâmetros, diferente da Grid Search, onde todas as combinações possíveis são tentadas.\n\n\n\n\n7.0.2 Agoritmos de Aprendizado de máquina\n\n7.0.2.1 Árvores de Decisão\nAs Árvores de Decisão são algoritmos de aprendizado supervisionado que podem ser utilizados tanto para classificação quanto para regressão. Possui uma estrutura hierárquica em árvore, tendo um nó raiz, ramificações, nós internos e nós folhas. Na análise de decisão, uma árvore de decisão pode ser usada para representar visualmente e explicitamente decisões e tomadas de decisão.\n\n\n\n\n\nÁrvore de Decisçao\n\n\n\n\nNa imagem acima podemos ver uma representação de uma árvore de decisão, que começa com um nó raiz, que não possui ramificações de entrada, a partir da qual a árvore se divide em ramos. Os ramos alimentam os nós internos ou nós decisão. Cada ramificação contém um conjunto de atributos ou regras de classificação, associado a um determinado rótulo de classe, que pode ser encontrado na extremidade da ramificação. O final dos ramos que não se dividem mais são os nós folha. Os nós folha representam todos os resultados possíveis dentro do conjunto de dados.\nO tipo de estrutura de fluxograma é fácil de ser interpretado e cria uma representação que permite que diferentes grupos entendam melhor porque uma determinada decisão foi tomada.\nO algoritmo de Árvore de Decisão possui uma estratégia que busca identificar os pontos de divisão ideias dentro de uma árvore. O processo de divisão é repetido até que todos ou a maioria dos registros tenham sido classificados em um rótulo de classe específico. Classificar ou não todos os pontos de dados como conjuntos depende principalmente da complexidade do algoritmo. Árvores menores ou mais simples, são mais fáceis de atingir nós de folhas puros, onde pontos de dados caem em uma única classe, além de serem mais fáceis de visualizar e interpretar as decisões. Em contrapartida, aumentar o tamanho de uma árvore dificulta manter essa pureza, podendo resultar em poucos dados caindo em uma determinada subárvore, como consequência, isso pode levar ao overfitting. Portanto, esses algoritmos têm preferência por árvores pequenas e simples, adicionando complexidade apenas quando realmente necessário. Árvores de decisão geralmente crescem de forma arbitrária, portanto é preciso decidir quais recursos escolher e quais condições usar para dividir uma árvore, além de saber quando parar essa divisão.\n\n7.0.2.1.1 Medidas de seleção de atributo\nComo mencionado anteriormente, uma Árvore de decisão é feita a partir de atributos e decidir qual atributo colocar na raiz ou em diferentes níveis da Árvore pode ser complicado. Aqui a aleatoriedade na escolha não é uma boa opção, podendo gerar resultados com baixa precisão. Para o problema de seleção de atributos, temos alguns critérios como: Entropia e ganho de informação, índice de Gini , taxa de ganho, entre outros.\n\nEntropia: Entropia é uma medida de aleatoriedade (impureza) dos valores de uma amostra. Quanto maior a entropia, mais difícil é tirar conclusões dessa informação. É definida pela seguinte fórmula:\n\n\\[\nEntropia(S) = - \\sum_{i=1}^{c} p_{i}\\,log_2\\,pi\n\\]\nOnde, S representa o Estado atual e \\(p_i\\) é a probabilidade de um evento \\(i\\) do estado S.\nOs valores variam entre 0 e 1. Para selecionar o melhor recurso para dividir e encontrar a Árvore ideal, deve-se buscar o atributo com menor entropia.\n\nGanho de Informação: Ganho é usado para medir o quão bem um atributo separa os exemplos de treino de acordo com sua classificação alvo. Quanto maior o ganho de informação, melhor. Ou seja, na construção de uma Árvore de Decisão buscamos um atributo que retorne o maior ganho de informação e a menor entropia. Ganho de informação é calculado a diferença entre a entropia antes da divisão e a entropia média após a divisão do conjunto de dados. Segue a fórmula:\n\n\\[\nIG(Y,X) = Entropia(Y) - Entropia (Y|X)\n\\]\nPodemos visualizar a fórmula como: subtraímos a entropia de \\(Y|X\\) da entropia apenas de \\(Y\\) para calcular a redução da incerteza sobre \\(Y\\) dada uma informação adicional \\(X\\) sobre \\(Y\\).\n\nÍndice de Gini: Também conhecido como impureza de Gini, o Índice de Gini calcula a quantidade de probabilidade de um recurso específico que é classificado incorretamente quando selecionado aleatoriamente. Se todos os elementos estiverem vinculados a uma única classe, ela pode ser chamada de pura. É calculado subtraindo a soma das probabilidades ao quadrado de cada classe de um. Segue a fórmula:\n\n\\[\nGN = 1- \\sum_{i=1}^{n} (p_i)^2\n\\]\nOnde \\(pi\\) denota a probabilidade de um elemento ser classificado para uma classe distinta. O Índice de Gini é semelhante a entropia. Varia entre 0 e 1, onde 0 expressa a pureza da classificação e 1 indica a distribuição aleatória dos elementos em várias classes.\n\nTaxa de Ganho: Taxa de ganho tenta diminuir o viés do ganho de informação. Leva em consideração o número de ramificações que resultariam antes de fazer a divisão. Ele corrige o ganho de informação levando em consideração as informações intrínsecas de uma divisão. A Informação Intrínseca (II) é definida como a entropia das proporções do subconjunto de dados. Segue a fórmula: \\[\nII = -\\sum_{i=1}^n \\frac{N(t_i)}{N(t)} *log{_2}  \\frac{N(t_i)}{N(t)}\n\\]\nOnde \\(N(t_i)\\) é o número de vezes que \\(t_i\\) ocorre dividido pela contagem total de eventos \\(N(t)\\) onde \\(t\\) é o conjunto de eventos.\nA Taxa de Ganho então é dada por:\n\\[\nGR = \\frac{Ganho\\ de\\ Informação}{Informação\\ intrínseca}\n\\]\n\nPara todas as variáveis ​​preditoras, aquela que fornece a maior taxa de ganho é escolhida para a divisão.\n\n\n7.0.2.1.2 Tipos de Árvores de Decisão\nMencionamos anteriormente que o algoritmo de Árvores de Decisão pode ser usado tanto para classificação quanto para regressão, e é claro, existem vários tipos de algoritmos. Vamos falar um pouco sobre os mais famosos.\n\nID3 (Iterative Dichotomiser 3): Esse algoritmo usa uma abordagem gananciosa de cima para baixo para construir a Árvore. Em outras palavras, ele começa a construir a Árvore de cima e de para cada iteração seleciona o melhor recurso no momento para criar um nó. Geralmente é usado apenas para problemas de classificação com recursos nominais. O algoritmo utiliza o Ganho de Informação para encontrar o melhor recurso.\nC4.5: Este algoritmo é uma melhoria em relação ao ID3. Ele pode usar a Taxa de Ganho como função para encontrar o melhor recurso.\nCART: CART é uma abreviação de “classification and regression trees” (Árvores de classificação e Regressão). Como o próprio nome já diz, pode ser usado tanto para classificação quanto para regressão. O algoritmo busca o melhor critério através do Índice de Gini.\n\n\n\n7.0.2.1.3 Podando Árvores de Decisão\nComo mencionamos anteriormente, Árvores complexas podem causar overfitting, cabe dizer que as Árvores são os algoritmos mais suscetíveis ao overfitting. Em alguns casos, a Árvore poderá se ramificar inúmeras vezes, gerando uma folha para cada observação, ou seja, fornecendo 100% de precisão no conjunto de treino, logo, é preciso ter um limite.\nPodar uma Árvore de Decisão consiste em remover partes da Árvore que não fornecem poder para classificar instâncias. A poda pode ser distinguida em:\n\nPré-poda: A Árvore é interrompida antes de concluir a classificação do conjunto de treinamento. É um método alternativo que tenta interromper o processo de construção da árvore antes que ele produza folhas com amostras muito pequenas. Essa heurística é conhecida como parada antecipada. Em cada estágio de divisão da árvore, é verificado o erro de validação cruzada. Se o erro não diminuir significativamente, então paramos. A parada antecipada pode prejudicar o ajuste, parando muito cedo.\nPós-poda: Normalmente a técnica mais utilizada, permite que a Árvore classifique o conjunto de treinamento antes de podá-la. Este método consiste em, a partir da Árvore não podada, pega uma sequência de subárvores (podadas) e escolher a melhor por meio da validação cruzada. É escolhida a que tem maior precisão no conjunto de treinamento validado cruzadamente.\n\n\n\n7.0.2.1.4 Floresta Aleatória (Random Forest)\nA Floresta Aleatória é um algoritmo que funciona a partir de Árvores de Decisão. Ele constrói diferentes Árvores em diferentes amostras e leva combina as saídas para alcançar um único resultado, levando em consideração a maioria de votos para classificação e média em caso de regressão. O algoritmo serve tanto para regressão quanto para regressão, mas aqui temos uma novidade, ele pode lidar com os dois, apesar de apresentar melhores resultados para problemas de classificação.\n\n\n7.0.2.1.5 Funcionamento do algoritmo\nAntes de falarmos de fato sobre o funcionamento do algoritmo, vamos entender de forma básica o método dos conjuntos. O método dos conjuntos, também conhecido como ensemble methods, pode ser definido resumidamente como, combinar vários modelos. No caso da Floresta Aleatória, uma combinação de Árvores de decisão. Os modelos são combinados e suas previsões são agregadas para identificar o resultado mais popular. Os métodos de conjunto mais populares são bagging e boosting. Em resumo o método bagging, que é usado em Floresta Aleatória, cria um subconjunto de treinamento diferente a partir dos dados de treinamento de amostra com substituição, o que significa que os pontos de dados individuais podem ser escolhidos mais de uma vez. Depos, dependendo do tipo da tarefa, classificação ou regressão, a saída final é baseada na votação da maioria ou a média. O método boosting é utilizado para outros algoritmos como, por exemplo, XGBOOST, portanto, falaremos sobre ele mais pra frente.\nCerto, entendido o método dos conjuntos podemos entrar no algoritmo em si. É basicamente uma extensão do método bagging, já que utiliza o método bagging e a aleatoriedade para criar uma floresta não correlacionada de Árvores de Decisão. A aleatoriedade se dá ao fato que o modelo utiliza uma amostragem aleatória do conjunto de dados de treinamento ao construir árvores e subconjuntos aleatórios de recursos considerados ao dividir os nós, o que garante baixa correlação entre as Árvores.\nOs algoritmos possuem alguns hiperparâmetros, que como explicado anteriormente, precisam ser definidos antes do treinamento do modelo. Os três principais são: o tamanho do nó, o número de Árvores e o número de recursos amostrados. A partir daí o algoritmo pode ser usado para resolver problemas de regressão ou classificação.\n\n\n\n7.0.2.2 Algoritmos de boosting\nComo falamos anteriormente, boosting é uma técnica de método dos conjuntos. Diferente de bagging que trabalha de forma paralela, considerando os modelos independentes uns dos outros, boosting trabalha de forma diferente. Boosting se refere a algoritmos que convertem alunos fracos em alunos fortes. Trabalha de maneira sequencial, ajustando iterativamente o peso da observação de acordo com a última classificação, assim, diminui o erro do viés e constrói preditivos fortes. Em outras palavras, a técnica consiste em, primeiramente, construir um modelo inicial com os dados de treinamento. Depois, um outro modelo é construído visando corrigir os erros do modelo anterior, atribuindo pesos caso uma entrada seja fornecida erroneamente. O processo continua e adiciona modelos até que todo o conjunto de dados de treino seja previsto corretamente ou o número máximo de modelos seja adicionado. Boosting é um algoritmo genérico e não um modelo, vamos então apresentar os principais modelos baseados no método boosting: Adaptative Bossting (AdaBoost), Gradient Boosting e XGBoost.\n\n7.0.2.2.1 AdaBoost\nAntes de entrarmos no algoritmo AdaBoost de fato, vamos relembrar o Algoritmo de Floresta Aleatória. Em Floresta Aleatória, o algoritmo cria diversas Árvores que consistem em um nó inicial com várias nós folhas. Não existem regras quanto ao tamanho de cada Árvore, assim, pode haver umas maiores que outras. No modelo AdaBoost, as Árvores possuem um nível, ou seja, apenas 1 divisão conhecido como Stump (toco).\nReforçando, o algoritmo tem apenas um nó com duas folhas. Tocos são aprendizes fracos, pois as técnicas de reforço preferem isso. Como em AdaBoost o erro do primeiro toco influência nos outros, a ordem dos tocos é muito importante. Reforçando, o algoritmo irá atribuir pesos mais altos aos pontos classificados erroneamente. Todos os pontos que têm pesos maiores recebem mais importância no próximo modelo.\n\n\n7.0.2.2.2 Funcionamento do algoritmo\nPara entendermos de uma forma um pouco mais clara, vamos fazer um passo a passo. Antes do primeiro passo de fato, todos os pontos de dados irão receber algum peso. Inicialmente, os pesos serão iguais para todos. O peso é calculado na forma \\(W = \\frac{1}{N}\\), onde \\(N\\) é o número de observações.\n\nPasso 1: Em primeiro lugar, o algoritmo pega o primeiro recurso e cria o primeiro tronco. Ele irá criar o mesmo número de tocos que o número de recursos, a partir daí, temos as primeiras Árvores de Decisão. O modelo então calcula o índice de Gini para cada Árovre e seleciona aquela com o índice mais baixo para ser o primeiro aprendiz base.\nPasso 2: O segundo passo é calcular o desempenho do toco com um método conhecido como “Importance” ou “Influence”. É calculado usando a seguinte fórmula:\n\\[\n\\mbox{Performance} (\\alpha) = \\frac{1}{2} log(\\frac{1- Erro\\ total }{Erro\\ total})\n\\]O valor sempre será um valor entre 0 e 1, onde 0 indica um toco perfeito e 1 um toco horrível. Calcular o desempenho de um toco é importante pois é preciso atualizar o peso da amostra antes de prosseguir para o próximo modelo. Se o mesmo peso for apicado, a saída recebida será a do primeiro modelo. Previsões erradas receberão mais peso, enquanto os pesos das previsões erradas serão diminuídos. Assim, no próximo modelo com os pesos atualizados, sera dada mais preferência aos pontos com pesos maiores.\nPasso 3: O próximo passo é atualizar os pesos. A atualização é feita utilizando a seguinte fórmula:\n\\[\n\\mbox{Novo peso} = \\mbox{peso antigo} * e^{\\pm Performance (\\alpha)}\n\\] A quantidade \\(\\alpha\\) será negativa quando a amostra for classificada corretamente e positiva quando classificada incorretamente. Vale ressaltar que a soma total de todos os pesos deve ser igual a 1. Em muitos casos os pesos atualizados não irão somar 1, então será necessário normalizar os pesos. A normalização é feita dividindo cada peso pela soma total dos pesos atualizados. Após a normalização dso pesos da amostra, a soma será 1.\n\nPasso 4: Agora, para verificar se os erros diminuíram ou não, a próxima etapa é criar um novo conjunto de dados. Para isso, com base nos novos pesos amostrais, nossas observações serão divididas em baldes, basicamente são intervalos de valores de \\(\\alpha\\). A partir daí, o algoritmo seleciona números aleatórios entre 0 e 1. Ele verifica em qual balde o valor selecionado pertence e seleciona esse registro no novo conjunto de dados. Como registros classificados incorretamente têm pesos amostrais maiores, existe mais probabilidade de serem selecionados várias vezes.\n\nPasso 5: Por fim, com o novo conjunto de dados, o algoritmo cria uma nova Árvore de decisão (toco) e repete o processo desde o primeiro passo até passar por todos os tocos. O processo é repetido até que um erro de treinamento baixo seja alcançado. Suponha um algoritmo simples que tenha construído 3 Árvores de Decisão de maneira sequencial. Assim, o conjunto de dados teste passará por todas as Árvores e, da mesma forma que em Florestas Aleatórias, a classe será selecionada com base na maioria. A partir daí é possível fazer previsões para o conjunto de teste.\n\n\n\n7.0.2.2.3 Gradient Boosting (Aumento de Gradiente)\nO modelo Gradient Boosting, também é um modelo de reforço, ou seja, utiliza técnicas de conjunto que pode ser usado tanto para regressão quanto para classificação. Este modelo possui algumas semelhanças com o Ada Boost e como já vimos sobre ele, aqui vamos focar um pouco mais em suas diferenças.\nA principal diferença deste modelo para o Ada Boost está no que ele faz com os valores subajustados do seu antecessor. Enquanto Ada Boost ajusta os pesos a cada interação, Gradient Boosting tenta ajustar o novo preditor aos erros residuais cometidos pelo preditor anterior.\nOutra diferença é que, enquanto Ada Boost começa construindo um toco, Gradient Boost começa fazendo uma única folha. Esta folha representa uma estimativa inicial para os pesos das amostras. Em caso de um problema de regressão, ao tentar prever um valor contínuo, o primeiro palpite é o valor médio. A partir daí, o modelo constrói uma Árvore. Aqui, a Árvore é maior que um toco, mas o modelo ainda restringe o tamanho dessa Árvore.\n\n\n7.0.2.2.4 Funcionamento do modelo\nVamos entender um pouco melhor o funcionamento do modelo fazendo um passo a passo resumido, assim como foi feito em Ada Boost. Vamos exemplificar também focando no modelo de Gradient Boosting para regressão.\n\nPasso 1: Como falamos anteriormente, o modelo começa criando uma única folha. Como estamos em um cenário de dados contínuos, essa folha então é a média das observações. Essa seria nossa previsão para o modelo base.\nPasso 2: O próximo passo é calcular os pseudo-resíduos, que basicamente é o valor observado - valor previsto. O termo pseudo-resíduo é baseado na regressão linear, onde a diferença entre os valores observados e os valores previstos resulta em resíduos. Utilizando os resíduos como alvo é possível gerar novas previsões. Neste caso, as previsões serão os valores de erros.\nPasso 3: Agora é combinado a folha original com a nova Árvore para fazer uma nova predição a partir dos dados de treinamento.\n\nEm resumo, o algoritmo começa com uma única folha. Em seguida é adicionada uma Árvore com base nos resíduos e dimensiona a contribuição das árvores para a previsão final com a taxa de aprendizado. A partir daí continua adicionando Árvores com base nos erros cometidos pela Árvore anterior.\nO modelo possui alguns hiperparâmetros importantes como, o número de estimadores, a profundidade máxima da Árvore, a divisão mínima das amostras, entre outros. Para obter valores precisos desses hiperparâmetros é possível utilizar as técnicas mencionadas anteriormente.\nResumimos bastante os passos e o funcionamento do modelo, mas deve-se ter em mente que existe uma matemática carregada por trás desses modelos. Cada passo, cada função perdida é utilizada por um motivo comprovado.\n\n\n\n7.0.2.3 Support Vector Machine (SVM)\nPara finalizar, vamos introduzir um último algoritmo muito conhecido no ramo de aprendizado de máquinas. Support Vector Machine, ou SVM, é um algoritmo de aprendizagem supervisionada muito popular que pode ser usado tanto para classificação quanto para regressão. Entretando, esse algoritmo é usado principalmente para classificação, então, focaremos mais nessa parte.\nA ideia basica do algoritmo é relativamente simples: criar uma linha ou um hiperplano que separa os dados em classe para que possamos colcoar facilmente o novo ponto de dados na categoria correta futuramente.\n\n7.0.2.3.1 Como o modelo funciona\nA ideia inicial do SVM é encontrar uma linha ou hiperplano entre os dados de duas classes. O algoritmo então escolhe pontos ou vetores extremos que ajudam a criar o hiperplano. Esses casos extremos são os vetores de suporte (daí o nome). O SVM recebe os dados de entrada e gera uma linha que separa essas classes da melhor forma possível.\nUm exemplo bem simples para entender o modelo é: suponha que tenhamos um conjunto de dados com duas classes, bolas vermelhas e quadrados azuis. precisamos então encontrar uma linha que separe esse conjunto de dados em duas classes (vermelho e azul).\nComo estamos em um espaço 2-d, uma única linha reta pode separar essa margem. Mas existe uma infinidade de linhas que podem separar essas classes, o SVM então busca encontrar a ideal. Mas como?\nO SVM encontra o ponto mais próximo das linhas de ambas as classes. Esses pontos são chamados de vetores de suporte. Esses pontos são chamados de vetores de suporte e a distância entre os vetores e o hiperplano é chamada de margem. O SVM então busca maximizar essa margem, onde, o hiperplano com margem máxima é chamado de hiperplano ótimo.\nCerto, mas nem sempre estamos um cenário onde nossos dados estão linearmente organizados. Neste caso, não podemos apenas desenhar uma única linha reta. Neste cenário, precisamos adicionar mais uma dimensão. Se para dados lineares trabalhos com duas dimensões (x e y), para dados lineares teremos uma terceira dimensão (z). Deixamos as coordenadas no eixo z serem governadas pela restrição,\n\\[z =x^2 + y^2\\]\n\nAssim, a coordenada z é basicamente o quadrado da distância do ponto à origem.\nFazendo isso, os dados se tornam linearmente separáveis. Seja z = k, onde k é uma constante. Como \\(z =x^2 + y^2\\), temos \\(k =x^2 + y^2\\); que é uma equação de um círculo. Com isso, é possível projetar o separador linar em dimensão superior de volta as dimensões originais usando esta transformação. Concluindo, podemos então classificar os dados adicionando uma dimensão extra a eles, fazendo com que se tornem linearmente separáveis. Encontrar a transformação correta para qualquer conjunto de dados não é tão fácil, mas existem recursos que podem ser usados na implementação do algoritmo para ajudar neste trabalho."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados Aplicada à Saúde Materno-Infantil",
    "section": "",
    "text": "Prefácio"
  },
  {
    "objectID": "intro.html#bases-de-dados",
    "href": "intro.html#bases-de-dados",
    "title": "1  Introdução",
    "section": "1.1 Bases de dados",
    "text": "1.1 Bases de dados\nOs dados considerados nas aplicações deste livro são provenientes do Sistema de Informação da Vigilância Epidemiológica da Gripe (SIVEP-Gripe), sistema oficial para o registro dos casos e óbitos por Síndrome Respiratória Aguda Grave (SRAG) disponibilizado pelo Ministério da Saúde. Os dados correspondem a registros de gestantes e puérperas de 10 a 55 anos hospitalizadas com SRAG por COVID-19 confirmada por teste de PCR. Também é utilizados o conjunto de dados do Sistema de Informação sobre Nascidos Vivos (SINASC) obtido por meio da Plataforma de Ciência de Dados aplicada à Saúde (PCDaS) da Fundação Oswaldo Cruz (Fiocruz). Os conjuntos de dados são utilizado para ilustrar e demonstrar diversos aspectos dos conceitos abordados no texto, e podem ser baixados em https://github.com/observatorioobstetrico/dados_livro_cd_saude.\n\n1.1.1 Dados de COVID-19 em gestantes e puérperas\nEssa base consiste em 11.523 registros de gestantes e puérperas diagnosticadas com COVID-19 no período de março de 2020 a dezembro de 2021. Alguns estudos conduzidos pelo OOBr usaram esses dados, dentre os quais podem ser citados: Características demográficas e epidemiológicas sobre mulheres grávidas e puérperas que morreram de Síndrome Respiratória Aguda Grave no Brasil, Mortalidade materna associada à COVID-19 no Brasil em 2020 e 2021: comparação com mulheres não grávidas e homens e Desfechos da COVID-19 em puérperas, gestantes e mulheres não gestantes e nem puérperas hospitalizadas.\nO dicionários das variáveis a ser considerado neste livro está na Tabela 1.1.\n\n\nTabela 1.1: Dicionário das variáveis da base de dados de COVID-19 em gestantes e puérperas.\n\n\n\n\n\n\n\nVariável\nDescrição\nValores\n\n\n\n\nDT_NOTIFIC\nData de preenchimento da ficha de notificação\nDia/Mês/Ano\n\n\nDT_SIN_PRI\nData de primeiros sintomas do caso\nDia/Mês/Ano\n\n\nDT_NASC\nData de nascimento da gestante ou puérpera\nDia/Mês/Ano\n\n\nDT_INTERNA\nData em que gestante ou puérpera foi hospitalizada\nDia/Mês/Ano\n\n\nSEM_PRI\nSemana epidemiológica do início dos sintomas\n1 a 52\n\n\nCS_RACA\nRaça da gestante ou puérpera\n1- branca; 2- preta; 3- amarela; 4- parda; 5-indígena; 9- ignorado\n\n\nCS_ESCOL_N\nNível de escolaridade da gestante ou puérpera\n0- sem escolaridade (analfabeto); 1- fundamental 1° ciclo (1ª a 5ª série); 2- fundamental 2 (6ª a 9ª série); 3- medio (1° ao 3° ano); 4- superior; 5- não se aplica; 9- ignorado\n\n\nidade\nIdade, em anos, da gestante ou puérpera\n10 a 55\n\n\nCS_GESTANT\nMomento gestacional ou puerpério\n1- 1° trimestre; 2- 2° trimestre; 3- 3° trimestre; 4- idade gestacional ignorada; 5- não; 9- ignorado\n\n\nPUERPERA\nSe paciente é puérpera ou parturiente (mulher que pariu recentemente - até 45 dias do parto)\n1- sim; 2- não; 9- ignorado\n\n\nSG_UF\nSigla do estado de residência da gestante ou puérpera\nSigla padronizada pelo IBGE\n\n\nID_MN_RESI\nNome do município de residência da gestante ou puérpera\nNomes padronizados pelo IBGE\n\n\nCO_MUN_RES\nCódigo do município de residência da gestante ou puérpera\nCódigo definido pelo IBGE\n\n\nCS_ZONA\nTipo de zona de residência da gestante ou puérpera\n1- urbana; 2- rural; 3- periurbana; 9- ignorado\n\n\nFEBRE\nSe gestante ou puérpera manifestou sintoma de febre\n1- sim; 2- não; 9- ignorado\n\n\nTOSSE\nSe gestante ou puérpera manifestou sintoma de tosse\n1- sim; 2- não; 9- ignorado\n\n\nGARGANTA\nSe gestante ou puérpera manifestou sintoma de dor de garganta\n1- sim; 2- não; 9- ignorado\n\n\nDISPNEIA\nSe gestante ou puérpera manifestou sintoma de dispneia\n1- sim; 2- não; 9- ignorado\n\n\nDESC_RESP\nSe gestante ou puérpera manifestou sintoma de desconforto respiratório\n1- sim; 2- não; 9- ignorado\n\n\nSATURACAO\nSe gestante ou puérpera manifestou sintoma de saturação\n1- sim; 2- não; 9- ignorado\n\n\nDIARREIA\nSe gestante ou puérpera manifestou sintoma de diarreia\n1- sim; 2- não; 9- ignorado\n\n\nVOMITO\nSe gestante ou puérpera manifestou sintoma de vômito\n1- sim; 2- não; 9- ignorado\n\n\nDOR_ABD\nSe gestante ou puérpera manifestou sintoma de dor abdominal\n1- sim; 2- não; 9- ignorado\n\n\nFADIGA\nSe gestante ou puérpera manifestou sintoma de fadiga\n1- sim; 2- não; 9- ignorado\n\n\nPERD_OLFT\nSe gestante ou puérpera manifestou sintoma de perda de olfato\n1- sim; 2- não; 9- ignorado\n\n\nPERD_PALA\nSe gestante ou puérpera manifestou sintoma de perda de paladar\n1- sim; 2- não; 9- ignorado\n\n\nASMA\nSe gestante ou puérpera tem asma\n1- sim; 2- não; 9- ignorado\n\n\nDIABETES\nSe gestante ou puérpera tem diabetes mellitus\n1- sim; 2- não; 9- ignorado\n\n\nNEUROLOGIC\nSe gestante ou puérpera tem doença neurológica\n1- sim; 2- não; 9- ignorado\n\n\nPNEUMOPATI\nSe gestante ou puérpera tem outra pneumopatia crônica\n1- sim; 2- não; 9- ignorado\n\n\nIMUNODEPRE\nSe gestante ou puérpera tem imunodeficiência ou imunodepressão (diminuição da função do sistema imunológico)\n1- sim; 2- não; 9- ignorado\n\n\nRENAL\nSe gestante ou puérpera tem doença renal crônica\n1- sim; 2- não; 9- ignorado\n\n\nOBESIDADE\nSe gestante ou puérpera tem obesidade\n1- sim; 2- não; 9- ignorado\n\n\nCARDIOPATI\nSe gestante ou puérpera tem doença cardiovascular crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEMATOLOGI\nSe gestante ou puérpera tem doença hematológica crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEPATICA\nSe gestante ou puérpera tem doença hepática crônica\n1- sim; 2- não; 9- ignorado\n\n\nVACINA_COV\nSe gestante ou puérpera recebeu vacina COVID-19\n1- sim; 2- não; 9- ignorado\n\n\nDOSE_1_COV\nData em que gestante ou puérpera recebeu a 1ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nDOSE_2_COV\nData em que gestante ou puérpera recebeu a 2ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nFAB_COV_1\nFabricante da vacina que a gestante ou puérpera recebeu na 1ª dose\n\n\n\nFAB_COV_2\nFabricante da vacina que a gestante ou puérpera recebeu na 2ª dose\n\n\n\nSUPORT_VEN\nSe gestante ou puérpera precisou de ventilação mecânica; se sim, se foi invasiva ou não\n1- sim, invasivo; 2- sim, não invasivo; 3- não; 9- ignorado\n\n\nUTI\nSe gestante ou puérpera foi internada na UTI\n1- sim; 2- não; 9- ignorado\n\n\nDT_ENTUTI\nData de entrada da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nDT_SAIDUTI\nData de saída da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nEVOLUCAO\nEvolução do caso da gestante ou puérpera\n1- cura; 2- óbito; 3- óbito por outras causas; 9- ignorado\n\n\n\n\n\n\n1.1.2 INDICADORES OBSTÉTRICOS\nConsiste em um conjunto de dados com ??? observações sobre indicadores obstétricos a nível municipal do ano de 2019, sendo os seguintes indicadores:\n\nPrematuridade: Refere-se ao parto que ocorre em idade gestacional inferior a 37 semanas.\nGestação Múltipla: Diz respeito a uma gestação que gera dois ou mais fetos simultaneamente.\nParto Cesárea: Trata-se do tipo de parto em que a extração do feto é realizada por meio de intervenção cirúrgica.\nConsultas de Pré-Natal: Consiste no acompanhamento médico realizado pela gestante durante a gravidez. O ideal é que ocorram pelo menos 7 consultas médicas durante o período de pré-natal.\nApgar: É uma escala proposta em 1953 pela médica Virgínia Apgar, que atribui pontuações a 5 sinais do recém-nascido: frequência cardíaca, respiração, tônus muscular, irritabilidade reflexa e cor da pele. A escala varia de 0 a 10, e escores abaixo de 7 requerem atenção dos profissionais obstetras. Essa avaliação é feita no primeiro minuto após o nascimento (Apgar do primeiro minuto) e novamente após 5 minutos (Apgar do quinto minuto).\nAnomalia congênita: Refere-se a alterações estruturais ou funcionais que podem ser causadas por diversos fatores e têm origem durante a vida intrauterina.\n\nA obtenção dos indicadores utilizados foi realizada considerando as frequências relativas ao município de residência dos nascidos vivos no ano da coleta. Os indicadores obstétricos são calculados como percentuais válidos, ou seja, os dados faltantes da variável em questão são desconsiderados. Por exemplo:\n\nPercentual válido de prematuridade = (número de prematuros / número de nascidos vivos com informação de prematuridade) x 100.\n\nO Dicionário para esse conjunto de dados é apresentado em Tabela 1.2 .\n\n\nTabela 1.2: Dicionário das variáveis da base de dados de Indicadores Obstétricos.\n\n\n\n\n\n\nVariável\nDescrição\n\n\n\n\nuf\nUnidade Federativa (Estados + Distrito Federal)\n\n\nmunicipio\nMunicípio\n\n\ncodigo\nCódigo do município do IBGE\n\n\nnascidos_vivos\nNúmero de nascidos vivos\n\n\nporc_premat\nPercentual válido de prematuridade\n\n\nporc_gesta_multipla\nPercentual válido de gestações múltiplas\n\n\nporc_cesarea\nPercentual válido de partos cesáreas\n\n\nporc_0_consulta\nPercentual válido de nascidos com nenhuma consulta de pré-natal\n\n\nporc_7mais_consulta\nPercentual válido de nascidos com 7 ou mais consultas de pré-natal\n\n\nporc_apgar1_menor_7\nPercentual válido de Apgar de 1º minuto menor que 7\n\n\nporc_apgar5_menor_7\nPercentual válido de Apgar de 5º minuto menor que 7\n\n\nporc_anomalia\nPercentual válido de nascidos com anomalia congênita\n\n\nporc_peso_menor_2500\nPercentual válido de nascidos com peso menor que 2.500 gramas\n\n\nporc_fem\nPercentual válido de nascidos do sexo feminino"
  },
  {
    "objectID": "manipulacao_dados.html",
    "href": "manipulacao_dados.html",
    "title": "2  Manipulação de dados",
    "section": "",
    "text": "Neste capítulo falaremos alguns princípios básicos sobre manipulação de dados. Iremos trabalhar em um cenário mais próximo da realidade possível, ou seja, iremos trabalhar em cima de uma base de dados real. O objetivo é manipular a base e torná-la pronta para ser usada nos capítulos seguintes. Será mostrado desde como importar a base até como criar novas variáveis que poderão ser utilizadas em análises. Não será possível cobrir todo o ramo de manipulação em um só capítulo, mas iremos trabalhar com o máximo de ferramentas possíveis. Pacotes ou funções que não forem utilizadas aqui, mas que são interessantes serão mencionados ao longo do capítulo junto a links que contenham explicações de como utilizá-las. Vale ressaltar que estamos em um cenário mais básico e introdutório. Vamos começar.\n\n2.0.1 Importação de dados\nUm dos caminhos mais simples para importar dados no R é utilizando a função read.table(). Está função é simples pois ja vem instalada com o R, faz parte do pacote base utils, e importa arquivos nos formatos cvs e txt.\nA utilização do pacote é bem simples, não preciso carregá-lo na memória usando library().\n\ndados1 &lt;- read.table(file = \"dados.csv\", sep = \";\")\ndados2 &lt;- read.table(file = \"caminho-para-o-arquivo/dados.csv\", sep = \";\")\n\nObserve que na função temos os argumentos file e sep. O file indica o nome do arquivo que será importado e sep indica qual o símbolo separador de colunas, que neste caso é a virgula. Note também que usamos dois exemplos, o primeiro considera que o seu arquivo está no diretório de trabalho (quando criamos o projeto e colocamos os arquivos de dados na pasta criada pelo projeto), não sendo necessário especificar o caminho até do arquivo. O outro exemplo mostra como especificar o local do seu arquivo. A função possui mais argumentos que você pode explorar usando o help, mas no geral, esses dois são os mais utilizados.\n\n2.0.1.1 Extensão .txt ou .csv\nCaso esteja trabalhando com arquivos do tipo cvs ou txt o pacote readr irá servir muito bem. As funções deste pacote são bem rapidas e algumas delas são focadas em tranformar arquivos simples em data.frame. Aglumas funções do pacote são\n\nread_cvs(): para arquivos delimitados por vírgulas.\nread_cvs2(): para arquivos delimitados por ponto e vírgula.\nread_tsv(): para arquivos delimitados por tabulações.\nread_delim(): para aquivos com qualquer delimitador.\nread_fwf(): para arquivos compactos que devem ter a largura de cada coluna especificada.\nread_table(): para arquivos de texto tabulas com colunas separas por espaço.\n\nCaso esta seja a primeira vez que você ira utilizar este pacote, será necessário instalá-lo em seu computar. Você pode fazer isso utilizando a função install.packages(\"readr\") e é claro, antes de usar qualquer pacote que não faça parte do R base, você deve carregá-lo. Como exemplo, consideramos um arquivo chamado dados1 que queremos importar para o R.\n\nlibrary(readr)\ndados_csv &lt;- read_csv(file = \"caminho-para-o-arquivo/dados1.csv\")\ndados_txt &lt;- read_delim(file = \"caminho-para-o-arquivo/dados1.txt\", delim = \" \")\n\nApesar dos argumentos deste pacote serem semelhantes aos da função read.table(), devemos nos atentar a algumas diferenças. Aqui é o argumento delim que indica qual o separador das colunas no arquivo texto.\nVale ressaltar que para cada função read_, existe umas respectiva função write_ para exportar o arquivo no formato de interesse. Como exemplo, queremos salvar a base de dados mtcars na pasta do meu computador com o nome cars:\n\nwrite_csv(x = mtcars, path = \"cars.csv\")\nwrite_delim(x = mtcars, delim = \" \", path = \"cars.txt\")\n\n\n\n2.0.1.2 Arquivos em Excel\nArquivos em formato xlsx são muito utilizados, porém o R não possui uma função nativa para importar este tipo de arquivo. Existem diversos pacotes para importar dados neste e formato e os principais são redxl, xlsx, XLConnect e tydixl. Apesar destes pacotes terem objetivos semelhantes, cada um tem suas peculiaridades, então aconselhamos estudar cada um desses pacotes e assim decidir qual melhor atende às suas necessidades. Aqui vamos mostrar apenas o pacote readxl, pois é um dos mais facéis e diretos de se utilizar. Este pacote serve para importar e ler planilhas do Excel nos formatos xlsx ou xls. A seguir estão listadas algumas funções para importação e leitura de dados:\n\nread_excel(): esta função detecta automaticamente a extensão do arquivo, e importa arquivos do tipo xsl e xlsx.\nread_xsl(): importa arquivos no formato xsl.\nread_xlsx(): importa arquivos no formato xlsx.\n\nNovamente, é necessário à instalação e carregamento do pacote caso não o tenha em seu computador. Para exemplicar consideramos um arquivo chamado dados2 que queremos importar para o R.\n\nlibrary(readxl)\ndados_excel1 &lt;- read_excel(path = \"dados2.xls\")\ndados_excelx1 &lt;- read_excel(path = \"dados2.xlsx\")\n\nPor meio da função read_excel conseguimos importar tanto um arquivo no formato xls quanto no formato xlsx.\nPodemos também exportar um arquivo em excel (.xls e .xlsx) ao considerar a função write_xlsx do pacote writexl. Suponha que temos o interesse em salvar a base de dados dados em excel na pasta do computador (exportar) com o nome de dados_correto:\n\nlibrary(writexl)\nwrite_xlsx(dados, \"dados_correto.xlsx\")\n\n\n\n\n2.0.2 Análise de consistência e tratamento de dados\nO tratamento dos dados toma muitas vezes a maior parte do tempo de uma análise estatística.\nA análise de consistência consiste em realizar uma primeira análise dos dados com o intuito de encontrar inconsistências. São exemplos de inconsistências:\n\nboas práticas para nome das variáveis.\ncomo erros de digitação;\nindivíduos imputados mais de uma vez na planilha de dados de maneira errada;\nidentificar casos missings e avaliar se a observação está ausente de maneira correta ou não;\nidentificar as categorias de variáveis qualitativas.\n\nA partir daqui iremos trabalhar com a nossa base de dados de COVID-19 em gestantes e puérperas.\nImportando os dados\nComo já aprendemos a importar os dados, vamos direto ao ponto. Nos dados estão no forma rds que não foi mencionado anteriormente, mas o pacote readr tem uma função para importar este tipo de arquivo.\n\ndados &lt;- readr::read_rds(\"dados/dados_covid[SUJO].rds\")\nknitr::kable(head(dados))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDT_NOTIFIC\nDT_SIN_PRI\nDT_NASC\nDT_INTERNA\nSEM_PRI\nSG_UF\nID_MN_RESI\nCO_MUN_RES\nCS_ZONA\nCS_RACA\nCS_ESCOL_N\nidade\nCS_GESTANT\nPUERPERA\nFEBRE\nTOSSE\nGARGANTA\nDISPNEIA\nDESC_RESP\nSATURACAO\nDIARREIA\nVOMITO\nFADIGA\nPERD_OLFT\nPERD_PALA\nDOR_ABD\nCARDIOPATI\nHEMATOLOGI\nHEPATICA\nASMA\nDIABETES\nNEUROLOGIC\nPNEUMOPATI\nIMUNODEPRE\nRENAL\nOBESIDADE\nVACINA_COV\nDOSE_1_COV\nDOSE_2_COV\nFAB_COV_1\nFAB_COV_2\nDT_ENTUTI\nDT_SAIDUTI\nUTI\nSUPORT_VEN\nEVOLUCAO\n\n\n\n\n15/05/2020\n06/05/2020\n03/06/2003\n15/05/2020\n19\nCE\nMORRINHOS\n230890\nNA\n4\nNA\n16\n2\nNA\n1\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n18/05/2020\n10/05/2020\n07/07/1996\n15/05/2020\n20\nPR\nCURITIBA\n410690\n1\n1\n2\n23\n2\n2\n2\n2\n2\n1\n2\n2\n1\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n30/04/2020\n20/04/2020\n26/03/1996\n24/04/2020\n17\nSP\nSAO CAETANO DO SUL\n354880\n1\n9\n9\n24\n9\n1\n1\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n11/05/2020\n04/05/2020\n02/06/1986\n09/05/2020\n19\nPA\nMARABA\n150420\n1\n4\n4\n33\n5\n1\n1\n1\n2\n2\n1\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n01/07/2020\n12/06/2020\n11/12/1996\n30/06/2020\n24\nDF\nSANTA MARIA\n530150\n1\n9\nNA\n23\n5\n1\n2\n2\n2\n2\n2\n2\n1\n2\nNA\n1\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n09/06/2020\n09/06/2020\n09/12/1984\n09/06/2020\n24\nRO\nPORTO VELHO\n110020\n1\n4\n2\n35\n3\n2\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n\n\n\n\n2.0.2.1 Tratamento da base de dados\nInicialmente, vamos verificar os nomes das variáveis na base de dados por meio da função names. Note que os nomes estão, de certa forma, padronizados. Todos maíusculos (com exceção de “idade”), separados por “_”. Este ainda não é o cenário ideal para trabalharmos, mas poderia ser pior, contendo maiúsculas, espaços e acentos. Utilizar os dados com essas características não impossibilita as futuras análises, mas pode atrapalhar quando precisamos selecionar algumas dessas variáveis.\n\nnames(dados)\n\n [1] \"DT_NOTIFIC\" \"DT_SIN_PRI\" \"DT_NASC\"    \"DT_INTERNA\" \"SEM_PRI\"   \n [6] \"SG_UF\"      \"ID_MN_RESI\" \"CO_MUN_RES\" \"CS_ZONA\"    \"CS_RACA\"   \n[11] \"CS_ESCOL_N\" \"idade\"      \"CS_GESTANT\" \"PUERPERA\"   \"FEBRE\"     \n[16] \"TOSSE\"      \"GARGANTA\"   \"DISPNEIA\"   \"DESC_RESP\"  \"SATURACAO\" \n[21] \"DIARREIA\"   \"VOMITO\"     \"FADIGA\"     \"PERD_OLFT\"  \"PERD_PALA\" \n[26] \"DOR_ABD\"    \"CARDIOPATI\" \"HEMATOLOGI\" \"HEPATICA\"   \"ASMA\"      \n[31] \"DIABETES\"   \"NEUROLOGIC\" \"PNEUMOPATI\" \"IMUNODEPRE\" \"RENAL\"     \n[36] \"OBESIDADE\"  \"VACINA_COV\" \"DOSE_1_COV\" \"DOSE_2_COV\" \"FAB_COV_1\" \n[41] \"FAB_COV_2\"  \"DT_ENTUTI\"  \"DT_SAIDUTI\" \"UTI\"        \"SUPORT_VEN\"\n[46] \"EVOLUCAO\"  \n\n\numa boa prática consiste em padronizar os nomes das variáveis, até para facilitar a lembrança deles. Para isso, utilizaremos o pacote janitor para a arrumação da base de dados. Usamos a função clean_names() para primeiro ajuste dos nomes das variáveis.\n\ndados &lt;- janitor::clean_names(dados) \nnames(dados)\n\n [1] \"dt_notific\" \"dt_sin_pri\" \"dt_nasc\"    \"dt_interna\" \"sem_pri\"   \n [6] \"sg_uf\"      \"id_mn_resi\" \"co_mun_res\" \"cs_zona\"    \"cs_raca\"   \n[11] \"cs_escol_n\" \"idade\"      \"cs_gestant\" \"puerpera\"   \"febre\"     \n[16] \"tosse\"      \"garganta\"   \"dispneia\"   \"desc_resp\"  \"saturacao\" \n[21] \"diarreia\"   \"vomito\"     \"fadiga\"     \"perd_olft\"  \"perd_pala\" \n[26] \"dor_abd\"    \"cardiopati\" \"hematologi\" \"hepatica\"   \"asma\"      \n[31] \"diabetes\"   \"neurologic\" \"pneumopati\" \"imunodepre\" \"renal\"     \n[36] \"obesidade\"  \"vacina_cov\" \"dose_1_cov\" \"dose_2_cov\" \"fab_cov_1\" \n[41] \"fab_cov_2\"  \"dt_entuti\"  \"dt_saiduti\" \"uti\"        \"suport_ven\"\n[46] \"evolucao\"  \n\n\nVeja que ele deixou todos os nomes minúsculos. Neste caso não foi feito, mas a função substitui o espaço por “_” e tira acentos. Isso ajuda a evitar problemas futuros em algumas análises que não lidam muito bem com acentos e espaços nos nomes das variáveis.\nOutro problema comum é a presença de linhas e colunas vazias. Na base de dados em questão, não há linhas nem colunas em branco, como pode ser visto na saída abaixo.\n\njanitor::remove_empty(dados,\"rows\")\njanitor::remove_empty(dados,\"cols\")\n\n\n\n2.0.2.2 \nIdentificando casos duplicados\nOutra boa prática é identificar casos duplicados, isto é, identificar se há casos erroneamente repetidos. O ideal é utilizar variável chave do seu banco de dados, ou seja, aquela em que cada observação é única. Por exemplo, em uma base de dados de funcionários de uma empresa, uma variável chave poderia ser o CPF. Uma variável chave também pode ser a combinação de variáveis, gerando assim observações únicas. Para identificar casos duplicados, usamos a função get_dupes do pacote janitor. Em nosso banco de dados não tempos uma varíavel chave, então não vamos especificá-la na função, assim a função irá procurar observações repetidas considerando todas as variáveis, ou seja, linhas repetidas.\n\njanitor::get_dupes(dados)\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: dt_notific, dt_sin_pri, dt_nasc, dt_interna, sem_pri, sg_uf, id_mn_resi, co_mun_res, cs_zona, ... and 37 other variables\n\n\n [1] dt_notific dt_sin_pri dt_nasc    dt_interna sem_pri    sg_uf     \n [7] id_mn_resi co_mun_res cs_zona    cs_raca    cs_escol_n idade     \n[13] cs_gestant puerpera   febre      tosse      garganta   dispneia  \n[19] desc_resp  saturacao  diarreia   vomito     fadiga     perd_olft \n[25] perd_pala  dor_abd    cardiopati hematologi hepatica   asma      \n[31] diabetes   neurologic pneumopati imunodepre renal      obesidade \n[37] vacina_cov dose_1_cov dose_2_cov fab_cov_1  fab_cov_2  dt_entuti \n[43] dt_saiduti uti        suport_ven evolucao   dupe_count\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n\nEm nosso caso, não temos casos duplicados. Caso tivesse, seria necessário remover as linhas duplicadas. Isto pode ser feito com o uso da função distinct do pacote dplyr.\n\n\n\n2.0.3 Identificar problemas nas variáveis da base de dados\nOutra etapa importante na análise de consistência é identificar o tipo de variável e ver se o R está interpretando corretamente o tipo de cada variável.\nTemos na nossa base de dados variáveis de data, além de variáveis qualitativas e quantitativas (veja o dicionário das variáveis na em: refenciar parte). Assim, precisamos entender se o R realmente entendeu todas as variáveis da maneira correta. Uma maneira de identificar isso e também de ver algumas descritivas das variáveis que nos auxiliam a ver possíveis inconsistências na base de dados é a a função glimpse do pacote dplyr. A função skim do pacote skimr também pode ajudar nisso.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;chr&gt; \"15/05/2020\", \"18/05/2020\", \"30/04/2020\", \"11/05/2020\", \"01…\n$ dt_sin_pri &lt;chr&gt; \"06/05/2020\", \"10/05/2020\", \"20/04/2020\", \"04/05/2020\", \"12…\n$ dt_nasc    &lt;chr&gt; \"03/06/2003\", \"07/07/1996\", \"26/03/1996\", \"02/06/1986\", \"11…\n$ dt_interna &lt;chr&gt; \"15/05/2020\", \"15/05/2020\", \"24/04/2020\", \"09/05/2020\", \"30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;int&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;int&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;int&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;int&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;int&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;int&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;int&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;int&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;int&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;int&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;int&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"30/06/2020\", \"\", \"12/07/2020\",…\n$ dt_saiduti &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"29/07/2020\", \"\", \"\", \"\", \"\", \"…\n$ uti        &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;int&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nNo R, as variáveis qualititativas são nomeadas “factor”, as variáveis quantitativas são nomeadas “numeric” e as variáveis de data são “date”. Note que na importação dos dados o R não entendeu corretamente os tipos de variáveis. Mas vamos corrigir isso no que segue.\nComeçando pela data, vamos rodar o seguinte código:\n\ndados$dt_notific  &lt;- as.Date(dados$dt_notific, format = \"%d/%m/%Y\")\ndados$dt_sin_pri  &lt;- as.Date(dados$dt_sin_pri, format = \"%d/%m/%Y\")\ndados$dt_nasc  &lt;- as.Date(dados$dt_nasc, format = \"%d/%m/%Y\")\ndados$dt_interna  &lt;- as.Date(dados$dt_interna, format = \"%d/%m/%Y\")\ndados$dt_entuti  &lt;- as.Date(dados$dt_entuti, format = \"%d/%m/%Y\")\ndados$dt_saiduti  &lt;- as.Date(dados$dt_saiduti, format = \"%d/%m/%Y\")\n\nA função as.Date informa para o R que a variável indicada é de data. O argumento format indica o formato que está a data, nesse caso, “dia/mês/ano”. Aqui é possível verificar todos os formatos de datas da função. Vamos ver como ficou:\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;int&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;int&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;int&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;int&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;int&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;int&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;int&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;int&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;int&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;int&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;int&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;int&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nAgora vamos lidar com as variáveis qualitativas. Veja que “cs_zona” foi identificada como int. Isso acontece porque ela foi tabulada como número, como posteriormente variáveis deste tipo serão recodificadas de acordo com o dicionário, precisamos tratá-la como fator. Já as demais variáveis qualitativas estão identificadas como numeric, dbl ou chacacter pois na tabulação suas categorias estão codificadas com números ou textos. Para então dizer ao R o verdadeiro tipo dessas variáveis, vamos utilizar os seguintes comandos:\n\ndados$cs_raca &lt;- as.factor(dados$cs_raca)\ndados$cs_escol_n &lt;- as.factor(dados$cs_escol_n)\ndados$cs_gestant &lt;- as.factor(dados$cs_gestant)\ndados$puerpera &lt;- as.factor(dados$puerpera)\ndados$cs_zona &lt;- as.factor(dados$cs_zona)\ndados$febre &lt;- as.factor(dados$febre)\ndados$tosse &lt;- as.factor(dados$tosse)\ndados$suport_ven &lt;- as.factor(dados$suport_ven)\ndados$uti &lt;- as.factor(dados$uti)\ndados$evolucao &lt;- as.factor(dados$evolucao)\n\nUma forma um pouco mais eficiente de fazer isso é selecionar as variáveis por meio de um vetor, por exemplo, quero que as variáveis da coluna 10 até a coluna 20 sejam fatores. Podemos fazer isso com a ajuda a função lapply. Essa função, em resumo, nos possibilita aplicar uma função em uma lista de elementos e retorna uma lista de mesmo tamanho em que o resultado é a aplicação desta função a cada elemento da lista. Neste caso, aplicamos a função as.factor nas colunas selecionadas (lista de elementos). Veja como é feito.\n\n\ndados[,c(17:37)] &lt;- lapply(dados[,c(17:37)], as.factor)\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;fct&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;fct&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;fct&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;fct&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;fct&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;fct&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;fct&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;fct&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;fct&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nÓtimo! Corrigimos as inconsistências das variáveis qualitativas. Mas outra questão surge: como faço para usar um rótulo nos números codificados nas categorias das variáveis qualitativas? Para o grupo, por exemplo, ao invés de aparecer 1 quero que apareça “sim”. Para isso, vamos utilizar o pacote forcats que lida com variáveis qualitativas (categóricas). Para renomear as categorias das variáveis, vamos usar a função fct_recode desse pacote:\n\ndados$cs_raca &lt;- forcats::fct_recode(dados$cs_raca,\n                                   branca = \"1\",\n                                   preta = \"2\",\n                                   amarela = \"3\",\n                                   parda = \"4\",\n                                   indigena = \"5\",\n                                   ignorado = \"9\")\n\ndados$cs_escol_n &lt;- forcats::fct_recode(dados$cs_escol_n,\n                                     \"sem escola\"  = \"0\",\n                                     fund1 = \"1\",\n                                     fund2 = \"2\",\n                                     medio = \"3\",\n                                     superior = \"4\",\n                                     ignorado = \"9\")\n\ndados$cs_gestant &lt;- forcats::fct_recode(dados$cs_gestant,\n                                     \"1tri\" = \"1\",\n                                     \"2tri\" = \"2\",\n                                     \"3tri\" = \"3\",\n                                     IG_ig = \"4\",\n                                     nao = \"5\",\n                                     ignorado = \"9\")\n\ndados$puerpera &lt;- forcats::fct_recode(dados$puerpera,\n                                      sim = \"1\",\n                                      nao = \"2\",\n                                      ignorado = \"9\")\n\ndados$cs_zona &lt;- forcats::fct_recode(dados$cs_zona,\n                                  urbana = \"1\",\n                                  rural = \"2\",\n                                  periurbana = \"3\",\n                                  ignorado = \"9\")\n\ndados$febre &lt;- forcats::fct_recode(dados$febre,\n                                   sim = \"1\",\n                                   nao = \"2\",\n                                   ignorado = \"9\")\n\ndados$suport_ven &lt;-forcats::fct_recode(dados$suport_ven,\n                                       \"sim, invasivo\" = \"1\",\n                                       \"sim, nao invasivo\" = \"2\",\n                                       nao = \"3\",\n                                       ignorado = \"9\")\n\ndados$uti &lt;-forcats::fct_recode(dados$uti,\n                                       sim = \"1\",\n                                       nao = \"2\",\n                                       ignorado = \"9\")\n\ndados$evolucao &lt;-forcats::fct_recode(dados$evolucao,\n                                       cura = \"1\",\n                                       obito = \"2\",\n                                       \"obito por outras causas\" = \"3\",\n                                       ignorado = \"9\")\n\nEste tramanto foi feito para todas as variáveis qualitativas da base, mas por conta do tamanho do código, omitimos algumas da saída.\nFinalmente chegamos nas variáveis quantitativas. Uma forma de identificar problemas em variáveis quantitativas é avaliar os valores mínimo e máximo de cada variável e ver se tem algum valor impossível para a mesma. Em nosso caso podemos verificar a variável idade. Seria meio estranho encontrar alguém com valores extremamente altos ou negativos, concorda?! A função summary pode ser uma opção boa aqui, ela nos formece algumas medidas descritivas como, media, mínimo, máximo, entre outros.\n\nsummary(dados$idade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   25.00   30.00   30.25   35.00   55.00       9 \n\n\nAparentemente nossa variável esta dentro do esperado, sem valores inesperados.\n\n2.0.3.1 Transformação de dados\nÉ possível modificar ou criar novas variáveis na base de dados por meio da função mutate do pacote dplyr, você pode veificar melhor essa função clicando aqui. Também podemos criar categorias com base em alguma condição por meio da função case_when também do pacote dplyr, veja melhor aqui. Para ficar mais claro, vamos a um exemplo combinando as duas funções. Vamos criar a variável “faixa_et”, onde as observações serão as faixas etárias. São essas: “&lt;20”, “20-34” e “&gt;=”. Veja como faz:\n\ndados &lt;- dados |&gt; \n  mutate(faixa_et = case_when(\n    idade &lt; 20 ~ \"&lt;20\",\n    idade &gt;= 20 & idade &lt; 34 ~ \"20-34\",\n    idade &gt;= 34 ~ \"&gt;=34\"\n  ))\n\ntable(dados$faixa_et)#table nos mostra as observações da quela variável e a sua frequência. \n\n\n  &lt;20  &gt;=34 20-34 \n  714  3862  6938 \n\n\nAqui fizemos a utilização da função “pipe” |&gt; que agora está no pacote base do R, mas que antes era necessário carregá-la por meio de pacotes. Essa função é de extrema importância, facilita a programção no R de uma forma inimaginável. É válido dedicar um pouco de seu tempo para entender melhor essa função. Separamos alguns links que pode te ajudar a entender melhor e você pode acessá-los clickando aqui, aqui ou aqui. Como foi mencionado acima, a função foi adicionada ao R base há pouco tempo, então esses links se referem ao pipe “antigo”, mas fique tranquilo, a função é a mesma. Para resumir, o pipe pega a saída de uma função e a passa para outra função como um argumento. Isso nos permite vincular uma sequência de etapas de análise.\n\n\n2.0.3.2 Manipulação de datas\nAlgo interessante também é trabalhar com a varíavel de datas. Podemos calulcar a diferença entre duas datas no R de forma bem simples por meio da função difftime do pacote base do R. Para exemplificar vamos criar a variável “dias_uti” que vai ser ser quantos dias a pessoa ficou internada na uti. Vamos fazer isso calculando a diferença entre a data de saída e a data de entrada na uti e queremos o resultado em dias.\n\ndados$dias_uti &lt;- difftime(dados$dt_saiduti, dados$dt_entuti, units = \"days\")\nglimpse(dados)\n\nRows: 11,523\nColumns: 48\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nNote que não utilizamos a função mutate para criar está nova variável, utilizamos apenas o $ para representar a variável e atribuímos a função. Assim, o R já entende como uma variável.\n\n\n2.0.3.3 Manipulação de dados\nJá temos a nossa base de dados devidamente tratada para prosseguir com a análise descritiva, mas quando falamos de manipulação de dados, um leque de possibilidades aparece. Em diversos cenários precisamos filtrar observações, reordená-las, selecionar variáveis específicas, entre outras coisas. Não poderíamos deixar de mencionar o poderoso tidyverse. O tidyverse é um pacote que contém um coleção de outros pacotes que são utilizados para manipulação, exploração e visualização de dados e que compartilham uma filosofia de design bem parecida, por isso de forma combinada permitem que você consiga fazer inúmeros trabalhos. Os pacotes que fazem parte desse universo são: dplyr, tidyr, ggplot2, forcats, purrr, stringr, tibble e readr. Anteriormente já trabalhamos com alguns destes pacotes, mas agora é válido aprofundarmos um pouco mais em alguns deles. Aqui você irá acessar o site do tidyverse onde podera navegar por cada pacote e aprender mais sobre suas utilidades e aqui você irá acessar um post escrito pelo Laboratório de Data Scinence - UFES (daslab) que contem diversos exemplos práticos de uso de todos os pacotes do universo tidyverse. Neste capítulo iremos trabalhar com algumas funções específicas.\n\n2.0.3.3.1 Pacote dplyr\nO dplyr é extremamente útil e nos ajuda a resolver os desafios mais comuns de manipulação de dados.\nSuas principais funções são:\n\nfilter() - filtra linhas;\ngroup_by() - agrupa pela(s) variável(is) no argumento. Função muito útil quando usada a funçaõ summurise.\nsummarise() - reduz vários valores a um único resumo.\nselect() - seleciona colunas;\narrange() - ordena a base;\nmutate() - cria/modifica colunas.\n\nJá utilizamos algumas funções do pacote, vamos falar sobre outras. Como já avançamos um pouco sobre a utilização de funções, vamos combinar algumas funções, o que geralmente é feito no dia a dia.\n\n#criando um novo banco de dados selecionando 3 variáveis\ndados_tratamento &lt;- dados |&gt; \n  select(sg_uf, cs_zona, idade)\n\nglimpse(dados_tratamento)\n\nRows: 11,523\nColumns: 3\n$ sg_uf   &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\", \"M…\n$ cs_zona &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana, ur…\n$ idade   &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26, 20…\n\n\nAqui criamos a base “dados_tratamento” onde apenas selecionamos algumas colunas da base de dados inicial com a função select.\n\ndados_tratamento2 &lt;- dados_tratamento |&gt; \n  filter(cs_zona == \"urbana\") |&gt; \n  group_by(sg_uf) |&gt; \n  summarise(media = mean(idade, na.rm = TRUE)) |&gt; \n  arrange(desc(media))\n\nknitr::kable(head(dados_tratamento2))\n\n\n\n\nsg_uf\nmedia\n\n\n\n\nAP\n36.66667\n\n\nBA\n31.04152\n\n\nRR\n31.00000\n\n\nSP\n30.94237\n\n\nRJ\n30.93570\n\n\nMG\n30.93257\n\n\n\n\n\nVamos entender o código acima. Primeiro acessamos a base “dados_tratamento” e com função filter selecionamos apenas as observações “urbana”. Após isso utilizamos a função group_by para agrupar nossas observações pela variável “sg_uf” e por últimos, combinamos com a função summarise para criar a variável “media” que será a media da variável idade. Note que nesta função utilizamos o argumento na.rm - TRUE. Este argumento serve para indicar para a função se ela deve ou não remover valores NA's do cálculo, o default é FALSE. Como não é possível calcular a média de valores ausentes e temos variáveis ausentes, foi necessário utilizar este argumento. Caso contrário, Estados com valores faltantes ficariam com NA. Por último, utilizamos a função arrange para ordernar os dados em em ordem descrente pela variavel media. Uma dica para tentar entender melhor o funcionamento das funções é tentar refazer o código utilizando uma função de cada vez e ir vendo como fica. Então, em poucas linhas de códigos conseguimos criar uma base com a idade média dos Estados considerando apenas zonas urbanas, legal né?\n\n\n2.0.3.3.2 Pacote stringr\nUm desafio muito grande na manipulação de dados é extrair informações de caracteres. Em resumo caracteres são letras, símbolos, sinais, números que representem algo escrito, etc.. Essa sequência de caracteres formam o que chamamos de string. Diversas vezes encontramos variáveis com categorias não padronizadas, como, por exemplo, uma variável contendo “São Paulo”, “sao paulo” e “sp”. Apesar de representarem o mesmo estado, elas são diferentes. Nesse sentido, uma parte muito importante no tratamento de dados é “lapidar” esse conjunto de caracteres para que seja possível usá-los nas análises. Essa é a introdução do post do daslab onde é passado de uma maneira muito prática como trabalhar com strings utilizando o pacote stringr, la você vai aprender também sobre expressões regulares, que com certeza serão úteis em vários momentos da sua carreira. Link do post. Como as variáveis de texto do nosso banco de dados já estão bem padronizadas não será necessário realizar nenhum tratamento, mas por ser um pacote de extrema importância e que não havia sido mencionado ainda, deixamos ele aqui para que você possa se aprofundar mais. Como em nossa base dados as variáveis de texto estão padrozinados, não será necessário realizar nenhum tratamento.\n\n\n\n2.0.3.4 Manipulando o formato da base de dados\nEm certos casos é necessário mudar o formato das bases de dados, fazer com que colunas se tornem linhas vice-versa. Vamos utilizar a base “dados_tratamento”. Veja que ela está no formato long, em que as avaliações do mesmo indivíduo (variável de identificação de indivíduo é “registro”) estão nas linhas. Queremos que as zonas fiquem nas colunas, com as três colunas (vamos tirar valores ignorados): urbana, rural e periurbana, ou seja, queremos o formato wide. Um pacote do R que pode nos auxiliar a transformar formato long em wide e vice-versa é o tidyr. A função que usaremos é spread, como segue:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\ndados_formato &lt;- dados_tratamento |&gt; \n  filter(!is.na(cs_zona) & cs_zona != \"ignorado\") |&gt; \n  mutate(id = row_number())\n\nknitr::kable(head(dados_formato))\n\n\n\n\nsg_uf\ncs_zona\nidade\nid\n\n\n\n\nPR\nurbana\n23\n1\n\n\nSP\nurbana\n24\n2\n\n\nPA\nurbana\n33\n3\n\n\nDF\nurbana\n23\n4\n\n\nRO\nurbana\n35\n5\n\n\nPI\nurbana\n31\n6\n\n\n\n\n\nFizemos pequenas alterações na base de dados. Primeiro realizamos um filtro para retirarmos valores faltantes da variável “cs_zona”, pois essa passará a\n\ndados_formato2 &lt;- dados_formato |&gt; \n  pivot_wider(names_from = cs_zona, values_from = idade)  \n\nknitr::kable(head(dados_formato2))\n\n\n\n\nsg_uf\nid\nurbana\nrural\nperiurbana\n\n\n\n\nPR\n1\n23\nNA\nNA\n\n\nSP\n2\n24\nNA\nNA\n\n\nPA\n3\n33\nNA\nNA\n\n\nDF\n4\n23\nNA\nNA\n\n\nRO\n5\n35\nNA\nNA\n\n\nPI\n6\n31\nNA\nNA\n\n\n\n\n\n\ndados_formato3 &lt;- dados_formato2 |&gt; \n  pivot_longer(cols = c(\"urbana\",   \"rural\",    \"periurbana\"), names_to = \"cs_zona\", values_to = \"idade\")\n\nknitr::kable(head(dados_formato3))\n\n\n\n\nsg_uf\nid\ncs_zona\nidade\n\n\n\n\nPR\n1\nurbana\n23\n\n\nPR\n1\nrural\nNA\n\n\nPR\n1\nperiurbana\nNA\n\n\nSP\n2\nurbana\n24\n\n\nSP\n2\nrural\nNA\n\n\nSP\n2\nperiurbana\nNA\n\n\n\n\n\n\n\n2.0.3.5 Combinando bases de dados\nQuando estamos trabalhando com dados, nem sempre uma única base irá conter todas as informações que precisamos, na verdade, isso é mais comum do que se possa imaginar. Assim, saber juntar duas bases de dados é indispensável. Vamos começar então falando sobre chave primária. Em resumo, chave primária se refere a um ou mais campos, onde combinados (no caso de mais de uma chave primária), não se repete na mesma tabela. Em outras palavras, uma chave primária no meu banco dados seria uma variável onde as observações não se repetem ou a combinação de variáveis que tornam as observações únicas. Para exemplicar vamos pegar nossa base de dados e separar em duas, para que posteriormente possamos juntalas. Como em nossa base de dados não temos naturalmente nenhuma chave primária, vamos utilizar a função mutate(id = row_number()) para criarmos um identificar único para este exemplo. Após isso, vamos dividir a nossa base de dados em duas, mantendo em comum entre elas apenas a nossa chave primária, neste caso, a variável “id”\n\ndados &lt;- dados |&gt; \n  mutate(id = row_number()) |&gt; \n  select(id, everything())#selecionar variavel id e todas as outras \n\ndados1 &lt;- dados[, c(1:24)]\n\nglimpse(dados1)\n\nRows: 11,523\nColumns: 24\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n\ndados2 &lt;- dados[, c(1, 25:49)]\n\nglimpse(dados2)\n\nRows: 11,523\nColumns: 26\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nEm “dados1” selecionamos as colunas de 1 até a 24, onde a coluna 1 é a variável “id”. Em dados 2 selecionamos a coluna depois e as colunas de 25 até a 49. Agora temos dois banco de dados e precisamos juntá-los.\nHá algumas funções de combinação de duas bases de dados no pacote dplyr. Elas recebem três argumentos: a primeira base a ser declarada (x=), a segunda base a ser declarada (y=) e a variável de identificação informada no argumento by=. Aqui estão as funções mais úteis:\n\nleft_join() - retorna todas as linhas da base de dados no argumento x e todas as colunas das duas bases de dados. Linhas da base de dados de x sem correspondentes em y receberão NA na base de dados combinada.\nright_join() - retorna todas as linhas da base de dados no argumento y e todas as colunas das duas bases de dados. Linhas da base de dados de y sem correspondentes em x receberão NA na base de dados combinada.\nfull_join() - retorna todas as linhas e todas as colunas de x e de y. As linhas sem correspondência entre as bases receberão NA na base de dados combinada.\ninner_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y e todas as colunas das duas bases de dados.\nsemi_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y, mantendo apenas as colunas da base de dados de x.\nanti_join() - filtra a base de dados no argumento x para incluir apenas valores que não possuem correspondências na base de dados no argumento y.\n\nAssim sendo, no nosso exemplo, tanto as funções left_join(), right_join(), full_join() e inner_join() retornarão a mesma combinação, pois “dados1” e “dados2” possuem exatamente os mesmos indivíduos, ou seja, não há nenhuma linha que esteja em uma das bases de dados e que não está na outra. Este cenário foi um pouco mais simples, mas pense que no dia a dia você irá encontrar bases onde você precisará encontrar chaves primarias entre elas. Além disso, varios problemas podem vir acompanhados, por exemplo, imagine que para juntar duas bases você utilizará uma chave formada pela combinação de duas variáveis: UF e Município. Em uma base a sua UF está no formato de sigla e na outra está sendo representada pelo código da UF atribuido pelo IBGE. Já na variável de Município, Em uma base os dados estão todos padronizados, maiúsculos e sem acentuação, já na outra base está no formato “padrão” com a primeira letra maiúscula e acentuação. Veja que será necessário um bom tratamento de dados para pode juntar essas bases. Voltando para o nosso exemplo, vamos a prática.\n\ndados_todos &lt;- full_join(dados1, dados2, by=c(\"id\")) \n\nglimpse(dados_todos)\n\nRows: 11,523\nColumns: 49\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nPronto, temos nossa base completa e aprendemos um pouco sobre manipular dados. O pacote tidyverse será um grande aliado seu no R de forma geral. Como mencionado anteriormente, não cobrimos tudo o que é necessário saber para trabalhar com manipulação de dados, é necessário entender a demanda e pesquisar soluções. Saber traduzir o seu problema para que consiga pesquisar com mais facilidade é uma habilidade muito importante. Recomendamos que treine, trabalhe com diferentes tipos de dados, pesquise pacotes, funções, etc. Com o tempo fará com tranquilidade coisas que hoje considera difícil. Lembre-se, a pratica leva a perfeição."
  },
  {
    "objectID": "tabulacao.html#variáveis",
    "href": "tabulacao.html#variáveis",
    "title": "3  Tabulação de dados",
    "section": "3.1 Variáveis",
    "text": "3.1 Variáveis\nOs objetos apresentados, ou variáveis, podem ser denotados como o armazenamento de informações sobre a característica de interesse a respeito de cada unidade amostral, variáveis socioeconômicas como raça, renda e escolaridade são um ótimo exemplo. As variáveis podem ser divididas em dois tipos:\n\nVariáveis Qualitativas: cujos valores podem ser separados por categorias não numéricas. Sendo chamadas de variáveis qualitativas ordinais quando há presença de uma ordenação entre as categorias (Ex.: Escolaridade), e variáveis qualitativas nominais caso contrário (Ex.: Raça, Sexo)\nVariáveis Quantitativas: onde os valores são expressos em números resultantes de uma contagem ou mensuração. Podendo ser quantitativas discretas, quando resultam de um conjunto finito ou enumerável de possíveis valores (Ex.: Número de vitórias ou de filhos), ou ainda variáveis quantitativas continuas quando assumem valores em uma escala continua (Ex.: Peso, Altura).\n\nObserve as 10 unidades amostrais para as variáveis da base de dados COVID-19 para melhor compreensão, onde a idade representa variável quantitativa discreta, a raça represeta qualitativa nominal e a escolaridade é relativa a qualitativa ordinal.\n\n\n\n\n\n\nidade_anos\nraca\nescol\n\n\n\n\n5\n39\nparda\nsuperior\n\n\n6\n34\nbranca\nsuperior\n\n\n8\n29\nbranca\nmedio\n\n\n11\n28\nbranca\nmedio\n\n\n13\n37\nparda\nfund2\n\n\n16\n27\nbranca\nmedio\n\n\n17\n44\nbranca\nmedio\n\n\n23\n31\nbranca\nmedio\n\n\n24\n33\namarela\nmedio\n\n\n25\n25\nparda\nmedio\n\n\n\n\n\nPodemos olhar uma variável por outra perspectiva, assumindo um outro tipo de classificação. Isso pode soar um pouco estranho a princípio, mas olher o exemplo a seguir para melhor compreensão, considere a variável idade, podemos transformar em faixas de idade para classificação em criança, jovem, adulto e idoso. Observe:\n\n#criacao da variavel classificacao\nclassificacao &lt;- idade_anos |&gt;\n  lapply(function(x) ifelse(x &lt; 12, 'crianca',\n                            ifelse(x &lt; 25, 'jovem',\n                                   ifelse( x &lt; 60 ,'adulto','idoso'))))\n#tabela concatenando idade e classificacao\nclassificacao |&gt; \n  unlist() |&gt; \n  cbind(idade_anos) |&gt; \n  head(10) |&gt; knitr::kable()\n\n\n\n\n\nidade_anos\n\n\n\n\njovem\n24\n\n\nadulto\n31\n\n\nadulto\n27\n\n\njovem\n20\n\n\nadulto\n39\n\n\nadulto\n34\n\n\nadulto\n34\n\n\nadulto\n29\n\n\nadulto\n44\n\n\nadulto\n27\n\n\n\n\n\nAgora, temos uma variável categórica ordinal."
  },
  {
    "objectID": "tabulacao.html#como-tabular",
    "href": "tabulacao.html#como-tabular",
    "title": "3  Tabulação de dados",
    "section": "3.2 Como tabular",
    "text": "3.2 Como tabular\nÉ perceptível, até mesmo quando trabalhamos com DataFrames e matrizes, a forma proposta de visualização e armazenamento dessas variáveis. Por colunas onde cada coluna representa uma das características (no nosso exemplo, idade, raça e escolaridade).\nFazemos isso de forma a facilitar nossa análise, sendo cada linha um indíviduo e, cada uma das observações dentro dessa linha, suas características.\nAssim como discutido, podemos obter nossas bases de dados de diversas fontes, como planilhas excel, arquivos .csv, bases SQL, ou até mesmo criá-las no nosso próprio R script com a função data.frame() como já apresentado. Por ser mais intuitivo e mais utilizado no dia a dia, vamos tomar o excel para exemplificar todo o processo. Você irá notar que o processo é realizado de forma bem simples.\n\n\n\n\n\nTabulação das variáveis no excel\n\n\n\n\nCada uma das células receberá um valor x referente a alguma característica indicada pela coluna e um indivíduo representado pela linha, em nosso caso temos 3 características para cada uma das 4 observações."
  },
  {
    "objectID": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "href": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "title": "3  Tabulação de dados",
    "section": "3.3 Alguns problemas no meio do caminho",
    "text": "3.3 Alguns problemas no meio do caminho\nÉ valido ressaltar que é possível se deparar com alguns problemas que talvez possam vir a ser solucionados da maneira errada.\nA forma como tabulamos nossos dados pode vir a ser um facilitar ou empecilho em nossas análises, um belo exemplo é a forma citada anteriormente de classificação dos dados ou transformação para que sejam salvos em alguma outra categoria, como faixa etária ou idade.\nOutro problema é quando trabalhamos com dados que possas vir a ter mais de uma resposta. Por exemplo: Quais sintomas estava sentindo? O melhor a se fazer nesse caso é criar uma coluna para cada um dos possíveis sintomas.\n\n\n\n\n\nMais de uma opção de escolha na variável\n\n\n\n\nUma outra forma seria:\n\n\n\n\n\nMais de uma opção de escolha na variável, outra forma\n\n\n\n\nDevemos lembrar sempre de anexar um ID ou forma de identificação única para cada uma das observações. É possível criar uma ou trabalhar com alguma já existente, um exemplo de uma já existente é o próprio CPF ou RG quando trabalhamos com pessoas.\nVale ressaltar outras boas práticas ao realizar a tabulação:\n\nSe trabalhando com Excel ou Softwares parecidos, deixe a planilha apenas com a tabela de dados, evite armazenar na mesma planilha várias informações avulssas que não façam parte da sua tabela;\nNo R conseguimos especificar qual planilha de um arquivo .xlsx queremos transferir, porém pode vir a ser um pouco confuso as vezes, então é sugerido deixar todas as suas informações em uma única tabela em uma única planilha;\nPadronização é extremamente importante, salve todos os dados para cada coluna em apenas um determinado formato (Ex.: Coluna Idade - Integer, Coluna Raça - Character), lembrando sempre de manter um padrão de medida (cm, L), variáveis do tipo categórico tambem precisam de padronização (Evite coisas como: Não, nao, n, N, não);\nCuidado ao classificar dados faltantes, uma prática errada é preencher esses dados com 0, isso pode vir a atrapalhar toda sua análise\nFoi citado CPF como forma de identificação, mas pode haver casos em que teremos mais de uma linha contendo um mesmo indivíduo dependendo do nosso tipo de dados. Ou seja, esteja atento para que não haja duplicidade de variável identificadora ou ID."
  },
  {
    "objectID": "estimacao.html#estimadores-e-estimativas",
    "href": "estimacao.html#estimadores-e-estimativas",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.1 Estimadores e estimativas",
    "text": "5.1 Estimadores e estimativas\nAgora que estamos a parte de algumas definições que serão importantes para o entendimento do capítulo, podemos iniciar com o seu conteúdo propriamente dito. Em primeiro lugar, chamamos de estimador qualquer estatística cujos valores são utilizados para se estimar um parâmetro ou uma função de um parâmetro. Dessa forma, temos, portanto, que todo estimador será, também, uma variável aleatória, uma vez que eles são funções das variáveis aleatórias que compõem nossa amostra. Quando coletamos a amostra, observamos os valores das variáveis aleatórias que a compõem e os substituímos na expressão do estimador, obtemos o que chamamos de estimativa. Estimativas não são valores aleatórios, mas sim realizações de variáveis aleatórias (dos estimadores).\nDiversos métodos foram desenvolvidos ao longo dos anos para se encontrar estatísticas que possam ser utilizadas como estimadores. Entre os mais conhecidos, podemos citar: o método dos momentos, que encontra estimadores relacionando os momentos amostrais e populacionais; o método dos mínimos quadrados, a partir do qual encontramos o estimador que minimiza a chamada soma de quadrado dos erros; e o método da máxima verossimilhança, provavelmente o mais conhecido e utilizado, por meio do qual encontramos o estimador que maximiza a probabilidade de a amostra coletada ter sido observada, através da maximização da função de verossimilhança. Não entraremos em maiores detalhes sobre nenhum desses métodos, uma vez que nosso objetivo aqui é introduzir o conceito de estimação de forma mais intuitiva. Entretanto, caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o método da máxima verossimilhança e tudo aquilo que está por trás dele. O post está disponível  neste link .\nO problema da estimação não se resume somente a encontrar estimadores. De fato, existem infinitos estimadores para qualquer que seja o parâmetro que tenhamos interesse. A questão agora é, então, estabelecer critérios que nos permitam determinar o melhor estimador em um certo conjunto. Nesse contexto, podemos definir uma série de propriedades que os estimadores possuem ou podem possuir. São elas:\n\nVício: dizemos que um estimador é não viciado (ou não viesado) se o seu valor esperado coincide com o verdadeiro valor do parâmetro em questão. Em outras palavras, estimadores não viciados acertam, em média, o valor do parâmetro que estão estimando. Caso essa afirmação não seja verdadeira, dizemos que o estimador apresenta vício. O vício é a diferença entre o verdadeiro valor do parâmetro e o valor esperado de seu estimador.\nConsistência: dizemos que um estimador é consistente se, à medida que o tamanho da amostra aumenta, o seu valor esperado converge para o verdadeiro valor do parâmetro em questão e sua variância converge para zero. Dessa forma, estimadores consistentes não necessariamente são não viciados para tamanhos pequenos de amostra: eles só precisam ser não viciados quando esse tamanho é muito grande.\nErro quadrático médio: definimos o erro quadrático médio como sendo o valor esperado da diferença quadrática entre o estimador e o verdadeiro valor do parâmetro estimado. Tal como a variância de uma variável aleatória é uma medida da dispersão de seus valores em torno de sua média, o erro quadrático médio é uma medida da dispersão dos valores do estimador em torno do verdadeiro valor do parâmetro. Dessa forma, estimadores com erros quadráticos médios pequenos são preferíveis. Algo a se notar é que o erro quadrático médio pode ser reescrito como sendo a soma entre o quadrado do vício do estimador e sua variância. Com isso, para estimadores não viciados, o erro quadrático médio se reduz à variância do estimador.\n\nEm um mundo ideal, parece ser intuitivo que nossa busca pelo melhor estimador se dê através do erro quadrático médio, tentando encontrar aquele para o qual essa medida seja a menor possível. Essa tarefa é, entretanto, raramente possível. Em geral, o erro quadrático médio de um estimador é uma função do valor desconhecido do parâmetro a ser estimado, e é muito comum que, para dois estimadores de um parâmetro, seus erros quadráticos médios se entrelassem: para certos valores do parâmetro, o primeiro estimador pode ter o menor erro quadrático médio, enquanto para outros valores o segundo estimador é o que o tem. Com isso, em nossa busca pelo melhor estimador, é comum restringirmos o conjunto de todos os estimadores possíveis à classe dos estimadores não viciados, fazendo com que o erro quadrático médio seja apenas uma função da variância do estimador. Essa restrição é feita porque existem técnicas que nos permitem encontrar, entre os estimadores não viciados, aqueles que possuem a menor variância possível. Não trataremos dessas técnicas ao longo deste livro, mas essa é uma importante consideração para se entender o motivo de, em geral, trabalharmos com estimadores não viciados. De toda forma, passemos, então, para a próxima seção, na qual veremos alguns exemplos de estimadores pontuais utilizando os dados apresentados e tratados em capítulos anteriores."
  },
  {
    "objectID": "estimacao.html#estimação-pontual",
    "href": "estimacao.html#estimação-pontual",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.2 Estimação pontual",
    "text": "5.2 Estimação pontual\nPara iniciar esta seção, comecemos com uma definição. Chamamos de estimação pontual a técnica de estimação na qual utilizamos um único valor de uma estatística para representarmos, ou estimarmos, o valor desconhecido de um parâmetro de interesse. Chamamos essa estatística de estimador pontual, enquanto ao seu valor observado damos o nome de estimativa pontual. [completar]\n\n5.2.1 Estimação pontual da média de uma população\nVoltando aos exemplos apresentados na introdução do capítulo, suponha, primeiramente, que nosso parâmetro de interesse seja a idade média das gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período de março de 2020 a dezembro de 2021, a qual denotaremos por \\(\\mu\\). Como temos acesso a todos os registros dessa população, o valor desse parâmetro não é desconhecido, mas isso servirá de auxílio para exemplificar os métodos que aqui serão empregados. Criando um vetor contendo todos os elementos da população, temos:\n\ndados &lt;- readr::read_rds(\"dados/dados_covid[LIMPO].rds\")\npopulacao1 &lt;- dados$idade_anos[which(dados$evolucao == \"óbito\")]\nlength(populacao1)\n\n[1] 1266\n\n\nO processo envolvido na criação do vetor populacao1 é o seguinte: dentro do data frame dados, que contém todos os registros de nossa população, estamos selecionando o valor da variável idade_anos de todas as pacientes para as quais o valor da variável evolucao é “óbito”. O tamanho desse vetor, obtido por meio da função length(), do pacote básico {base}, é de 1266. Ou seja, a população de gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período considerado é composta por 1266 elementos. Para calcular o valor da idade média dessas mulheres, podemos utilizar a função mean(), também do pacote básico {base}, que calcula a média aritmética de um dado vetor.\n\nmean(populacao1)\n\n[1] 31.80806\n\n\nA saída do código nos revela o valor de \\(31.81\\) anos. Note que esse valor não representa uma estimativa; ele é, de fato, o verdadeiro valor do parâmetro \\(\\mu\\). O que aconteceria, entretanto, se tivéssemos acesso apenas a uma amostra da população em questão? Poderíamos garantir que os resultados obtidos seriam válidos para todas as gestantes e puérperas desse grupo? É o que começaremos a ver na subseção seguinte.\n\n5.2.1.1 Trabalhando com amostras da população\nPara exemplificar os conceitos de estimação definidos anteriormente, simularemos a retirada de amostras da população de gestantes e puérperas com a qual estamos trabalhando. Dentre as várias maneiras de se obter amostras de uma população, utilizaremos, aqui, a amostragem aleatória simples (AAS) com reposição, uma técnica de amostragem probabilística (ou seja, que atribui a cada elemento da população uma probabilidade, conhecida a priori, de pertencer à amostra), na qual todos os elementos da população possuem a mesma probabilidade de serem sorteados. Utilizaremos a AAS com reposição, que admite a possibilidade de um elemento ser selecionado mais de uma vez, por sua maior simplicidade teórica e por algumas implicações matemáticas e estatísticas que ela carrega, como a independência entre as unidades sorteadas.\nAntes da realização da amostragem, denotamos as variáveis a serem selecionadas por \\(X_1, X_2, ..., X_{n}\\), sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera hospitalizada pela COVID-19 e que faleceu em decorrência dessa doença, com \\(i = 1, 2, ..., n\\). Dizemos que essa sequência de variáveis aleatórias forma uma amostra aleatória de tamanho \\(n\\). Sendo, novamente, \\(\\mu\\) o parâmetro que representa a idade média da população em questão, e denotando por \\(\\sigma^2\\) o parâmetro que representa a variância populacional das idades dessas gestantes e puérperas, temos, ainda, que \\(E(X_i) = \\mu\\) e que \\(Var(X_i) = \\sigma^2\\).\nDentro do R, podemos obter uma amostra aleatória de tamanho, digamos, \\(n = 30\\), a partir da função sample(), do pacote básico {base}. Utilizaremos três argumentos dessa função: o primeiro, x, recebe o vetor de elementos do qual a amostra será retirada; o segundo, size, recebe o número de itens a serem sorteados; por fim, o terceiro argumento, replace, receberá o valor TRUE, indicando que a amostragem deve ser realizada com reposição. O código utilizado para a realização desse processo, bem como a amostra obtida, podem ser vistos abaixo.\n\nset.seed(43)\namostra1 &lt;- sample(x = populacao1, size = 30, replace = TRUE)\namostra1\n\n [1] 32 27 37 38 39 32 30 24 29 30 34 40 33 17 34 35 32 38 23 24 33 38 18 25 26\n[26] 32 25 36 29 32\n\n\nÉ importante ressaltar que, enquanto \\(X_1, X_2, ..., X_{30}\\) são variáveis aleatórias, os valores guardados no vetor amostra1 representam realizações das mesmas. Dessa forma, temos, para a amostra sorteada, que \\(x_1 = 32\\), \\(x_2 = 27\\), \\(x_3 = 37\\) e assim por diante. Essas realizações seriam, muito provavalmente, diferentes caso desempenhássemos o procedimento de retirada da amostra novamente. No presente caso, o código acima sempre resultará nos mesmos elementos, uma vez que estamos fixando a semente inicial do gerador de números pseudo-aleatórios do R por meio da função set.seed(), do pacote básico {base}. Desfixando a semente inicial do sorteio, entretanto, o resultado obtido através da função sample() seria diferente a cada vez que rodássemos o bloco de código. Observe.\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 33 27 17 27 24 28 29 30 32 32 31 41 25 30 26 41 26 27 33 27 33 24 39 36\n[26] 30 40 32 28 23\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 42 32 29 32 30 30 39 35 48 32 26 27 42 23 29 31 37 27 32 25 41 43 22 21\n[26] 30 34 22 30 37\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 22 33 32 30 30 31 36 37 37 23 28 33 38 40 21 23 38 33 23 37 27 31 43 39 41\n[26] 36 35 39 39 45\n\n\nCom a distinção entre variáveis aleatórias e suas realizações em mente, precisamos, nesse próximo caso, definir qual estimador utilizaremos para estimarmos o parâmetro em questão. Como temos interesse na idade média da população, uma escolha muito intuitiva seria utilizar a média aritmética dos valores amostrados como uma estimativa do valor desse parâmetro. Assim, considerando que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória de tamanho \\(n\\) dessa população, definimos o estimador da média amostral como sendo a estatística dada por\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{n}}{n} = \\frac{\\sum_{i = 1}^{n} X_i}{n}.\n\\]\nAlém de muito intuitivo, esse estimador é, também, não viciado e consistente. Podemos facilmente demonstrar essas propriedades de forma matemática, utilizando para isso propriedades de valores esperados. Quanto a ser não viciado, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da esperança o multiplicando:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nSendo a esperança da soma de variáveis aleatórias equivalente à soma das esperanças marginais de cada variável, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right)\n\\]\nPor fim, como \\(E(X_i) = \\mu\\),\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mu = \\frac{1}{n} (n\\mu) = \\mu.\n\\]\nLogo, como o valor esperado do estimador é igual ao parâmetro que ele estima, concluímos que \\(\\bar{X}\\) é um estimador não viciado. Para demonstrarmos que ele é, também, consistente, precisamos calcular sua variância. Assim,\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da variância elevando-o ao quadrado:\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nComo utilizamos a AAS com reposição para a retirada da amostra, garantimos que as variáveis aleatórias que a compõem são independentes. Assim, sendo a variância da soma de variáveis aleatórias independentes dada pela soma das variâncias marginais de cada variável, e como \\(Var(X_i) = \\sigma^2\\), temos:\n\\[\n\\begin{align}\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(X_i) \\\\ & = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\sigma^2 \\\\ & = \\frac{1}{n^2} (n \\sigma^2) \\\\ & = \\frac{\\sigma^2}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\bar{X}\\). Essa informação, aliada ao fato de \\(\\bar{X}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Voltando ao exemplo em que estávamos, como definimos que \\(n = 30\\), a expressão do estimador da média amostral se torna\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{30}}{30} = \\frac{\\sum_{i = 1}^{30} X_i}{30}.\n\\]\nSubstituindo os valores da amostra coletada e calculando sua média aritmética, obtemos:\n\nmean(amostra1)\n\n[1] 30.73333\n\n\nCom isso, concluímos que uma estimativa pontual para a idade média das gestantes e puérperas que faleceram em decorrência da COVID-19 é de \\(30.73\\). Esse valor é relativamente próximo do verdadeiro valor do parâmetro, o qual sabemos ser 31.81. Lembre-se, entretanto, que a estimativa obtida depende diretamente da amostra que foi coletada, uma vez que nosso estimador é uma função da amostra e, portanto, é uma variável aleatória. A cada vez que realizássemos um novo sorteio, o valor de nossa estimativa seria, muito provavelmente, diferente do anterior. Como nosso objetivo é fazer uma afirmação sobre o parâmetro \\(\\mu\\) a partir da amostra coletada, é interessante considerar que a validade dessa afirmação seria melhor compreendida se soubéssemos o que acontece com nosso estimador quando retiramos todas as amostras de mesmo tamanho possíveis de nossa população. Retomaremos essa discussão posteriormente. Buscaremos, agora, estimar um outro tipo de parâmetro: a proporção populacional.\n\n\n\n5.2.2 Estimação pontual da proporção populacional\nPara o segundo exemplo, suponha que o parâmetro no qual temos interesse seja a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas. Representaremos esse parâmetro por \\(p\\). Assim como no exemplo anterior, podemos calcular seu valor, uma vez que temos acesso a todos os registros dessa população. Note que, como estamos tratando da proporção válida, precisamos que nossa população seja composta apenas pelas mulheres para as quais o valor da variável diarreia foi preenchido de forma válida (ou seja, com sim ou não). Assim, temos:\n\npopulacao2 &lt;- dados$diarreia[which(!is.na(dados$diarreia) & dados$diarreia != \"ignorado\")]\nlength(populacao2)\n\n[1] 8472\n\nhead(populacao2, 20)\n\n [1] \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n[13] \"não\" \"sim\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n\n\nObservando as saídas acima, podemos notar que nossa população é formada por 8.472 elementos, que assumem valor sim, quando a gestante ou puérpera apresentou diarreia como um dos sintomas da COVID-19, e não, quando esse sintoma não foi apresentado. Para facilitar nosso trabalho a partir daqui, transformaremos os valores sim em 1 e os valores não em 0, utilizando para isso a função ifelse(), do pacote básico {base}. Essa função recebe três argumentos: o primeiro, test, recebe um vetor lógico; o segundo, yes, recebe o valor que a função deve retornar quando o dado elemento desse vetor lógico for verdadeiro; por fim, o terceiro argumento, no, recebe o valor que a função deve retornar quando o dado elemento do vetor lógico for falso. Observe o código e a saída abaixo.\n\npopulacao2_transformada &lt;- ifelse(populacao2 == \"sim\", yes = 1, no = 0)\nhead(populacao2_transformada, 20)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n\n\nCalculando, por fim, a proporção desejada, que nada mais será do que a média aritmética do vetor populacao2_transformada, uma vez que ele é formado por zeros e uns, temos:\n\nmean(populacao2_transformada)\n\n[1] 0.128305\n\n\nLogo, o valor do parâmetro \\(p\\) - a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas - é de 0,128, ou 12,8%. Vamos, agora, fingir que não tínhamos acesso a métodos de se calcular o valor desse parâmetro, tentando novamente estimá-lo por meio da coleta de amostras da população.\n\n5.2.2.1 Trabalhando com amostras de uma população com distribuição Bernoulli\nDiferentemente do que ocorria com o exemplo anterior, a amostra aleatória que agora coletaremos será composta por variáveis aleatórias para as quais sabemos a “forma” de sua distribuição de probabilidade. Podemos definir \\(Y_1, Y_2, ..., Y_{n}\\) como sendo uma amostra aleatória da distribuição Bernoulli com parâmetro \\(p\\), na qual \\(Y_i\\) recebe o valor 1, caso a \\(i\\)-ésima gestante ou puérpera sorteada tenha apresentado diarreia como um dos sintomas da COVID-19 (sucesso), e 0, caso contrário (fracasso), com \\(i = 1, 2, ..., n\\). O parâmetro \\(p\\) representa a probabilidade de sucesso (que sabemos ser de 0,128, apesar de estarmos fingindo que não temos essa informação). Como queremos estimar uma proporção, é intuitivo considerarmos como estimador a proporção das gestantes ou puérperas da amostra que apresentaram diarreia como sintoma. Assim, definimos o estimador da proporção amostral, denotado por \\(\\hat{p}\\), como sendo\n\\[\n\\hat{p} = \\frac{Y_1 + Y_2 + ... + Y_n}{n} = \\frac{\\sum_{i = 1}^{n} Y_i}{n}.\n\\]\nDe maneira similar ao que fizemos com o estimador da média amostral, \\(\\bar{X}\\), podemos demonstrar que o estimador da proporção amostral é, também, não viciado e consistente. Quanto à primeira propriedade, sabendo que \\(E(Y_i) = p\\), para \\(i = 1, 2, ..., n\\), temos:\n\\[\nE\\left(\\hat{p}\\right) = E \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n} \\sum_{i = 1}^{n} E\\left(Y_i \\right) =  \\frac{1}{n}  \\sum_{i = 1}^{n} p = \\frac{1}{n} (np) = p.\n\\]\nAssim, como \\(E(\\hat{p}) = p\\), podemos concluir que o estimador da proporção amostral é não viesado. Em outras palavras, esse estimador “acerta”, em média, o verdadeiro valor do parâmetro \\(p\\). Para a segunda propriedade, precisamos, primeiramente, calcular a variância de \\(\\hat{p}\\). Como \\(Y_i\\) segue distribuição \\(Bernoulli(p)\\), sabemos que \\(Var(Y_i) = p(1 - p)\\). Dessa forma, temos que\n\\[\n\\begin{align}\nVar(\\hat{p}) = Var \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(Y_i) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} p(1 - p) \\\\ &  = \\frac{1}{n^2} \\left[np(1 - p)\\right] \\\\ &  = \\frac{p(1 - p)}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\hat{p}\\). Essa informação, aliada ao fato de \\(\\hat{p}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Investigadas as propriedades do estimador, podemos partir para a retirada da amostra, utilizando novamente a função sample() para simular uma amostra de tamanho \\(n = 30\\) obtida por meio da AAS com reposição. A amostra coletada pode ser vista abaixo.\n\nset.seed(43)\namostra2 &lt;- sample(x = populacao2_transformada, size = 30, replace = TRUE)\namostra2\n\n [1] 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n\n\nAplicando os valores obtidos no estimador da proporção amostral, que nada mais é do que a média aritmética da amostra, temos:\n\nmean(amostra2)\n\n[1] 0.1333333\n\n\nCom isso, concluímos que uma estimativa pontual para a proporção válida de gestantes e puérperas hospitalizadas pela COVID-19 no período em estudo e que apresentaram diarreia como um dos sintomas da doença é de 0,133, ou de 13,3%. Novamente, essa estimativa depende diretamente da amostra obtida; novas amostragens quase certamente resultariam em estimativas diferentes para o parâmetro. Com isso, volta à tona a reflexão levantada no final do exemplo anterior, de que a validade de nossa afirmação sobre o verdadeiro valor de \\(p\\) seria melhor compreendida caso levássemos em consideração a distribuição de nosso estimador, \\(\\hat{p}\\). Conseguiríamos estudar o comportamento probabilístico de \\(\\hat{p}\\) caso aumentássemos o tamanho da amostra? A resposta, já adiantando, é sim. O que utilizamos para realizar esse estudo, entretanto, será visto na próxima seção.\n\n\n\n5.2.3 A distribuição amostral de estimadores\nComo vimos ao longo das seções anteriores, o problema da Inferência Estatística que queremos resolver consiste em fazer uma afirmação sobre um certo parâmetro de uma determinada população por meio de uma amostra. Para encará-lo, decidimos que nossa afirmação será baseada em uma certa estatística \\(T\\), para a qual demos o nome de estimador, que será uma função da amostra (\\(X_1, X_2, ..., X_n\\)). Quando coletamos a amostra, podemos obter um valor particular de \\(T\\), digamos \\(t_0\\), para o qual demos o nome de estimativa. E é com base nesse valor \\(t_0\\) que faremos a afirmação sobre o parâmetro de interesse. Para entendermos melhor a incerteza por trás de nossa afirmação, entretanto, seria de nosso interesse determinar qual é a distribuição de \\(T\\) quando a amostra, \\(X_1, X_2, ..., X_n\\), assume todos os valores possíveis. Chamamos essa distribuição de distribuição amostral da estatística T. Bussab e Moretin (referência) esquematizam o procedimento para a obtenção da distribuição amostral da seguinte maneira:\n\nA partir de uma determinada população \\(X\\), com certo parâmetro de interesse \\(\\theta\\), obtemos todas as amostras possíveis com um mesmo tamanho amostral \\(n\\), de acordo com uma certa técnica de amostragem;\nPara cada amostra obtida, calculamos o valor \\(t\\) da estatística \\(T\\);\nOs valores \\(t\\) formam uma nova população, cuja distribuição recebe o nome de distribuição amostral de \\(T\\).\n\nÉ muito comum, no entanto, que não sejamos capazes de coletar todas as amostras possíveis de uma população. Com isso, acabamos tendo que nos contentar em simular um número grande de amostras, para assim termos uma ideia do que acontece com a estatística de interesse. Para melhor entendermos as ideias apresentadas, consideremos os estimadores \\(\\bar{X}\\), a média amostral, e \\(\\hat{p}\\), a proporção amostral. Nos exemplos antecedentes, acabamos determinando, talvez sem perceber, a média e a variância das distribuições amostrais de ambos os estimadores quando estávamos demonstrando duas de suas propriedades - a falta de vício e a consistência. Retomando os resultados obtidos, encontramos que\n\n\\(E(\\bar{X} = \\mu)\\) e \\(Var(\\bar{X}) = \\displaystyle \\frac{\\sigma^2}{n}\\);\n\\(E(\\hat{p} = p)\\) e \\(Var(\\hat{p}) = \\displaystyle \\frac{p(1 - p)}{n}\\).\n\nMédias e variâncias não são, todavia, tudo aquilo que precisamos para determinar a distribuição amostral de estimadores. Precisamos, também, determinar sua “forma”. Para isso, coletaremos várias amostras e construiremos histogramas das distribuições de \\(\\bar{X}\\) e \\(\\hat{p}\\) para diferentes tamanhos de amostra. Comecemos simulando \\(M\\) = 100 amostras, cada uma com tamanho \\(n\\) = 15, da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo, a qual chamamos de populacao1. Utilizaremos para isso a função replicate(), do pacote básico {base}. Essa função recebe dois argumentos: o primeiro, n, recebe o número de replicações a serem feitas, enquanto o segundo, expr, recebe a expressão que será replicada. O resultado, guardado no objeto amostras_pop1, é uma matriz na qual o elemento \\([a_{ij}]\\) representa o \\(i\\)-ésimo elemento da \\(j\\)-ésima amostra, com \\(i = 1, 2, ..., 15\\) e \\(j = 1, 2, ..., 100\\). As cinco primeiras colunas dessa matriz podem ser vistas abaixo.\n\nset.seed(43)\nM &lt;- 100\nn &lt;- 15\namostras_pop1 &lt;- replicate(M, expr = sample(x = populacao1, size = n, replace = TRUE))\namostras_pop1[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]   32   35   33   26   33\n [2,]   27   32   33   41   42\n [3,]   37   38   27   26   32\n [4,]   38   23   17   27   29\n [5,]   39   24   27   33   32\n [6,]   32   33   24   27   30\n [7,]   30   38   28   33   30\n [8,]   24   18   29   24   39\n [9,]   29   25   30   39   35\n[10,]   30   26   32   36   48\n[11,]   34   32   32   30   32\n[12,]   40   25   31   40   26\n[13,]   33   36   41   32   27\n[14,]   17   29   25   28   42\n[15,]   34   32   30   23   23\n\n\nRepetimos o mesmo processo para a população de gestantes e puérperas com preenchimento válido da variável diarreia, que chamamos de populacao2_transformada. Novamente, as cinco primeiras colunas da matriz de amostras, que agora denominamos amostras_pop2, podem ser vistas abaixo.\n\nset.seed(43)\namostras_pop2 &lt;- replicate(M, expr = sample(x = populacao2_transformada, size = n, replace = TRUE))\namostras_pop2[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    0    0\n [2,]    0    0    0    0    0\n [3,]    0    1    0    0    0\n [4,]    0    0    0    0    0\n [5,]    0    0    0    1    1\n [6,]    0    0    0    0    0\n [7,]    0    0    0    0    0\n [8,]    1    0    0    0    0\n [9,]    1    0    0    0    0\n[10,]    0    0    0    1    0\n[11,]    0    0    0    0    0\n[12,]    0    0    0    1    1\n[13,]    0    0    0    0    0\n[14,]    0    1    0    0    0\n[15,]    0    0    0    1    0\n\n\nCom as amostras em mãos, o próximo passo é calcular o valor do respectivo estimador em cada uma delas. Realizaremos esse processo com a função apply(), também do pacote básico {base}, a qual permite que apliquemos qualquer função em todas as linhas ou colunas de uma matriz. Utilizaremos três de seus argumentos: o primeiro, X, recebe a matriz na qual queremos aplicar a função; o segundo, MARGIN, recebe a direção em que a função será aplicada (1 caso queiramos que a função seja aplicada nas linhas da matriz, ou 2 caso queiramos a aplicar em suas colunas); por fim, o terceiro, FUN, recebe a função que queremos aplicar. O código utilizado nesse processo, bem como parte dos vetores obtidos, podem ser vistos abaixo.\n\nx_barras &lt;- apply(X = amostras_pop1, MARGIN = 2, FUN = mean)\nhead(x_barras)\n\n[1] 31.73333 29.73333 29.26667 31.00000 33.33333 30.73333\n\np_chapeus &lt;- apply(X = amostras_pop2, MARGIN = 2, FUN = mean)\nhead(p_chapeus)\n\n[1] 0.1333333 0.1333333 0.0000000 0.2666667 0.1333333 0.2000000\n\n\nPor fim, criemos os histogramas da distribuição de cada estimador. Como já discutimos sobre a criação de histogramas no capítulo de Estatística Descritiva, o código abaixo deve ser familiar.\n\nlibrary(ggplot2)\n\nggplot(data.frame(x_barra = x_barras), aes(x = x_barra))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"lightblue\",\n    bins = 15,\n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(26, 36)) +\n  labs(\n    x = \"Idade média das gestantes e puérperas\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral da médias amostrais para n = 15\"\n    ) +\n  geom_vline(xintercept = mean(x_barras), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 34, \n    y = 0.3,\n    label = paste(\"Valor médio das estimativas:\", round(mean(x_barras), 3))\n    )\n\n\n\nggplot(data.frame(p_chapeu = p_chapeus), aes(x = p_chapeu))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"steelblue\", \n    bins = 15, \n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(0, 0.5)) +\n  labs(\n    x = \"Proporção de gestantes ou puérperas com diarreia como sintoma\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral das proporções amostrais para n = 15\"\n    ) + \n  geom_vline(xintercept = mean(p_chapeus), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 0.23, \n    y = 9,\n    label = paste(\"Valor médio das estimativas:\", round(mean(p_chapeus), 3))\n    )\n\n\n\n\nObservando os histogramas, podemos notar que, mesmo para um tamanho de amostra pequeno como \\(n\\) = 15, a distribuição de \\(\\bar{X}\\) se assemelha à distribuição normal, visto que apresenta o característico formato aproximado de sino e uma quase simetria em torno de sua média. Essa combinação de fatores nos sugere que \\(X_1, X_2, ..., X_{15}\\), as variáveis aleatórias que compõem as amostras da população de idades, seguem, também, uma distribuição simétrica em torno da média. Além disso, o valor médio das estimativas \\(\\bar{x}\\), de 31,843, está muito próximo do verdadeiro valor do parâmetro populacional \\(\\mu\\), que sabemos ser de 31,81 anos. Esse resultado já era esperado, uma vez que a distribuição de \\(\\bar{X}\\) está centrada em \\(\\mu\\). Pouco podemos dizer, entretanto, do histograma da distribuição amostral de \\(\\hat{p}\\) até o momento; apenas que sua média está muito próxima do verdadeiro valor de \\(p\\), que sabemos ser de 0,128, como já era esperado pelo mesmo motivo. Aumentemos, então, o tamanho das amostras, e observemos os resultados obtidos. Como a única modificação será o valor da variável n, ocultaremos os códigos utilizados para evitar uma maior poluição visual. Dessa forma, para \\(n\\) = 30, temos:\n\n\n\n\n\n\n\n\nPara \\(n = 50\\),\n\n\n\n\n\n\n\n\nPara \\(n = 100\\),\n\n\n\n\n\n\n\n\nPor fim, para \\(n = 200\\),\n\n\n\n\n\n\n\n\nObserve que, conforme aumentamos o tamanho das amostras, os histogramas de ambos os estimadores tendem a se concentrar cada vez mais em torno de suas respectivas médias, uma vez que as variância das estimativas se torna cada vez menor. Dessa forma, podemos concluir que estimativas obtidas a partir de tamanhos de amostra maiores têm uma maior probabilidade de “acertarem” o verdadeiro valor do parâmetro que estão estimando. É também notável que mesmo os histogramas das proporções amostrais aparentam convergir para o formato da distribuição normal conforme o valor de \\(n\\) aumenta. Esse fato, por incrível que pareça, não é coincidência: ele é decorrência direta do Teorema Central do Limite (TCL), o qual afirma que, independente da distribuição da população, quanto maior o tamanho amostral, mais próxima será a distribuição amostral da média de uma distribuição normal. Vale lembrar que a proporção amostral nada mais é do que um caso particular da média amostral em que os valores observados na amostra contém apenas zeros e uns, o que explica a aplicação do TCL nesse caso. Para sermos mais precisos, podemos dizer de forma aproximada que, para tamanhos suficientemente grandes de amostra,\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right) \\text{ e } \\hat{p}\\sim N\\left(p, \\frac{p(1-p)}{n} \\right).\n\\]\nCom isso, aqui terminamos o conteúdo referente à estimação pontual."
  },
  {
    "objectID": "estimacao.html#estimação-intervalar",
    "href": "estimacao.html#estimação-intervalar",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.3 Estimação intervalar",
    "text": "5.3 Estimação intervalar\nAté este ponto, todos os estimadores que apresentamos e discutimos são pontuais, uma vez que fornecem um único valor como estimativa para o parâmetro de interesse. Estimativas pontuais, por mais úteis que sejam, acabam fornecendo uma informação incompleta sobre o valor estimado do parâmetro em questão. Como estimadores são variáveis aleatórias e, portanto, possuem uma distribuição de probabilidade, seria de nosso interesse que a estimativa a ser apresentada levasse em consideração uma medida de seu possível erro. Essa essa medida pode ser, por exemplo, um intervalo relacionado à dimensão da confiança que temos de que o verdadeiro valor do parâmetro está sendo captado. Dessa forma, a partir daqui, entramos no campo da estimação intervalar. Dentro da Inferência Clássica, que estamos estudando neste capítulo, estimativas intervalares se dão a partir dos chamados intervalos de confiança. Intervalos de confiança incorporam, à estimativa pontual do parâmetro, informações a respeito da variabilidade do estimador. Além disso, eles são obtidos através da distribuição amostral de seus estimadores, o que justifica ainda mais o conteúdo que vimos na última subseção de estimação pontual.\nComo o intuito deste livro não é conter uma porção pesada de teoria, introduziremos o conceito de intervalos de confiança a partir de exemplos, realizando explicações sobre os elementos envolvidos em sua construção conforme seja necessário. Caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o melhor a teoria por trás dos intervalos de confiança, que conta também com o detalhamento de um dos principais métodos utilizados para a construção desses intervalos: o método da quantidade pivotal. O post está disponível  neste link . Com isso em mente, prossigamos para nosso primeiro exemplo: a criação de intervalos de confiança para a média amostral.\n\n5.3.1 Intervalos de confiança para a média amostral quando a variância populacional é conhecida\nUtilizando o exemplo já apresentado na seção anterior, considere que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Denotando, novamente, por \\(\\mu\\) a média populacional das idades dessas mulheres, e por \\(\\sigma^2\\) a variância populacional dessas idades, temos, ainda que \\(E(X_i) = \\mu\\) e \\(Var(X_i) = \\sigma^2\\). Suponha que queiramos estimar o valor de \\(\\mu\\), utilizando para isso o estimador \\(\\bar{X}\\). Suponha também, neste primeiro exemplo, que o valor de \\(\\sigma^2\\) é conhecido. Note que não estamos fazendo nenhuma suposição sobre a distribuição de probabilidade dessas variáveis. Dessa forma, precisaremos, a partir deste ponto, impor uma restrição: o tamanho da amostra deve ser grande o suficiente para que possamos aplicar o Teorema Central do Limite. Caso essa restrição seja cumprida, sabemos, por meio do TCL e de forma aproximada, que\n\\[\n\\bar{X} \\sim N \\left(\\mu, \\frac{\\sigma^2}{n} \\right).\n\\]\nSubtraindo de uma variável aleatória a sua média e dividindo o resultado por seu desvio padrão, obtemos o que chamamos de variável aleatória padronizadas. Uma variável aleatória padronizada tem média igual a zero e variância igual a um. Aplicando esse resultado em nosso estimador, \\(\\bar{X}\\), obtemos uma nova variável, a qual chamaremos de \\(Z\\), cuja distribuição estará totalmente definida, o que será de grande utilidade na construção de nosso intervalo. Observe.\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\] Como conhecemos a distribuição de probabilidade de \\(Z\\), podemos, para um certo valor \\(\\alpha\\), com \\(0 &lt; \\alpha &lt; 1\\), encontrar valores \\(z_1\\) e \\(z_2\\), com \\(z_1 &lt; z_2\\), tais que\n\\[\nP(z_1 &lt; Z &lt; z_2) = 1 - \\alpha.  \\qquad \\qquad \\text{(I)}\n\\]\nChamamos o valor \\(1 - \\alpha\\) de coeficiente de confiança. Sua interpretação será feita posteriormente. Quanto à probabilidade acima, note que existem infinitos valores de \\(z_1\\) e \\(z_2\\) que a satisfazem. Como queremos encontrar um intervalo que contenha os valores mais plausíveis do parâmetro em estudo, é de nosso interesse que a amplitude desse intervalo seja a menor possível, sendo a amplitude de um intervalo definida como a diferença entre seus extremos superior e inferior. E, para que esse interesse seja cumprido, é necessário que os valores de \\(z_1\\) e \\(z_2\\) sejam os mais próximos possíveis. Para distribuições simétricas em torno do zero, como é o caso da distribuição normal padrão, podemos mostrar que a amplitude do intervalo será mínima se os valores de \\(z_1\\) e \\(z_2\\) forem opostos, ou seja, se \\(z_1 = -z_2\\). Com isso, precisamos apenas encontrar um valor \\(z\\) tal que\n\\[\nP(Z \\leqslant z) = 1 - \\frac{\\alpha}{2}\n\\]\nA este valor, o qual denotamos por \\(z_{1 - \\alpha/2}\\), damos o nome de quantil de ordem \\(1 - \\alpha/2\\). Um quantil de ordem \\(k\\) de uma variável aleatória, com \\(0 &lt; k &lt; 1\\), nada mais é que o ponto tal que, quando nele aplicada a função de distribuição acumulada da variável, a probabilidade obtida é igual a \\(k\\) (a ordem que o quantil representa). Em uma situação prática, na qual teríamos um valor definido de \\(\\alpha\\), poderíamos utilizar uma tabela da distribuição normal padrão para encontrar o valor de \\(z_{1 - \\alpha/2}\\), ou mesmo utilizar a função qnorm(), do pacote básico {stats}, para realizar esse processo. A função qnorm(), bem como a família de funções do R que seguem a estrutura “qnome_da_distribuição()”, representa a função quantílica: para uma dada probabilidade e para dados valores dos parâmetros da distribuição, a função retorna o quantil cuja ordem é a probabilidade estipulada em seus arguementos. Com isso em mente, podemos reescrever \\(z_1\\) e \\(z_2\\) como sendo\n\\[\nz_1 = -z_{1 - \\alpha/2} \\text{ e } z_2 = z_{1 - \\alpha/2}.\n\\]\nPara que a explicação acima seja melhor absorvida, observe o gráfico a seguir, que representa a curva da densidade de probabilidade da distribuição normal padrão.\n\n\n\n\n\nFigura 1: Para uma confiança de (100 - \\(\\alpha\\))%, a área em cada cauda da distribuição deverá ser de \\(\\alpha\\)/2 para que o intervalo seja o menor possível.\n\n\n\n\nVoltando à probabilidade definida em \\(\\text{(I)}\\), a atualizando com os resultados obtidos e reescrevendo \\(Z\\), temos:\n\\[\nP(z_1 &lt; Z &lt; z_2) = P \\left(-z_{1 - \\alpha/2} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma}  &lt; z_{1 - \\alpha/2} \\right)  = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-z_{1 - \\alpha/2} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} &lt; z_{1 - \\alpha/2} \\right) \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\sigma &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; z_{1 - \\alpha/2}\\sigma \\right)  \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\  &\n= P\\left(-\\bar{X} + -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\ & = P\\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\] Portanto, quando a variância populacional é conhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\; \\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nA interpretação do resultado acima deve ser feita com cuidado. É preciso entender que a expressão \\(IC(\\mu,\\ 1 - \\alpha)\\) envolve uma variável aleatória, \\(\\bar{X}\\), fazendo com que o intervalo obtido também seja aleatório. Para o intervalo aleatório encontrado acima, podemos dizer que a probabilidade aproximada de ele conter o verdadeiro valor do parâmetro \\(\\mu\\) é de \\(1 - \\alpha\\). Aproximada, nesse caso, porque estamos utilizando o TCL para fazer uma aproximação da distribuição de probabilidade de \\(\\bar{X}\\); caso a população seguisse distribuição normal, essa probabilidade seria exata. De qualquer forma, quando coletamos a amostra e observamos uma estimativa \\(\\bar{x}\\), obtemos um intervalo numérico, que chamamos de intervalo de confiança observado. A partir desse ponto, não existem mais quantidades aleatórias na expressão do intervalo, uma vez que, na Inferência Clássica, os parâmetros, por mais que possam ser desconhecidos, são quantidades fixas. Dessa forma, não podemos mais afirmar que um intervalo de confiança observado possui probabilidade \\(1 - \\alpha\\) de conter o verdadeiro valor do parâmetro. Podemos apenas dizer que temos uma confiança considerável de que esse intervalo contém o verdadeiro valor do parâmetro. A medida da nossa confiança é de \\(1 - \\alpha\\) porque, antes de colhermos a amostra, \\(1 - \\alpha\\) era a probabilidade aproximada de que o intervalo aleatório contivesse o verdadeiro valor de \\(\\mu\\).\nComo a distinção entre confiança e probabilidade pode ser difícil de se entender, uma interpretação conveniente para intervalos de confiança é a seguinte: se obtivéssemos várias amostras de mesmo tamanho e, para cada uma delas, calculássemos os correspondentes intervalos de confiança com coeficiente de confiança \\(1 - \\alpha\\), esperaríamos que a proporção de intervalos que contivessem o verdadeiro valor do parâmetro fosse igual a \\(1 - \\alpha\\).\nPor fim, antes de realizarmos um exemplo numérico, podemos fazer algumas considerações a respeito da escolha do valor de \\(\\alpha\\). É possível mostrar que, conforme aumentamos o coeficiente de confiança, a amplitude do intervalo aumenta. Isso, claro, é algo que deveríamos esperar, visto que intervalos maiores possuem naturalmente uma maior chance de conterem um valor desconhecido. Com isso, para que os intervalos sejam o mais informativo possível, mantendo uma confiança elevada, é necessário que selecionemos \\(\\alpha\\) de forma balanceada, sendo uma escolha muito comum o valor 0,05 (nesse caso, temos que o coeficiente de confiança é de 0,95). É muito mais interessante, por exemplo, um intervalo de confiança que diz que o verdadeiro valor do salário médio de um estatístico está entre 3,5 a 6 salários mínimos do que um intervalo que diz que esse valor está entre 2 a 7,5 salários mínimos. Apesar de, com o segundo intervalo, termos uma maior confiança de que o verdeiro valor do salário médio está sendo captado, a qualidade da informação que extraímos dele é consideravelmente pior do que aquela obtida com o primeiro intervalo.\n\n5.3.1.1 Um exemplo numérico\nSubstituindo as letras por números, e continuando o exemplo em que estávamos, vamos, agora, obter, através do R, um intervalo de 95% confiança para \\(\\mu\\). Lembre-se que aqui \\(\\mu\\) é a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021. Como precisamos que nossa amostra seja suficientemente grande para que possamos aplicar o TCL, utilizaremos \\(n = 50\\), uma vez que, observando os histogramas criados na seção de Distribuição Amostral, a distribuição de \\(bar{X}\\) já se aproxima satisfatóriamente bem da distribuição normal a partir desse ponto. Além disso, como a expressão do intervalo de confiança obtido acima leva em consideração que a variância populacional é conhecida, precisaremos, também, dessa informação. Para calculá-la, teremos de multiplicar o resultado da função var() por \\(\\frac{N - 1}{N}\\), sendo \\(N\\) o tamanho da população, uma vez que essa função utiliza \\(N - 1\\) como denominador para o cálculo da variância. Observe o código abaixo.\n\nsigma2 &lt;- var(populacao1) * (length(populacao1) - 1)/length(populacao1)\nsigma2\n\n[1] 45.97659\n\n\nObtendo uma amostra de tamanho \\(n = 50\\) por meio da função sample(), e a armazenando no vetor amostra_ic_media, temos:\n\nset.seed(34)\namostra_ic_media &lt;- sample(x = populacao1, size = 50, replace = TRUE)\nhead(amostra_ic_media, 20)\n\n [1] 26 39 32 35 35 35 31 21 25 40 33 32 19 22 19 30 40 31 38 36\n\n\nNesse próximo passo, criaremos uma função, a qual chamaremos de ic_media_caso1(), que calculará intervalos de confiança para o parâmetro \\(\\mu\\) quando a variância populacional é conhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função possuirá três argumentos: dados, que receberá o vetor de valores observados na amostra; sigma, que receberá o valor do desvio padrão populacional; e alpha, que receberá o valor necessário para se obter o coeficiente de confiança associado ao intervalo, que nesse caso será de 0,05. Utilizaremos a já explicada função qnorm() para encontrar o valor do quantil de ordem \\(1 - \\alpha\\) da normal padrão (não utilizaremos os argumentos mean e sigma dessa função, uma vez que seus valores padrões são, respectivamente, 1 e 0). O limite inferior do intervalo será guardado no vetor limite_inferior, enquanto o superior será guardado em limite_superior. A função retornará um data frame contendo algumas medidas referentes à amostra e o intervalo de confiança propriamente dito. O resultado do processo pode ser visto abaixo.\n\nic_media_caso1 &lt;- function(amostra, sigma, alfa) {\n  media &lt;- mean(amostra)\n  n &lt;- length(amostra)\n  z &lt;- round(qnorm(p = 1 - alfa/2), 3)\n  limite_inferior &lt;- round(media - z * sigma/sqrt(n), 3)\n  limite_superior &lt;- round(media + z * sigma/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n, \n      estimativa_pontual = media,\n      limite_inferior, \n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso1(amostra = amostra_ic_media, sigma = sqrt(sigma2), alfa = 0.05)\n\n   n estimativa_pontual limite_inferior limite_superior amplitude\n1 50              32.16          30.281          34.039     3.758\n\n\nPortanto, um intervalo de 95% de confiança para a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo é de (28,401; 32,159). Intervalos de confiança podem, inclusive, ser utilizados como substitutos para testes de hipóteses, dos quais falaremos no próximo capítulo. Caso quiséssemos, por exemplo, testar a hipótese de que o valor de \\(\\mu\\) é igual a 27, rejeitaríamos a hipótese nula sob um nível de significância de 0,05, uma vez que 27 não está contido no intervalo de 95% de confiança para \\(\\mu\\) obtido acima.\n\n\n\n5.3.2 Intervalos de confiança para a média amostral quando a variância populacional é desconhecida\nConsidere, assim como no exemplo anterior, que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Suponha que queiramos, novamente, estimar o valor de \\(\\mu\\), mas que agora o valor da variância populacional \\(\\sigma^2\\) é desconhecido. Ainda com a restrição de que o tamanho da amostra deve ser suficientemente grande para que possamos aplicar o Teorema Central do Limite, sabemos, como visto anteriormente, que\n\\[\nZ  = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\]\nLembre-se, entretanto, que estamos no caso em que \\(\\sigma^2\\) é desconhecido, e por isso não podemos utilizar a variável acima para construirmos o intervalo de confiança, uma vez que o intervalo encontrado seria função de \\(\\sigma\\). Dessa forma, é intuitivo que em algum momento utilizemos um estimador da variância populacional na expressão do intervalo. Por sorte, existe um conhecido resultado dentro da Estatística que fornecerá uma função que nos seja conveniente. Definindo, primeiramente, o estimador da variância amostral, denotado por \\(S^2\\), como sendo\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})}{n - 1},\n\\] sabemos ser válido que\n\\[\nT = \\frac{\\sqrt{n} \\left(\\bar{X} - \\mu \\right)}{S} \\sim t_{n - 1},\n\\] onde \\(S\\) é a raiz quadrada de \\(S^2\\) e \\(t_{n - 1}\\) representa a distribuição de probabilidade T de Student com \\(n - 1\\) graus de liberdade. A demonstração desse resultado pode ser consultada no post do Observatório que referenciamos no início da seção. A partir daqui, o processo para obtermos a expressão do intervalo de confiança será muito semelhante ao que realizamos no exemplo anterior. Como a distribuição da variável aleatória \\(T\\) é totalmente conhecida, uma vez que, em uma situação prática, o valor do tamanho da amostra \\(n\\) estaria definido, podemos encontrar valores \\(t_{1_{(n-1)}}\\) e \\(t_{2_{(n-1)}}\\), com \\(t_{1_{(n-1)}} &lt; t_{2_{(n-1)}}\\), tais que\n\\[\nP\\left(t_{1_{(n-1)}} &lt; T &lt; t_{2_{(n-1)}} \\right) = 1 - \\alpha. \\qquad \\qquad \\text{(II)}\n\\]\nComo a distribuição T de Student, assim como a distribuição normal padrão, é simétrica em torno de zero, os valores de \\(t_1\\) e \\(t_2\\) que geram a menor amplitude possível para o intervalo de confiança serão dados por\n\\[\nt_{1_{(n-1)}} = -t_{(n - 1;\\;1 -\\alpha/2)} \\text{ e } t_{2_{(n-1)}} = t_{(n - 1;\\;1 -\\alpha/2)}.\n\\]\nEm outras palavras, para encontrarmos os valores desses pontos, basta que calculemos o quantil de ordem \\(1 - \\alpha/2\\) da distribuição T de Student com \\(n - 1\\) graus de liberdade. Voltando à probabilidade definida em \\(\\text{(II)}\\), a atualizando com os resultados obtidos e reescrevendo \\(T\\), temos:\n\\[\nP\\left(t_{1_{(n-1)}} &lt; T &lt; t_{2_{(n-1)}} \\right) = P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; -t_{(n - 1;\\;1 -\\alpha/2)} \\right) = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; t_{(n - 1;\\;1 -\\alpha/2)} \\right) \\\\ &\n= P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} S &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; t_{(n - 1;\\;1 -\\alpha/2)} S\\right) \\\\ & =\nP\\left(-t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ & =\nP\\left(-\\bar{X} + -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ &\n= P\\left(\\bar{X} - t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\]\nPortanto, quando a variância populacional é desconhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança de \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}}; \\bar{X} + t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}} \\right).\n\\]\nAlgo interessante a se observar é que, tanto o intervalo aleatório acima, quanto o intervalo aleatório encontrado no exemplo anterior, possuem probabilidade 1 - \\(\\alpha\\) de conterem o verdadeiro valor do parâmetro \\(\\mu\\), mesmo que suas estruturas sejam diferentes. Entretanto, como na construção do intervalo apresentado no presente exemplo foi utilizado um estimador de \\(\\sigma\\), a amplitude média dos intervalos de confiança obtidos por esse método será maior do que a amplitude média dos intervalos obtidos quando utilizamos o método apresentado no exemplo anterior. Com isso, dizemos que o intervalo de confiança acima é menos informativo, uma vez que o intervalo de valores plausíveis para \\(\\mu\\), nesse caso, será maior que o do intervalo de confiança construído anteriormente. Esse problema é mais evidente para tamanhos pequenos de amostra, e torna-se menos relevante conforme o valor de \\(n\\) aumenta.\n\n5.3.2.1 Um exemplo numérico\nPara demonstrar a diferença entre o intervalo de confiança derivado acima e aquele derivado no exemplo anterior, podemos aproveitar o mesmo exemplo sobre a média das idades das gestantes e puérperas previamente discutido. Novamente, criaremos uma função, a qual chamaremos de ic_media_caso2(), que retornará o intervalo de confiança para a média populacional quando a variância populacional é desconhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função se dará de forma similar àquela criada anteriormente, sendo as únicas diferenças o cálculo do desvio padrão amostral, feito com a função sd(), do pacote básico {stats} e a obtenção do quantil da distribuição T de Student através da função qt(), do mesmo pacote. O resultado do processo pode ser visto abaixo. Lembre-se que o vetor amostra_ic_media foi criado no exemplo anterior.\n\nic_media_caso2 &lt;- function(dados, alfa) {\n  media &lt;- mean(dados)\n  n &lt;- length(dados)\n  S &lt;- round(sd(dados), 3)\n  t &lt;- qt(1 - alfa/2, n - 1)\n  limite_inferior &lt;- round(media - t * S/sqrt(n), 3)\n  limite_superior &lt;- round(media + t * S/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n,\n      S, \n      estimativa_pontual = media,\n      limite_inferior,\n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso2(dados = amostra_ic_media, alfa = 0.05)\n\n   n     S estimativa_pontual limite_inferior limite_superior amplitude\n1 50 6.634              32.16          30.275          34.045      3.77\n\n\nComo podemos perceber, o intervalo de confiança obtido acima apresenta uma amplitude levemente maior do que o intervalo obtido anteriormente (3,77 contra 3,758). Dessa forma, apesar de a diferença não ser gritante, podemos perceber que a utilização de uma estimativa para o valor da variância populacional acarretou, como já explicado anteriormente, em um intervalo levemente menos informativo a respeito do verdadeiro valor da média populacional. Essa diferença na amplitude, entretanto, não é regra: poderíamos ter coletado uma amostra em que a amplitude do intervalo de confiança construído pelo presente método fosse menor. Além disso, como o tamanho da amostra é grande o suficiente, a diferença nas amplitudes dos dois intervalos passa a ser praticamente desconsiderável. Com isso, aqui encerramos a construção de intervalos de confiança para a média de uma população. Partamos, agora, para a última seção desse capítulo, na qual trataremos da obtenção de intervalos de confiança para a proporção populacional.\n\n\n\n5.3.3 Intervalos de confiança para a proporção populacional\nA fazer"
  },
  {
    "objectID": "naosupervisionado.html#métodos-de-agrupamentos",
    "href": "naosupervisionado.html#métodos-de-agrupamentos",
    "title": "9  Análise de Agrupamentos",
    "section": "9.1 Métodos de Agrupamentos",
    "text": "9.1 Métodos de Agrupamentos\nNeste capítulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Além disso, temos os métodos hierárquicos, nos quais os grupos são organizados em uma estrutura de árvore.\nOutra abordagem interessante é considerar a densidade dos pontos no espaço. Nesse caso, procuramos identificar regiões mais densas separadas por áreas menos povoadas. Esses métodos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na análise dos dados.\nTambém existe uma classe de métodos que utiliza técnicas de decomposição espectral. Esses métodos reduzem a dimensionalidade dos dados, preservando as informações relevantes dos grupos presentes. São os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.\nCada uma dessas abordagens possui suas próprias características, vantagens e limitações.\n\n9.1.1 Métodos por Particionamento\nOs métodos por particionamento são comumente utilizados para agrupar dados, onde cada partição representa um cluster. Esses métodos são baseados em distância e envolvem a realocação iterativa das observações entre os clusters para obter um particionamento otimizado.\nA escolha do número de clusters é um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum é o método do cotovelo, que considera a relação entre a variância total intraclusters e o número de grupos criados. O método do cotovelo considera que aumentar o número de clusters reduz a variância, mas em algum ponto, não há melhora significativa na granularidade do agrupamento. Esse ponto ótimo, que indica o número adequado de clusters, é identificado no gráfico por uma curva tracejada.\n\n\n\n\n\nA variância total intraclusters é calculada utilizando as distâncias euclidianas quadráticas entre as observações e o centróide do respectivo grupo. O centróide \\(c_l\\) de um grupo \\(C_l\\) é obtido através da média das observações atribuídas a esse cluster, utilizando a fórmula:\n\\[c_l = \\frac{1}{|\\mathcal{C}_l|} \\sum{i \\in \\mathcal{C}_l} \\mathbf{x}_i\\]\nA variância total intraclusters é calculada como a soma das distâncias euclidianas quadráticas entre as observações e os respectivos centróides, utilizando a fórmula:\n\\[\\sum_{l=1}^{K} \\sum_{i \\in \\mathcal{C}_l} |\\mathbf{x}_i - \\mathbf{c}_l|^2\\]\nEssas são algumas das abordagens dos métodos por particionamento, aqui será considerado o k-médias e o k-medóides com os algorítmos PAM e CLARA, que serão apresentados a seguir com exemplos de aplicações.\n\n9.1.1.1 K-médias\nO K-médias é um método amplamente utilizado para agrupamento de dados. Ele busca encontrar K partições dos dados, minimizando a variância. O algoritmo de (Lloyd 1982) é comumente usado para realizar o K-médias. Ele envolve os seguintes passos:\n\nescolha dos K centróides iniciais;\nparticionamento dos dados com base na menor distância para cada centróide;\natualização dos centróides com as novas observações atribuídas a eles;\nrepetição dos passos 2 e 3 até que não haja mais mudança de agrupamento. É possível definir um número máximo de iterações para otimizar o método computacionalmente.\n\nUma alternativa é o algoritmo de (Hartigan e Wong 1979), que adiciona uma etapa de validação para alterar os agrupamentos. A cada iteração, verifica-se se houve atualização nos centróides dos grupos. Nesse caso, um novo objeto só é atribuído a um cluster se a soma das distâncias quadráticas diminuir.\nNo entanto, o método K-médias apresenta limitações ao lidar com clusters de formas não convencionais ou grupos com tamanhos muito discrepantes. Além disso, ele é sensível a outliers, pois a inclusão de um dado extremo pode influenciar significativamente o valor do centróide. A aplicação para o software R, tanto do método de agrupamento quanto a escolha do número de clusters K pelo método do cotovelo, segue abaixo, será considerado os dados padronizados para retirar qualquer tendência em função da diferença de escala ou amplitude dos dados:\n\nset.seed (1122)\n#BIBLIOTECAS\nlibrary(ggplot2)\n## padronizacao dos dados \n\ndados_norm &lt;- as.data.frame(scale(dados_indicadores[,-c(1:4)]))\n\n## escolhendo k pelo metodo do cotovelo\n\ncotovelo_kmedias &lt;- factoextra::fviz_nbclust(dados_norm ,\n kmeans,\n method = \"wss\") +\n geom_vline( xintercept = 7, linetype = 2) +\n labs(x = \"Numero de Grupos\", y = \"Variancia Total Intragrupo\", title = \"K-medias\")\n\n## ajustando k-medias com o numero de grupos escolhido\n\nk_medias &lt;- kmeans(dados_norm,\n                        centers = 7)\n\nA função kmeans é uma ferramenta poderosa disponível no R para realizar o agrupamento de dados utilizando o método K-médias. A função kmeans retorna três principais objetos:\n\nCluster_centers: É uma matriz que representa os centróides finais de cada cluster. Cada linha dessa matriz representa um centróide, com as coordenadas correspondentes às variáveis do conjunto de dados.\nCluster_assignment: É um vetor que contém as atribuições de cada observação a um determinado cluster. Cada elemento desse vetor representa o número do cluster ao qual a observação foi atribuída. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.\nWithin_cluster_sum_of_squares: É um valor que representa a soma dos quadrados das distâncias de cada observação em relação ao seu respectivo centróide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homogêneo é o cluster.\n\n\n\n9.1.1.2 K-medóides\nEm situações com valores extremos, os algoritmos K-medóides surgem como uma alternativa ao cálculo do centróide, evitando a influência excessiva desses valores na representação central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por (Kaufman e Rousseeuw 2009) considera um custo para as trocas de medóides a cada iteração. O custo é calculado como a diferença da variância total intragrupo considerando um novo medóide (observação não medóide) em comparação com o medóide atual. A variância total intragrupo é uma medida da dispersão dos pontos dentro de um grupo.\nPara realizar o agrupamento, o algoritmo PAM segue os seguintes passos:\n\nEscolha inicial dos \\(K\\) medóides a partir do conjunto de dados;\nAs observações não selecionadas como medóides são atribuídas ao grupo cujo medóide é o mais próximo;\nSelecionar aleatoriamente uma observação não medóide \\(o_r\\);\nCalcular o custo de se mudar o medóide atual para \\(o_r\\);\nCaso o custo seja menor que 0, realizar a troca de medóide;\nRepetir os passos 2 a 5 até que não haja mais mudanças de agrupamento.\n\nO custo de mudança do medóide atual para outra observação é calculado como a diferença da variância total intragrupo considerando a nova observação como representante em comparação com o medóide atual.\nAlém disso, é comum utilizar a medida de distância absoluta no lugar da distância euclidiana quadrática para calcular a distância entre os pontos e os medóides. O método pode ser visto abaixo:\n\n## escolhendo k pelo metodo do cotovelo\ncotovelo_pam &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::pam,\n                      method = \"wss\") +\n                geom_vline( xintercept = 7, linetype = 2) +\n                labs(x = \"Numero de Grupos\",\n                      y = \"Variancia Total Intragrupo\",\n                      title = \"PAM\")\n\npam &lt;- cluster::pam(dados_norm ,\n                      k = 7)\n\nA função cluster::pam no R retorna os seguintes elementos:\n\nmedoids: Um vetor contendo os índices das observações selecionadas como medóides finais de cada cluster.\nclustering: Um vetor contendo os rótulos dos clusters aos quais cada observação foi atribuída.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, incluindo o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\n\nEsses elementos fornecem informações sobre os medóides finais selecionados, a atribuição de clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento e informações adicionais sobre cada cluster.\nPara lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a variância total intragrupo para cada agrupamento gerado. A partição que apresentar menor variância total intragrupo é selecionada como o resultado final do algoritmo. Observe abaixo a aplicação para o R:\n\ncotovelo_clara &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::clara ,\n                      method = \"wss\") +\n                  geom_vline( xintercept = 7, linetype = 2) +\n                  labs(x = \"Numero de Grupos\",\n                        y = \"Variancia Total Intragrupo\",\n                        title = \"CLARA\")\nclara &lt;- cluster::clara(dados_norm ,\n                          k = 7, samples = 10)\n\nA função cluster::clara no R retorna os seguintes resultados:\n\nmedoids: Um objeto pamobject contendo os medóides finais de cada cluster.\nclustering: Um vetor com os rótulos dos clusters atribuídos a cada observação.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, como o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\nsamples: Uma lista contendo os índices das observações selecionadas em cada subamostra.\ncall: A chamada original da função cluster::clara que foi utilizada.\n\nEsses resultados fornecem detalhes sobre os medóides finais escolhidos, a atribuição dos clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento, informações adicionais sobre os clusters, as subamostras utilizadas e a chamada original da função.\n\n\n\n9.1.2 Métodos Hierárquicos\nOs métodos hierárquicos são utilizados para agrupar dados em diferentes níveis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.\nNa abordagem aglomerativa, os grupos são construídos a partir do nível mais baixo, onde cada observação forma um cluster separado, até atingir o nível mais alto, onde todos os dados estão em um único grupo. A fusão dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, também conhecidas como linkages, podem ser definidas da seguinte forma:\n\nMétodo do vizinho mais próximo (Single linkages): Considera a menor distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\min_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo do vizinho mais distante (Complete linkages): Utiliza a maior distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\max_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo da média das distâncias (Average linkages): Calcula a média das distâncias entre as observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\frac{1}{|C_l||C_{l'}|}\\sum_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo do centróide (Centróide linkages): Considera a distância entre os centróides de cada grupo como medida de dissimilaridade.\n\n\\[\nd(C_l,C_{l'}) = d^2(c_l,c_{l'})\n\\]\n\nMétodo de Ward: Minimiza a variância dentro dos grupos ao fundir os clusters que levam à menor variação possível.\n\n\\[\nd(C_l,C_{l'}) = \\frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'})\n\\]\nNo contexto dos métodos hierárquicos de agrupamento, a abordagem aglomerativa é amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada iteração é necessário identificar a melhor divisão do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hierárquico aglomerativo consiste em:\n\nCada observação é inicialmente atribuída a um cluster separado.\nCom base no método de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.\nOs dois grupos com a menor dissimilaridade são fundidos em um único grupo.\nRepetem-se os passos 2 e 3 até que todas as observações estejam em um único grupo.\n\nJá na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo único que contém todas as observações e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observações. O algoritmo DIANA segue os seguintes passos:\n\nTodas as observações são agrupadas em um único grupo.\nA observação com a maior dissimilaridade média em relação aos pontos do mesmo grupo é separada em um novo grupo.\nCada observação do grupo inicial é atribuída ao novo grupo se a dissimilaridade média em relação aos objetos desse grupo for menor do que a dissimilaridade média em relação aos demais pontos do grupo inicial.\n\n4.Calcula-se o diâmetro de todos os grupos (a maior dissimilaridade entre duas observações) e seleciona-se o grupo com o maior diâmetro.\n\nRepetem-se os passos 2 a 4 até que cada observação esteja em um grupo separado.\n\nNo agrupamento hierárquico, a visualização dos clusters é feita por meio de um dendrograma, um gráfico ramificado que mostra as junções e divisões dos clusters. A altura do ramo no primeiro nó do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o número de grupos a partir do dendrograma, busca-se uma grande diferença de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma característica dos métodos hierárquicos é que as decisões de agrupamento ou divisão não são desfeitas, ou seja, não há troca de observações entre os clusters. Decisões de união ou divisão mal feitas podem resultar em grupos de baixa qualidade. Além disso, esses métodos não são bem dimensionados, pois cada decisão de mesclagem ou divisão requer a avaliação de muitos objetos ou clusters. Pelo exemplo abaixo, uma possível resposta de número adequado de clusters seria de dois ou três grupos.\n Os agrupamentos hierárquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das funções hclust e cluster::diana, é obtido um objeto da classe “hclust” e da clase “diana” respectivamente, esses objetos contém informações sobre o agrupamento hierárquico realizado, incluindo a estrutura do dendrograma, as distâncias entre os objetos e outras propriedades relacionadas. Seguido pela seleção, com base no dendrograma, do número K ideal de clusters (Não necessariamente só um K), e o “corte” da árvore aglomerativa no valor ideal identificado. É apresentado abaixo para todos os métodos citados a aplicação para o R, supondo a distância utilizada como a euclidiana calculada no início do capítulo\n\nlibrary(ggdendro)\n#AGLOMERATIVOS ############\n#METODO WARD -----\n##CRIAR O OBJETO HCLUST\nagl_ward &lt;- hclust(dist_euclidian , method = \"ward.D2\")\n\n# PLOT DO DENDROGRAMA \ndendograma_ward &lt;-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,\n main = \"Ward - cortado em H = 20\")\n\n# \"corte\" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS\nagl_ward_res &lt;- cutree(agl_ward , k = 3:8)\n\n# segue o mesmo para todos os outros metodos\n\n#METODO SINGLE LINKAGE -----\nagl_single &lt;- hclust(dist_euclidian , method = \"single\")\n\ndendograma_single &lt;- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,\n                        main = \"Vizinho mais Proximo - cortado em H = 4\",\n                          xlab = \"\")\n\nagl_single_res &lt;- cutree(agl_single , k = 3:8)\n\n#METODO COMPLETE LINKAGE -----\nagl_complete &lt;-  hclust(dist_euclidian , method = \"complete\")\n\ndendograma_complete &lt;- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,\n                             main = \"Vizinho mais Distante - cortado em H = 10\")\n\nagl_complete_res &lt;- cutree(agl_complete , k = 3:8)\n\n#METODO AVARAGE LINKAGE -----\nagl_ave &lt;- hclust(dist_euclidian , method = \"average\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,\n                                main = \" Media das Distancias - cortado em H = 15\")\n\nagl_ave_res &lt;- cutree(agl_ave, k = 3:8)\n\n#METODO CENTROIDE LINKAGE -----\nagl_cent &lt;- hclust(dist_euclidian , method = \"centroid\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,\n                            main = \" Centroide - cortado em H = 15\")\n\nagl_cent_res &lt;- cutree(agl_cent , k = 3:8)\n\n####### DIVISIVO (DIANA) #########\ndiana &lt;- cluster::diana(dist_euclidian ,\n                            diss = TRUE ,\n                            metric = \"euclidean\",\n                            keep.diss = FALSE ,\n                            keep.data = FALSE\n                            )\n\ndendrograma_diana &lt;-  plot(cut(as.dendrogram(diana), h = 7)$upper ,\n                                   main = \"Diana - cortado em H = 7\")\n\ndiana_res &lt;- cutree(diana , k = 3:8)"
  },
  {
    "objectID": "naosupervisionado.html#métodos-de-validação",
    "href": "naosupervisionado.html#métodos-de-validação",
    "title": "9  Análise de Agrupamentos",
    "section": "9.2 Métodos de Validação",
    "text": "9.2 Métodos de Validação\n\n\n\n\nHartigan, John A, e Manchek A Wong. 1979. «Algorithm AS 136: A k-means clustering algorithm». Journal of the royal statistical society. series c (applied statistics) 28 (1): 100–108.\n\n\nKaufman, Leonard, e Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. «Least squares quantization in PCM». IEEE transactions on information theory 28 (2): 129–37."
  },
  {
    "objectID": "tutorialr.html#sobre-o-software-r",
    "href": "tutorialr.html#sobre-o-software-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.1 Sobre o software R",
    "text": "A.1 Sobre o software R\nR é um ambiente computacional e uma linguagem de programação para manipulação, análise e visualização de dados. Para essas finalidades, ele é considerado um dos melhores e um dos mais utilizados dentre os ambientes computacionais disponíveis. O R é mantido pela R Development Core Team e está disponível para diferentes sistemas operacionais: Linux, Mac e Windows.\nO software é livre, ou seja, gratuito, com código aberto em uma linguagem acessível. Nele, estão implementadas muitas metodologias estatísticas. Muitas dessas fazem parte do ambiente base do R e outras acompanham o ambiente sob a forma de pacotes, o que o torna altamente flexível. Os pacotes são bibliotecas com funções extras devidamente documentadas criadas para ajudar a resolver problemas de diferentes áreas do conhecimento.\nO R possui uma comunidade extremamente ativa, engajada desde o aprimoramento de ferramentas e desenvolvimento de novas bibliotecas, até o suporte aos usuários. Sobre o desenvolvimento de novas bibliotecas, um pesquisador em Estatística que desenvolve um novo modelo estatístico pode disponibilizá-lo em um pacote acessível aos usuários que se interessem pelo modelo, por exemplo. Além disso, a disponibilidade e compartilhamento da pesquisa em um pacote no R é uma boa prática quando falamos de reprodutibilidade na ciência. Ainda nesse ponto, realizar as análises de uma pesquisa aplicada em um programa livre e acessível a todos é um dos principais pontos para permitir reprodutibilidade.\nOptar por programar em R também implica na escolha de uma IDE (Integrated Development Environment). Uma IDE é um ambiente de desenvolvimento integrado onde podem ser combinadas ferramentas utilizadas no desenvolvimento de aplicações, como um editor de código ou uma ferramenta de preenchimento inteligente de código. Para o R, a IDE mais popular entre os usuários é o RStudio. O RStudio é um conjunto de ferramentas integradas projetadas para editar e executar os códigos em R. Assim, quando for o interesse utilizar o R, basta abrir o RStudio (R é automaticamente carregado)."
  },
  {
    "objectID": "tutorialr.html#instalação-do-r",
    "href": "tutorialr.html#instalação-do-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.2 Instalação do R",
    "text": "A.2 Instalação do R\nA seguir, será apresentado o passo a passo de como instalar o R e o RStudio para os três sistemas operacionais: Windows, MAC e Linux, respectivamente.\n\nA.2.1 R no Windows\nA forma mais simples de instalar o R consiste em primeiramente acessar a página do software pelo endereço https://cloud.r-project.org/. Ao acessar a página haverão três opções para download, sendo cada uma referente a um sistema operacional em específico. Assim, para conseguir instalar o software em um sistema operacional Windows basta primeiramente clicar no link Download R for Windows.\n\n\n\n\n\nPasso 1\n\n\n\n\nQuatro subdiretórios irão surgir, dentre eles é necessário clicar na base, pois este contém a distribuição base do R para instalação.\n\n\n\n\n\nPasso 2\n\n\n\n\nO subdiretório base irá redirecionar para uma página que contém o link de download do arquivo de instalação do software. Este por sua vez, pode ser identificado como Download + versão atual do R + for Windows.\n\n\n\n\n\nPasso 3\n\n\n\n\nFeito isso, um arquivo executável será baixado no computador, o qual, ao abri-lo, deverá escolher o idioma (português brasileiro) e simplesmente clicar em Avançar toda vez que o cliente de instalação requerer.\n\n\n\n\n\nPasso 4\n\n\n\n\n\n\n\n\n\nPasso 5\n\n\n\n\nAssim, uma instalação padrão do software será instalada no computador.\n\n\nA.2.2 R no MAC\nDa mesma forma a qual iniciamos a instalação do R no Windows também iniciaremos no MAC, onde é necessário acessar o endereço https://cloud.r-project.org/ e clicar no link Download R for macOS.\n\n\n\n\n\nPasso 1\n\n\n\n\nO link irá redirecionar para uma página com arquivos de extensão .pkg típicos de macOS. É importante verificar qual versão disponível é a ideal para seu sistema. A versão do tipo arm64.pkg é referente a versão mais recente do macOS na data deste material.\n\n\n\n\n\nPasso 2\n\n\n\n\nTendo feito o download do arquivo, basta abri-lo para um cliente de instalação ficar disponível, e então, para efetuar uma instalação padrão deve-se seguir as instruções do cliente sem customizações aditivas assim como foi feito para o Windows.\n\n\nA.2.3 R no Linux\nA instalação do R no Linux depende da distribuição sendo utilizada. Basta acessar o mesmo endereço https://cloud.r-project.org/ utilizado na instalação dos outros sistemas, e clicar no link Download R for Linux.\n\n\n\n\n\nPasso 1\n\n\n\n\nFeito isso irá aparecer as opções de distribuições para Linux em que o software está disponível para download, basta selecionar a distribuição compatível. Caso sua distribuição for Ubuntu por exemplo, clicamos nela no respectivo link.\n\n\n\n\n\nPasso 2\n\n\n\n\nAssim, irá ser redirecionado para uma página com as devidas instruções de instalação do R para a distribuição escolhida. Basta seguir as instruções para efetuar uma instalação padrão do software."
  },
  {
    "objectID": "tutorialr.html#instalação-do-rstudio",
    "href": "tutorialr.html#instalação-do-rstudio",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.3 Instalação do RStudio",
    "text": "A.3 Instalação do RStudio\nO RStudio é um conjunto de ferramentas integradas projetadas (IDE - Integrated Development Environment) da linguagem R para auxiliar na produtividade ao utilizar o R. Embora não seja obrigatório o seu uso, é um consenso na comunidade de que o uso do RStudio facilita o aprendizado enquanto acelera a produtividade do usuário, tornando-o indispensável principalmente para iniciantes.\nNo ano de 2022, RStudio iniciou um processo de transição de nome onde passou a se chamar Posit. O objetivo por de trás desse processo se dá na inclusão da comunidade de Python ao R, dado o crescimento notório do Python na área de análise de dados nos últimos anos e que ambas as linguagens se complementam.\nO primeiro passo para instalar o RStudio é acessar o site da Posit e ir até a página de download que pode ser acessada pelo endereço https://posit.co/download/rstudio-desktop/. Feito isso, a página irá apresentar algumas opções, dentre elas uma breve tabela com arquivos executáveis mais recentes disponíveis de instalação do RStudio.\n\n\n\n\n\nArquivos executáveis de instalação\n\n\n\n\nDentre os arquivos executáveis está a versão mais recente para Windows (retângulo vermelho), macOS (retângulo azul) e para diferentes distribuições do Linux (retângulo verde). É preciso fazer o download conforme o seu sistema operacional.\nApós o download basta abrir o arquivo executável baixado e seguir as instruções do cliente para que a instalação seja feita.\n\n\n\n\n\nRStudio aberto pela primeira vez"
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-rstudio",
    "href": "tutorialr.html#primeiros-passos-no-rstudio",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.4 Primeiros passos no RStudio",
    "text": "A.4 Primeiros passos no RStudio\nO RStudio é uma ferramenta que por padrão é dividida em quatro painéis, sendo que cada um deles contêm abas com diferentes utilidades.\n\n\n\n\n\nPainéis do RStudio\n\n\n\n\nA seguir descrevemos melhor os painéis e algumas abas comumente utilizadas do RStudio:\n Editor/Scripts: local para escrever códigos (principalmente arquivos em formato .R).\n Console: onde se executa os códigos e visualiza resultados.\n Aqui, é possível acessar todos os objetos criados em Environment e o histórico de códigos executados em History e conectar fonte de dados em Connections.\n Nessa área, temos diversas utilidades frequentemente utilizadas:\n\npodemos acessar arquivos e pastas do computador pela aba Files;\nna aba Plots, visualizamos resultados em que são gerados figuras (como gráficos e tabelas), caso um comando desse tipo tenha sido executado;\nem Packages, podemos manusear pacotes (instalar, atualizar ou deletar);\nna aba Help temos acesso à documentação de uma determinada função quando utilizado o comando help() ou ?. Uma função nada mais é do que uma estrutura de código pronta com a forma de acesso nome(argumento) que recebe argumentos de entrada e retorna uma resposta. O próprio comando help() é uma função.\n\nO usuário pode alterar as configurações padrões do RStudio ao acessar as opções globais.\n\n\n\n\n\nOpções globais\n\n\n\n\nPara usuários iniciantes, é recomendável configurar a aparência e estrutura (layout) dos painéis conforme a própria preferência para tornar a experiência de uso mais confortável.\n\n\n\n\n\nMenu de aparência\n\n\n\n\nPodemos alterar o layout pelo menu Panel Layout. Usualmente, os painéis são estruturados de forma que o painel Console fique ao lado do painel de Script (Source/Editor), facilitando a visualização dos comandos rodados.\n\n\n\n\n\nMenu de estruturação dos painéis\n\n\n\n\n\nA.4.1 Projetos\nUma funcionalidade importante é a criação de projetos, permitindo dividir o trabalho em múltiplos ambientes, cada um com o seu diretório, documentos e workspace.\nPara criar um projeto, os seguintes passos podem ser seguidos:\n\nClique na opção File do menu, e então em New Project.\nClique em New Directory.\nClique em New Project.\nEscreva o nome do diretório (pasta) onde deseja manter seu projeto, exemplo: “my_project”.\nClique no botão Create Project.\n\nPara criar um novo script para escrever os códigos, vá em File -&gt; New File -&gt; R Script.\n\n\nA.4.2 Boas práticas\nComente bem o seu código: é possível fazer comentários usando o símbolo #. É sempre bom explicar o que uma variável armazena, o que uma função faz, por que alguns parâmetros são passados para uma determinada função, qual é o objetivo de um trecho de código, etc.\nEvite linhas de código muito longas: usar linhas de código mais curtas ajuda na leitura do código.\nEscreva um código organizado. Por exemplo, adote um padrão no uso de minúsculas e maiúsculas, uma lógica única na organização de pastas e arquivos, pode ser adotada uma breve descrição (como comentário) indicando o que um determinado script faz.\nCarregue todos os pacotes que irá usar sempre no início do arquivo: quando alguém abrir o seu código será fácil identificar quais são os pacotes que devem ser instalados e quais dependências podem existir."
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-r",
    "href": "tutorialr.html#primeiros-passos-no-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.5 Primeiros passos no R",
    "text": "A.5 Primeiros passos no R\nO código pode ser escrito no Script e então ser executado ao apertar o botão Run (localizado no painel de Script) ou com o atalho no teclado Ctrl + Enter. É importante salientar que, apenas a linha em que o símbolo de inserção de código (barra vertical do cursor) estiver é que será executada. Para executar múltiplas linhas simultaneamente, é necessário selecionar as linhas desejadas e então utilizar o comando de execução mencionado.\nOutra forma de escrever e executar códigos é através do painel Console. Normalmente, o Console é utilizado para executar códigos sem muitas linhas de estruturação ou para fazer testes rápidos (ex: uso do R como calculadora). Para rodar o código diretamente pelo painel Console, basta escrevê-lo na linha em que contém o símbolo &gt;, o qual indica que o R está pronto para receber comandos, e então pressionar a tecla Enter.\n\nA.5.1 R como calculadora\nUma das utilidades do R é utilizá-lo como uma calculadora, onde podemos realizar contas matemáticas simples até as mais complexas.\nPor padrão, o R entende as linhas de códigos da esquerda para a direita e de cima para baixo. No entanto, ao se deparar com operações matemáticas, ele respeita algumas prioridades. A operação com maior para a menor prioridade é: potenciação &gt; multiplicação ou divisão &gt; adição ou subtração. Caso haja a necessidade de alterar essa ordem, isso pode ser feito utilizando parênteses.\n\n# Adição.\n10 + 15\n\n[1] 25\n\n# Subtração.\n10 - 2\n\n[1] 8\n\n# Multiplicação.\n2 * 10\n\n[1] 20\n\n# Divisão.\n30/2\n\n[1] 15\n\n# Raiz quadrada.\nsqrt(4)\n\n[1] 2\n\n# Potência.\n2^2\n\n[1] 4\n\n# Potência &gt; Multiplicação &gt; Soma.\n2^2 + 5 * 2\n\n[1] 14\n\n# Multiplicação &gt; Potência &gt; Soma.\n2^2 + (5 * 2)\n\n[1] 14\n\n# Potência &gt; Soma &gt; Multiplicação.\n2 * (2^2 + 5) \n\n[1] 18\n\n\nCaso um comando incompleto seja dado, como 10 ^, o R mostrará um +. Isso não tem a ver com a soma e apenas que o R está esperando que o comando que estava sendo escrito seja finalizado. Para recomeçar, basta terminar a escrita do comando ou apenas pressionar Esc.\nVale também ressaltar que se um comando que o R não reconhece for dado, ele retornará uma mensagem de erro.\n\n\nA.5.2 Atribuição\nOs objetos (também chamados de variáveis) são “locais” onde são guardadas informações (números, textos etc). O ato de “guardar” informações dentro de objetos é chamado de atribuição, e pode ser feito com &lt;- ou =. Embora ambas as formas funcionem, na prática, o sinal &lt;- é usualmente utilizado para atribuições enquanto que o sinal = é utilizado para configurar argumentos de funções.\n\n# Variável x recebe o número 5 de diferentes formas.\nx &lt;- 5 \n\nx = 5\n\ny = (2^2 + 6) - 4\nx &lt;- y - 1\n\nUm ponto importante a se atentar é que o R é case sensitive, isto é, faz a diferenciação entre as letras minúsculas e maiúsculas. Portanto, x é diferente de X.\n\n# Dica: Podemos obter o output do comando ao colocá-lo em volta de ().\n(x &lt;- 10/2)\n\n[1] 5\n\n# Ao chamar X obteremos um erro, pois a variável criada era minúscula.\nX\n\nError in eval(expr, envir, enclos): objeto 'X' não encontrado\n\n\n\n\nA.5.3 Objetos em R\nExistem cinco classes básicas de objetos no R:\n\nCharacter: “UAH!”\nNumeric: 0.95 (números reais)\nInteger: 100515 (inteiros)\nComplex: 2 + 5i (números complexos, a + bi)\nLogical: TRUE (booleanos, TRUE/FALSE)\n\nApós realizar a atribuição, podemos verificar a classe do objeto com a função class().\n\n# Character/texto, deve estar entre aspas \"\".\nx &lt;- \"gestante\"; \nclass(x) \n\n[1] \"character\"\n\n# Numeric/números reais.\nx &lt;- 0.9 \nclass(x) \n\n[1] \"numeric\"\n\n# Integer/números inteiros, tem que ser atribuído com o valor acompanhado de um ‘L’.\nx &lt;- 5L\nclass(x)\n\n[1] \"integer\"\n\n# Complex/números complexos.\nx &lt;- 2 + 5i\nclass(x)\n\n[1] \"complex\"\n\n# logical/valores lógicos.\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n\nOs valores lógicos são apresentados em letra maiúscula. Isso é muito importante, pois o R diferencia letras maiúsculas de minúsculas. Então, valores lógicos só são reconhecidos se escritos como TRUE ou FALSE. Além disso, cada valor lógico assume um valor numérico, sendo TRUE referente ao valor 1 e FALSE referente ao valor 0.\n\n# Operações matemáticas com valores lógicos.\n(TRUE*2)^2 + TRUE + FALSE + 2*TRUE\n\n[1] 7\n\n\nMuitas vezes é do interesse do usuário apagar objetos que foram criados, principalmente se for rodar códigos prontos em um ambiente que outra pessoa estava trabalhando, pois pode haver objetos já criados com os mesmos nomes dos que se encontram no código/script de interesse, o que poderá levar a erros e dificuldades de execução. A remoção de objetos pode ser feito com a função rm() ou remove().\n\n# Criando o objeto x.\nx &lt;- 20\nx\n\n[1] 20\n\n# Removendo o objeto x.\nrm(x)\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\n# Removendo todos os objetos criados.\n(x &lt;- 1)\n\n[1] 1\n\n(y &lt;- 2)\n\n[1] 2\n\nrm(list=ls())\n\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\ny\n\nError in eval(expr, envir, enclos): objeto 'y' não encontrado\n\n\nVale notar que ao utilizar a função rm() ou a função remove() para remover todos os objetos criados, é necessário incluir um argumento chamado list onde utilizamos o sinal de = para especificar os objetos a serem deletados. A função ls() lista todos os objetos criados até o momento.\n\n\nA.5.4 Vetores\nNo R a estrutura mais básica de dados é chamada de Vector (vetor), podendo aparecer no formado Atomic (atômico) ou no formado de list (lista). Dentre os vetores atômicos existem quatro tipos, sendo eles: Character, Integer, Double e Logical.\n\nCom vetores podemos atribuir vários valores a um mesmo objeto. Para entrar com vários números (ou nomes, ou qualquer outro grupo de coisas), precisamos usar uma função para dizer ao programa que os valores serão combinados em um único vetor. Para criar vetores atômicos a função c() é a mais usual por podermos criar vetores atômicos de todos os tipos diretamente. Também podemos utilizar a função seq() e o símbolo : para criar vetores do tipo Integer, e a função rep() que é capaz de criar vetores Double, por exemplo. Além disso, podemos verificar o tipo do vetor com a função typeof().\n\n# Vetor Double com a função c().\n(vetor1 &lt;- c(2.5, 3, 4/5))\n\n[1] 2.5 3.0 0.8\n\ntypeof(vetor1)\n\n[1] \"double\"\n\n# Vetor Integer com a função c().\n(vetor2 &lt;- c(5L, 7L, 9L))\n\n[1] 5 7 9\n\ntypeof(vetor2)\n\n[1] \"integer\"\n\n# Vetor Character com a função c().\n(vetor3 &lt;- c(\"hospital1\", \"hospital2\"))\n\n[1] \"hospital1\" \"hospital2\"\n\ntypeof(vetor3)\n\n[1] \"character\"\n\n# Vetor Logical com a função c().\n(vetor4 &lt;- c(TRUE, FALSE, FALSE, TRUE))\n\n[1]  TRUE FALSE FALSE  TRUE\n\ntypeof(vetor4)\n\n[1] \"logical\"\n\n# Vetor Integer com a função seq().\n(vetor5 &lt;- seq(1, 5))\n\n[1] 1 2 3 4 5\n\ntypeof(vetor5)\n\n[1] \"integer\"\n\n# Vetor Integer com o símbolo :.\n(vetor6 &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ntypeof(vetor6)\n\n[1] \"integer\"\n\n# Vetor Double com a função rep(). \n(vetor7 &lt;- rep(1,10))\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\ntypeof(vetor7)\n\n[1] \"double\"\n\n\nÉ comum o usuário querer saber o tamanho do vetor que ele está trabalhando, isso pode ser feito com a função length(). Além disso, é importante ter certeza de que estamos trabalhando com um vetor atômico, o que pode ser verificado com a função is.vector().\n\n# Podemos construir um vetor com vetores dentro da função c().\n(vetor &lt;- c(c(1, 2), rep(1, 2), seq(1, 2), 1:2))\n\n[1] 1 2 1 1 1 2 1 2\n\nis.vector(vetor)\n\n[1] TRUE\n\ntypeof(vetor)\n\n[1] \"double\"\n\nlength(vetor)\n\n[1] 8\n\n\nObserve que é possível criar um vetor com elementos de diferentes tipos. Sabemos que a função rep() gera um vetor de tipo Double e a seq() gera um vetor de tipo Integer, e ao criar um vetor utilizando a função c() em conjunto com estas obtemos um vetor de tipo Double, de forma que o R priorizou este tipo ao invés do Integer. No R isso é chamado de coerção, onde o vetor sendo criado irá manter o tipo de maior prioridade dentre os seus elementos, e os elementos de tipos com menor prioridade serão convertidos para o tipo prioritário. Isso ocorre, pois todos os elementos de um vetor atômico devem ter o mesmo tipo. Para os tipos apresentados temos como o de menor prioridade para o maior: Logical &lt; Integer &lt; Double &lt; Character. Além disso, se considerarmos Complex e List, teremos List com maior prioridade seguido de Character e Complex.\nPode ser do interesse do usuário visualizar elementos específicos que existem dentro de um vetor, isso pode ser feito ao especificar a posição do elemento dentro do vetor entre os símbolos [].\n\n# vetor com varios elementos.\nvet &lt;- c(TRUE, 5, 7L, \"hospital\")\ntypeof(vet)\n\n[1] \"character\"\n\n# elemento de posição 3.\nvet[3]\n\n[1] \"7\"\n\n# elementos das posições 2, 3 e 4.\nvet[2:4]\n\n[1] \"5\"        \"7\"        \"hospital\"\n\n\nAs operações vetoriais podem ser realizadas de maneira bastante intuitiva, pois em vetores atômicos as operações são realizadas elemento a elemento.\n\n# Operações com vetores.\nvetor1 &lt;- c(4, 9, 16)\n(vetor1_menos1 &lt;- vetor1 - 1)\n\n[1]  3  8 15\n\n(vetor1_vezes2 &lt;- vetor1 * 2)\n\n[1]  8 18 32\n\n(vetor1_dividido2 &lt;- vetor1/2)\n\n[1] 2.0 4.5 8.0\n\n(vetor1_raiz &lt;- sqrt(vetor1))\n\n[1] 2 3 4\n\nvetor2 &lt;- c(1, 2, 3)\n(vetor1_mais_vetor2 &lt;- vetor1 + vetor2)\n\n[1]  5 11 19\n\n\nVamos agora considerar vetores de pesos (quilos) e alturas (metros) de 6 pessoas.\n\n# Vetores de peso e de quilo.\n(peso &lt;- c(62, 70, 52, 98, 90, 70))\n\n[1] 62 70 52 98 90 70\n\n(altura &lt;- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61))\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Obs: note que o separador decimal do R é um . (ponto).\n\nPodemos a partir dessas informações calcular o IMC. Vale lembrar que o IMC é dado pelo peso (em kg) dividido pela altura (em metros) ao quadrado.\n\n(imc &lt;- peso/(altura^2))\n\n[1] 21.45329 21.13271 16.97959 26.03890 26.58318 27.00513\n\n\nÉ importante saber que, no R, vetores são a base dos demais objetos. Objetos com apenas um elemento, por exemplo, não são considerados escalares, mas vetores de tamanho um. Em outras palavras, os próprios elementos de um vetor são também vetores.\n\nelemento1 &lt;- \"\"\nis.vector(elemento1)\n\n[1] TRUE\n\nlength(elemento1)\n\n[1] 1\n\nelemento2 &lt;- 5\nis.vector(elemento2)\n\n[1] TRUE\n\nlength(elemento2)\n\n[1] 1\n\nelemento3 &lt;- TRUE\nis.vector(elemento3)\n\n[1] TRUE\n\nlength(elemento3)\n\n[1] 1\n\n\nAlém dos vetores de formato atômico também existem os de formado lista, que diferente dos atômicos, as listas podem ter elementos de tipos diferentes de forma que não há necessidade do R efetuar coerções. Para criar listas no R podemos utilizar a função list().\n\n# Lista com vários tipos de elementos (inclusive listas).\n(lista &lt;- list(5, \"hospital\", list(1:5), c(rep(1, 2)), seq(1, 2)))\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] \"hospital\"\n\n[[3]]\n[[3]][[1]]\n[1] 1 2 3 4 5\n\n\n[[4]]\n[1] 1 1\n\n[[5]]\n[1] 1 2\n\nis.vector(lista)\n\n[1] TRUE\n\ntypeof(lista)\n\n[1] \"list\"\n\nlength(lista)\n\n[1] 5\n\n# Dica: podemos verificar a estrutura de qualquer objeto com a função str().\nstr(lista)\n\nList of 5\n $ : num 5\n $ : chr \"hospital\"\n $ :List of 1\n  ..$ : int [1:5] 1 2 3 4 5\n $ : num [1:2] 1 1\n $ : int [1:2] 1 2\n\n# Dica: podemos retornar uma lista para vetor atômico com a função unlist().\nunlist(lista)\n\n [1] \"5\"        \"hospital\" \"1\"        \"2\"        \"3\"        \"4\"       \n [7] \"5\"        \"1\"        \"1\"        \"1\"        \"2\"       \n\n\n\n\nA.5.5 Matrizes\nMatrizes são vetores numéricos com duas dimensões, sendo estas a linha e a coluna às quais o elemento pertence. No R podemos criar matrizes com a função matrix().\n\n# Criando uma matriz de 16 elementos com 4 linhas e 4 colunas.\n(matri &lt;- matrix(seq(1,16), nrow = 4, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nstr(matri)\n\n int [1:4, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Podemos verificar se é uma matriz com a função is.matrix().\nis.matrix(matri)\n\n[1] TRUE\n\n\nNote que os números de 1 a 16 foram dispostos na matriz coluna por coluna, ou seja, preenchendo de cima para baixo e depois da esquerda para a direita. Isso ocorre por padrão, pois a função matrix() possui um argumento chamado byrow = FALSE em que, para criar uma matriz em que é preenchida de elementos por linha, basta alterar o argumento para byrow = TRUE. Além disso, a função seq() está gerando os elementos da matriz enquanto o argumento nrow indica o número de linhas e ncol o número de colunas.\nPara visualizar um elemento específico de uma matriz podemos utilizar o mesmo método que usamos com vetores. Lembrando que matrizes ainda são vetores, porém, com uma dimensão a mais. Então, para visualizar um elemento específico devemos indicar a posição do elemento para todas as dimensões existentes, no caso das matrizes, para linha e coluna.\n\n# Obtendo linhas, colunas e elementos específicos.\nmatri[3,  ]   # seleciona a 3ª linha.\n\n[1]  3  7 11 15\n\nmatri[ , 2]   # seleciona a 2ª coluna.\n\n[1] 5 6 7 8\n\nmatri[1, 2]   # seleciona o elemento da primeira linha e segunda coluna.\n\n[1] 5\n\n\nPerceba que cada linha e cada coluna de uma matriz é um vetor (uma dimensão). Assim, podemos alterar uma linha ou uma coluna atribuindo um vetor de interesse, por exemplo.\n\n# substituindo a primeira linha e quarta coluna da matriz.\nmatri[1, ] &lt;- c(9, 9, 9, 9)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    9\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nmatri[, 4] &lt;- rep(1, 4)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    1\n[2,]    2    6   10    1\n[3,]    3    7   11    1\n[4,]    4    8   12    1\n\n\nÉ de importância para o usuário verificar o tamanho (número de elementos) quando se trata de vetores. Porém, quando se trata de matrizes, é importante conhecer as dimensões além do número de elementos. Para verificar as dimensões de uma matriz podemos utilizar a função dim(), enquanto para o tamanho (número de elementos) ainda podemos utilizar a função length().\n\n# Verificando o tamanho e dimensões da matriz.\nlength(matri)\n\n[1] 16\n\ndim(matri)\n\n[1] 4 4\n\n\nComo sabemos que as linhas e colunas de uma matriz são vetores, podemos adicionar mais linhas e colunas a ela com os elementos que queremos. Para concatenar linhas e colunas em uma matriz podemos utilizar as funções rbind() e cbind() respectivamente.\n\nvet1 &lt;- c(99, 98, 97, 95)\nvet2 &lt;- c(0, 5, 7, 9, 99) \n(matri &lt;- rbind(matri, vet1))\n\n     [,1] [,2] [,3] [,4]\n        9    9    9    1\n        2    6   10    1\n        3    7   11    1\n        4    8   12    1\nvet1   99   98   97   95\n\n(matri &lt;- cbind(matri, vet2))\n\n                 vet2\n      9  9  9  1    0\n      2  6 10  1    5\n      3  7 11  1    7\n      4  8 12  1    9\nvet1 99 98 97 95   99\n\n\nOperações matemáticas entre matrizes e elementos são realizadas elemento a elemento assim como vetores. Porém, quando se trata de matrizes, é de interesse efetuar a multiplicação matricial clássica, o que pode ser feito com a operação %*% respeitando a equidade do número de colunas da matriz que pré-multiplica e o número de linhas da matriz que pós-multiplica.\n\n# Criando duas matrizes 2x2 (duas linhas e duas colunas).\n(matriz1 &lt;- matrix(c(rep(1, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    2\n\n(matriz2 &lt;- matrix(c(rep(2, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n# Soma duas matrizes (elemento a elemento).\nmatriz1 + matriz2\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    3    4\n\n# Subtrai duas matrizes (elemento a elemento).\nmatriz1 - matriz2\n\n     [,1] [,2]\n[1,]   -1    0\n[2,]   -1    0\n\n# Divide duas matrizes (elemento a elemento).\nmatriz1/matriz2\n\n     [,1] [,2]\n[1,]  0.5    1\n[2,]  0.5    1\n\n# Multiplica duas matrizes (elemento a elemento).\nmatriz1 * matriz2\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    2    4\n\n# Multiplicação matricial clássica.\nmatriz1 %*% matriz2\n\n     [,1] [,2]\n[1,]    6    6\n[2,]    6    6\n\n# Potência de uma matriz (elemento a elemento).\n(matriz3 &lt;- matriz2^2)\n\n     [,1] [,2]\n[1,]    4    4\n[2,]    4    4\n\n# Raiz quadrada de uma matriz (elemento a elemento).\nsqrt(matriz3)\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nA.5.6 Fatores\nÉ muito comum termos que lidar com variáveis categóricas, ou seja, variáveis que possuem categorias intrínsecas em sua natureza. No R, existe uma classe de objetos chamada Fatores especificamente para representar esse tipo de variável (nominal e ordinal). Os fatores podem ser vistos como vetores de elementos numéricos inteiros (pois são assim internamente representados no R) e possuem rótulos (labels). Consequentemente, são vetores do tipo Double.\n\n# Criando um vetor/variável com a informação do sexo de 7 pessoas. \n(sexo1 &lt;- c(\"Mulher\", \"Homem\", \"Homem\", \"Mulher\", \"Mulher\", \"Mulher\", \"Homem\"))\n\n[1] \"Mulher\" \"Homem\"  \"Homem\"  \"Mulher\" \"Mulher\" \"Mulher\" \"Homem\" \n\n# Verificando a classe da variável sexo1.\nclass(sexo1)\n\n[1] \"character\"\n\n# Transformando em fator.\n(sexo2 &lt;- as.factor(sexo1))\n\n[1] Mulher Homem  Homem  Mulher Mulher Mulher Homem \nLevels: Homem Mulher\n\nclass(sexo2)\n\n[1] \"factor\"\n\n# Verificando os levels da variável de classe factor (sexo2).\nlevels(sexo2)\n\n[1] \"Homem\"  \"Mulher\"\n\n\nPodemos verificar que a variável é representada internamente por elementos numéricos inteiros ao tentar transformá-la em um vetor numérico com a função as.numeric().\n\n# Ao transformar sexo1 obteremos um vetor de dados faltantes (NA) por coerção.\nas.numeric(sexo1)\n\nWarning: NAs introduzidos por coerção\n\n\n[1] NA NA NA NA NA NA NA\n\n# Ao transformar sexo2 obteremos um vetor double com valores inteiros.\n(sexo2_num &lt;- as.numeric(sexo2))\n\n[1] 2 1 1 2 2 2 1\n\ntypeof(sexo2_num)\n\n[1] \"double\"\n\n\nFatores possuem levels em ordem alfabética, e isso pode influenciar diretamente na hora de construir gráficos e realizar aplicações de modelos.\n\n\nA.5.7 Data Frame\nTrata-se de uma “tabela de dados” onde as colunas são as variáveis e as linhas são os registros, e as colunas podem ser de classes diferentes. Logo, a principal diferença entre data frame e matriz é que matrizes só podem conter elementos da mesma classe.\nPara criar data frame no R é utilizado a função data.frame().\n\n# Colunas/variáveis para o data frame.\nID &lt;- seq(1,6)\npes &lt;- c(62, 70, 52, 98, 90, 70)\nalt &lt;- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61)\nimc &lt;- pes/(alt^2)\n\n# Criando o data frame.\n(dados &lt;- data.frame(ID = ID, peso = pes, altura = alt, imc = imc))\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nPodemos pensar na estrutura de um data frame da mesma forma que de uma matriz. Se por acaso for do interesse olhar os dados de altura, por exemplo, basta acessar a coluna três do data frame.\n\n# Selecionando a variável \"altura\".\ndados[, 3]\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n\nEmbora possamos usar os mesmos métodos discutidos na seção de matrizes, quando se trata de data frames, usualmente selecionamos as variáveis de interesse sem ter que saber em qual coluna ela está. Isso pode ser feito ao utilizar o símbolo $, dessa forma a coluna será selecionada em forma de vetor.\n\n# Selecionando a variável \"altura\".\ndados$altura\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Dica: também é possível fazer a seleção de colunas da seguinte forma:\ndados[, c(\"altura\", \"peso\")]\n\n  altura peso\n1   1.70   62\n2   1.82   70\n3   1.75   52\n4   1.94   98\n5   1.84   90\n6   1.61   70\n\n\nUtilizando o mesmo símbolo podemos adicionar ou deletar colunas.\n\n# Adicionando a variável \"grupo\".\ngr &lt;- c(rep(1,3),rep(2,3))\ndados$grupo &lt;- gr\ndados\n\n  ID peso altura      imc grupo\n1  1   62   1.70 21.45329     1\n2  2   70   1.82 21.13271     1\n3  3   52   1.75 16.97959     1\n4  4   98   1.94 26.03890     2\n5  5   90   1.84 26.58318     2\n6  6   70   1.61 27.00513     2\n\n# Deletando a variável \"grupo\".\ndados$grupo &lt;- NULL\ndados\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nNote que ao adicionar variáveis a um data frame essa variável tem que ter o mesmo número de elementos que as demais variáveis, caso isso não seja respeitado o R ira retornar um erro.\nA estrutura de data frame é provavelmente a mais utilizada no dia a dia de quem analisa dados. Sabendo disso, existem algumas funções que são importantes de um usuário de R ter em mente.\n\nhead() - Mostra as primeiras 6 linhas.\ntail() - Mostra as últimas 6 linhas.\ndim() - Número de linhas e de colunas.\nnames() - Os nomes das colunas (variáveis).\nstr() - Estrutura do data frame. Mostra, entre outras coisas, a classe de cada coluna.\n\nAlgumas dessas funções já foram abordadas ao longo do texto. As funções de visualização head() e tail() possuem um argumento chamado n o qual podemos customizar o número de linhas que queremos visualizar.\n\nhead(dados, n = 4)\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n\ntail(dados, n = 4)\n\n  ID peso altura      imc\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\ndim(dados)\n\n[1] 6 4\n\nnames(dados)\n\n[1] \"ID\"     \"peso\"   \"altura\" \"imc\"   \n\nstr(dados)\n\n'data.frame':   6 obs. of  4 variables:\n $ ID    : int  1 2 3 4 5 6\n $ peso  : num  62 70 52 98 90 70\n $ altura: num  1.7 1.82 1.75 1.94 1.84 1.61\n $ imc   : num  21.5 21.1 17 26 26.6 ...\n\n\nCada coluna do data frame pode ser interpretada como um vetor. Dessa forma, as operações de vetores discutidas anteriormente são válidas.\n\n# Cria uma coluna do produto de peso por altura.\ndados$pesovezesaltura &lt;- dados$peso * dados$altura\ndados\n\n  ID peso altura      imc pesovezesaltura\n1  1   62   1.70 21.45329          105.40\n2  2   70   1.82 21.13271          127.40\n3  3   52   1.75 16.97959           91.00\n4  4   98   1.94 26.03890          190.12\n5  5   90   1.84 26.58318          165.60\n6  6   70   1.61 27.00513          112.70\n\n# Cria uma coluna de peso + cinco.\ndados$peso5 &lt;- dados$peso + 5\ndados\n\n  ID peso altura      imc pesovezesaltura peso5\n1  1   62   1.70 21.45329          105.40    67\n2  2   70   1.82 21.13271          127.40    75\n3  3   52   1.75 16.97959           91.00    57\n4  4   98   1.94 26.03890          190.12   103\n5  5   90   1.84 26.58318          165.60    95\n6  6   70   1.61 27.00513          112.70    75\n\n# Cria uma coluna da metade do peso original.\ndados$pesometade &lt;-  dados$peso/2\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n\n\n\nA.5.8 Operadores lógicos\nSabemos que TRUE e FALSE são objetos que pertencem à classe logical, além de terem representação numérica de 1 e 0 respectivamente.\nA operação lógica nada mais é do que um teste que retorna verdadeiro (TRUE) ou falso (FALSE). Assim, podemos realizar comparações entre valores utilizando alguns operadores específicos.\n\n# Verifica se 9 é igual a 12.\n9 == 12\n\n[1] FALSE\n\n# Verifica se 12 é igual a 12.\n12 == 12\n\n[1] TRUE\n\n# Verifica se 9 é diferente de 12.\n9 != 12\n\n[1] TRUE\n\n# Verifica se 9 é maior que 5.\n9 &gt; 5\n\n[1] TRUE\n\n# Verifica se 9 é maior ou igual a 9.\n9 &gt;= 9\n\n[1] TRUE\n\n# Verifica se 4 é menor que 4.\n4 &lt; 4\n\n[1] FALSE\n\n# Verifica se 4 é menor ou igual que 4.\n4 &lt;= 4\n\n[1] TRUE\n\n\nPodemos notar que estes operadores funcionam bem com números, mas isso não é verdade quando se trata de objetos do tipo character (texto). Dentre esses, o operador == apenas funciona com números e o != funciona normalmente tanto com números quanto para textos. Os operadores &gt;, &gt;=, &lt; e &lt;= funcionam com textos pensando na ordem alfabética destes.\nPodemos utilizar operadores de comparação múltipla mais usuais em conjunto com estes discutidos para tornar as comparações ainda mais dinâmicas.\n\nE: & - será verdadeiro se todas operações forem TRUE.\n\n\nx &lt;- 17\n\n# Verifica se x &gt; 9 é verdadeiro E x &lt; 50 é verdadeiro.\n(x &gt; 9) & (x &lt; 50)\n\n[1] TRUE\n\n# Verifica se x &lt; 9 é verdadeiro E x &lt; 50 é verdadeiro E x &gt; 17 é verdadeiro.\n(x &gt; 9) & (x &lt; 50) & (x &gt; 17)\n\n[1] FALSE\n\n\n\nOU: | - será verdadeiro se pelomenos uma operação for TRUE.\n\n\nx &lt;- 17\n\n# Verifica se x &lt; 9 é verdadeiro OU x &lt; 50 é verdadeiro.\n(x &lt; 9) | (x &lt; 50)\n\n[1] TRUE\n\n# Verifica se x &lt; 9 é verdadeiro OU x &gt; 50 é verdadeiro OU x &lt;= 17 é verdadeiro.\n(x &lt; 9) | (x &gt; 50) | (x &lt;= 17)\n\n[1] TRUE\n\n\n\nNegação: ! - nega a resposta lógica da comparação.\n\n\nx &lt;- 17\n\n# Retorna TRUE se x &lt; 50 for FALSE, e FALSE caso contrário. \n!(x &lt; 50)\n\n[1] FALSE\n\n\nPodemos verificar se um valor (ou conjunto de valores) está contido em um vetor utilizando o operador %in%.\n\nex &lt;- 1:15\n\n# Verifica se os valores 3 e 5 fazem parte dos elementos do vetor ex.\nc(3, 5) %in% ex\n\n[1] TRUE TRUE\n\n# Dica: o operador %in% também funciona com character:\ntexto &lt;- c(\"hospital1\", \"hospital2\", \"hospital3\", \"hospital4\", \"hospital5\")\nc(\"hospital5\", \"UTI\") %in% texto\n\n[1]  TRUE FALSE\n\n\nTodos esses operadores podem ser utilizados ao manipular data frames. Iremos aproveitar o data frame criado anteriormente e adicionar mais duas colunas de textos para realizar alguns testes.\n\n# Visualizando o data frame criado anteriormente\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n# Adicionando a coluna sexo.\ndados$sexo &lt;- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"M\")\n\n# Adicionando a coluna olhos (preenchimento impreciso = F).\ndados$olhos &lt;- c(\"preto\", \"castanho\", \"F\", \"preto\", \"azul\", \"F\")\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Utilizando o operador %in% para obter as linhas com a cor dos olhos imprecisa.\ndados[dados$olhos %in% dados$sexo, ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo olhos\n3  3   52   1.75 16.97959            91.0    57         26    M     F\n6  6   70   1.61 27.00513           112.7    75         35    M     F\n\n# %in% com ! para obter as linhas com a cor dos olhos correta.\ndados[!(dados$olhos %in% dados$sexo), ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n\n# Linhas onde o peso é menor que o imc + 40. Retorna apenas colunas peso e imc.\ndados[(dados$peso &lt; (dados$imc + 40)), c(\"peso\", \"imc\")]\n\n  peso      imc\n3   52 16.97959\n\n\n\n\nA.5.9 Dados faltantes, infinitos e indefinições matemáticas\nDados faltantes é uma das coisas mais comuns em bases de dados, podendo surgir por diferentes fatores. No R, dados faltantes são representados por NA e é um símbolo que todo usuário deve conhecer e saber lidar. Além do NA, símbolos como NaN e Inf também são muito comuns no dia a dia.\n\nNA (Not Available): dado faltante/indisponível.\nNaN (Not a Number): indefinições matemáticas. Como 0/0 e log(-1).\nInf (Infinito): número muito grande ou o limite matemático. Aceita sinal negativo (-Inf).\n\n\nx &lt;- c(1, 6, 9)\n\n# Retorna NA\nx[4]\n\n[1] NA\n\n# Retorna NaN\nlog(-10)\n\nWarning in log(-10): NaNs produzidos\n\n\n[1] NaN\n\n# Retorna Inf\n10^14321\n\n[1] Inf\n\n\nAo lidar com bases de dados é necessário saber verificar se ela apresenta dados faltantes.\n\n# Base de dados que estamos usando.\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Adiciona linhas com dados faltantes.\ndados &lt;- rbind(dados, c(6, NA, 1.75, NA, 125, 99, 50, \"M\", \"castanho\"))\ndados &lt;- rbind(dados, c(9, 50, NA, 50, 127, 97, 55, \"F\", \"azul\"))\n\n# Deleta colunas que não iremos mais usar.\ndados[, c(\"pesovezesaltura\", \"peso5\", \"pesometade\")] &lt;- NULL\ndados\n\n  ID peso altura              imc sexo    olhos\n1  1   62    1.7 21.4532871972318    M    preto\n2  2   70   1.82 21.1327134404057    F castanho\n3  3   52   1.75 16.9795918367347    M        F\n4  4   98   1.94 26.0388989265597    F    preto\n5  5   90   1.84 26.5831758034026    F     azul\n6  6   70   1.61 27.0051309748852    M        F\n7  6 &lt;NA&gt;   1.75             &lt;NA&gt;    M castanho\n8  9   50   &lt;NA&gt;               50    F     azul\n\n# Ao incluir NA a variável imc passou a apresentar mais casas decimais.\n# Dica: podemos arredondar os valores do vetor alterados com a função round().\ndados[1:6, \"imc\"] &lt;- round(as.numeric(dados[1:6, \"imc\"]), digits = 2) \ndados\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n7  6 &lt;NA&gt;   1.75  &lt;NA&gt;    M castanho\n8  9   50   &lt;NA&gt;    50    F     azul\n\n# Avalia se os elementos da coluna peso são NA ou não.\nis.na(dados$peso)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n# Verifica se existe pelomenos 1 dado faltante no data frame.\nany(is.na(dados))\n\n[1] TRUE\n\n# Filtra apenas as linhas com NA na variável peso.\ndados[is.na(dados$peso),]\n\n  ID peso altura  imc sexo    olhos\n7  6 &lt;NA&gt;   1.75 &lt;NA&gt;    M castanho\n\n# Dica: as funções na.omit() e complete.cases() podem remover linhas com NA.\nna.omit(dados)\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\ndados[complete.cases(dados), ]\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\n\nPara lidar com dados faltantes é importante ter pelo menos uma ideia do motivo para eles existirem na base de dados sendo analisada. Muitas vezes não temos ideia desse motivo, e a melhor estratégia acaba sendo analisar os dados, incluindo e reportando com transparência os dados faltantes. Ao analisar dados sem excluir os casos faltantes, muitas vezes nos deparamos com erros inesperados que ocorrem por tentarmos usar funções que não estão considerando esses casos. Situações como essas exigem uma atenção a mais do usuário, tendo que pesquisar e ler documentações de funções para ter certeza do que a função sendo usada está fazendo.\n\n# Criando um vetor com dados faltante.\nvetor1 &lt;- c(NA, 1, 1, 1, 5)\n\n# mean() calcula a média do vetor.\nmean(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nmean(vetor1, na.rm = TRUE)\n\n[1] 2\n\n# sum() calcula a soma dos elementos do vetor.\nsum(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nsum(vetor1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\nA.5.10 Condicionamento: If e else\nAs estruturas if e else, também chamadas de condicionais, servem para executar códigos apenas se uma condição (teste lógico) for satisfeita.\n\nvalor1 &lt;- 224\nvalor2 &lt;- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta &lt;- 10 # resposta é 10.\n} else { # caso contrário.\n  resposta &lt;- 15 # respota é 15.\n  }\nresposta\n\n[1] 15\n\n\nVeja que o R só executa o conteúdo das chaves {} se a expressão dentro dos parênteses () retornar TRUE. Além disso, note que a condição de igualdade é representada por dois iguais (==). Como dito anteriormente, apenas um igual (=) é símbolo de atribuição (preferível &lt;-), em argumentos de estruturas condicionais queremos realizar comparações.\nPara utilizar mais condições podemos utilizar o else if ().\n\nvalor1 &lt;- 224\nvalor2 &lt;- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta &lt;- 10 # resposta é 10.\n} else if (valor1 &gt; valor2) { # Se não, então valor1 é maior que valor2 ?\n  resposta &lt;- 15 # então a resposta é 15.\n  } else { # caso contrário.\n    resposta &lt;- 25 # respota é 25.\n    }\nresposta\n\n[1] 25\n\n\n\n\nA.5.11 Iterador for\nO for serve para repetir uma mesma tarefa para um conjunto de valores diferentes (realiza um loop). Cada repetição é chamada de iteração.\nComo exemplo, considere o vetor atribuído ao objeto vetor1 como segue:\n\nvetor1 &lt;- c(1,20,50,60,100)\n\nPodemos criar um novo vetor que seja formado por cada elemento do vetor1 dividido por sua posição.\n\nvetor2 &lt;- NULL\nfor (i in 1: length(vetor1)){\n  vetor2[i] &lt;- vetor1[i]/i\n}\nvetor2\n\n[1]  1.00000 10.00000 16.66667 15.00000 20.00000\n\n\nNote que primeiro definimos o objeto vetor2, recebendo NULL. O NULL representa a ausência de um objeto e serve para já declarar algum objeto que receberá valor na sequência. Ao rodar o for, o vetor2 passa a ser um vetor de tamanho 5 (tamanho do vetor1).\nNo exemplo, temos 5 iterações e para cada valor de i, correndo de 1 até 5 (tamanho do vetor1), pegamos o valor do vetor1 na posição i e dividimos por i. Assim, formamos o vetor2.\n\n\nA.5.12 Funções\nFunções no R são nomes que guardam um código de R. A ideia é que sempre que rodar a função com os seus argumentos, o código que ela guarda será executado e o resultado será retornado.\nJá usamos anteriormente algumas funções que estão na base do R. Por exemplo, quando usamos class() para entender a classe do objeto que o R está entendendo. Colocamos um argumento dentro do parêntese e o R retornou qual a classe do objeto em questão.\nImportantes:\n\nSe a função tiver mais de um argumento, eles são sempre separados por vírgulas;\nCada função tem os seus próprios argumentos. Para saber quais são e como usar os argumentos de uma função, basta acessar a sua documentação. Uma forma de fazer isso é pela função help(), cujo argumento é o nome da função em questão.\n\n\nhelp(mean)\n\nVeja que abrirá a documentação sobre a função mean no menu “Help” do RStudio. Assim, é possível ver os argumentos e exemplos de uso da função.\nAinda sobre funções já presentes no R, vamos considerar agora a função sample. Veja a documentação dessa função para ver o que ela faz.\n\nhelp(sample)\n\nA função sample retorna uma amostra de um vetor com tamanho especificado em um de seus argumentos com ou sem reposição. Ela apresenta quatro argumentos na forma sample(x, size, replace = FALSE, prob = NULL), em que: x é o vetor do qual será amostrado o número de elementos especificado no argumento size, replace indica se é com ou sem reposição e prob é para especificar probabilidades de seleção.\nPodemos usar essa função para amostrar de um objeto dois elementos (size = 2) em uma seleção com reposição (replace = TRUE) e que a probabilidade de seleção seja a mesma para todos os elementos do vetor. No caso da probabilidade, como podemos ver na documentação da função sample, o default (padrão se o usuário não mudar o argumento) é ser a mesma probabilidade de seleção para todos os elementos. Assim, se o usuário nada especificar para esse argumento, o R entenderá o seu default. O mesmo vale para o argumento replace: caso fosse o interesse fazer a seleção sem reposição, não precisaríamos colocar esse argumento por seu default ser FALSE.\n\n\n[1] 10 20\n\n\nTambém poderíamos usar a mesma função sem colocar o nome dos argumentos, desde que o usuário entenda o que ela está fazendo.\n\nsample(vetor_am, 2 , TRUE) \n\n[1] 10.0  0.5\n\n\nNesse caso, é importante que se respeite a ordem dos argumentos: o vetor tem que ser o primeiro, o segundo argumento é size e assim por diante.\nVale ressaltar que as duas últimas saídas não necessariamente serão as mesmas, porque é feito um sorteio aleatório de dois elementos de vetor_am em cada uma delas.\nAlém de usar funções já prontas, podemos criar novas funções. Suponha que queremos criar uma função de dois argumentos que retorna o primeiro mais três vezes o segundo argumento.\n\nf_conta &lt;- function(x, y) {\n  out &lt;- x + 3 * y\n  return(out)\n}\n\nA função acima possui:\n\nnome: f_conta.\nargumentos: x e y.\no corpo out: &lt;- x + 3 * y.\no que retorna: return(out).\n\nPara chamar a função e utilizá-la basta chamar pelo nome com os devidos argumentos, assim como temos feito até então.\n\nf_conta(x = 10, y = 20)\n\n[1] 70\n\n\nVeja que o cálculo acima retorna exatamente o mesmo que o seguinte:\n\nf_conta(y = 20, x = 10)\n\n[1] 70\n\n\nIsso acontece porque a ordem dos argumentos foi alterada, porém, mantendo seus devidos nomes. Se não quiser colocar os nomes dos argumentos, precisa tomar cuidado para não errar a ordem deles. Isso porque:\n\nf_conta(10,20)\n\n[1] 70\n\n\né diferente de:\n\nf_conta(20,10)\n\n[1] 50\n\n\n\n\nA.5.13 Como obter ajuda no R\nListamos aqui 3 maneiras para buscar ajuda no R:\n\nHelp/documentação do R (comandos help(nome_da_funcao) ou ?nome_da_funcao). Como exemplo:\n\n\nhelp(mean) \n?mean\n\n\nGoogle: especificar a linguagem é de suma importância na pesquisa, além de deixar o problema ou a função bem claro.\n\n\n\n\n\n\nPesquisa no Google\n\n\n\n\n\nComunidade: O Stack Overflow e o Stack Overflow em Português são sites de perguntas e respostas amplamente utilizados por todas as linguagens de programação.\n\n\n\nA.5.14 Pacotes\nComo dito quando falamos “Sobre o R”, o R apresenta funções na sua base e também em forma de pacotes (conjunto de funções bem documentado), que precisam ser instalados (uma vez no seu computador) e carregados na sessão de utilização do R (carregado em toda sessão aberta).\nDificilmente uma análise será feita apenas com as funções básicas do R e dificilmente não vai existir um pacote com as funções que você precisa. Por esse motivo, falamos a seguir em como instalar e carregar pacotes.\n\nA.5.14.1 Instalação de pacotes\n\nVia CRAN:\n\n\ninstall.packages(\"nome-do-pacote\")\n\nExemplo: Instalação do pacote dplyr.\n\ninstall.packages(\"dplyr\")\n\nNote que o nome do pacote está entre aspas.\n\nVia Github: Para instalar via Github precisa primeiramente instalar o pacote devtools.\n\n\ndevtools::install_github(\"nome-do-repo/nome-do-pacote\")\n\nExemplo:\n\ndevtools::install_github(\"tidyverse/dplyr\")\n\n\n\nA.5.14.2 Carregar pacotes\nUma vez que um pacote de interesse está instalado em sua máquina, para carregá-lo na sessão atual do R é só rodar a seguinte linha de comando:\n\nlibrary(nome-do-pacote)\n\nVeja que para carregar o pacote não se usa aspas.\nComo exemplo, o carregamento do pacote dplyr:\n\nlibrary(dplyr)\n\nSó é necessário instalar o pacote uma vez, mas é necessário carregá-lo toda vez que começar uma nova sessão.\nDado que o pacote está carregado ao rodar a função library(), todas as funções desse pacote podem ser usadas sem problemas.\nCaso você não queira carregar o pacote e apenas usar uma função específica do pacote, você pode usar nome-do-pacote::nome-da-funcao. Por exemplo:\n\ndplyr::distinct(...)\n\nTendo carregado o pacote dplyr anteriormente (pela função library()), não seria necessário colocar dplyr:: antes da função distinct do pacote.\n\n\n\nA.5.15 Materiais complementares\nLivros e Artigos:\n\nCritical Thinking in Clinical Research. Felipe Fregni & Ben M. W. Illigens. 2018.\nCHAPTER 3: Selecting the Study Population. In: Critical Thinking in Clinical Research by Felipe Fregni and Ben Illigens. Oxford University Press 2018.\nFandino W. Formulating a good research question: Pearls and pitfalls. Indian J Anaesth. 2019;63(8):611–616. doi:10.4103/ija.IJA_198_19\nRiva JJ, Malik KM, Burnie SJ, Endicott AR, Busse JW. What is your research question? An introduction to the PICOT format for clinicians. J Can Chiropr Assoc. 2012;56(3):167–171.\nExternal validity, generalizability, and knowledge utilization. Ferguson L1. J Nurs Scholarsh. 2004;36(1):16-22.\nPeter M Rothwell; Commentary: External validity of results of randomized trials: disentangling a complex concept, International Journal of Epidemiology, Volume 39, Issue 1, 1 February 2010, Pages 94–96, https://doi.org/10.1093/ije/dyp305\n\nSites:\n\nhttps://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/1-data-display-and-summary\nhttp://www.sthda.com/english/wiki/statistical-tests-and-assumptions"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Anton, Howard, and Chris Rorres. 2001. Álgebra Linear\nCom Aplicações. Vol. 8. Bookman Porto\nAlegre.\n\n\nHartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A\nk-Means Clustering Algorithm.” Journal of the Royal\nStatistical Society. Series c (Applied Statistics) 28 (1): 100–108.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied\nMultivariate Statistical Analysis. Vol. 5. 8. Prentice hall Upper\nSaddle River, NJ.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. “Least Squares Quantization in PCM.”\nIEEE Transactions on Information Theory 28 (2): 129–37."
  },
  {
    "objectID": "supervisionada.html#conceitos-iniciais",
    "href": "supervisionada.html#conceitos-iniciais",
    "title": "7  Aprendizado supervisionado",
    "section": "7.1 Conceitos iniciais",
    "text": "7.1 Conceitos iniciais\nAprendizado supervisionado pode ser definido como a tarefa de aprender uma função que mapeia uma entrada em uma saída e isso é feito com base em exemplos e treinos. Em outras palavras, uma máquina é treinada para encontrar soluções chamadas rótulos, onde esses rótulos identificam alguma característica. Apesar de também poder ser usada para regressão, o aprendizado supervisionado tem como tarefa típica a classificação. Um exemplo bem simples de classificação é: suponha que eu queira classificar imagens de animais, nesse caso possuo um banco de dados com imagens de cachorros e gatos. Quero que meu algoritmo classifique as imagens identificando o tipo do animal na imagem. Para isso o algoritmo é treinado utilizando vários exemplos para que ele consiga classificar novas imagens posteriormente. Outra tarefa é predizer um valor com base em características, por exemplo, prever o valor de um carro dado um conjunto de características (quilometragem, idade, marca, etc.) chamadas preditores. Este tipo de tarefa é chamada regressão. Para treinar o sistema é preciso incluir diversos exemplos, assim o banco de dados é separado em treino e teste, onde o é feito o treinamento na base treino para posteriormente serem feitos os testes de predição e avaliação da qualidade do ajuste na base teste.\n\n7.1.1 Dificuldades gerais do machine learnig\nComo dito anteriormente, a idéia geral do aprendizado de máquina é construir um algoritmo para solucionar os meus problemas, onde esse algoritmo será treinado com dados. Mas, o que acontece se o meu algoritmo for ruim ou os dados estiverem ruins?\n\n7.1.1.1 Quantidade insuficiente de dados\nFalando sobre dados ruins, o primeiro problema é a quantidade de dados. Já parou pra pensar em quão difícil é treinar uma máquina? Voltando ao exemplo anterior, para você aprender a diferenciar um cachorro de um gato quando era criança, bastou alguém lhe apontar qual era qual algumas vezes você se tornou capaz de diferenciar cães de gatos independente das características. Uma máquina não consegue fazer isso facilmente, é necessário uma quantidade grande de dados para a maioria dos algoritmos, até mesmo para problemas simples como o do exemplo citado e para problemas complexos, como reconhecimento de imagem ou fala você pode precisar de milhões de exemplos.\n\n\n7.1.1.2 Dados de treino não representativos\nComo mencionado anteriormente, o treinamento de um algoritmo é feito por meio de uma base de dados, onde está é separada em dados de treinamento e de teste, para que eu possa usá-lo e generalizá-lo em dados futuros. Dados de treinamento que não representem bem os dados que serão usados no futuro podem ser um modelo que não funcionará bem. Utilizando o exemplo do algoritmo de regressão onde o objetivo era prever os valores dos carros com base em suas características. Digamos que meu algoritmo foi treinado com uma base de dados de carros apenas do estado de São Paulo, mas meu algoritmo será utilizado para prever carros de todo o país, pode ser que não funcione tão bem. Os estados podem alterar significativamente os preços dos carros por meio de impostos, por exemplo. É de extrema importância utilizar um conjunto de dados de treino que represente bem os dados que você deseja generalizar. Isso pode não ser uma tarefa fácil, pode encontrar problemas com amostras, principalmente se ela for muito pequena e até mesmo uma amostra grande pode não ser representativa.\n\n\n7.1.1.3 Qualidade dos dados\nComo pode ter imaginado, a qualidade dos dados também é de extrema importância. Dados com discrepâncias, vários erros, e gerados a partir de medições de baixa qualidade fará com que fique mais difícil o seu algoritmo identificar padrões e tomar decisões. Se você convive com pessoas do ramo da ciência de dados em geral, é bem provável que já tenha ouvido alguém dizer algo do tipo: “gastamos a maior parte do nosso tempo para limpar os dados”. Isso não é em vão. Na maioria dos casos, principalmente no ramo de aprendizagem de máquinas é gasto um enorme tempo para limpar os dados pois pode influenciar muito na qualidade do modelo. Por exemplo, se algumas informações forem muito discrepantes, é preciso decidir entre tentar corrigir ou excluí-las. Se uma variável tiver uma quantidade significativa de valores faltantes, deverá ser decidido se essas observações serão excluídas ou se será possível utilizar métodos de imputação de dados. Treinar mais de um modelo com diferentes decisões tomadas sobre os dados também pode ser efetivo.\n\n\n7.1.1.4 Sobreajustamento dos dados (Overfitting)\nO sobreajustamento é um conceito que ocorre quando nosso modelo (não só um modelo de aprendizado de máquinas), se ajusta exatamente aos nossos dados de treinamento. Ouvir isso uma primeira vez pode parecer excelente, ou até mesmo o cenário ideal, afinal, queremos que o nosso modelo se ajuste o máximo possível, certo? bom.. não exatamente. O que acontece neste caso, é que o modelo mostra-se adequado apenas para os dados de treino, como se o modelo tivesse apenas decorado os dados de treino e não fosse capaz de generalizar para outros dados nunca vistos antes. Assim, o desempenho do nosso modelo quando usado em novos dados cai drasticamente. Algumas razões que podem levar a um sobreajustamento: base de treino muito pequena, não contendo dados suficientes para representar bem todos valores de entrada possíveis; grande quantidade de informações irrelevantes (dados ruidosos); treinamento excessivo em um único conjunto de amostra; modelo muito complexo, fazendo com que ele aprenda os ruídos nos dados de treinamento. Agora que sabemos o problema que é um sobreajustamento e as razões que podem levar a isso, precisamos falar sobre como evitar que isso aconteça. Existem algumas técnicas comumente utilizadas.\n\nRegularização: Foi dito anteriormente que uma razão para o sobreajustamento é a complexidade do modelo, então, faz sentido diminuirmos sua complexidade. Isso pode ser feito removendo ou diminuindo o número de parâmetros.\nParada antecipada: Quando um modelo está sendo treinado por rodadas de repetição, é possível avaliar cara uma dessa repetição. Normalmente o desempenho de um modelo melhora a cada repetição, mas chega um momento em que começa a acontecer o sobreajustamento. A ideia da parada antecipada é pausar o treinamento antes que chegue a esse ponto.\nAumento de dados: Essa técnica consiste em aumentar ligeiramente os dados da amostra toda vez que o modelo os processa, ou seja, injetar dados limpos e relevantes nos dados de treino. Isso faz com que os conjuntos de treino pareçam “exclusivos” do modelo, impedindo que ele aprenda suas características. Mas isso deve ser feito com moderação, pode injetar dados que não estão limpos pode fazer mais mal do que bem. Além disso, não é um método garantido.\n\nExistem outras técnicas que podem ser utilizadas para evitar o sobreajustamento. Mas precisamos falar também sobre como detectá-los.\nUma forma “não técnica” e que não deve ser a sua única forma de tentar identificar o sobreajustamento é por meio da visualização gráfica. A visualização gráfica pode ser usada apenas para levantar hipóteses, nunca para tomar uma decisão final. Até mesmo porque nem sempre é possível verificar esse problema visualmente. Talvez a técnica mais eficiente para isso é a Validação Cruzada k-fold (k-fold Cross Validation). Vamos falar sobre posteriormente.\n\n\n7.1.1.5 Subajustamento dos dados (Underfitting)\nComo pode ter imaginado, subajustamento é o oposto do sobreajustamento. Ocorre quando seu modelo é muito simples para aprender a estrutura dos dados. O subajustamento leva a um erro elevado tanto nos dados de treino quanto nos dados de teste. Pode ocorrer quando o modelo não foi treinado por tempo suficiente ou as variáveis ​​de entrada não são significativas o suficiente para determinar uma relação significativa entre as variáveis ​​de entrada e saída. Aqui também estamos em um cenário a ser evitado e apesar de ser contrário ao sobreajustamento, as técnicas tanto para identificar quanto para evitar o problema são semelhantes. Um adendo, geralmente, identificar um subajustamento é mais fácil que identificar um sobreajustamento.\n\n\n\n7.1.2 Modelo de Regressão Linear\nJá temos uma breve noção sobre o que é aprendizado supervisionado, agora vamos introduzir o funcionamento básico de um modelo. Como foi mencionado, aprendizado supervisionado é usado principalmente para métodos de classificação e regressão. Um modelo de regressão linear, como o próprio nome já diz, se enquadra nos métodos de regressão. A regressão consiste em modelar um valor de previsão com base em variáveis independentes. De forma mais geral, o modelo consiste em fazer uma previsão “simples” calculando uma ponderação entre as somas dos recursos de entrada e uma constante chamada intercepto. Assim, obtemos uma relação linear entre a variável de saída e as variáveis de entrada. A linha de regressão é a linha de melhor ajuste para o modelo.\n\n\n\n\n\nFonte: https://is.gd/Z9yiHp\n\n\n\n\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nonde:\n\n\\(\\hat y\\) é o valor predito\n\\(n\\) o número de características\n\\(x_i\\) é a \\(i^{th}\\) característica\n\\(\\beta_j\\) é o \\(j^{th}\\) parâmetro do modelo\n\nCerto, temos uma definição matemática do nosso modelo, mas como posso treiná-lo? Treinar um modelo significa também definir os parâmetros para que o modelo se ajuste melhor aos meus dados. Em outras palavras, um modelo treinado irá se ajustar à melhor linha para prever o valor de \\(y\\) para um dado valor de \\(x\\). Assim, ao encontrar os melhores valores de \\(\\beta 's\\) obtemos a melhor linha de ajuste. Para isso, primeiro precisamos de uma medida de quão bem (ou mal) o modelo se ajusta aos meus dados. Posteriormente será discutido quais as medidas mais comuns para avaliação de modelos de regressão.\nExistem algumas suposições importantes que devem ser feitas para utilizar um modelo de regressão linear. Estas são algumas verificações formais durante a construção de um modelo de regressão linear, o que garante a obtenção do melhor resultado possível do conjunto de dados fornecido.\n\nSuposição de linearidade: A regressão linear assume que a relação entre a entrada e saída é linear. Pode parecer um pouco óbvio, mas em alguns casos onde, em um primeiro olhar, faça sentido usar uma regressão linear, nossos dados não permitam isso. Pode ser necessário transformar os dados.\nHomocedasticidade: Homocedasticidade é uma situação em que o termo de erro é o mesmo para todos os valores de variáveis ​​independentes. Com homocedasticidade, não deve haver uma distribuição padrão clara de dados no gráfico de dispersão.\nErros normalmente distribuídos: A regressão linear assume que o termo de erro deve seguir o padrão de distribuição normal. Se os termos de erro não forem normalmente distribuídos, os intervalos de confiança se tornarão muito amplos ou muito estreitos, o que pode causar dificuldades em encontrar coeficientes. Você pode obter algum benefício usando transformações (por exemplo, log ou BoxCox) em suas variáveis ​​para tornar sua distribuição mais gaussiana.\nMulticolinearidade: O modelo de regressão linear não assume nenhuma autocorrelação em termos de erro. Se houver alguma correlação no termo de erro, isso reduzirá drasticamente a precisão do modelo. A autocorrelação geralmente ocorre se houver uma dependência entre os erros residuais. Considere calcular correlações pareadas para seus dados de entrada e remover os mais correlacionados.\n\n\n\n7.1.3 Modelo de Regressão logística\nAlguns algoritmos de regressão podem ser usados para classificação (o contrário também é válido). A regressão logística é um dos algoritmos mais populares do machine learning e geralmente é usada para estimar a probabilidade de que uma instância pertença a uma classe. Por exemplo, qual a probabilidade de que o objeto de uma imagem seja um cachorro? ou um gato? Neste caso, se a probabilidade estimada for maior que 50%, então o modelo pode prever que naquela imagem tem um cachorro (classe rotulada como “1”), se for menor, prevê que é um gato (classe rotulada como “0”). Este tipo de regressão pode retornar valores categóricos ou discretos, como: Sim ou Não, 0 ou 1, verdadeiro ou falso, entre outros. Mas aqui, ela fornece os valores probabilísticos que estão entre 0 e 1. Apesar de ser semelhante a regressão linear, aqui não ajustamos uma linha de regressão, mas sim uma função logística em forma de “S” que prevê os dois valores máximos (0 ou 1).\n\n\n\n\n\nFonte: https://is.gd/87NSTU\n\n\n\n\nA equação de regressão Logística pode ser obtida a partir da equação de Regressão Linear.\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nO problema de usar essa abordagem é que podemos prever probabilidades negativas em alguns casos e valores maiores que 1 em outros. Essas previsões não são sensatas, pois sabemos que a verdadeira probabilidade deve ser um número entre 0 e 1. Para resolver esse problema, devemos modelar \\(\\hat y\\) usando uma função que forneça saídas entre 0 e 1 para todos os valores de \\(\\hat y\\). Na regressão logística usamos a função logística como sendo:\n\\[\n\\hat y = \\frac{e^{\\beta_0+\\beta_1X}}{1 + e^{\\beta_0+\\beta_1X}}\n\\]\nDepois de algumas manipulações, chegamos que\n\\[\n\\frac{\\hat y}{1- \\hat y} = e^{\\beta_0+\\beta_1X}\n\\]\nMas precisamos variar de \\(-\\infty\\) até \\(\\infty\\), então pegue o logaritmo da equação e temos:\n\\[\n\\log\\bigg[\\frac{\\hat y}{1- \\hat y} \\bigg ] = {\\beta_0+\\beta_1X}\n\\]\nExistem alguns tipos de regressão logística:\n\nBinomial: Aqui deve haver apenas dois tipos de possíveis variáveis, como 0 ou 1, Falso ou Verdadeiro, etc.\nMultinomial: Pode também haver 3 ou mais tipos não ordenados possíveis da variável dependente, como, cachorro, gato ou tigre.\nOrdinal: Na regressão logística ordinal, pode haver 3 ou mais tipos ordenados possíveis de variáveis ​​dependentes, como “baixo”, “médio” ou “alto”.\n\n\n\n7.1.4 Medidas de desempenho\nAo desenvolver projetos de Aprendizado de Máquinas no geral, é preciso sempre verificar a qualidade do modelo. Assim como não existe um modelo padrão para resolver todos os problemas, não existe uma única métrica para avaliar a qualidade do modelo. Saber qual métrica é mais apropriada para determinado cenário é crucial, pois escolhas erradas podem gerar modelos problemáticos.\n\n7.1.4.1 Modelos de regressão\nExistem algumas métricas importantes para medirmos a qualidade de um modelo de regressão.\n\nRoot Mean Squared Error (RMSE): Uma medida muito comum usada em um modelo de regressão é a Root Mean Squared Error (REQM). Em resumo, o REQM é uma medida que mostra o quão espalhados estão esses resíduos. A métrica possui valor mínimo 0 e sem valor máximo. Quanto maior esse número, pior o modelo.\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}  (\\hat y_i - y_i)^2}\n\\]\nUma vantagem dessa métrica é que predições muito distantes do real aumentam o valor da medida com facilidade, o que torna a métrica bem vinda em problemas onde erros grandes não são tolerados.\nMean Absolute Error (MAE): Esta medida é bem simples de entender. Nada mais é que a média do erro que cada ponto tem em relação a linha de regressão. É um pouco parecido com o que RMSE faz, a diferença é que aqui, erros grandes não afetam tanto a medida. A sua interpretação é: quanto maior o MAE, maior é o erro do modelo. Apesar de ser simples, não deve ser usado em todos os modelos. Esta medida é bastante sensível a valores discrepantes (outliers). Portanto, deve-se avaliar os dados antes de utilizar esta métrica.\n\\[\nMAE = \\frac{1}{N}\\sum|y_i - \\hat{y_i}|\n\\]\n\\(R^2\\): R-quadrado é uma medida que visa explicar qual a porcentagem de variância que pôde ser prevista pelo modelo, ou seja, o qua “próximo” as medidas reais estão dos nossos dados. Segue a fórmula:\n\\[\nR^2 = 1 - \\frac{\\sum(y_i - {\\hat{y_i}})^2} {\\sum(y_i - {\\bar{y}})^2}\n\\]\nonde, \\(\\hat{y_i}\\) é o valor predito, \\(\\bar{y}\\) é o valor médio das amostras e \\(y_i\\) é o valor observado. O valor resultante varia de 0 a 1. Quanto maior o valor, melhor o modelo. Por exemplo, caso tivéssemos avaliando um modelo e tivéssemos \\(R^2 = 0.87\\), entende-se então que 87% da variância de nossos dados podem ser explicadas pelo modelo. Esta medida possui algumas limitações importantes: só pode ser aplicada perfeitamente para modelos univariados; é uma medida enviesada, pode definição e em casos de overfitting, o valor da métrica ainda continua alto.\n\n\n\n7.1.4.2 Modelos de classificação\n\nAcurácia: A acurácia nos diz quantos de nossos exemplos foram classificados corretamente e é dada pela seguinte fórmula:\n\\[\n\\mbox{acurácia}   =  \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nOnde TP = True Positive (Verdadeiro Positivo), TN = True Negative (Verdadeiro Negativo), FP = False Negative (Falso Negativo) e FN = False Negative (Falso Negativo).\nA métrica então é definida pela razão entre os acertos e o total (erros + acertos). É uma métrica extremamente fácil de ser usada e interpretada. Por exemplo, se tenho 100 observações e meu modelo classificou 80 corretamente, então a acurácia do meu modelo é 80%. Apesar de simples, essa métrica pode não ser adequada em alguns casos.\nUma desvantagem é que podemos obter uma acurácia alta, mas o nosso modelo pode ter uma performance inadequada. Por exemplo, pense que temos um conjunto de dados de animais com 100 observações, sendo 90 cachorros e 10 gatos. Se tivermos um modelo que sempre classificará todas as observações como cachorro, ainda teríamos um modelo com acurácia de 90%. É uma métrica boa, mas não estamos avaliando o nosso modelo de uma boa forma. Se introduzirmos novos dados que venham de uma forma mais equilibrada, ou seja, com quantidade de cachorros e gatos parecidas, o modelo se comportaria de forma ruim. Portanto, para conjuntos de dados desbalanceados, outras métricas são mais eficientes.\nOutra desvantagem é que esta medida atribui o mesmo peso para ambos os erros. Por exemplo, em um modelo que classifica exames de câncer entre positivo e negativo para a doença. Um modelo com acurácia 90% aparentemente é um bom modelo onde os 10% de erro podem ter sido falsos negativos ou falsos positivos. Porém, o erro por falso negativo aqui é bem mais grave.\nPrecisão: Essa métrica é definida pela razão entre a quantidade de observações classificadas corretamente como positivos e o total de classificados como positivos, segue a fórmula:\n\n\\[\n\\mbox{precisão}  =  \\frac{TP}{TP + FP}\n\\]\nEsta medida atribui um peso maior para os erros por falso positivo. Pode ser entendida como a resposta para a pergunta: das observações classificadas como positivos, quantos são verdadeiramente positivos? então, neste caso, se a precisão fosse de 90%, é esperado que a cada 100 observações classificadas como positivos, apenas 90 são de fato positivos.\n\nSensibilidade: Essa métrica avalia então a capacidade do modelo detectar com sucesso resultados classificados como positivos em relação a todos os pontos de dados positivos.\n\\[\n\\mbox{sensibilidade} = \\frac{TP}{TP + FN}\n\\]\nEspecificidade: Ao contrário da sensibilidade, a especificidade avalia a capacidade do modelo detectar resultados negativos. Segue a fórmula:\n\n\\[\n\\mbox{especificidade} = \\frac{TN}{TN + FP}\n\\]\n\n\n7.1.4.3 Matriz de confusão\nMatriz de confusão é uma matriz usada para avaliar o desempenho de um modelo de classificação. A matriz compara os valores de destino reais com os previstos pelo modelo. A matriz é N x N onde é N é o número de classes. Para um problema de classificação binária teríamos uma matriz 2 x 2. As matrizes de confusão revelam quando um modelo confunde consistentemente duas classes, simplificando a determinação da probabilidade de os resultados de um modelo serem confiáveis.\n\n\n\n\n\n\n\n\n\n\n\n\nObservado\n\n\n\nPredito\nNegativo\nPositivo\n\n\n\n\nNegativo\nVerdadeiro Negativo (TN)\nFalso Negativo (FN)\n\n\nPositivo\nFalso Positivo (FP)\nVerdadeiro Positivo (TP)\n\n\n\n\n\n\n\nNa matriz de confusão colocamos os valores reais nas colunas e os valores preditos nas linhas. Assim o cruzamento das linhas e das colunas passam a ser nossas métricas. (não é incomum vermos versões invertidas)\n\n\n\n7.1.5 Curva AUC-ROC\nQuando queremos verificar ou visualizar o desempenho de um modelo de classificação binária (0, 1), podemos utilzar a AUC (Area Under The Curve) ROC (Receiver Operating Characteristics). Está é uma das métricas de avaliação de desempenho de modelos de classificação mais importantes.\n\n7.1.5.1 O que é a curva AUC-ROC\nA curva ROC é uma métrica de avaliação para problemas de classificação binária. É uma curva de probabilidade que plota o TPR (sensibilidade) contra o FPR (1 - especificidade). A AUC (Area Under the Curve) é a medida da capacidade de um classificador para distinguir entre classes e é usada com um resumo da curva ROC. Quanto maior a AUC, melhor o modelo está em distinguir entre classes positivas e negativas. Por analogia, quanto maior a AUC, melhor o modelo está em distinguir entre pacientes com a doença e sem a doença.\nQuando AUC = 1, o modelo é capaz de distinguir perfeitamente os pontos positivos e negativos corretamente. Porém, se AUC = 0, o modelo estaria predizendo todos valores negativos como positivos e vice e versa. Quando AUC= 0,5, o modelo não é capaz de distinguir os valores. Neste caso o modelo está prevendo cada classe de forma aleatória ou de forma constante para todos os dados. Já quando 0,5 &lt; AUC &lt; 1, há uma grande chance de que o modelo consiga distinguir os valores negativos das classes positivas. Aqui o modelo é capaz de detectar mais números de verdadeiros positivos e verdadeiros negativos do que falsos negativos e falsos positivos.\nA curva ROC é produzida calculando e plotando a taxa de verdadeiros positivos em relação à taxa de falsos positivos para um único classificador em vários limites . Por exemplo, na regressão logística, o limiar seria a probabilidade prevista de uma observação pertencente à classe positiva. Normalmente, na regressão logística, se uma observação é prevista como positiva com probabilidade &gt; 0,5, ela é rotulada como positiva. No entanto, poderíamos realmente escolher qualquer limite entre 0 e 1 (0,1, 0,3, 0,6, 0,99, etc.) — e as curvas ROC nos ajudam a visualizar como essas escolhas afetam o desempenho do classificador.\nA figura abaixo demonstra como alguns modelos teóricos podem plotar a curva ROC. A linha cinza pontilhada representa um classificador que não é melhor do que a adivinhação aleatória, seria a linha diagonal. Um modelo com uma taxa de verdadeiros positivos de 100% e falsos negativos de 0% seria plotado sobre a linha da esquerda e a de cima. Quase todos os exemplos do mundo real cairão em algum lugar sobre essas duas linhas tendo o cenário 0,5 &lt; AUC &lt; 1, como exemplo, a linha azul da figura. Normalmente procuramos um classificador que mantenha uma alta taxa de verdadeiros positivos e, ao mesmo tempo, uma baixa taxa de falsos positivos. Embora seja útil visualizar a curva ROC, em muitos casos podemos reduzir essas informações a uma única métrica, a AUC.\n\n\n\n\n\n\n\nFonte: https://is.gd/iyWHe8\n\n\n\n\nUm grande ponto positivo da curva ROC é que ela permita que encontremos um limite de classificação adequado ao nosso problema específico\nPor exemplo, se estivéssemos avaliando um classificador de spam de e-mail, gostaríamos que a taxa de falsos positivos fosse muito, muito baixa. Não queremos que alguém perca um e-mail importante para o filtro de spam só porque nosso algoritmo foi muito agressivo. Provavelmente até permitiríamos uma boa quantidade de e-mails de spam reais (verdadeiros positivos) através do filtro apenas para garantir que nenhum e-mail importante fosse perdido.\nPor outro lado, se nosso classificador está prevendo se alguém tem uma doença terminal, podemos aceitar um número maior de falsos positivos (diagnosticado incorretamente a doença), apenas para garantir que não perderemos nenhum verdadeiro positivo (pessoas que realmente têm a doença).\nAlém disso, também podemos comparar o desempenho de diferentes classificadores para o mesmo problema.\nExistem outras métricas de desempenho que não foram apresentadas aqui, mas devemos ressaltar alguns pontos. Não existe uma métrica certa ou errada, devemos apenas nos atentar e buscar a que melhor atende o nosso problema. É possível também utilizar mais de uma métrica para o mesmo modelo.\n\n\n\n7.1.6 Validação Cruzada (Cross-Validation)\nAté aqui falamos um pouco sobre alguns problemas que podem ser encontrados no aprendizado de máquinas e superficialmente sobre dois modelos de regressão. Vamos falar agora sobre um método que é bem utilizado para validar a estabilidade do seu modelo. Como mencionamos anteriormente, não podemos simplesmente ajustar um modelo aos meus dados de treino e esperar que ele funcione perfeitamente, ou até mesmo esperar que aquele seja o melhor modelo possível para fazer alguma validação. Falamos um pouco sobre isso quando discutimos sobre sobreajustamento e subajustamento. Então, vamos nos aprofundar sobre um método que nos garanta que o nosso modelo obteve a maioria dos padrões dos dados corretos sem captar muitos ruídos.\nO que é validação cruzada?\nValidação cruzada é uma técnica para avaliar um modelo de aprendizado de máquina e testar o seu desempenho. Pode ajudar a comparar e selecionar um modelo mais apropriado para o nosso problema. É bem fácil de entender, de implementar e tende a ter um viés menor do que outros métodos usados para o mesmo objetivo. Por isso é uma ferramenta tão utilizada. Tanto a validação cruzada quanto outros algoritmos funcionam de maneira semelhante, consiste em: dividir o conjunto de dados em treino e teste; treinar o modelo no conjunto treino; validar o modelo no conjunto teste e repetir as etapas anteriores algumas vezes. Dentro da validação cruzada existem diversas técnicas onde umas são mais utilizadas. Já mencionamos anteriormente o método\nk-fold, mas exite também os métodos, hold-out, leave-p-out, k-fold stratified, entre outros. Vamos falar sobre alguns deles.\n\nHold-Out Cross Validation: Esta é a técnica mais simples e comum. Ele consiste em remover uma parte dos dados de treinamento e usá-la para obter previsões do modelo treinado no restante dos dados. A estimativa de erro informa como nosso modelo está se saindo em dados não vistos ou no conjunto de validação. A implementação é extremamente fácil e existem pacotes que podem ajudar nisso. Mas apesar disto, esse método tem um grande desvantagem. Se estivermos trabalhando com um conjunto de dados que não é completamente uniforme, podemos acabar em uma situação difícil após a separação. O conjunto de treino pode não representar muito bem o conjunto de teste, ou seja, os conjuntos podem ser bem diferentes, onde um é mais fácil do que o outro.\nK-Fold Cross Validation: O K-Fold pode se apresentar como um técnica que minimiza as desvantagens do método Hold-Out apresentando uma nova maneira de dividir o banco de dados. Neste método os dados são divididos em k subconjuntos (daí o nome). O método de validação é repetido k vezes, onde, a cada vez, um dos k subconjuntos é usado como conjunto de teste e os outros k-1 conjuntos são unidos para formar o conjunto de treinamento. A estimativa de erro é a média de todas as k tentativas. Como cada ponto de dados chega a um conjunto de validação exatamente uma vez e a um conjunto de treinamento k-1 vezes, isso reduz significativamente o viés. Como “regra geral”, k=5 ou k=10 é escolhido, mas não existe nada fixo. Comparando diretamente ao método Hold-Out, o método K-Fold tende a ser melhor, mas também possui uma desvantagem. Aumentar o k resultado no treinamento de mais modelos e o processo de treinamento pode ser custoso e demorado.\nLeave-P-Out Cross Validation: Este método consiste em criar todos os conjuntos de treinamento e testes possíveis usando p amostras como conjunto de teste. Em outras palavras, deixa p pontos de dados fora dos dados de treino, ou seja, se houver n pontos de dados na amostra original, np amostras são usadas para treinar o modelo p pontos são usadas como conjunto teste. Como se pode imaginar, este método é extremamente exaustivo, tendo em vista que é preciso validar o modelo para todas as combinações possíveis e para um p demasiadamente grande, pode ser computacionalmente inviável.\n\nO método de validação cruzada também pode nos ajudar a ajustar hiperparâmetros, falaremos sobre isso posteriormente.\n\n\n7.1.7 Hiperparâmetros\nJá discutimos anteriormente sobre treinamento de um modelo e mencionamos os Hiperparâmetros, vamos agora discutir de um modo geral o que eles são e qual a sua importância. Hiperparâmetros são atributos que controlam o treinamento de um modelo. Eles ajudam a direcionar um modelo, melhorando seu desempenho e evitando que ele aprenda somente com os dados de treino, ou seja, evitar o overfitting e underfitting. Por exemplo, no processo de configuração de uma rede neural, precisamos decidir quantas camadas ocultas de nós precisam ser usadas entre a camada de entrada e a camada de saída, assim como quantos nós cada camada vai usar. Esses parâmetros não estão diretamente ligados aos dados de treino e não são aprendidos diretamente pelos modelos. Parâmetros são as variáveis ​​que o algoritmo de Machine Learning usa para prever resultados com base na entrada de dados históricos. Já os hiperparâmetros são variáveis ​​que são especificadas ao longo do processo de construção do modelo. Com isso os hiperparâmetros são fornecidos antes dos parâmetros, ou podemos dizer que os hiperparâmetros são utilizados para avaliar os parâmetros ideais do modelo. Encontrar a melhor combinação de hiperparâmetros pode fazer a diferença no seu modelo.\nExemplos comuns de hiperparâmetros\n\nNúmero de árvores em um algoritmo de Random Forest.\nA escolha da função custo ou perda que o modelo usará.\nNúmero de cadamadas ocultas em redes neurais.\nQuantidade mínima de observações dentro de um nó.\nTaxa de divisão de treino e teste.\nProporção de linhas para sortear por árvore.\n\n\n7.1.7.1 Ajuste de hiperparâmetro\nO ajuste de hiperparâmetro consiste em encontrar a configuração de hiperparâmetro que irá resultar no melhor desempenho do modelo. A busca manual pode ser utilizada para localizar os hiperparâmetros ótimos, utilizando uma abordagem de acerto e erro, mas obviamente isso levaria muito tempo. As técnicas mais utilizadas para isso são o Grid Search ou Random Search.\n\nGrid Search: Essa é uma técnica de ajuste que tenta calcular os valores ótimos dos hiperparâmetros. É calculado exaustivamente o desempenho do modelo para cada combinação de todos os hiperparâmetros fornecidos previamente e escolhe, a partir dai, o valor ideal para os hiperparâmetros. Apesar de ser uma técnica que foge da abordagem manual, é um processo demorado e computacionalmente caro.\nRandom Search: Random Search é um método que usa combinações aleatórias de hiperparâmetros para treinar o modelo. As melhores combinações são usadas. A diferença para o Grid Search é que aqui não é especificado um conjunto de valores para os hiperparâmetros. Em vez disso, os valores de cada hiperparâmetro são amostrados a partir de uma distribuição. Essa técnica permite controlar o número de tentativas de combinações de hiperparâmetros, diferente da Grid Search, onde todas as combinações possíveis são tentadas."
  },
  {
    "objectID": "supervisionada.html#agoritmos-de-aprendizado-de-máquina",
    "href": "supervisionada.html#agoritmos-de-aprendizado-de-máquina",
    "title": "7  Aprendizado supervisionado",
    "section": "7.2 Agoritmos de Aprendizado de máquina",
    "text": "7.2 Agoritmos de Aprendizado de máquina\n\n7.2.1 Árvores de Decisão\nAs Árvores de Decisão são algoritmos de aprendizado supervisionado que podem ser utilizados tanto para classificação quanto para regressão. Possui uma estrutura hierárquica em árvore, tendo um nó raiz, ramificações, nós internos e nós folhas. Na análise de decisão, uma árvore de decisão pode ser usada para representar visualmente e explicitamente decisões e tomadas de decisão.\n\n\n\n\n\nÁrvore de Decisçao\n\n\n\n\nNa imagem acima podemos ver uma representação de uma árvore de decisão, que começa com um nó raiz, que não possui ramificações de entrada, a partir da qual a árvore se divide em ramos. Os ramos alimentam os nós internos ou nós decisão. Cada ramificação contém um conjunto de atributos ou regras de classificação, associado a um determinado rótulo de classe, que pode ser encontrado na extremidade da ramificação. O final dos ramos que não se dividem mais são os nós folha. Os nós folha representam todos os resultados possíveis dentro do conjunto de dados.\nO tipo de estrutura de fluxograma é fácil de ser interpretado e cria uma representação que permite que diferentes grupos entendam melhor porque uma determinada decisão foi tomada.\nO algoritmo de Árvore de Decisão possui uma estratégia que busca identificar os pontos de divisão ideias dentro de uma árvore. O processo de divisão é repetido até que todos ou a maioria dos registros tenham sido classificados em um rótulo de classe específico. Classificar ou não todos os pontos de dados como conjuntos depende principalmente da complexidade do algoritmo. Árvores menores ou mais simples, são mais fáceis de atingir nós de folhas puros, onde pontos de dados caem em uma única classe, além de serem mais fáceis de visualizar e interpretar as decisões. Em contrapartida, aumentar o tamanho de uma árvore dificulta manter essa pureza, podendo resultar em poucos dados caindo em uma determinada subárvore, como consequência, isso pode levar ao overfitting. Portanto, esses algoritmos têm preferência por árvores pequenas e simples, adicionando complexidade apenas quando realmente necessário. Árvores de decisão geralmente crescem de forma arbitrária, portanto é preciso decidir quais recursos escolher e quais condições usar para dividir uma árvore, além de saber quando parar essa divisão.\n\n7.2.1.1 Medidas de seleção de atributo\nComo mencionado anteriormente, uma Árvore de decisão é feita a partir de atributos e decidir qual atributo colocar na raiz ou em diferentes níveis da Árvore pode ser complicado. Aqui a aleatoriedade na escolha não é uma boa opção, podendo gerar resultados com baixa precisão. Para o problema de seleção de atributos, temos alguns critérios como: Entropia e ganho de informação, índice de Gini , taxa de ganho, entre outros.\n\nEntropia: Entropia é uma medida de aleatoriedade (impureza) dos valores de uma amostra. Quanto maior a entropia, mais difícil é tirar conclusões dessa informação. É definida pela seguinte fórmula:\n\n\\[\nEntropia(S) = - \\sum_{i=1}^{c} p_{i}\\,log_2\\,pi\n\\]\nOnde, S representa o Estado atual e \\(p_i\\) é a probabilidade de um evento \\(i\\) do estado S.\nOs valores variam entre 0 e 1. Para selecionar o melhor recurso para dividir e encontrar a Árvore ideal, deve-se buscar o atributo com menor entropia.\n\nGanho de Informação: Ganho é usado para medir o quão bem um atributo separa os exemplos de treino de acordo com sua classificação alvo. Quanto maior o ganho de informação, melhor. Ou seja, na construção de uma Árvore de Decisão buscamos um atributo que retorne o maior ganho de informação e a menor entropia. Ganho de informação é calculado a diferença entre a entropia antes da divisão e a entropia média após a divisão do conjunto de dados. Segue a fórmula:\n\n\\[\nIG(Y,X) = Entropia(Y) - Entropia (Y|X)\n\\]\nPodemos visualizar a fórmula como: subtraímos a entropia de \\(Y|X\\) da entropia apenas de \\(Y\\) para calcular a redução da incerteza sobre \\(Y\\) dada uma informação adicional \\(X\\) sobre \\(Y\\).\n\nÍndice de Gini: Também conhecido como impureza de Gini, o Índice de Gini calcula a quantidade de probabilidade de um recurso específico que é classificado incorretamente quando selecionado aleatoriamente. Se todos os elementos estiverem vinculados a uma única classe, ela pode ser chamada de pura. É calculado subtraindo a soma das probabilidades ao quadrado de cada classe de um. Segue a fórmula:\n\n\\[\nGN = 1- \\sum_{i=1}^{n} (p_i)^2\n\\]\nOnde \\(pi\\) denota a probabilidade de um elemento ser classificado para uma classe distinta. O Índice de Gini é semelhante a entropia. Varia entre 0 e 1, onde 0 expressa a pureza da classificação e 1 indica a distribuição aleatória dos elementos em várias classes.\n\nTaxa de Ganho: Taxa de ganho tenta diminuir o viés do ganho de informação. Leva em consideração o número de ramificações que resultariam antes de fazer a divisão. Ele corrige o ganho de informação levando em consideração as informações intrínsecas de uma divisão. A Informação Intrínseca (II) é definida como a entropia das proporções do subconjunto de dados. Segue a fórmula: \\[\nII = -\\sum_{i=1}^n \\frac{N(t_i)}{N(t)} *log{_2}  \\frac{N(t_i)}{N(t)}\n\\]\nOnde \\(N(t_i)\\) é o número de vezes que \\(t_i\\) ocorre dividido pela contagem total de eventos \\(N(t)\\) onde \\(t\\) é o conjunto de eventos.\nA Taxa de Ganho então é dada por:\n\\[\nGR = \\frac{Ganho\\ de\\ Informação}{Informação\\ intrínseca}\n\\]\n\nPara todas as variáveis ​​preditoras, aquela que fornece a maior taxa de ganho é escolhida para a divisão.\n\n\n7.2.1.2 Tipos de Árvores de Decisão\nMencionamos anteriormente que o algoritmo de Árvores de Decisão pode ser usado tanto para classificação quanto para regressão, e é claro, existem vários tipos de algoritmos. Vamos falar um pouco sobre os mais famosos.\n\nID3 (Iterative Dichotomiser 3): Esse algoritmo usa uma abordagem gananciosa de cima para baixo para construir a Árvore. Em outras palavras, ele começa a construir a Árvore de cima e de para cada iteração seleciona o melhor recurso no momento para criar um nó. Geralmente é usado apenas para problemas de classificação com recursos nominais. O algoritmo utiliza o Ganho de Informação para encontrar o melhor recurso.\nC4.5: Este algoritmo é uma melhoria em relação ao ID3. Ele pode usar a Taxa de Ganho como função para encontrar o melhor recurso.\nCART: CART é uma abreviação de “classification and regression trees” (Árvores de classificação e Regressão). Como o próprio nome já diz, pode ser usado tanto para classificação quanto para regressão. O algoritmo busca o melhor critério através do Índice de Gini.\n\n\n\n7.2.1.3 Podando Árvores de Decisão\nComo mencionamos anteriormente, Árvores complexas podem causar overfitting, cabe dizer que as Árvores são os algoritmos mais suscetíveis ao overfitting. Em alguns casos, a Árvore poderá se ramificar inúmeras vezes, gerando uma folha para cada observação, ou seja, fornecendo 100% de precisão no conjunto de treino, logo, é preciso ter um limite.\nPodar uma Árvore de Decisão consiste em remover partes da Árvore que não fornecem poder para classificar instâncias. A poda pode ser distinguida em:\n\nPré-poda: A Árvore é interrompida antes de concluir a classificação do conjunto de treinamento. É um método alternativo que tenta interromper o processo de construção da árvore antes que ele produza folhas com amostras muito pequenas. Essa heurística é conhecida como parada antecipada. Em cada estágio de divisão da árvore, é verificado o erro de validação cruzada. Se o erro não diminuir significativamente, então paramos. A parada antecipada pode prejudicar o ajuste, parando muito cedo.\nPós-poda: Normalmente a técnica mais utilizada, permite que a Árvore classifique o conjunto de treinamento antes de podá-la. Este método consiste em, a partir da Árvore não podada, pega uma sequência de subárvores (podadas) e escolher a melhor por meio da validação cruzada. É escolhida a que tem maior precisão no conjunto de treinamento validado cruzadamente.\n\n\n\n7.2.1.4 Floresta Aleatória (Random Forest)\nA Floresta Aleatória é um algoritmo que funciona a partir de Árvores de Decisão. Ele constrói diferentes Árvores em diferentes amostras e leva combina as saídas para alcançar um único resultado, levando em consideração a maioria de votos para classificação e média em caso de regressão. O algoritmo serve tanto para regressão quanto para regressão, mas aqui temos uma novidade, ele pode lidar com os dois, apesar de apresentar melhores resultados para problemas de classificação.\n\n\n7.2.1.5 Funcionamento do algoritmo\nAntes de falarmos de fato sobre o funcionamento do algoritmo, vamos entender de forma básica o método dos conjuntos. O método dos conjuntos, também conhecido como ensemble methods, pode ser definido resumidamente como, combinar vários modelos. No caso da Floresta Aleatória, uma combinação de Árvores de decisão. Os modelos são combinados e suas previsões são agregadas para identificar o resultado mais popular. Os métodos de conjunto mais populares são bagging e boosting. Em resumo o método bagging, que é usado em Floresta Aleatória, cria um subconjunto de treinamento diferente a partir dos dados de treinamento de amostra com substituição, o que significa que os pontos de dados individuais podem ser escolhidos mais de uma vez. Depos, dependendo do tipo da tarefa, classificação ou regressão, a saída final é baseada na votação da maioria ou a média. O método boosting é utilizado para outros algoritmos como, por exemplo, XGBOOST, portanto, falaremos sobre ele mais pra frente.\nCerto, entendido o método dos conjuntos podemos entrar no algoritmo em si. É basicamente uma extensão do método bagging, já que utiliza o método bagging e a aleatoriedade para criar uma floresta não correlacionada de Árvores de Decisão. A aleatoriedade se dá ao fato que o modelo utiliza uma amostragem aleatória do conjunto de dados de treinamento ao construir árvores e subconjuntos aleatórios de recursos considerados ao dividir os nós, o que garante baixa correlação entre as Árvores.\nOs algoritmos possuem alguns hiperparâmetros, que como explicado anteriormente, precisam ser definidos antes do treinamento do modelo. Os três principais são: o tamanho do nó, o número de Árvores e o número de recursos amostrados. A partir daí o algoritmo pode ser usado para resolver problemas de regressão ou classificação.\n\n\n\n7.2.2 Algoritmos de boosting\nComo falamos anteriormente, boosting é uma técnica de método dos conjuntos. Diferente de bagging que trabalha de forma paralela, considerando os modelos independentes uns dos outros, boosting trabalha de forma diferente. Boosting se refere a algoritmos que convertem alunos fracos em alunos fortes. Trabalha de maneira sequencial, ajustando iterativamente o peso da observação de acordo com a última classificação, assim, diminui o erro do viés e constrói preditivos fortes. Em outras palavras, a técnica consiste em, primeiramente, construir um modelo inicial com os dados de treinamento. Depois, um outro modelo é construído visando corrigir os erros do modelo anterior, atribuindo pesos caso uma entrada seja fornecida erroneamente. O processo continua e adiciona modelos até que todo o conjunto de dados de treino seja previsto corretamente ou o número máximo de modelos seja adicionado. Boosting é um algoritmo genérico e não um modelo, vamos então apresentar os principais modelos baseados no método boosting: Adaptative Bossting (AdaBoost), Gradient Boosting e XGBoost.\n\n7.2.2.1 AdaBoost\nAntes de entrarmos no algoritmo AdaBoost de fato, vamos relembrar o Algoritmo de Floresta Aleatória. Em Floresta Aleatória, o algoritmo cria diversas Árvores que consistem em um nó inicial com várias nós folhas. Não existem regras quanto ao tamanho de cada Árvore, assim, pode haver umas maiores que outras. No modelo AdaBoost, as Árvores possuem um nível, ou seja, apenas 1 divisão conhecido como Stump (toco).\nReforçando, o algoritmo tem apenas um nó com duas folhas. Tocos são aprendizes fracos, pois as técnicas de reforço preferem isso. Como em AdaBoost o erro do primeiro toco influência nos outros, a ordem dos tocos é muito importante. Reforçando, o algoritmo irá atribuir pesos mais altos aos pontos classificados erroneamente. Todos os pontos que têm pesos maiores recebem mais importância no próximo modelo.\n\n\n7.2.2.2 Funcionamento do algoritmo\nPara entendermos de uma forma um pouco mais clara, vamos fazer um passo a passo. Antes do primeiro passo de fato, todos os pontos de dados irão receber algum peso. Inicialmente, os pesos serão iguais para todos. O peso é calculado na forma \\(W = \\frac{1}{N}\\), onde \\(N\\) é o número de observações.\n\nPasso 1: Em primeiro lugar, o algoritmo pega o primeiro recurso e cria o primeiro tronco. Ele irá criar o mesmo número de tocos que o número de recursos, a partir daí, temos as primeiras Árvores de Decisão. O modelo então calcula o índice de Gini para cada Árovre e seleciona aquela com o índice mais baixo para ser o primeiro aprendiz base.\nPasso 2: O segundo passo é calcular o desempenho do toco com um método conhecido como “Importance” ou “Influence”. É calculado usando a seguinte fórmula:\n\\[\n\\mbox{Performance} (\\alpha) = \\frac{1}{2} log(\\frac{1- Erro\\ total }{Erro\\ total})\n\\]O valor sempre será um valor entre 0 e 1, onde 0 indica um toco perfeito e 1 um toco horrível. Calcular o desempenho de um toco é importante pois é preciso atualizar o peso da amostra antes de prosseguir para o próximo modelo. Se o mesmo peso for apicado, a saída recebida será a do primeiro modelo. Previsões erradas receberão mais peso, enquanto os pesos das previsões erradas serão diminuídos. Assim, no próximo modelo com os pesos atualizados, sera dada mais preferência aos pontos com pesos maiores.\nPasso 3: O próximo passo é atualizar os pesos. A atualização é feita utilizando a seguinte fórmula:\n\\[\n\\mbox{Novo peso} = \\mbox{peso antigo} * e^{\\pm Performance (\\alpha)}\n\\] A quantidade \\(\\alpha\\) será negativa quando a amostra for classificada corretamente e positiva quando classificada incorretamente. Vale ressaltar que a soma total de todos os pesos deve ser igual a 1. Em muitos casos os pesos atualizados não irão somar 1, então será necessário normalizar os pesos. A normalização é feita dividindo cada peso pela soma total dos pesos atualizados. Após a normalização dso pesos da amostra, a soma será 1.\n\nPasso 4: Agora, para verificar se os erros diminuíram ou não, a próxima etapa é criar um novo conjunto de dados. Para isso, com base nos novos pesos amostrais, nossas observações serão divididas em baldes, basicamente são intervalos de valores de \\(\\alpha\\). A partir daí, o algoritmo seleciona números aleatórios entre 0 e 1. Ele verifica em qual balde o valor selecionado pertence e seleciona esse registro no novo conjunto de dados. Como registros classificados incorretamente têm pesos amostrais maiores, existe mais probabilidade de serem selecionados várias vezes.\n\nPasso 5: Por fim, com o novo conjunto de dados, o algoritmo cria uma nova Árvore de decisão (toco) e repete o processo desde o primeiro passo até passar por todos os tocos. O processo é repetido até que um erro de treinamento baixo seja alcançado. Suponha um algoritmo simples que tenha construído 3 Árvores de Decisão de maneira sequencial. Assim, o conjunto de dados teste passará por todas as Árvores e, da mesma forma que em Florestas Aleatórias, a classe será selecionada com base na maioria. A partir daí é possível fazer previsões para o conjunto de teste.\n\n\n\n7.2.2.3 Gradient Boosting (Aumento de Gradiente)\nO modelo Gradient Boosting, também é um modelo de reforço, ou seja, utiliza técnicas de conjunto que pode ser usado tanto para regressão quanto para classificação. Este modelo possui algumas semelhanças com o Ada Boost e como já vimos sobre ele, aqui vamos focar um pouco mais em suas diferenças.\nA principal diferença deste modelo para o Ada Boost está no que ele faz com os valores subajustados do seu antecessor. Enquanto Ada Boost ajusta os pesos a cada interação, Gradient Boosting tenta ajustar o novo preditor aos erros residuais cometidos pelo preditor anterior.\nOutra diferença é que, enquanto Ada Boost começa construindo um toco, Gradient Boost começa fazendo uma única folha. Esta folha representa uma estimativa inicial para os pesos das amostras. Em caso de um problema de regressão, ao tentar prever um valor contínuo, o primeiro palpite é o valor médio. A partir daí, o modelo constrói uma Árvore. Aqui, a Árvore é maior que um toco, mas o modelo ainda restringe o tamanho dessa Árvore.\n\n\n7.2.2.4 Funcionamento do modelo\nVamos entender um pouco melhor o funcionamento do modelo fazendo um passo a passo resumido, assim como foi feito em Ada Boost. Vamos exemplificar também focando no modelo de Gradient Boosting para regressão.\n\nPasso 1: Como falamos anteriormente, o modelo começa criando uma única folha. Como estamos em um cenário de dados contínuos, essa folha então é a média das observações. Essa seria nossa previsão para o modelo base.\nPasso 2: O próximo passo é calcular os pseudo-resíduos, que basicamente é o valor observado - valor previsto. O termo pseudo-resíduo é baseado na regressão linear, onde a diferença entre os valores observados e os valores previstos resulta em resíduos. Utilizando os resíduos como alvo é possível gerar novas previsões. Neste caso, as previsões serão os valores de erros.\nPasso 3: Agora é combinado a folha original com a nova Árvore para fazer uma nova predição a partir dos dados de treinamento.\n\nEm resumo, o algoritmo começa com uma única folha. Em seguida é adicionada uma Árvore com base nos resíduos e dimensiona a contribuição das árvores para a previsão final com a taxa de aprendizado. A partir daí continua adicionando Árvores com base nos erros cometidos pela Árvore anterior.\nO modelo possui alguns hiperparâmetros importantes como, o número de estimadores, a profundidade máxima da Árvore, a divisão mínima das amostras, entre outros. Para obter valores precisos desses hiperparâmetros é possível utilizar as técnicas mencionadas anteriormente.\nResumimos bastante os passos e o funcionamento do modelo, mas deve-se ter em mente que existe uma matemática carregada por trás desses modelos. Cada passo, cada função perdida é utilizada por um motivo comprovado.\n\n\n\n7.2.3 Support Vector Machine (SVM)\nPara finalizar, vamos introduzir um último algoritmo muito conhecido no ramo de aprendizado de máquinas. Support Vector Machine, ou SVM, é um algoritmo de aprendizagem supervisionada muito popular que pode ser usado tanto para classificação quanto para regressão. Entretando, esse algoritmo é usado principalmente para classificação, então, focaremos mais nessa parte.\nA ideia basica do algoritmo é relativamente simples: criar uma linha ou um hiperplano que separa os dados em classe para que possamos colcoar facilmente o novo ponto de dados na categoria correta futuramente.\n\n7.2.3.1 Como o modelo funciona\nA ideia inicial do SVM é encontrar uma linha ou hiperplano entre os dados de duas classes. O algoritmo então escolhe pontos ou vetores extremos que ajudam a criar o hiperplano. Esses casos extremos são os vetores de suporte (daí o nome). O SVM recebe os dados de entrada e gera uma linha que separa essas classes da melhor forma possível.\nUm exemplo bem simples para entender o modelo é: suponha que tenhamos um conjunto de dados com duas classes, bolas vermelhas e quadrados azuis. precisamos então encontrar uma linha que separe esse conjunto de dados em duas classes (vermelho e azul).\nComo estamos em um espaço 2-d, uma única linha reta pode separar essa margem. Mas existe uma infinidade de linhas que podem separar essas classes, o SVM então busca encontrar a ideal. Mas como?\nO SVM encontra o ponto mais próximo das linhas de ambas as classes. Esses pontos são chamados de vetores de suporte. Esses pontos são chamados de vetores de suporte e a distância entre os vetores e o hiperplano é chamada de margem. O SVM então busca maximizar essa margem, onde, o hiperplano com margem máxima é chamado de hiperplano ótimo.\nCerto, mas nem sempre estamos um cenário onde nossos dados estão linearmente organizados. Neste caso, não podemos apenas desenhar uma única linha reta. Neste cenário, precisamos adicionar mais uma dimensão. Se para dados lineares trabalhos com duas dimensões (x e y), para dados lineares teremos uma terceira dimensão (z). Deixamos as coordenadas no eixo z serem governadas pela restrição,\n\\[z =x^2 + y^2\\]\n\nAssim, a coordenada z é basicamente o quadrado da distância do ponto à origem.\nFazendo isso, os dados se tornam linearmente separáveis. Seja z = k, onde k é uma constante. Como \\(z =x^2 + y^2\\), temos \\(k =x^2 + y^2\\); que é uma equação de um círculo. Com isso, é possível projetar o separador linar em dimensão superior de volta as dimensões originais usando esta transformação. Concluindo, podemos então classificar os dados adicionando uma dimensão extra a eles, fazendo com que se tornem linearmente separáveis. Encontrar a transformação correta para qualquer conjunto de dados não é tão fácil, mas existem recursos que podem ser usados na implementação do algoritmo para ajudar neste trabalho."
  },
  {
    "objectID": "supervisionada.html#considerações-finais",
    "href": "supervisionada.html#considerações-finais",
    "title": "7  Aprendizado supervisionado",
    "section": "7.3 Considerações finais",
    "text": "7.3 Considerações finais\nNeste capítulo buscamos abordar os principais tópicos do aprendizado supervisionado. O objetivo aqui foi dar uma breve noção sobre o funcionamento dos algoritmos, diferenças nos métodos de funciomanetos, possíveis problemas e dificuldades. Aqui foi apenas o pontapé inicial para que você consiga entender e implementar algoritmos supervisionados."
  }
]