[
  {
    "objectID": "naosupervisionado.html#análise-de-agrupamentos",
    "href": "naosupervisionado.html#análise-de-agrupamentos",
    "title": "9  Aprendizado Não Supervisionado",
    "section": "9.1 Análise de Agrupamentos",
    "text": "9.1 Análise de Agrupamentos\nComo descrito anteriormente e reforçado aqui, na análise de agrupamento, buscamos identificar regiões no espaço dos dados que possuam um grande número de observações próximas umas das outras. Essas regiões são chamadas de clusters. A ideia é agrupar indivíduos que sejam semelhantes entre si e diferentes dos indivíduos em outros clusters. Essa técnica é chamada de aprendizado não supervisionado, pois não utilizamos uma variável específica como referência para avaliar o resultado do agrupamento.\nFormalmente, os clusters são definidos da seguinte forma:\n\nCada cluster é um grupo de observações;\nTodos os indivíduos pertencem a pelo menos um cluster;\nDois clusters diferentes não possuem observações em comum.\n\nAo realizar o agrupamento de dados, é importante utilizar um método que maximize as diferenças entre os clusters, ao mesmo tempo que minimiza as diferenças dentro de cada cluster. Para isso, são utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferenças entre as observações.\nAs medidas de dissimilaridade mais comumente usadas são a distância euclidiana e a distância euclidiana quadrática, como apresentado abaixo respectivamente:\n\\[\n\\begin{split}\nd(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\\\\nd^2(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2\n\\end{split}\n\\]\nOutras medidas menos utilizadas incluem a distância absoluta e a distância de Mahalanobis, que leva em consideração a matriz de covariância, respectivamente representadas como:\n\\[\n\\begin{split}\nd_a(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\\\\nd_M(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_i')' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_i')}\n\\end{split}\n\\]\nUma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados é por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade \\(a(x_i,x_j)\\) entre cada par de objetos \\(x_i\\) e \\(x_j\\) com \\(i,j = 1,2,\\dots,N.\\)\n\\[\n\\begin{align}\nA =\n\\begin{bmatrix}\n          a(x_1,x_1) & a(x_1,x_2) & \\cdots &a(x_1,x_N) \\\\\n         a(x_2,x_1) & a(x_2,x_2) & \\cdots & a(x_2,x_N)  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           a(x_N,x_1) & a(x_N,x_2) & \\cdots & a(x_N,x_N)\n         \\end{bmatrix}.\n  \\end{align}\n\\]\nAs matrizes de dissimilaridade podem ser obtidas com apoio da função dist(), onde o tipo de distância (Euclidiana por exemplo), é passada no parâmetro da função, method , veja a seguir, um exemplo aplicado ao conjunto de indicadores obstétricos, esse DataSet séra o referncial para a sessão atual, será considerado apenas as colunas dos indicadores.\n\ndist_euclidian &lt;- dist(scale(dados_indicadores[,-c(1:4)]), method = \"euclidean\")\n\nO códio acima cria e armazena um objeto do tipo dist que será utilizado em exemplos futuros.\nA análise de agrupamento é uma ferramenta valiosa que permite identificar estratos em uma população e detectar outliers. É importante considerar a escalabilidade do método, sua capacidade de lidar com diferentes tipos de variáveis e clusters de formatos variados. Além disso, a robustez em relação a outliers e a capacidade de agrupar dados de alta dimensionalidade são considerações essenciais. Existem diversos métodos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas próximas sessões, exploraremos os métodos considerados e suas aplicações adequadas, bem como métodos de avaliação de qualidade para os agrupamentos.\nNeste capítulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Além disso, temos os métodos hierárquicos, nos quais os grupos são organizados em uma estrutura de árvore.\nOutra abordagem interessante é considerar a densidade dos pontos no espaço. Nesse caso, procuramos identificar regiões mais densas separadas por áreas menos povoadas. Esses métodos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na análise dos dados, aqui será considerado o DBSCAN.\nTambém existe uma classe de métodos que utiliza técnicas de decomposição espectral. Esses métodos reduzem a dimensionalidade dos dados, preservando as informações relevantes dos grupos presentes. São os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.\nCada uma dessas abordagens possui suas próprias características, vantagens e limitações.\n\n9.1.1 Métodos por Particionamento\nOs métodos por particionamento são comumente utilizados para agrupar dados, onde cada partição representa um cluster. Esses métodos são baseados em distância e envolvem a realocação iterativa das observações entre os clusters para obter um particionamento otimizado.\nA escolha do número de clusters é um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum é o método do cotovelo, que considera a relação entre a variância total intraclusters e o número de grupos criados. O método do cotovelo considera que aumentar o número de clusters reduz a variância, mas em algum ponto, não há melhora significativa na granularidade do agrupamento. Esse ponto ótimo, que indica o número adequado de clusters, é identificado no gráfico por uma curva tracejada (veja Figura 9.1).\n\n\n\nFigura 9.1: Screeplot para seleção de número de clusters\n\n\nA variância total intraclusters é calculada utilizando as distâncias euclidianas quadráticas entre as observações e o centróide do respectivo grupo. O centróide \\(c_l\\) de um grupo \\(C_l\\) é obtido através da média das observações atribuídas a esse cluster, utilizando a fórmula:\n\\[c_l = \\frac{1}{|\\mathcal{C}_l|} \\sum{i \\in \\mathcal{C}_l} \\mathbf{x}_i\\]\nA variância total intraclusters é calculada como a soma das distâncias euclidianas quadráticas entre as observações e os respectivos centróides, utilizando a fórmula:\n\\[\\sum_{l=1}^{K} \\sum_{i \\in \\mathcal{C}_l} |\\mathbf{x}_i - \\mathbf{c}_l|^2\\]\nEssas são algumas das abordagens dos métodos por particionamento, aqui será considerado o k-médias e o k-medóides com os algorítmos PAM e CLARA, que serão apresentados a seguir com exemplos de aplicações.\n\n9.1.1.1 K-médias\nO K-médias é um método amplamente utilizado para agrupamento de dados. Ele busca encontrar K partições dos dados, minimizando a variância. O algoritmo de (Lloyd 1982) é comumente usado para realizar o K-médias. Ele envolve os seguintes passos:\n\nescolha dos K centróides iniciais;\nparticionamento dos dados com base na menor distância para cada centróide;\natualização dos centróides com as novas observações atribuídas a eles;\nrepetição dos passos 2 e 3 até que não haja mais mudança de agrupamento. É possível definir um número máximo de iterações para otimizar o método computacionalmente.\n\nUma alternativa é o algoritmo de (Hartigan e Wong 1979), que adiciona uma etapa de validação para alterar os agrupamentos. A cada iteração, verifica-se se houve atualização nos centróides dos grupos. Nesse caso, um novo objeto só é atribuído a um cluster se a soma das distâncias quadráticas diminuir.\nNo entanto, o método K-médias apresenta limitações ao lidar com clusters de formas não convencionais ou grupos com tamanhos muito discrepantes. Além disso, ele é sensível a outliers, pois a inclusão de um dado extremo pode influenciar significativamente o valor do centróide. A aplicação para o software R, tanto do método de agrupamento quanto a escolha do número de clusters K pelo método do cotovelo, segue abaixo, será considerado os dados padronizados para retirar qualquer tendência em função da diferença de escala ou amplitude dos dados:\n\nset.seed (1122)\n#BIBLIOTECAS\nlibrary(ggplot2)\n## padronizacao dos dados \n\ndados_norm &lt;- as.data.frame(scale(dados_indicadores[,-c(1:4)]))\n\n## escolhendo k pelo metodo do cotovelo\n\ncotovelo_kmedias &lt;- factoextra::fviz_nbclust(dados_norm ,\n kmeans,\n method = \"wss\") +\n geom_vline( xintercept = 7, linetype = 2) +\n labs(x = \"Numero de Grupos\", y = \"Variancia Total Intragrupo\", title = \"K-medias\")\n\n## ajustando k-medias com o numero de grupos escolhido\n\nk_medias &lt;- kmeans(dados_norm,\n                        centers = 7)\n\nA função kmeans é uma ferramenta poderosa disponível no R para realizar o agrupamento de dados utilizando o método K-médias. A função kmeans retorna três principais objetos:\n\nCluster_centers: É uma matriz que representa os centróides finais de cada cluster. Cada linha dessa matriz representa um centróide, com as coordenadas correspondentes às variáveis do conjunto de dados.\nCluster_assignment: É um vetor que contém as atribuições de cada observação a um determinado cluster. Cada elemento desse vetor representa o número do cluster ao qual a observação foi atribuída. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.\nWithin_cluster_sum_of_squares: É um valor que representa a soma dos quadrados das distâncias de cada observação em relação ao seu respectivo centróide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homogêneo é o cluster.\n\n\n\n9.1.1.2 K-medóides\nEm situações com valores extremos, os algoritmos K-medóides surgem como uma alternativa ao cálculo do centróide, evitando a influência excessiva desses valores na representação central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por (Kaufman e Rousseeuw 2009) considera um custo para as trocas de medóides a cada iteração. O custo é calculado como a diferença da variância total intragrupo considerando um novo medóide (observação não medóide) em comparação com o medóide atual. A variância total intragrupo é uma medida da dispersão dos pontos dentro de um grupo.\nPara realizar o agrupamento, o algoritmo PAM segue os seguintes passos:\n\nEscolha inicial dos \\(K\\) medóides a partir do conjunto de dados;\nAs observações não selecionadas como medóides são atribuídas ao grupo cujo medóide é o mais próximo;\nSelecionar aleatoriamente uma observação não medóide \\(o_r\\);\nCalcular o custo de se mudar o medóide atual para \\(o_r\\);\nCaso o custo seja menor que 0, realizar a troca de medóide;\nRepetir os passos 2 a 5 até que não haja mais mudanças de agrupamento.\n\nO custo de mudança do medóide atual para outra observação é calculado como a diferença da variância total intragrupo considerando a nova observação como representante em comparação com o medóide atual.\nAlém disso, é comum utilizar a medida de distância absoluta no lugar da distância euclidiana quadrática para calcular a distância entre os pontos e os medóides. O método pode ser visto abaixo:\n\n## escolhendo k pelo metodo do cotovelo\ncotovelo_pam &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::pam,\n                      method = \"wss\") +\n                geom_vline( xintercept = 7, linetype = 2) +\n                labs(x = \"Numero de Grupos\",\n                      y = \"Variancia Total Intragrupo\",\n                      title = \"PAM\")\n\npam &lt;- cluster::pam(dados_norm ,\n                      k = 7)\n\nA função cluster::pam no R retorna os seguintes elementos:\n\nmedoids: Um vetor contendo os índices das observações selecionadas como medóides finais de cada cluster.\nclustering: Um vetor contendo os rótulos dos clusters aos quais cada observação foi atribuída.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, incluindo o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\n\nEsses elementos fornecem informações sobre os medóides finais selecionados, a atribuição de clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento e informações adicionais sobre cada cluster.\nPara lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a variância total intragrupo para cada agrupamento gerado. A partição que apresentar menor variância total intragrupo é selecionada como o resultado final do algoritmo. Observe abaixo a aplicação para o R:\n\ncotovelo_clara &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::clara ,\n                      method = \"wss\") +\n                  geom_vline( xintercept = 7, linetype = 2) +\n                  labs(x = \"Numero de Grupos\",\n                        y = \"Variancia Total Intragrupo\",\n                        title = \"CLARA\")\nclara &lt;- cluster::clara(dados_norm ,\n                          k = 7, samples = 10)\n\nA função cluster::clara no R retorna os seguintes resultados:\n\nmedoids: Um objeto pamobject contendo os medóides finais de cada cluster.\nclustering: Um vetor com os rótulos dos clusters atribuídos a cada observação.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, como o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\nsamples: Uma lista contendo os índices das observações selecionadas em cada subamostra.\ncall: A chamada original da função cluster::clara que foi utilizada.\n\nEsses resultados fornecem detalhes sobre os medóides finais escolhidos, a atribuição dos clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento, informações adicionais sobre os clusters, as subamostras utilizadas e a chamada original da função.\n\n\n\n9.1.2 Métodos Hierárquicos\nOs métodos hierárquicos são utilizados para agrupar dados em diferentes níveis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.\nNa abordagem aglomerativa, os grupos são construídos a partir do nível mais baixo, onde cada observação forma um cluster separado, até atingir o nível mais alto, onde todos os dados estão em um único grupo. A fusão dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, também conhecidas como linkages, podem ser definidas da seguinte forma:\n\nMétodo do vizinho mais próximo (Single linkages): Considera a menor distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\min_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo do vizinho mais distante (Complete linkages): Utiliza a maior distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\max_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo da média das distâncias (Average linkages): Calcula a média das distâncias entre as observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\frac{1}{|C_l||C_{l'}|}\\sum_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo do centróide (Centróide linkages): Considera a distância entre os centróides de cada grupo como medida de dissimilaridade.\n\n\\[\nd(C_l,C_{l'}) = d^2(c_l,c_{l'}) .\n\\]\n\nMétodo de Ward: Minimiza a variância dentro dos grupos ao fundir os clusters que levam à menor variação possível.\n\n\\[\nd(C_l,C_{l'}) = \\frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'}) .\n\\]\nNo contexto dos métodos hierárquicos de agrupamento, a abordagem aglomerativa é amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada iteração é necessário identificar a melhor divisão do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hierárquico aglomerativo consiste em:\n\nCada observação é inicialmente atribuída a um cluster separado.\nCom base no método de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.\nOs dois grupos com a menor dissimilaridade são fundidos em um único grupo.\nRepetem-se os passos 2 e 3 até que todas as observações estejam em um único grupo.\n\nJá na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo único que contém todas as observações e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observações. O algoritmo DIANA segue os seguintes passos:\n\nTodas as observações são agrupadas em um único grupo.\nA observação com a maior dissimilaridade média em relação aos pontos do mesmo grupo é separada em um novo grupo.\nCada observação do grupo inicial é atribuída ao novo grupo se a dissimilaridade média em relação aos objetos desse grupo for menor do que a dissimilaridade média em relação aos demais pontos do grupo inicial.\n\n4.Calcula-se o diâmetro de todos os grupos (a maior dissimilaridade entre duas observações) e seleciona-se o grupo com o maior diâmetro.\n\nRepetem-se os passos 2 a 4 até que cada observação esteja em um grupo separado.\n\nNo agrupamento hierárquico, a visualização dos clusters é feita por meio de um dendrograma, um gráfico ramificado que mostra as junções e divisões dos clusters. A altura do ramo no primeiro nó do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o número de grupos a partir do dendrograma, busca-se uma grande diferença de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma característica dos métodos hierárquicos é que as decisões de agrupamento ou divisão não são desfeitas, ou seja, não há troca de observações entre os clusters. Decisões de união ou divisão mal feitas podem resultar em grupos de baixa qualidade. Além disso, esses métodos não são bem dimensionados, pois cada decisão de mesclagem ou divisão requer a avaliação de muitos objetos ou clusters. Pelo exemplo abaixo na Figura 9.2, uma possível resposta de número adequado de clusters seria de dois ou três grupos.\n\n\n\nFigura 9.2: Exemplo de Dendrograma\n\n\nOs agrupamentos hierárquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das funções hclust e cluster::diana, é obtido um objeto da classe “hclust” e da clase “diana” respectivamente, esses objetos contém informações sobre o agrupamento hierárquico realizado, incluindo a estrutura do dendrograma, as distâncias entre os objetos e outras propriedades relacionadas. Seguido pela seleção, com base no dendrograma, do número K ideal de clusters (Não necessariamente só um K), e o “corte” da árvore aglomerativa no valor ideal identificado. É apresentado abaixo para todos os métodos citados a aplicação para o R, supondo a distância utilizada como a euclidiana calculada no início do capítulo.\n\nlibrary(ggdendro)\n#AGLOMERATIVOS ############\n#METODO WARD -----\n##CRIAR O OBJETO HCLUST\nagl_ward &lt;- hclust(dist_euclidian , method = \"ward.D2\")\n\n# PLOT DO DENDROGRAMA \ndendograma_ward &lt;-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,\n main = \"Ward - cortado em H = 20\")\n\n# \"corte\" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS\nagl_ward_res &lt;- cutree(agl_ward , k = 3:8)\n\n# segue o mesmo para todos os outros metodos\n\n#METODO SINGLE LINKAGE -----\nagl_single &lt;- hclust(dist_euclidian , method = \"single\")\n\ndendograma_single &lt;- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,\n                        main = \"Vizinho mais Proximo - cortado em H = 4\",\n                          xlab = \"\")\n\nagl_single_res &lt;- cutree(agl_single , k = 3:8)\n\n#METODO COMPLETE LINKAGE -----\nagl_complete &lt;-  hclust(dist_euclidian , method = \"complete\")\n\ndendograma_complete &lt;- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,\n                             main = \"Vizinho mais Distante - cortado em H = 10\")\n\nagl_complete_res &lt;- cutree(agl_complete , k = 3:8)\n\n#METODO AVARAGE LINKAGE -----\nagl_ave &lt;- hclust(dist_euclidian , method = \"average\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,\n                                main = \" Media das Distancias - cortado em H = 15\")\n\nagl_ave_res &lt;- cutree(agl_ave, k = 3:8)\n\n#METODO CENTROIDE LINKAGE -----\nagl_cent &lt;- hclust(dist_euclidian , method = \"centroid\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,\n                            main = \" Centroide - cortado em H = 15\")\n\nagl_cent_res &lt;- cutree(agl_cent , k = 3:8)\n\n####### DIVISIVO (DIANA) #########\ndiana &lt;- cluster::diana(dist_euclidian ,\n                            diss = TRUE ,\n                            metric = \"euclidean\",\n                            keep.diss = FALSE ,\n                            keep.data = FALSE\n                            )\n\ndendrograma_diana &lt;-  plot(cut(as.dendrogram(diana), h = 7)$upper ,\n                                   main = \"Diana - cortado em H = 7\")\n\ndiana_res &lt;- cutree(diana , k = 3:8)\n\n\n\n9.1.3 DBSCAN\nO método DBSCAN (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de agrupamento que foi proposto para encontrar clusters com formas arbitrárias, ou seja, clusters que não necessariamente possuem uma forma esférica ou convexa. Ele busca identificar as regiões mais densas do espaço vetorial separadas por regiões com menos objetos, como é possível ver na Figura 9.3 sua eficiência em relação a outros métodos.\n\n\n\nFigura 9.3: Aplicação DBSCAN, Fonte: Boyke et al. (2021)\n\n\nA ideia geral do algoritmo DBSCAN é identificar os clusters de forma que a densidade de pontos ao redor de cada ponto de um grupo seja maior que um limite estabelecido. Ele utiliza dois parâmetros principais: \\(\\epsilon\\) e MinPts, para entendimento do agrupamento DBSCAN observe a definição dos seguintes termos:\n\n\\(\\epsilon\\)-vizinhos: Os \\(\\epsilon\\)-vizinhos de um ponto \\(x_j\\) são os objetos cuja distância para \\(x_i\\) seja menor ou igual a \\(\\epsilon\\), ou seja, a vizinhança de \\(x_i\\) é composta pelos objetos que estão dentro de um raio \\(\\epsilon\\) ao redor de \\(x_i\\).\nPonto de Núcleo: Um ponto \\(x_i\\) é considerado um ponto de núcleo se o número de seus \\(\\epsilon\\)-vizinhos (ou seja, os pontos dentro da vizinhança de \\(x_i\\)) for maior ou igual a um valor mínimo estabelecido chamado MinPts.\nPonto de Borda: Um ponto \\(x_i\\) é considerado um ponto de borda se o número de seus \\(\\epsilon\\) -vizinhos for menor do que MinPts, mas ele está dentro da vizinhança de algum ponto de núcleo.\nDiretamente alcançável por densidade: Um ponto \\(x_j\\) é diretamente alcançável por densidade por um ponto \\(x_i\\) se \\(x_j\\) é um \\(\\epsilon\\)-vizinho de \\(x_i\\) e se \\(x_i\\) é um ponto de núcleo.\nAlcançável por densidade: Um ponto \\(x_i\\) é alcançável por densidade por um ponto \\(x_j\\) se existe uma cadeia de pontos \\(x_{i'}\\) , onde $i’ N \\mathbb{N} $, em que \\(x_1 = x_j, x_N = x_i\\) e \\(x_{i' + 1}\\) é diretamente alcançável por densidade por \\(x_{i'}\\).\nConectado por densidade: Um ponto \\(x_i\\) é conectado por densidade a um ponto \\(x_j\\) se existe um terceiro ponto \\(x_{i'}\\) em que ambos são alcançáveis por densidade por \\(x_{i'}\\).\n\nCom base nesses critérios, o DBSCAN define os grupos da seguinte forma:\n\nSe um ponto \\(x_i\\) é alcançável por densidade por um ponto \\(x_j\\), então ambos pertencem ao mesmo cluster \\(C_l\\).\nTodos os pontos de um cluster são conectados por densidade entre si.\n\nUma das vantagens do DBSCAN é sua capacidade de identificar outliers no conjunto de dados, uma vez que ele não atribui todos os objetos a grupos. Os outliers são definidos como os pontos que não são atribuídos a nenhum cluster. O algoritmo DBSCAN funciona da seguinte maneira:\n\nInicialmente, todas as observações da base de dados são classificadas como “não visitadas”.\nEm seguida, os seguintes passos são executados repetidamente:\n(a). Selecionar aleatoriamente uma observação “não visitada” \\(x_i\\).\n(b). Verificar o número de \\(epsilon\\)-vizinhos da observação \\(x_i\\) selecionada:\n\nSe o número de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for maior ou igual a MinPts, um novo cluster é criado para \\(x_i\\), e todos os objetos na \\(\\epsilon\\)-vizinhança de \\(x_i\\) são adicionados a um conjunto candidato, \\(M\\). O algoritmo adiciona iterativamente a \\(C_l\\) todos os objetos em 𝑀 que não pertencem a nenhum cluster.\nSe o número de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for menor do que MinPts, \\(x_i\\) é marcado como um ponto de ruído (outlier).\n\nRepetir os passos 1 e 2 até que não haja mais observações “não visitadas”.\n\nOs parâmetros \\(\\epsilon\\) e MinPts afetam diretamente o resultado do agrupamento, determinando o tamanho mínimo de cada grupo e a distância entre os clusters. Uma das limitações do DBSCAN é o fato de que esses parâmetros são globais, ou seja, definidos para todos os grupos formados, o que pode não ser ideal quando diferentes clusters possuem densidades de pontos distintas. Para a seleção do valor de MinPts é comum que seja baseado no problema em si ou conhecimento prévio. Porém, uma proposta para seleção é o MinPts = 2×\\(p\\) pode ser um bom valor para o parâmetro. Já o \\(\\epsilon\\) costuma ser baseado no número mínimo de pontos definido, em que é construído o gráfico das distâncias de cada ponto para os MinPts−1 vizinhos mais próximos e escolhido o valor “cotovelo”, ou seja, onde começa a ocorrer um crescimento mais acentuado nas distâncias ordenadas. Abaixo vemos como esse método se dá de forma aplicada para o software de programação tanto da seleção do melhor \\(\\epsilon\\) quanto da aplicação do modelo.\n\n## selecao do melhor eps\n dbscan::kNNdist(dados_diss ,\n k = 3, all = FALSE)\n## ajuste com o eps selecionado\n dbscan &lt;- dbscan::dbscan(dados_norm , eps = 2, minPts = 4)\n\nEssa função retorna os seguintes elementos:\n\ncluster: É um vetor que atribui um número de cluster a cada ponto de dados no conjunto de entrada. Os pontos que não pertencem a nenhum cluster são rotulados como “0” ou como “outlier”.\neps: É o valor do raio (epsilon) usado no algoritmo.\nminPts: É o número mínimo de pontos necessários para formar um cluster.\nborder: É um vetor lógico que indica se cada ponto é um ponto de borda.\nreachability: É um vetor que armazena os valores de reachability distance de cada ponto.\ncore_dist: É um vetor que contém as distâncias de densidade mínima( core distance ) para cada ponto.\n\nUma técnica relacionada ao DBSCAN que lida com a limitação dos parâmetros globais é o algoritmo OPTICS (Ordering Points To Identify the Clustering Structure), proposto por (Ankerst et al. 1999). O OPTICS ordena os dados de forma que as observações em grupos mais densos estarão mais próximas na ordenação. Esse algoritmo pode ser uma opção para explorar diferentes densidades de pontos em diferentes clusters.\n\n\n9.1.4 Agrupamento Espectral\nAgrupar conjuntos de dados com alta dimensionalidade pode ser desafiador para alguns métodos de agrupamento. A grande quantidade de variáveis em alta dimensão pode aumentar significativamente o custo computacional e dificultar a separação adequada dos grupos. Nesse contexto, os métodos de agrupamento espectral surgem como uma abordagem promissora, pois permitem reduzir a dimensionalidade dos dados sem perder a informação contida nas variáveis, além de melhorar a separação entre grupos que podem não estar claramente distintos devido à alta dimensionalidade.\nOs algoritmos de agrupamento espectral usam uma medida de dissimilaridade para representar o conjunto de dados como um grafo. Nessa representação, os vértices do grafo correspondem às observações (\\(V = v_1, ..., v_N\\)), e as arestas (\\(A\\)) são definidas por uma matriz de similaridade, onde \\(a(x_i, x_{i'})\\) representa o peso da aresta que conecta os vértices \\(x_i\\) e \\(x_{i'}\\). O agrupamento em \\(K\\) clusters é então formulado como um problema de corte de arestas, que pode ser computacionalmente complexo. Para contornar essa complexidade, é aplicada a teoria espectral.\nO algoritmo de Ng, Jordan e Weiss (NJW) é um exemplo de algoritmo de agrupamento espectral. Ele requer a definição prévia de um parâmetro, assim como o algoritmo K-médias. Esse parâmetro é o número de grupos \\(K\\) a serem formados. O algoritmo NJW segue os seguintes passos:\n\nCalcula-se a matriz de similaridade \\(A\\), usando um parâmetro escalar \\(\\sigma\\). A similaridade entre cada par de pontos \\(x_i\\) e \\(x_{i'}\\) é medida usando uma função de similaridade, como a função Gaussiana. A matriz \\(A\\) é preenchida com os valores calculados.\nCalcula-se a matriz de graus \\(D\\), onde cada elemento \\(d_{ij}\\) na diagonal principal representa a soma das similaridades da linha \\(i\\) da matriz \\(A\\). A matriz \\(D\\) captura a estrutura de conectividade do conjunto de dados.\nConstrói-se a matriz Laplaciana \\(L\\), que é uma forma de normalização de \\(A\\). No algoritmo NJW, a matriz Laplaciana é definida como \\(L = D^{(-1/2)} 𝐴 𝐷^{(-1/2)}\\). Essa normalização realça as diferenças entre os grupos de dados.\nIdentificam-se os \\(K\\) maiores autovalores de \\(L\\) e seus respectivos autovetores associados, que são armazenados em uma matriz \\(Z\\) de tamanho \\(N \\times K\\).\nDefine-se uma nova matriz \\(Y\\), normalizada a partir de \\(Z\\). Cada elemento \\(y_{ii'}\\) de \\(Y\\) é calculado dividindo-se o elemento \\(z_{ii'}\\) de \\(Z\\) pelo produto da raiz quadrada do elemento diagonal correspondente a \\(z_{ii}\\) e a raiz quadrada do elemento diagonal correspondente a \\(z_{i'}\\). Essa normalização ajusta as magnitudes dos dados.\nCom a redução da dimensionalidade dos dados na matriz \\(Y\\) (de tamanho \\(N \\times K\\)), pode-se aplicar um algoritmo de agrupamento, como o K-médias, para realizar o agrupamento dos dados.\n\n\nspectral_clustering &lt;- function(data , # dados\n                                k) # numero de grupos\n                                {\n    # matriz de dissimilaridade\n    A &lt;- as.matrix(KRLS::gausskernel (data , 2)) # sigma^2 = 1\n    diag(A) &lt;- 0\n    \n    # matriz Laplaciana\n    L = diag(1 / sqrt(rowSums(A))) %*% A %*% diag(1 / sqrt(rowSums(A)))\n    \n    # matriz de autovetores associados aos k maiores autovalores\n    auto &lt;- eigen(L, symmetric = TRUE)\n    X &lt;- auto$vectors[, 1:k]\n    \n    # matriz normalizada\n    Y &lt;- X / sqrt(rowSums(X ^ 2))\n    \n    # agrupamente k medias na matriz redimensionalizada\n    set.seed (1122)\n    k_means &lt;- kmeans(Y, centers = k)\n    \n    return(list(\n    clusters = k_means$cluster , #retornar grupos\n    auto = auto , #retornar autovalores e autovetores\n    autovetores = X, #retornar autovetores dos K maiores autovalores\n    normalizada = Y #retornar matriz normalizada\n    ))\n}\n\nA escolha dos parâmetros \\(K\\) e \\(\\sigma\\) pode ser feita de forma arbitrária, dependendo do objetivo do estudo. No entanto, existem abordagens e técnicas disponíveis para auxiliar na seleção desses parâmetros. No exemplo acima , o parâmetro \\(\\sigma\\) foi fixado em 1, e o número de grupos \\(K\\) foi escolhido com base na análise dos primeiros autovalores da matriz Laplaciana \\(L\\). O valor ótimo de \\(K\\) é aquele em que ocorre uma queda significativa dos autovalores, uma vez que os maiores autovalores da matriz Laplaciana tendem a ser próximos de 1.\n\n\n9.1.5 Validação do Modelo\nNo problema do agrupamento, não é possível verificar o grau de acerto do resultado obtido, uma vez que os verdadeiros grupos não são conhecidos a priori. Portanto, é necessário aplicar algum tipo de validação à partição final.\nQuatro índices de avaliação interna da qualidade do agrupamento são os principais a serem utilizados atualmente: Davies-Bouldin (DB), Dunn (D), Silhueta (S) e Calinski-Harabasz (CH).\n\n9.1.5.1 Davies-Bouldin (DB):\nO índice DB mede a similaridade média entre cada grupo e seu grupo mais similar dentre os demais clusters. A distância média \\(\\delta_l\\) de um grupo \\(l\\) às suas observações é calculada em relação a um valor referencial \\(m_l\\). A fórmula para \\(\\delta_l\\) é:\n\\[\n\\delta_{l} = \\left(\\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} \\left \\| x_{i} - m_{l} \\right \\|^q\\right)^{\\frac{1}{q}}.\n\\]\nonde \\(n_l\\) é o número de observações no grupo \\(l\\), \\(x_i\\) é uma observação, \\(m_l\\) é o valor referencial (centróide ou medóide) e \\(q\\) é um valor pré-definido, onde os valores mais utilizados são \\(q = 1\\) e \\(q= 2\\). Para o índice, é utilizada também distância entre os grupos \\(\\Delta_{ll'}\\), que é obtida como a distância entre os valores referenciais de cada grupo. A fórmula para \\(\\Delta_{ll'}\\) é:\n\\[\n\\Delta_{ll'} = \\left( \\sum_{j=1}^{p} |m_{jl} - m_{jl'}|^{t} \\right)^{\\frac{1}{t}}.\n\\]\nonde \\(t \\in \\mathbb{N}\\) é pré-definido e geralmente adotado como \\(t = 1\\) (distâncias absolutas) ou \\(t=2\\) (distância euclidiana). Assim, o índice de Davies-Bouldin (DB) fica definido por:\n\\[\nDB = \\frac{1}{K} \\sum_{l=1}^{K} \\max_{l \\neq l'} \\left( \\frac{\\delta_{l} + \\delta_{l'}}{\\Delta_{ll'}} \\right).\n\\] Nesse caso, buscamos agrupar observações de forma a minimizar a variância intragrupo e maximizar a diferença entre eles. Portanto, valores menores do índice de Davies-Bouldin são considerados melhores. Observe a métrica aplicada ao conjunto de dados de agrupamento obtido pelo k-médias (As medidas seguintes também serão aplicadas ao mesmo conjunto).\n\n#Metrica DB usando Centroide\nclusterSim::index.DB(dados_norm ,\n                     k_medias$cluster)$DB\n#Metrica DB usando Medoide\nclusterSim::index.DB(dados_norm ,\n                      k_medias$cluster ,\n                      d = dist_euclidian ,\n                      centrotypes = \"medoids\"\n                      )$DB \n\n\n\n9.1.5.2 Dunn (D):\nO índice D mede a razão entre a separação dos grupos e a variância dentro deles. A separação entre dois grupos \\(l\\) e \\(l'\\) é calculada pela distância do vizinho mais próximo, dessa forma buscamos valores grandes para essa medida de avaliação. A variância intragrupo é representada pelo diâmetro \\(diam_l\\) do cluster. A fórmula para o índice D é:\n\\[\nD = \\frac{{\\min_{l,l' \\in \\{1,...,K\\}; l \\neq l'} d(C_{l}, C_{l'})}}{{\\max_{l \\in \\{1,...,K\\}} diam_{l}}}.\n\\]\n\n#Metrica D\n clValid::dunn(distance = dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.3 Silhueta (S):\nO índice de Silhueta(S) mede a qualidade do agrupamento considerando as distâncias de cada ponto em relação às observações do mesmo grupo e aos demais clusters formados. Para cada observação \\(i\\) pertencente ao grupo \\(C_l\\), definimos as medidas \\(a_i\\) e \\(b_i\\) da seguinte maneira:\n$$\n\\[\\begin{split}\na_i = \\frac{1}{n_{l}-1} \\sum_{i' \\neq i, i' \\in C_{l}}^{n_l} d(x_{i}, x_{i'}),\\\\\n\nb_i = \\min_{l \\neq l'} \\left( \\frac{1}{n_{l'}} \\sum_{i' \\in C_{l'}}^{n_{l'}} d(x_{i}, x_{i'}) \\right).\n\\end{split}\\]\n$$\nonde \\(d(x_i,x_{i'})\\) representa a distância entre as observações \\(x_i\\) e \\(x_{i'}\\). A partir dessas medidas, calculamos a silhueta \\(s_i\\) da observação \\(i\\), com \\(i = 1, ..., N\\), usando a fórmula:\n\\[\ns_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}.\n\\]\nO coeficiente de Silhueta S global é obtido por:\n\\[\nS = \\frac{1}{K} \\sum_{l=1}^{K} \\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} s_i.\n\\] Dessa forma, valores maiores de S indicam agrupamentos mais densos e separados, o que é considerado um cenário adequado para um agrupamento bem-sucedido.\n\n#Metrica S\nclusterSim::index.S(dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.4 Calinski-Harabasz (CH):\nO índice de Calinski-Harabasz (CH) considera a variância intra-grupo, chamada de \\(WGGS_l\\), de cada cluster \\(C_l\\) gerado, levando em conta a distância quadrática de cada observação em relação ao seu valor de referência \\(m_l\\), que pode ser um centróide ou medóide. A variância intra-grupo total,\\(WGGS\\), é calculada como a soma das variâncias intra-grupo de cada cluster:\n\\[\nWGGS = \\sum_{l=1}^{K} \\sum_{i=1}^{n_{l}} d^2(x_{i}, m_l).\n\\]\nAlém disso, o cálculo do índice CH utiliza uma medida de dispersão \\(BGSS\\) entre os grupos, que é obtida através da soma ponderada das distâncias quadráticas do valor de referência de cada cluster em relação a um valor central global \\(m\\). Essa medida é calculada da seguinte forma:\n\\[\nBGSS = \\sum_{l=1}^{K} n_{l} d^2(m_l, m).\n\\]\nDessa forma, o índice CH de Calinski-Harabasz é dado por:\n\\[\nCH = \\frac{(N - K) }{K - 1}\\times \\frac{BGSS}{WGSS}.\n\\]\nOnde, valores maiores do índice CH indicam grupos com menor variância e bem separados, o que é considerado uma boa característica de um agrupamento.\n\n#Metrica CH usando Medoide\nclusterSim::index.G1(dados_norm , k_medias$cluster , \n                     d = dist_euclidian , centrotypes = \"medoids\")\n\n#Metrica CH usando centroide\nclusterSim::index.G1(dados_norm , k_medias$cluster)\n\n\n\n\n\nAnkerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, e Jörg Sander. 1999. «OPTICS: Ordering points to identify the clustering structure». ACM Sigmod record 28 (2): 49–60.\n\n\nHartigan, John A, e Manchek A Wong. 1979. «Algorithm AS 136: A k-means clustering algorithm». Journal of the royal statistical society. series c (applied statistics) 28 (1): 100–108.\n\n\nKaufman, Leonard, e Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. «Least squares quantization in PCM». IEEE transactions on information theory 28 (2): 129–37."
  },
  {
    "objectID": "PCA.html#introdução.",
    "href": "PCA.html#introdução.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.1 Introdução.",
    "text": "10.1 Introdução.\nA Análise de Componentes Principais (PCA) é uma técnica que busca resumir a variação presente em um conjunto de dados multivariados através de combinações lineares de suas variáveis originais, que são correlacionadas. O objetivo principal é reduzir a dimensionalidade dos dados, representando um grande número de variáveis originais em um número menor de componentes principais. Essas novas variáveis são ordenadas em ordem decrescente de importância, de modo que a primeira componente principal capture a maior quantidade possível da variabilidade total dos dados, e as subsequentes capturem cada vez menos.\nA primeira componente principal é calculada de forma a maximizar a variância explicada pelos dados, ou seja, ela é a direção ao longo da qual os dados apresentam a maior variação possível. As demais componentes são ortogonais à primeira e são combinações lineares das variáveis originais, indicando a importância relativa de cada variável naquele componente. Note então, que por essa definição já podemos perceber a relação com os autovetores e autovetores associados.\nO PCA tem diversas aplicações em áreas como estatística, engenharia e ciência de dados, sendo usado para resumir grandes conjuntos de dados, detectar padrões e estruturas latentes, identificar outliers e reduzir o ruído presente nos dados. Além disso, ele é comumente utilizado como uma técnica de pré-processamento de dados para outras técnicas de análise, como regressão e clustering.\nUma aplicação comum do PCA é na identificação de variáveis latentes em um conjunto de dados, que não são diretamente observáveis, mas podem ser inferidas a partir de outras variáveis observáveis correlacionadas entre si.Um exemplo de variável latente na obstetrícia pode ser a “saúde fetal” ou até mesmo a “saúde materna”. Não pode ser diretamente medida ou observada, mas pode ser inferida a partir de múltiplas variáveis observáveis, como a frequência cardíaca fetal, a pressão arterial materna entre outras."
  },
  {
    "objectID": "PCA.html#como-realizar-a-análise-de-componentes-principais.",
    "href": "PCA.html#como-realizar-a-análise-de-componentes-principais.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.2 Como realizar a análise de componentes principais.",
    "text": "10.2 Como realizar a análise de componentes principais.\nO primeiro passo é entender a definição matemática real das componentes principais. Seja \\(\\boldsymbol{X}\\) um vetor aleatório com \\(\\boldsymbol{\\mu} = E(\\boldsymbol{X})\\) e \\(\\boldsymbol \\Sigma = Var(\\boldsymbol{X})\\) e \\((\\lambda_i,e_i), i = 1,\\dots,p\\) os pares de autovalores e autovetores normalizados associados de \\(\\Sigma\\). Então,\n\\[\n\\begin{split}\n\\boldsymbol{Y} = \\boldsymbol{ O'}\\boldsymbol{X},\\quad \\textrm{com}\\quad \\boldsymbol{O} = [\\boldsymbol{e}_1,\\boldsymbol{e}_2,\\dots,\\boldsymbol{e}_p],\\textrm{ os componentes principais de }\\boldsymbol X\\\\\n\\textrm{ou seja}\\\\\n\\boldsymbol Y =\n\\begin{bmatrix}\nY_1\\\\\n\\vdots\\\\\nY_p\n\\end{bmatrix} \\textrm{ com  } \\quad Y_1 = \\boldsymbol e_1'\\boldsymbol X = e_{11}X_1 + e_{12}X_2 +  \\dots + e_{1p}X_p,\n\\end{split}\n\\]\na primeira componente principal.\nOs elementos na matriz \\(\\boldsymbol{O}\\), denominados loadings, representam as relações entre as variáveis originais e as componentes principais. Cada coluna da matriz \\(\\boldsymbol{O}\\) corresponde a uma componente principal, e as linhas correspondem às variáveis originais. Os elementos \\(e_{ij}\\) são os coeficientes de ponderação que indicam a contribuição de cada variável para cada componente principal. Em outras palavras, eles representam a importância relativa das variáveis na construção das componentes principais.\nComo citado, as componentes então seguem sendo combinações lineares das variáveis originais e os autovetores correspondentes. Os componentes principais de \\(\\boldsymbol{X}\\), \\(\\boldsymbol{Y}\\), são tais que,\n\\[\n\\begin{split}\n\\boldsymbol\\mu_y = E(\\boldsymbol Y) = E(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'E(\\boldsymbol X) = \\boldsymbol O'\\boldsymbol\\mu_x\\\\\n\\boldsymbol\\Sigma_y = Var(\\boldsymbol Y) = Var(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'Var(\\boldsymbol X)\\boldsymbol O = \\boldsymbol{O'\\Sigma_xO= \\Lambda}.\n\\end{split}\n\\]\nOu seja, as componentes principais são construídas de forma que elas sejam não correlacionadas entre si (\\(cov(Y_i,Y_j) = 0\\), para todo \\(i \\neq j\\)) e tenham variâncias iguais aos autovalores correspondentes (\\(Var(Y_i) = \\lambda_i\\)). A prova desse resultado pode ser vista em (Johnson, Wichern, et al. 2002, 5:432).\nÉ de conhecimento geral, como um dos objetivos da análise de dados, a compreensão da distribuição bem como variabilidade dos dados. Podemos então descrever a variância total da população como sendo o somatório de todos os autovalores \\(\\lambda_i\\). A partir disso, podemos escrever a proporção da variância total explicada pela \\(j\\)-ésima componente, como sendo:\n\\[\n\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i}; \\qquad \\forall j =1,\\dots,p.\n\\] Essa definição é muito útil para reduzir a dimensionalidade dos dados, pois nos permite capturar uma proporção significativa da variabilidade total com um número menor de componentes. Dessa forma, para algum \\(p\\) significativamente grande, podemos utilizar \\(d &lt; p\\) componentes ao invés das \\(p\\) variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas \\(d\\) componentes.\nSe \\(Y_i = \\boldsymbol{e'_iX}, i =1\\dots,p\\) são as componentes principais obtidas da matriz de covariância, então\n\\[\n\\rho_{Y_i,X_j} = \\frac{e_{ij}\\sqrt{\\lambda_i}}{\\sigma_{jj}}, \\quad \\forall i,j=1,\\dots p,\n\\]\nSão os coeficientes de correlação entre a componente \\(Y_i\\) e a variável \\(X_j\\). Esse valor auxiliará na compreensão entre a relação indivídual de \\(X_j\\) a uma componente principal \\(Y_i\\), porém não explica a relação desta variável em presença das outras. Por isso, alguns altores recomendam o uso único do valor de \\(e_{ij}\\) para compreender a relação variável-componente. Ambos os resultados serão importântes para compreensão da componente. Observe abaixo a aplicação de todos os conceitos discutidos no software utilizado ( R ), com os devidos comentários.\n\n#Importacao dos dados\ndados &lt;- readRDS('dados/dados_indicadores.rds')\n\n#Matriz de Covariancia\nmatriz_cov &lt;- cov(dados |&gt; dplyr::select_if(is.numeric))\n\n#Calcular os autovalores e autovetores\nresultado &lt;- eigen(matriz_cov)\nautovalores &lt;- resultado$values\nautovetores &lt;- resultado$vectors\n\n#Calcular as componentes principais\ncomponentes_principais &lt;- as.matrix(dados |&gt; dplyr::select_if(is.numeric)) %*% autovetores\n\n#Calcular as proporceos da variancia total explicadas por cada componente\nprop_variancia &lt;- autovalores / sum(autovalores)\n\n#Calcular os coeficientes de correlacao entre as variaveis originais e as componentes\ncoef_correlacao &lt;- autovetores * sqrt(autovalores) / matriz_cov\n\n##### OU DE FORMA ANALOGA TAMBEM PODEMOS FAZER ###########\npca_dados &lt;- princomp((dados |&gt; dplyr::select_if(is.numeric)))\n\nOnde a função princomp é uma função em R que realiza a análise de componentes principais. Essa função retorna um objeto do tipo “princomp”, que contém várias informações sobre a análise realizada. Alguns dos principais elementos retornados por essa função são:\n\nsdev: Vetor contendo os desvios padrão estimados das componentes principais.\nloadings: Matriz de carga, que contém os coeficientes de ponderação das variáveis originais em cada componente principal. Cada coluna representa um componente principal, e as linhas correspondem às variáveis originais.\ncenter: Vetor contendo as médias das variáveis originais usadas na análise.\nscale: Vetor contendo os desvios padrão das variáveis originais usadas na análise.\nn.obs: Número de observações usadas na análise.\nscores: Matriz de escores, que contém os valores das observações nas componentes principais. Cada coluna representa um componente principal, e as linhas correspondem às observações.\ncall: A chamada da função princomp.\n\nAs componentes principais podem ser obtidas a partir de variáveis padronizadas. A padronização das variáveis é comumente realizada na análise de componentes principais para garantir que as variáveis tenham a mesma escala e evitar que uma variável com maior variabilidade domine a análise em detrimento das outras. Bem como estudado em variáveis aleatórias, a padronização das variáveis é feita subtraindo-se a média de cada variável e dividindo-se pelo desvio padrão. Dessa forma, todas as variáveis terão média zero e desvio padrão igual a um.\nNo código anterior, a padronização das variáveis não foi realizada, porem é recomendado padronizar as variáveis antes de realizar qualquer análise de componentes principais, isso pode ser feito de maneira simples utilizando o comando scale. Ao realizar essa transformação, trabalhar com a matriz de covariâncias das variáveis transformadas equivale a trabalhar com a matriz de correlação."
  },
  {
    "objectID": "PCA.html#número-de-componentes-principais-e-redução-da-dimensionalidade.",
    "href": "PCA.html#número-de-componentes-principais-e-redução-da-dimensionalidade.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.3 Número de componentes principais e ReduçãO da dimensionalidade.",
    "text": "10.3 Número de componentes principais e ReduçãO da dimensionalidade.\nA escolha do número ideal de componentes principais é uma etapa crucial ao aplicar a Análise de Componentes Principais (PCA) em um conjunto de dados. A seleção adequada de componentes pode garantir a retenção da maior parte da variância dos dados, ao mesmo tempo em que reduz a dimensionalidade do problema. Abaixo são apresentados alguns dos métodos atualmente utilizados para número \\(d\\) de componentes retidas.\n\nVariância explicada\n\nPlotar a porcentagem de variância explicada de cada componente individual e a porcentagem de variância total capturada por todos os componentes principais. Este é o método mais avançado e eficaz que pode ser usado para selecionar o melhor número de componentes principais para o conjunto de dados. Neste método, criamos o seguinte tipo de gráfico apresentado na Figura 10.1.\n\n#Padronizacao dos dados e criacao do modelo PCA\npca_dados &lt;- princomp(scale(dados |&gt; dplyr::select_if(is.numeric)))\n\n#Scree plot para numero de variaveis\nfactoextra::fviz_eig(pca_dados,\n                     addlabels = TRUE, \n                     linecolor = \"Red\", ylim = c(0, 50)) +\n ggplot2::labs(x = \"Dimensões\", y = \"Porcentagem de Variância Explicada\")\n\n\n\n\nFigura 10.1: Screeplot para seleção de número de clusters\n\n\nO número de barras é igual ao número de variáveis no conjunto de dados original. Neste gráfico, cada barra mostra a porcentagem de variância explicada de cada componente individual. Ao observar esse gráfico, podemos decidir quantas componentes devem ser mantidas com base em algum critério como porcentagem mínima que deseja-se manter, exemplo 80% ou 90%, mantendo por exemplo 8 ou 7 componentes. Ou ainda podemos nos basear na linha vermelha usando o método do cotovelo aplicado em aprendizado não supervisionado.\n\nSeguir a regra de Kaiser\n\nDe acordo com a regra de Kaiser, é recomendado manter todos os componentes com autovalores maiores que 1.\nVocê pode obter os autovalores da seguinte forma:\n\npca_dados$sdev ^2\n\nEm seguida, você pode selecionar os componentes com autovalores maiores que 1. Ao seguir essa regra, é melhor combinar isso com o gráfico de porcentagem de variância explicada discutido no Método anterior. Às vezes, um autovalor ligeiramente menor que 1 (por exemplo, 0.95) pode capturar uma quantidade significativa de variância nos dados. Portanto, é melhor mantê-lo também se ele mostrar uma barra relativamente alta no gráfico de porcentagem de variância explicada.\n\nMétrica de avaliação de desempenho\n\nEste método só pode ser usado se você planeja realizar tarefas de regressão ou classificação com o conjunto de dados reduzido (transformado) após aplicar o PCA.\nUsando o gráfico discutido no Método de variância explicada, você pode selecionar o número inicial de componentes principais e obter o conjunto de dados reduzido (transformado). Em seguida, você constrói um modelo de regressão ou classificação e mede seu desempenho por meio do RMSE ou da pontuação de precisão. Em seguida, você altera ligeiramente o número de componentes principais, constrói o modelo novamente e mede a pontuação de desempenho. Após repetir essas etapas várias vezes, você pode selecionar o melhor número de componentes principais que realmente oferece a melhor pontuação de desempenho.\nObserve que a pontuação de desempenho do modelo depende de outros fatores também. Por exemplo, depende do estado aleatório da divisão dos conjuntos de treinamento e teste, quantidade de dados, desequilíbrio de classes, número de árvores no modelo (se o modelo for uma floresta aleatória ou uma variante similar), número de iterações definidas durante a otimização, valores discrepantes e valores ausentes nos dados, etc. Portanto, tenha cuidado ao usar este método!"
  },
  {
    "objectID": "PCA.html#interpretação-do-componentes-principais-de-uma-amostra.",
    "href": "PCA.html#interpretação-do-componentes-principais-de-uma-amostra.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.4 Interpretação do componentes principais de uma amostra.",
    "text": "10.4 Interpretação do componentes principais de uma amostra.\nApós realizar a Análise de Componentes Principais (PCA) em um conjunto de dados, é importante interpretar os componentes principais resultantes para entender a estrutura e os padrões presentes nos dados. A interpretação dos componentes principais envolve analisar os coeficientes de ponderação das variáveis originais em cada componente, ou loadings, como apresentado nas sessões anteriores.\nUma forma comum de interpretar os loadings é observar os valores absolutos e as diferenças entre eles. Valores absolutos altos indicam uma forte contribuição da variável para a respectiva componente principal, enquanto valores baixos indicam uma contribuição fraca. Além disso, diferenças significativas entre os valores absolutos dos loadings de uma variável em diferentes componentes podem indicar a presença de padrões específicos relacionados a essa variável.\nPor exemplo, suponha que estamos analisando um conjunto de dados com indicadores de saúde que inclui variáveis como taxa de mortalidade infantil, taxa de natalidade, gastos com saúde per capita, entre outras. Após realizar a PCA, obtemos as componentes principais. Podemos então examinar os loadings para interpretar os padrões presentes nos dados.\nSuponha que a primeira componente principal tenha altos loadings positivos para as variáveis de taxa de natalidade e gastos com saúde per capita, enquanto tem um loading negativo para a taxa de mortalidade infantil. Isso indica que a primeira componente principal está capturando um padrão em que países com maiores taxas de natalidade e maiores gastos com saúde per capita tendem a ter menores taxas de mortalidade infantil.\nDa mesma forma, a segunda componente principal pode ter altos loadings positivos para as variáveis de incidência de doenças e taxa de vacinação, indicando que ela está capturando um padrão em que países com maiores taxas de incidência de doenças e maiores taxas de vacinação tendem a ter menores valores nessa componente principal.\nA interpretação dos componentes principais também pode ser facilitada ao observar os valores dos coeficientes de correlação entre as variáveis originais e as componentes principais. Esses coeficientes de correlação indicam a força e a direção da relação entre cada variável e cada componente. Valores absolutos altos indicam uma correlação forte, enquanto valores próximos a zero indicam uma correlação fraca.\n\n\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Ankerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, and Jörg Sander.\n1999. “OPTICS: Ordering Points to Identify the Clustering\nStructure.” ACM Sigmod Record 28 (2): 49–60.\n\n\nAnton, Howard, and Chris Rorres. 2001. Álgebra Linear\nCom Aplicações. Vol. 8. Bookman Porto\nAlegre.\n\n\nHartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A\nk-Means Clustering Algorithm.” Journal of the Royal\nStatistical Society. Series c (Applied Statistics) 28 (1): 100–108.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied\nMultivariate Statistical Analysis. Vol. 5. 8. Prentice hall Upper\nSaddle River, NJ.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. “Least Squares Quantization in PCM.”\nIEEE Transactions on Information Theory 28 (2): 129–37."
  }
]