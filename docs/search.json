[
  {
    "objectID": "PCA.html#introdução.",
    "href": "PCA.html#introdução.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.1 Introdução.",
    "text": "10.1 Introdução.\nA Análise de Componentes Principais (PCA) é uma técnica que busca resumir a variação presente em um conjunto de dados multivariados através de combinações lineares de suas variáveis originais, que são correlacionadas. O objetivo principal é reduzir a dimensionalidade dos dados, representando um grande número de variáveis originais em um número menor de componentes principais. Essas novas variáveis são ordenadas em ordem decrescente de importância, de modo que a primeira componente principal capture a maior quantidade possível da variabilidade total dos dados, e as subsequentes capturem cada vez menos.\nA primeira componente principal é calculada de forma a maximizar a variância explicada pelos dados, ou seja, ela é a direção ao longo da qual os dados apresentam a maior variação possível. As demais componentes são ortogonais à primeira e são combinações lineares das variáveis originais, indicando a importância relativa de cada variável naquele componente. Note então, que por essa definição já podemos perceber a relação com os autovetores e autovetores associados.\nO PCA tem diversas aplicações em áreas como estatística, engenharia e ciência de dados, sendo usado para resumir grandes conjuntos de dados, detectar padrões e estruturas latentes, identificar outliers e reduzir o ruído presente nos dados. Além disso, ele é comumente utilizado como uma técnica de pré-processamento de dados para outras técnicas de análise, como regressão e clustering.\nUma aplicação comum do PCA é na identificação de variáveis latentes em um conjunto de dados, que não são diretamente observáveis, mas podem ser inferidas a partir de outras variáveis observáveis correlacionadas entre si.Um exemplo de variável latente na obstetrícia pode ser a “saúde fetal” ou até mesmo a “saúde materna”. Não pode ser diretamente medida ou observada, mas pode ser inferida a partir de múltiplas variáveis observáveis, como a frequência cardíaca fetal, a pressão arterial materna entre outras."
  },
  {
    "objectID": "PCA.html#como-realizar-a-análise-de-componentes-principais.",
    "href": "PCA.html#como-realizar-a-análise-de-componentes-principais.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.2 Como realizar a análise de componentes principais.",
    "text": "10.2 Como realizar a análise de componentes principais.\nO primeiro passo é entender a definição matemática real das componentes principais. Seja \\(\\boldsymbol{X}\\) um vetor aleatório com \\(\\boldsymbol{\\mu} = E(\\boldsymbol{X})\\) e \\(\\boldsymbol \\Sigma = Var(\\boldsymbol{X})\\) e \\((\\lambda_i,e_i), i = 1,\\dots,p\\) os pares de autovalores e autovetores normalizados associados de \\(\\Sigma\\). Então,\n\\[\n\\begin{split}\n\\boldsymbol{Y} = \\boldsymbol{ O'}\\boldsymbol{X},\\quad \\textrm{com}\\quad \\boldsymbol{O} = [\\boldsymbol{e}_1,\\boldsymbol{e}_2,\\dots,\\boldsymbol{e}_p],\\textrm{ os componentes principais de }\\boldsymbol X\\\\\n\\textrm{ou seja}\\\\\n\\boldsymbol Y =\n\\begin{bmatrix}\nY_1\\\\\n\\vdots\\\\\nY_p\n\\end{bmatrix} \\textrm{ com  } \\quad Y_1 = \\boldsymbol e_1'\\boldsymbol X = e_{11}X_1 + e_{12}X_2 +  \\dots + e_{1p}X_p.\n\\end{split}\n\\]\nA primeira componente principal. Como citado, as componentes então seguem sendo combinações lineares das variáveis originais e os autovetores correspondentes. Os componentes principais de \\(\\boldsymbol{X}\\), \\(\\boldsymbol{Y}\\), são tais que,\n\\[\n\\begin{split}\n\\boldsymbol\\mu_y = E(\\boldsymbol Y) = E(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'E(\\boldsymbol X) = \\boldsymbol O'\\boldsymbol\\mu_x\\\\\n\\boldsymbol\\Sigma_y = Var(\\boldsymbol Y) = Var(\\boldsymbol O'\\boldsymbol X) = \\boldsymbol O'Var(\\boldsymbol X)\\boldsymbol O = \\boldsymbol{O'\\Sigma_xO= \\Lambda}.\n\\end{split}\n\\]\nOu seja\n\\[\ncov(Y_i,Y_j) = 0, \\forall i \\neq j \\textrm{ e } Var(Y_i) = \\lambda_i\n\\]\nA prova desse resultado pode ser vista em (Johnson, Wichern, et al. 2002, 5:432).\nÉ de conhecimento geral, como um dos objetivos da análise de dados sendo a compreensão da distribuição bem como variabilidade dos dados. Podemos então descrever a variância total da população como sendo o somatório de todos os autovalores \\(\\lambda_i\\). A partir disso, podemos escrever a proporção da variância total explicada pela \\(j\\)-ésima componente, como sendo:\n\\[\n\\frac{\\lambda_j}{\\sum_{i=1}^p \\lambda_i}; \\qquad \\forall j =1,\\dots,p.\n\\]\nDessa forma, para algum \\(p\\) significativamente grande, podemos utilizar \\(d &lt; p\\) componentes ao invés das \\(p\\) variáveis originais, considerando que, podemos descrever uma proporção relativamente alta da variância com essas \\(d\\) componentes.\nSe \\(Y_i = \\boldsymbol{e'_iX}, i =1\\dots,p\\) são as componentes principais obtidas da matriz de covariância, então\n\\[\n\\rho_{Y_i,X_j} = \\frac{e_{ij}\\sqrt{\\lambda_i}}{\\sigma_{jj}}, \\quad \\forall i,j=1,\\dots p,\n\\]\nSão os coeficientes de correlação entre a componente \\(Y_i\\) e a variável \\(X_j\\). Esse valor auxiliará na compreensão entre a relação indivídual de \\(X_j\\) a uma componente principal \\(Y_i\\), porém não explica a relação desta variável em presença das outras. Por isso, alguns altores recomendam o uso único do valor de \\(e_{ij}\\) para compreender a relação variável-componente. Ambos os resultados serão importântes para compreensão da componente.\nExemplo"
  },
  {
    "objectID": "PCA.html#componentes-principais-por-variáveis-padronizadas.",
    "href": "PCA.html#componentes-principais-por-variáveis-padronizadas.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.3 Componentes principais por variáveis padronizadas.",
    "text": "10.3 Componentes principais por variáveis padronizadas.\nExemplo"
  },
  {
    "objectID": "PCA.html#número-de-componentes-principais.",
    "href": "PCA.html#número-de-componentes-principais.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.4 Número de componentes principais.",
    "text": "10.4 Número de componentes principais.\nExemplo"
  },
  {
    "objectID": "PCA.html#redução-de-dimensionalidade-com-o-pca.",
    "href": "PCA.html#redução-de-dimensionalidade-com-o-pca.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.5 Redução de dimensionalidade com o PCA.",
    "text": "10.5 Redução de dimensionalidade com o PCA.\nExemplo"
  },
  {
    "objectID": "PCA.html#interpretação-do-componentes-principais-de-uma-amostra.",
    "href": "PCA.html#interpretação-do-componentes-principais-de-uma-amostra.",
    "title": "10  Análise de Componentes Principais (PCA).",
    "section": "10.6 Interpretação do componentes principais de uma amostra.",
    "text": "10.6 Interpretação do componentes principais de uma amostra.\nExemplo\n\n\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "naosupervisionado.html",
    "href": "naosupervisionado.html",
    "title": "9  Aprendizado Não Supervisionado",
    "section": "",
    "text": "10 Análise de Agrupamentos\nComo descrito anteriormente e reforçado aqui, na análise de agrupamento, buscamos identificar regiões no espaço dos dados que possuam um grande número de observações próximas umas das outras. Essas regiões são chamadas de clusters. A ideia é agrupar indivíduos que sejam semelhantes entre si e diferentes dos indivíduos em outros clusters. Essa técnica é chamada de aprendizado não supervisionado, pois não utilizamos uma variável específica como referência para avaliar o resultado do agrupamento.\nFormalmente, os clusters são definidos da seguinte forma:\nAo realizar o agrupamento de dados, é importante utilizar um método que maximize as diferenças entre os clusters, ao mesmo tempo que minimiza as diferenças dentro de cada cluster. Para isso, são utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferenças entre as observações.\nAs medidas de dissimilaridade mais comumente usadas são a distância euclidiana e a distância euclidiana quadrática, como apresentado abaixo respectivamente:\n\\[\n\\begin{split}\nd(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\\\\nd^2(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2\n\\end{split}\n\\]\nOutras medidas menos utilizadas incluem a distância absoluta e a distância de Mahalanobis, que leva em consideração a matriz de covariância, respectivamente representadas como:\n\\[\n\\begin{split}\nd_a(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\\\\nd_M(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_i')' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_i')}\n\\end{split}\n\\]\nUma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados é por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade \\(a(x_i,x_j)\\) entre cada par de objetos \\(x_i\\) e \\(x_j\\) com \\(i,j = 1,2,\\dots,N.\\)\n\\[\n\\begin{align}\nA =\n\\begin{bmatrix}\n          a(x_1,x_1) & a(x_1,x_2) & \\cdots &a(x_1,x_N) \\\\\n         a(x_2,x_1) & a(x_2,x_2) & \\cdots & a(x_2,x_N)  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           a(x_N,x_1) & a(x_N,x_2) & \\cdots & a(x_N,x_N)\n         \\end{bmatrix}.\n  \\end{align}\n\\]\nAs matrizes de dissimilaridade podem ser obtidas com apoio da função dist(), onde o tipo de distância (Euclidiana por exemplo), é passada no parâmetro da função, method , veja a seguir, um exemplo aplicado ao conjunto de indicadores obstétricos, esse DataSet séra o referncial para a sessão atual, será considerado apenas as colunas dos indicadores.\ndist_euclidian &lt;- dist(scale(dados_indicadores[,-c(1:4)]), method = \"euclidean\")\nO códio acima cria e armazena um objeto do tipo dist que será utilizado em exemplos futuros.\nA análise de agrupamento é uma ferramenta valiosa que permite identificar estratos em uma população e detectar outliers. É importante considerar a escalabilidade do método, sua capacidade de lidar com diferentes tipos de variáveis e clusters de formatos variados. Além disso, a robustez em relação a outliers e a capacidade de agrupar dados de alta dimensionalidade são considerações essenciais. Existem diversos métodos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas próximas sessões, exploraremos os métodos considerados e suas aplicações adequadas, bem como métodos de avaliação de qualidade para os agrupamentos."
  },
  {
    "objectID": "naosupervisionado.html#métodos-de-agrupamentos",
    "href": "naosupervisionado.html#métodos-de-agrupamentos",
    "title": "9  Aprendizado Não Supervisionado",
    "section": "10.1 Métodos de Agrupamentos",
    "text": "10.1 Métodos de Agrupamentos\nNeste capítulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Além disso, temos os métodos hierárquicos, nos quais os grupos são organizados em uma estrutura de árvore.\nOutra abordagem interessante é considerar a densidade dos pontos no espaço. Nesse caso, procuramos identificar regiões mais densas separadas por áreas menos povoadas. Esses métodos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na análise dos dados.\nTambém existe uma classe de métodos que utiliza técnicas de decomposição espectral. Esses métodos reduzem a dimensionalidade dos dados, preservando as informações relevantes dos grupos presentes. São os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.\nCada uma dessas abordagens possui suas próprias características, vantagens e limitações.\n\n10.1.1 Métodos por Particionamento\nOs métodos por particionamento são comumente utilizados para agrupar dados, onde cada partição representa um cluster. Esses métodos são baseados em distância e envolvem a realocação iterativa das observações entre os clusters para obter um particionamento otimizado.\nA escolha do número de clusters é um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum é o método do cotovelo, que considera a relação entre a variância total intraclusters e o número de grupos criados. O método do cotovelo considera que aumentar o número de clusters reduz a variância, mas em algum ponto, não há melhora significativa na granularidade do agrupamento. Esse ponto ótimo, que indica o número adequado de clusters, é identificado no gráfico por uma curva tracejada.\n\n\n\n\n\nA variância total intraclusters é calculada utilizando as distâncias euclidianas quadráticas entre as observações e o centróide do respectivo grupo. O centróide \\(c_l\\) de um grupo \\(C_l\\) é obtido através da média das observações atribuídas a esse cluster, utilizando a fórmula:\n\\[c_l = \\frac{1}{|\\mathcal{C}_l|} \\sum{i \\in \\mathcal{C}_l} \\mathbf{x}_i\\]\nA variância total intraclusters é calculada como a soma das distâncias euclidianas quadráticas entre as observações e os respectivos centróides, utilizando a fórmula:\n\\[\\sum_{l=1}^{K} \\sum_{i \\in \\mathcal{C}_l} |\\mathbf{x}_i - \\mathbf{c}_l|^2\\]\nEssas são algumas das abordagens dos métodos por particionamento, aqui será considerado o k-médias e o k-medóides com os algorítmos PAM e CLARA, que serão apresentados a seguir com exemplos de aplicações.\n\n10.1.1.1 K-médias\nO K-médias é um método amplamente utilizado para agrupamento de dados. Ele busca encontrar K partições dos dados, minimizando a variância. O algoritmo de (Lloyd 1982) é comumente usado para realizar o K-médias. Ele envolve os seguintes passos:\n\nescolha dos K centróides iniciais;\nparticionamento dos dados com base na menor distância para cada centróide;\natualização dos centróides com as novas observações atribuídas a eles;\nrepetição dos passos 2 e 3 até que não haja mais mudança de agrupamento. É possível definir um número máximo de iterações para otimizar o método computacionalmente.\n\nUma alternativa é o algoritmo de (Hartigan e Wong 1979), que adiciona uma etapa de validação para alterar os agrupamentos. A cada iteração, verifica-se se houve atualização nos centróides dos grupos. Nesse caso, um novo objeto só é atribuído a um cluster se a soma das distâncias quadráticas diminuir.\nNo entanto, o método K-médias apresenta limitações ao lidar com clusters de formas não convencionais ou grupos com tamanhos muito discrepantes. Além disso, ele é sensível a outliers, pois a inclusão de um dado extremo pode influenciar significativamente o valor do centróide. A aplicação para o software R, tanto do método de agrupamento quanto a escolha do número de clusters K pelo método do cotovelo, segue abaixo, será considerado os dados padronizados para retirar qualquer tendência em função da diferença de escala ou amplitude dos dados:\n\nset.seed (1122)\n#BIBLIOTECAS\nlibrary(ggplot2)\n## padronizacao dos dados \n\ndados_norm &lt;- as.data.frame(scale(dados_indicadores[,-c(1:4)]))\n\n## escolhendo k pelo metodo do cotovelo\n\ncotovelo_kmedias &lt;- factoextra::fviz_nbclust(dados_norm ,\n kmeans,\n method = \"wss\") +\n geom_vline( xintercept = 7, linetype = 2) +\n labs(x = \"Numero de Grupos\", y = \"Variancia Total Intragrupo\", title = \"K-medias\")\n\n## ajustando k-medias com o numero de grupos escolhido\n\nk_medias &lt;- kmeans(dados_norm,\n                        centers = 7)\n\nA função kmeans é uma ferramenta poderosa disponível no R para realizar o agrupamento de dados utilizando o método K-médias. A função kmeans retorna três principais objetos:\n\nCluster_centers: É uma matriz que representa os centróides finais de cada cluster. Cada linha dessa matriz representa um centróide, com as coordenadas correspondentes às variáveis do conjunto de dados.\nCluster_assignment: É um vetor que contém as atribuições de cada observação a um determinado cluster. Cada elemento desse vetor representa o número do cluster ao qual a observação foi atribuída. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.\nWithin_cluster_sum_of_squares: É um valor que representa a soma dos quadrados das distâncias de cada observação em relação ao seu respectivo centróide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homogêneo é o cluster.\n\n\n\n10.1.1.2 K-medóides\nEm situações com valores extremos, os algoritmos K-medóides surgem como uma alternativa ao cálculo do centróide, evitando a influência excessiva desses valores na representação central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por (Kaufman e Rousseeuw 2009) considera um custo para as trocas de medóides a cada iteração. O custo é calculado como a diferença da variância total intragrupo considerando um novo medóide (observação não medóide) em comparação com o medóide atual. A variância total intragrupo é uma medida da dispersão dos pontos dentro de um grupo.\nPara realizar o agrupamento, o algoritmo PAM segue os seguintes passos:\n\nEscolha inicial dos \\(K\\) medóides a partir do conjunto de dados;\nAs observações não selecionadas como medóides são atribuídas ao grupo cujo medóide é o mais próximo;\nSelecionar aleatoriamente uma observação não medóide \\(o_r\\);\nCalcular o custo de se mudar o medóide atual para \\(o_r\\);\nCaso o custo seja menor que 0, realizar a troca de medóide;\nRepetir os passos 2 a 5 até que não haja mais mudanças de agrupamento.\n\nO custo de mudança do medóide atual para outra observação é calculado como a diferença da variância total intragrupo considerando a nova observação como representante em comparação com o medóide atual.\nAlém disso, é comum utilizar a medida de distância absoluta no lugar da distância euclidiana quadrática para calcular a distância entre os pontos e os medóides. O método pode ser visto abaixo:\n\n## escolhendo k pelo metodo do cotovelo\ncotovelo_pam &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::pam,\n                      method = \"wss\") +\n                geom_vline( xintercept = 7, linetype = 2) +\n                labs(x = \"Numero de Grupos\",\n                      y = \"Variancia Total Intragrupo\",\n                      title = \"PAM\")\n\npam &lt;- cluster::pam(dados_norm ,\n                      k = 7)\n\nA função cluster::pam no R retorna os seguintes elementos:\n\nmedoids: Um vetor contendo os índices das observações selecionadas como medóides finais de cada cluster.\nclustering: Um vetor contendo os rótulos dos clusters aos quais cada observação foi atribuída.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, incluindo o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\n\nEsses elementos fornecem informações sobre os medóides finais selecionados, a atribuição de clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento e informações adicionais sobre cada cluster.\nPara lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a variância total intragrupo para cada agrupamento gerado. A partição que apresentar menor variância total intragrupo é selecionada como o resultado final do algoritmo. Observe abaixo a aplicação para o R:\n\ncotovelo_clara &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::clara ,\n                      method = \"wss\") +\n                  geom_vline( xintercept = 7, linetype = 2) +\n                  labs(x = \"Numero de Grupos\",\n                        y = \"Variancia Total Intragrupo\",\n                        title = \"CLARA\")\nclara &lt;- cluster::clara(dados_norm ,\n                          k = 7, samples = 10)\n\nA função cluster::clara no R retorna os seguintes resultados:\n\nmedoids: Um objeto pamobject contendo os medóides finais de cada cluster.\nclustering: Um vetor com os rótulos dos clusters atribuídos a cada observação.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, como o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\nsamples: Uma lista contendo os índices das observações selecionadas em cada subamostra.\ncall: A chamada original da função cluster::clara que foi utilizada.\n\nEsses resultados fornecem detalhes sobre os medóides finais escolhidos, a atribuição dos clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento, informações adicionais sobre os clusters, as subamostras utilizadas e a chamada original da função.\n\n\n\n10.1.2 Métodos Hierárquicos\nOs métodos hierárquicos são utilizados para agrupar dados em diferentes níveis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.\nNa abordagem aglomerativa, os grupos são construídos a partir do nível mais baixo, onde cada observação forma um cluster separado, até atingir o nível mais alto, onde todos os dados estão em um único grupo. A fusão dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, também conhecidas como linkages, podem ser definidas da seguinte forma:\n\nMétodo do vizinho mais próximo (Single linkages): Considera a menor distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\min_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo do vizinho mais distante (Complete linkages): Utiliza a maior distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\max_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo da média das distâncias (Average linkages): Calcula a média das distâncias entre as observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\frac{1}{|C_l||C_{l'}|}\\sum_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j)\n\\]\n\nMétodo do centróide (Centróide linkages): Considera a distância entre os centróides de cada grupo como medida de dissimilaridade.\n\n\\[\nd(C_l,C_{l'}) = d^2(c_l,c_{l'})\n\\]\n\nMétodo de Ward: Minimiza a variância dentro dos grupos ao fundir os clusters que levam à menor variação possível.\n\n\\[\nd(C_l,C_{l'}) = \\frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'})\n\\]\nNo contexto dos métodos hierárquicos de agrupamento, a abordagem aglomerativa é amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada iteração é necessário identificar a melhor divisão do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hierárquico aglomerativo consiste em:\n\nCada observação é inicialmente atribuída a um cluster separado.\nCom base no método de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.\nOs dois grupos com a menor dissimilaridade são fundidos em um único grupo.\nRepetem-se os passos 2 e 3 até que todas as observações estejam em um único grupo.\n\nJá na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo único que contém todas as observações e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observações. O algoritmo DIANA segue os seguintes passos:\n\nTodas as observações são agrupadas em um único grupo.\nA observação com a maior dissimilaridade média em relação aos pontos do mesmo grupo é separada em um novo grupo.\nCada observação do grupo inicial é atribuída ao novo grupo se a dissimilaridade média em relação aos objetos desse grupo for menor do que a dissimilaridade média em relação aos demais pontos do grupo inicial.\n\n4.Calcula-se o diâmetro de todos os grupos (a maior dissimilaridade entre duas observações) e seleciona-se o grupo com o maior diâmetro.\n\nRepetem-se os passos 2 a 4 até que cada observação esteja em um grupo separado.\n\nNo agrupamento hierárquico, a visualização dos clusters é feita por meio de um dendrograma, um gráfico ramificado que mostra as junções e divisões dos clusters. A altura do ramo no primeiro nó do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o número de grupos a partir do dendrograma, busca-se uma grande diferença de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma característica dos métodos hierárquicos é que as decisões de agrupamento ou divisão não são desfeitas, ou seja, não há troca de observações entre os clusters. Decisões de união ou divisão mal feitas podem resultar em grupos de baixa qualidade. Além disso, esses métodos não são bem dimensionados, pois cada decisão de mesclagem ou divisão requer a avaliação de muitos objetos ou clusters. Pelo exemplo abaixo, uma possível resposta de número adequado de clusters seria de dois ou três grupos.\n Os agrupamentos hierárquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das funções hclust e cluster::diana, é obtido um objeto da classe “hclust” e da clase “diana” respectivamente, esses objetos contém informações sobre o agrupamento hierárquico realizado, incluindo a estrutura do dendrograma, as distâncias entre os objetos e outras propriedades relacionadas. Seguido pela seleção, com base no dendrograma, do número K ideal de clusters (Não necessariamente só um K), e o “corte” da árvore aglomerativa no valor ideal identificado. É apresentado abaixo para todos os métodos citados a aplicação para o R, supondo a distância utilizada como a euclidiana calculada no início do capítulo\n\nlibrary(ggdendro)\n#AGLOMERATIVOS ############\n#METODO WARD -----\n##CRIAR O OBJETO HCLUST\nagl_ward &lt;- hclust(dist_euclidian , method = \"ward.D2\")\n\n# PLOT DO DENDROGRAMA \ndendograma_ward &lt;-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,\n main = \"Ward - cortado em H = 20\")\n\n# \"corte\" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS\nagl_ward_res &lt;- cutree(agl_ward , k = 3:8)\n\n# segue o mesmo para todos os outros metodos\n\n#METODO SINGLE LINKAGE -----\nagl_single &lt;- hclust(dist_euclidian , method = \"single\")\n\ndendograma_single &lt;- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,\n                        main = \"Vizinho mais Proximo - cortado em H = 4\",\n                          xlab = \"\")\n\nagl_single_res &lt;- cutree(agl_single , k = 3:8)\n\n#METODO COMPLETE LINKAGE -----\nagl_complete &lt;-  hclust(dist_euclidian , method = \"complete\")\n\ndendograma_complete &lt;- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,\n                             main = \"Vizinho mais Distante - cortado em H = 10\")\n\nagl_complete_res &lt;- cutree(agl_complete , k = 3:8)\n\n#METODO AVARAGE LINKAGE -----\nagl_ave &lt;- hclust(dist_euclidian , method = \"average\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,\n                                main = \" Media das Distancias - cortado em H = 15\")\n\nagl_ave_res &lt;- cutree(agl_ave, k = 3:8)\n\n#METODO CENTROIDE LINKAGE -----\nagl_cent &lt;- hclust(dist_euclidian , method = \"centroid\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,\n                            main = \" Centroide - cortado em H = 15\")\n\nagl_cent_res &lt;- cutree(agl_cent , k = 3:8)\n\n####### DIVISIVO (DIANA) #########\ndiana &lt;- cluster::diana(dist_euclidian ,\n                            diss = TRUE ,\n                            metric = \"euclidean\",\n                            keep.diss = FALSE ,\n                            keep.data = FALSE\n                            )\n\ndendrograma_diana &lt;-  plot(cut(as.dendrogram(diana), h = 7)$upper ,\n                                   main = \"Diana - cortado em H = 7\")\n\ndiana_res &lt;- cutree(diana , k = 3:8)"
  },
  {
    "objectID": "naosupervisionado.html#métodos-de-validação",
    "href": "naosupervisionado.html#métodos-de-validação",
    "title": "9  Aprendizado Não Supervisionado",
    "section": "10.2 Métodos de Validação",
    "text": "10.2 Métodos de Validação\n\n\n\n\nHartigan, John A, e Manchek A Wong. 1979. «Algorithm AS 136: A k-means clustering algorithm». Journal of the royal statistical society. series c (applied statistics) 28 (1): 100–108.\n\n\nKaufman, Leonard, e Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. «Least squares quantization in PCM». IEEE transactions on information theory 28 (2): 129–37."
  },
  {
    "objectID": "introducao_algebra.html#definições-importantes",
    "href": "introducao_algebra.html#definições-importantes",
    "title": "8  Alguns conceitos básicos de algebra",
    "section": "8.1 Definições importantes",
    "text": "8.1 Definições importantes\n\n8.1.1 Vetor Aleatório\nUm vetor aleatório \\(\\boldsymbol{X}\\) é um vetor contendo \\(p\\) componentes, onde cada componente é uma variável aleatória \\(X_i\\), para \\(i = 1, 2, ..., p\\).\n\\[\n\\begin{align}\n  \\boldsymbol{X} &= \\begin{bmatrix}\n           X_{1} \\\\\n           X_{2} \\\\\n           \\vdots \\\\\n           X_{p}\n         \\end{bmatrix}\n  \\end{align}\n\\]\nSuponha que o objeto em estudo é saúde sanguínea, o vetor de variáveis em estudo poderia vir a ser o número de vitaminas do tipo A, B e D no sangue, ou seja, um vetor com três componentes. O vetor transposto do vetor aleatório \\(\\boldsymbol{X}\\) é denotado por \\(\\boldsymbol{X}' = [X_1 X_2 X_3 ...X_p]\\).\n\n\n8.1.2 Vetor de Médias\nO vetor \\(\\boldsymbol{\\mu}\\) é chamado vetor de médias quando \\(E(\\boldsymbol{X}) = \\boldsymbol{\\mu}\\), onde \\(\\boldsymbol{X}\\) é um vetor aleatório. Dessa forma,\n\\[\n\\begin{align}\n  E(\\boldsymbol{X}) &= \\begin{bmatrix}\n           E(X_{1}) \\\\\n           E(X_{2}) \\\\\n           \\vdots \\\\\n           E(X_{p})\n         \\end{bmatrix}\n  \\end{align} = \\boldsymbol{\\mu} = \\begin{bmatrix}\n           \\mu_1 \\\\\n           \\mu_2 \\\\\n           \\vdots \\\\\n           \\mu_p\n         \\end{bmatrix}.\n\\]\nOu seja, sendo cada componente \\(X_i\\) do vetor uma variável aleatória, \\(\\mu_i\\) representa a respectiva esperança dessa variável.\n\n\n8.1.3 Matriz de Covariâncias\nA matriz de variâncias e covariâncias do vetor \\(\\boldsymbol{X}\\) é denotada por\n\\[\nCov(\\boldsymbol{X}) = V(\\boldsymbol{X}) = Var(\\boldsymbol{X}) =\n\\boldsymbol{\\Sigma}_{p\\times p} = \\begin{bmatrix}\n           \\sigma_{11} & \\sigma_{12} & \\cdots & \\sigma_{1p}  \\\\\n          \\sigma_{21} & \\sigma_{22} & \\cdots & \\sigma_{2p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\sigma_{p1} & \\sigma_{p2} & \\cdots & \\sigma_{pp}\n         \\end{bmatrix}.\n\\]\nOnde \\(\\sigma_{ii}\\) representa a variância do elemento \\(X_i\\) do vetor aleatório e \\(\\sigma_{ij} = E[(X_i- \\mu_i)(X_j - \\mu_j)]\\) a covariância entre as componentes \\(X_i\\) e \\(X_j\\), \\(\\forall\\quad i,j = 1,\\dots,p\\). A matriz de covariância é uma matriz simétrica, sua transposta é igual a ela mesma, ou seja \\(\\boldsymbol\\Sigma = \\boldsymbol\\Sigma'\\). Sendo tambem não negativa definida, \\(a'\\boldsymbol\\Sigma a \\geq 0\\) para todo vetor de constantes pertencentes aos reais. A matriz de covariância pode ser facilmente calcula utilizando a função cov() da linguagem R.\n\n\n8.1.4 Matriz de correlação\nA matriz de correlação do vetor \\(\\boldsymbol{X}\\) é denotada por,\n\\[\n\\boldsymbol{P}_{p\\times p} = \\begin{bmatrix}\n           1 & \\rho_{12} & \\rho_{13}& ... & \\rho_{1p}  \\\\\n          \\rho_{21} & 1 & \\rho\n          _{23}&... & \\rho_{2p}  \\\\\n          \\rho_{31} & \\rho_{32} & 1 &... & \\rho_{3p}  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           \\rho_{p1} & \\rho_{p2} &\\rho_{p3}& ... & 1\n         \\end{bmatrix}.\n\\]\nEm que,\n\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii}\\sigma_{jj}}} = \\frac{\\sigma_{ij}}{\\sigma_i\\sigma_j}.\n\\]\nSendo assim, \\(\\rho_{ij}\\) a correlação entre a i-ésima e a j-ésima componente do vetor aleatório \\(\\boldsymbol{X}\\), \\(\\forall i,j = 1,\\dots, p\\), em que, se \\(j = i\\) a correlação assume o valor máximo de 1. De forma análoga à matriz de covariância, pode-se utilizar a função cor(), para obter a matriz de correlação \\(\\boldsymbol{P}\\) do vetor aleatório \\(\\boldsymbol{X}\\).\nUm dos conceitos fundamentais da álgebra linear, bem como da estatística multivariada é o de autovalores e autovetores de uma matriz. A decomposição espectral de uma matriz é a representação de uma matriz simétrica em termos de seus autovetores e autovalores. Os autovetores são vetores especiais que não mudam de direção quando multiplicados pela matriz original, mas apenas são escalados por um fator conhecido, como autovalor correspondente.\n\n\n8.1.5 Auto Valores e Auto Vetores\nEm estatística multivariada, trabalhamos habitualmente com matrizes quadradas de covariância e correlação, ou seja, matrizes em que o número de colunas e linhas é o mesmo. Nesse contexto, um vetor não nulo \\(\\boldsymbol{e}\\) é denominado autovetor de uma matriz quadrada \\(\\boldsymbol\\Sigma_{p\\times p}\\) se \\(\\boldsymbol\\Sigma \\boldsymbol{e}\\) for um múltiplo escalar de \\(\\boldsymbol{e}\\), isto é,\n\\[\n\\boldsymbol\\Sigma \\boldsymbol{e} = \\lambda \\boldsymbol{e},\n\\]\ncom algum escalar \\(\\lambda\\). O escalar \\(\\lambda\\) é denominado de autovalor de \\(\\boldsymbol\\Sigma\\), e dizemos que \\(\\boldsymbol{e}\\) é um autovetor associado a \\(\\lambda\\). Os autovetores são usados para reduzir a dimensionalidade de um conjunto de dados e extrair informações importantes sobre as relações entre as variáveis.\nNote que é possível descrever a equação acima da seguinte forma:\n\\[\n\\begin{split}\n\\boldsymbol\\Sigma \\boldsymbol{e} = \\lambda \\boldsymbol{e}\\\\\n\\lambda\\boldsymbol{e}-\\boldsymbol\\Sigma\\boldsymbol{e}  = 0 \\\\\n(\\lambda \\boldsymbol I - \\boldsymbol\\Sigma)\\boldsymbol{e} = 0.\n\\end{split}\n\\]\nem que \\(\\boldsymbol I\\) é a matriz identidade de mesma dimensão que \\(\\boldsymbol\\Sigma\\).\nUma importante relação entre autovalores e autovetores é dada pela equação característica de \\(\\boldsymbol\\Sigma\\), que é obtida resolvendo a equação \\((\\lambda \\boldsymbol I - \\boldsymbol\\Sigma)\\boldsymbol{e} = 0\\) para \\(\\lambda\\). Para que a equação tenha solução não trivial, ou seja, que o vetor \\(\\boldsymbol{e}\\) não seja o vetor nulo, é necessário que o determinante da matriz \\((\\lambda \\boldsymbol I - \\boldsymbol\\Sigma)\\) seja igual a zero. Essa equação polinomial é definida como equação característica de \\(\\boldsymbol\\Sigma\\). Os autovalores podem ser encontrados resolvendo a equação para \\(\\lambda\\). Uma definição mais formal seria:\n\n\n8.1.6 Equação característica\nA equação característica de uma matriz quadrada \\(\\boldsymbol{\\Sigma}\\) de ordem \\(p\\) é definida como a equação polinomial de grau \\(p\\) dada por:\n\\[\ndet(\\lambda \\boldsymbol I - \\boldsymbol\\Sigma) = 0.\n\\]\nonde os valores de \\(\\lambda\\) que satisfazem a equação são determinados como os autovalores de \\(\\boldsymbol\\Sigma\\) e det representa o determinante da matriz resultante da equação. Para esclarecimento, suponha como exemplo que,\n\\[\n\\boldsymbol\\Sigma = \\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\]\nEntão,\n\\[\n\\begin{split}\ndet\\left(\\begin{bmatrix}\n\\lambda& 0\\\\\n0 & \\lambda\n\\end{bmatrix}\n-  \n\\begin{bmatrix}\n8 & -2 \\\\\n-2 & 5\n\\end{bmatrix}\n\\right) = 0\\\\\ndet\\left(\\begin{bmatrix}\n\\lambda - 8 & 2 \\\\\n2 & \\lambda-5\n\\end{bmatrix}\n\\right) = 0 \\\\\n(\\lambda - 8)\\times(\\lambda-5) - (2)\\times(2) = 0\\\\\n\\lambda^2 - 13\\lambda + 36 = 0\n\\end{split}\n\\]\nResolvendo a equação obtemos os valores de \\(\\lambda_1 = 9\\) e \\(lambda_2 = 4\\), podemos encontrar os autovetores \\(\\boldsymbol{e}_i\\) associados seguindo a definição. Para \\(\\lambda_1\\) temos:\n\\[\n\\begin{bmatrix}\n\\lambda_1-8&2\\\\\n2 & \\lambda_1-5\n\\end{bmatrix}\n\\begin{bmatrix}\ne_{11}\\\\\ne_{12}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n1&2\\\\\n2 & 4\n\\end{bmatrix}\n\\begin{bmatrix}\ne_{11}\\\\\ne_{12}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\rightarrow e_{11} = - 2e_{12}.\n\\]\nDa mesma forma para \\(\\lambda_2\\):\n\\[\n\\begin{bmatrix}\n\\lambda_2-8&2\\\\\n2 & \\lambda_2-5\n\\end{bmatrix}\n\\begin{bmatrix}\ne_{21}\\\\\ne_{22}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\\\\\n\\begin{bmatrix}\n-4&2\\\\\n2 & -1\n\\end{bmatrix}\n\\begin{bmatrix}\ne_{21}\\\\\ne_{22}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0\\\\\n0\n\\end{bmatrix}\n\\rightarrow 2e_{21} = e_{22}.\n\\]\nApós encontrar os autovalores de uma matriz \\(\\boldsymbol{\\Sigma}\\), é importante observar que para cada autovalor há infinitos autovetores possíveis. Para evitar essa ambiguidade, é comum restringir os autovetores a um conjunto de vetores normalizados. Um vetor \\(\\boldsymbol{e}_i\\) é normalizado quando a sua norma euclidiana é igual a 1, isto é, quando:\n\\[\n||\\boldsymbol{e}_i|| = \\sqrt{e^2_{i1} + e^2_{i2} + \\dots + e^2_{ip}} = 1.\n\\]\nDessa forma, garantimos que os autovetores estão no mesmo espaço de dimensão \\(p\\) que as variáveis originais. Para o exemplo então, temos:\n\\[\n\\begin{split}\ne_{11} =- 2e_{12}\\\\\n\\sqrt{e_{11}^2 + e_{12}^2 } = 1\\\\\n\\sqrt{(-2e_{12})^2 + e_{12}^2 } = 1\\\\\n\\sqrt{5e_{12}^2 } = 1\\\\\ne_{12} = \\frac{1}{\\sqrt{5}}  \\approx 0.45\\\\\ne_{11} \\approx - 0.90\n\\end{split}\n\\]\nO mesmo se segue para \\(\\lambda_2\\), onde obtemos o vetor \\(\\boldsymbol{e}_2' = [-0.45,-0.90]\\), assim então os vetores normalizados:\n\\[\n\\boldsymbol{e}_1 = \\begin{bmatrix}\n-0.90\\\\\n0.45\n\\end{bmatrix};\\qquad\\boldsymbol{e}_2 = \\begin{bmatrix}\n-0.45\\\\\n-0.90\n\\end{bmatrix}.\n\\]\nPara entender melhor a relação entre matriz de covariância/correlação e seus autovetores e autovalores normalizados, utilizamos o conceito de decomposição espectral. Através desse teorema, podemos decompor a matriz em questão em seus autovetores e autovalores normalizados, o que nos permite obter informações valiosas sobre as relações entre as variáveis originais."
  },
  {
    "objectID": "introducao_algebra.html#decomposição-espectral-de-matrizes-de-correlação-e-covariância-em-seus-autovetores-e-autovalores-normalizados.",
    "href": "introducao_algebra.html#decomposição-espectral-de-matrizes-de-correlação-e-covariância-em-seus-autovetores-e-autovalores-normalizados.",
    "title": "8  Alguns conceitos básicos de algebra",
    "section": "8.2 Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.",
    "text": "8.2 Decomposição Espectral de Matrizes de correlação e Covariância em seus Autovetores e Autovalores normalizados.\nPara uma matriz quadrada simétrica \\(\\boldsymbol\\Sigma\\) de ordem \\(p\\), é possível encontrar uma matriz \\(\\boldsymbol{O}\\) cujas colunas consistem nos autovetores normalizados de \\(\\boldsymbol\\Sigma\\), e uma matriz diagonal \\(\\boldsymbol \\Lambda\\) cujos elementos diagonais são os autovalores correspondentes, de tal forma que \\(\\boldsymbol\\Sigma\\) pode ser escrito como \\(\\boldsymbol\\Sigma = \\boldsymbol O \\boldsymbol \\Lambda \\boldsymbol O'\\). Da mesma forma, podemos dizer que:\n\\[\n\\boldsymbol O'\\boldsymbol\\Sigma \\boldsymbol O = \\begin{bmatrix}\n\\lambda_1 & 0 & 0 &\\dots & 0\\\\\n0&\\lambda_2& 0 & \\dots & 0 \\\\\n0 & 0 &\\lambda_3 &\\dots & 0\\\\\n\\vdots& \\vdots & \\vdots & \\ddots & \\vdots\\\\\n0 & 0 & 0 & \\dots& \\lambda_p\n\\end{bmatrix} = \\boldsymbol\\Lambda\n\\]\nOnde \\(\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\lambda_p\\geq0\\) são os autovalores da matriz \\(\\boldsymbol\\Sigma\\). Nesse caso, dizemos que a matriz \\(\\boldsymbol \\Sigma\\) é similar à matriz \\(\\boldsymbol\\Lambda\\), que implica em:\n\n\\(det(\\boldsymbol\\Sigma) = det(\\boldsymbol\\Lambda) = \\prod^p_{i=1} \\lambda_i\\);\ntraço\\((\\boldsymbol\\Sigma) =\\) traço\\((\\boldsymbol\\Lambda) = \\lambda_1 +\\dots+\\lambda_p\\).\n\nTem-se que a i-ésima coluna da matriz \\(\\boldsymbol O\\) é o autovetor normalizado \\(\\boldsymbol{e}_i\\) relacionado ao autovalor \\(\\lambda_i\\). Então a matriz \\(\\boldsymbol O\\) é dada por \\(O = [\\boldsymbol{e}_1,\\boldsymbol{e}_2,\\dots,\\boldsymbol{e}_p]\\). De forma simples, podemos ver que:\n\\[\n\\Sigma = \\boldsymbol{O \\Lambda O'}= \\sum_{i=1}^p \\lambda_i \\boldsymbol{e}_i \\boldsymbol{e}_i'\n\\]\nOs autovetores normalizados em \\(\\mathbf{O}\\) são ortogonais entre si e formam uma base para o espaço de dimensão \\(p\\) em que a matriz \\(\\mathbf{\\Sigma}\\) atua. Cada autovetor representa uma direção no espaço em que a variabilidade dos dados é máxima. Os autovalores em \\(\\mathbf{\\Lambda}\\) representam a variância explicada por cada direção (ou autovetor) correspondente. Os autovalores são ordenados em ordem decrescente, de forma que o primeiro autovalor corresponde à direção de maior variabilidade e assim por diante. Observe na figura abaixo, onde é representado um conjunto de dados gerados aleatoriamente de 3 componentes (\\(X_1,X_2,X_3\\)), e o comportamento de cada um dos autovetores para cada uma das dimensões.\n\n\n\n\n\n\nOs autovetores são representados por setas que partem do centro de massa dos dados e indicam a direção e a magnitude da variação dos dados nessa direção. A magnitude da variação é dada pelo valor do autovalor correspondente a cada autovetor, que indica a importância daquele eixo.\nDentro do R é possível realizar a decomposição espectral usando a função eigen(), da seguinte forma:\n\nsigma &lt;-matrix(c(8,-2,-2,5),nrow = 2)\nsigma\n\n     [,1] [,2]\n[1,]    8   -2\n[2,]   -2    5\n\neigen(sigma)\n\neigen() decomposition\n$values\n[1] 9 4\n\n$vectors\n           [,1]       [,2]\n[1,] -0.8944272 -0.4472136\n[2,]  0.4472136 -0.8944272\n\n\nNos devolvendo os autovalores e autovetores normalizados para a matriz inserida na função, os mesmos calculados de forma manual anteriormente.\n\n\n\n\nAnton, Howard, e Chris Rorres. 2001. Álgebra linear com aplicações. Vol. 8. Bookman Porto Alegre.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied multivariate statistical analysis. Vol. 5. 8. Prentice hall Upper Saddle River, NJ."
  },
  {
    "objectID": "naosupervisionado.html#análise-de-agrupamentos",
    "href": "naosupervisionado.html#análise-de-agrupamentos",
    "title": "9  Aprendizado Não Supervisionado",
    "section": "9.1 Análise de Agrupamentos",
    "text": "9.1 Análise de Agrupamentos\nComo descrito anteriormente e reforçado aqui, na análise de agrupamento, buscamos identificar regiões no espaço dos dados que possuam um grande número de observações próximas umas das outras. Essas regiões são chamadas de clusters. A ideia é agrupar indivíduos que sejam semelhantes entre si e diferentes dos indivíduos em outros clusters. Essa técnica é chamada de aprendizado não supervisionado, pois não utilizamos uma variável específica como referência para avaliar o resultado do agrupamento.\nFormalmente, os clusters são definidos da seguinte forma:\n\nCada cluster é um grupo de observações;\nTodos os indivíduos pertencem a pelo menos um cluster;\nDois clusters diferentes não possuem observações em comum.\n\nAo realizar o agrupamento de dados, é importante utilizar um método que maximize as diferenças entre os clusters, ao mesmo tempo que minimiza as diferenças dentro de cada cluster. Para isso, são utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferenças entre as observações.\nAs medidas de dissimilaridade mais comumente usadas são a distância euclidiana e a distância euclidiana quadrática, como apresentado abaixo respectivamente:\n\\[\n\\begin{split}\nd(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{\\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\\\\nd^2(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2\n\\end{split}\n\\]\nOutras medidas menos utilizadas incluem a distância absoluta e a distância de Mahalanobis, que leva em consideração a matriz de covariância, respectivamente representadas como:\n\\[\n\\begin{split}\nd_a(\\mathbf{x}_i, \\mathbf{x}_i') = \\sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\\\\nd_M(\\mathbf{x}_i, \\mathbf{x}_i') = \\sqrt{(\\mathbf{x}_i - \\mathbf{x}_i')' \\mathbf{S}^{-1} (\\mathbf{x}_i - \\mathbf{x}_i')}\n\\end{split}\n\\]\nUma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados é por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade \\(a(x_i,x_j)\\) entre cada par de objetos \\(x_i\\) e \\(x_j\\) com \\(i,j = 1,2,\\dots,N.\\)\n\\[\n\\begin{align}\nA =\n\\begin{bmatrix}\n          a(x_1,x_1) & a(x_1,x_2) & \\cdots &a(x_1,x_N) \\\\\n         a(x_2,x_1) & a(x_2,x_2) & \\cdots & a(x_2,x_N)  \\\\\n            \\vdots &\\vdots & \\ddots &\\vdots \\\\\n           a(x_N,x_1) & a(x_N,x_2) & \\cdots & a(x_N,x_N)\n         \\end{bmatrix}.\n  \\end{align}\n\\]\nAs matrizes de dissimilaridade podem ser obtidas com apoio da função dist(), onde o tipo de distância (Euclidiana por exemplo), é passada no parâmetro da função, method , veja a seguir, um exemplo aplicado ao conjunto de indicadores obstétricos, esse DataSet séra o referncial para a sessão atual, será considerado apenas as colunas dos indicadores.\n\ndist_euclidian &lt;- dist(scale(dados_indicadores[,-c(1:4)]), method = \"euclidean\")\n\nO códio acima cria e armazena um objeto do tipo dist que será utilizado em exemplos futuros.\nA análise de agrupamento é uma ferramenta valiosa que permite identificar estratos em uma população e detectar outliers. É importante considerar a escalabilidade do método, sua capacidade de lidar com diferentes tipos de variáveis e clusters de formatos variados. Além disso, a robustez em relação a outliers e a capacidade de agrupar dados de alta dimensionalidade são considerações essenciais. Existem diversos métodos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas próximas sessões, exploraremos os métodos considerados e suas aplicações adequadas, bem como métodos de avaliação de qualidade para os agrupamentos.\nNeste capítulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Além disso, temos os métodos hierárquicos, nos quais os grupos são organizados em uma estrutura de árvore.\nOutra abordagem interessante é considerar a densidade dos pontos no espaço. Nesse caso, procuramos identificar regiões mais densas separadas por áreas menos povoadas. Esses métodos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na análise dos dados, aqui será considerado o DBSCAN.\nTambém existe uma classe de métodos que utiliza técnicas de decomposição espectral. Esses métodos reduzem a dimensionalidade dos dados, preservando as informações relevantes dos grupos presentes. São os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.\nCada uma dessas abordagens possui suas próprias características, vantagens e limitações.\n\n9.1.1 Métodos por Particionamento\nOs métodos por particionamento são comumente utilizados para agrupar dados, onde cada partição representa um cluster. Esses métodos são baseados em distância e envolvem a realocação iterativa das observações entre os clusters para obter um particionamento otimizado.\nA escolha do número de clusters é um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum é o método do cotovelo, que considera a relação entre a variância total intraclusters e o número de grupos criados. O método do cotovelo considera que aumentar o número de clusters reduz a variância, mas em algum ponto, não há melhora significativa na granularidade do agrupamento. Esse ponto ótimo, que indica o número adequado de clusters, é identificado no gráfico por uma curva tracejada.\n\n\n\n\n\nA variância total intraclusters é calculada utilizando as distâncias euclidianas quadráticas entre as observações e o centróide do respectivo grupo. O centróide \\(c_l\\) de um grupo \\(C_l\\) é obtido através da média das observações atribuídas a esse cluster, utilizando a fórmula:\n\\[c_l = \\frac{1}{|\\mathcal{C}_l|} \\sum{i \\in \\mathcal{C}_l} \\mathbf{x}_i\\]\nA variância total intraclusters é calculada como a soma das distâncias euclidianas quadráticas entre as observações e os respectivos centróides, utilizando a fórmula:\n\\[\\sum_{l=1}^{K} \\sum_{i \\in \\mathcal{C}_l} |\\mathbf{x}_i - \\mathbf{c}_l|^2\\]\nEssas são algumas das abordagens dos métodos por particionamento, aqui será considerado o k-médias e o k-medóides com os algorítmos PAM e CLARA, que serão apresentados a seguir com exemplos de aplicações.\n\n9.1.1.1 K-médias\nO K-médias é um método amplamente utilizado para agrupamento de dados. Ele busca encontrar K partições dos dados, minimizando a variância. O algoritmo de (Lloyd 1982) é comumente usado para realizar o K-médias. Ele envolve os seguintes passos:\n\nescolha dos K centróides iniciais;\nparticionamento dos dados com base na menor distância para cada centróide;\natualização dos centróides com as novas observações atribuídas a eles;\nrepetição dos passos 2 e 3 até que não haja mais mudança de agrupamento. É possível definir um número máximo de iterações para otimizar o método computacionalmente.\n\nUma alternativa é o algoritmo de (Hartigan e Wong 1979), que adiciona uma etapa de validação para alterar os agrupamentos. A cada iteração, verifica-se se houve atualização nos centróides dos grupos. Nesse caso, um novo objeto só é atribuído a um cluster se a soma das distâncias quadráticas diminuir.\nNo entanto, o método K-médias apresenta limitações ao lidar com clusters de formas não convencionais ou grupos com tamanhos muito discrepantes. Além disso, ele é sensível a outliers, pois a inclusão de um dado extremo pode influenciar significativamente o valor do centróide. A aplicação para o software R, tanto do método de agrupamento quanto a escolha do número de clusters K pelo método do cotovelo, segue abaixo, será considerado os dados padronizados para retirar qualquer tendência em função da diferença de escala ou amplitude dos dados:\n\nset.seed (1122)\n#BIBLIOTECAS\nlibrary(ggplot2)\n## padronizacao dos dados \n\ndados_norm &lt;- as.data.frame(scale(dados_indicadores[,-c(1:4)]))\n\n## escolhendo k pelo metodo do cotovelo\n\ncotovelo_kmedias &lt;- factoextra::fviz_nbclust(dados_norm ,\n kmeans,\n method = \"wss\") +\n geom_vline( xintercept = 7, linetype = 2) +\n labs(x = \"Numero de Grupos\", y = \"Variancia Total Intragrupo\", title = \"K-medias\")\n\n## ajustando k-medias com o numero de grupos escolhido\n\nk_medias &lt;- kmeans(dados_norm,\n                        centers = 7)\n\nA função kmeans é uma ferramenta poderosa disponível no R para realizar o agrupamento de dados utilizando o método K-médias. A função kmeans retorna três principais objetos:\n\nCluster_centers: É uma matriz que representa os centróides finais de cada cluster. Cada linha dessa matriz representa um centróide, com as coordenadas correspondentes às variáveis do conjunto de dados.\nCluster_assignment: É um vetor que contém as atribuições de cada observação a um determinado cluster. Cada elemento desse vetor representa o número do cluster ao qual a observação foi atribuída. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.\nWithin_cluster_sum_of_squares: É um valor que representa a soma dos quadrados das distâncias de cada observação em relação ao seu respectivo centróide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homogêneo é o cluster.\n\n\n\n9.1.1.2 K-medóides\nEm situações com valores extremos, os algoritmos K-medóides surgem como uma alternativa ao cálculo do centróide, evitando a influência excessiva desses valores na representação central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por (Kaufman e Rousseeuw 2009) considera um custo para as trocas de medóides a cada iteração. O custo é calculado como a diferença da variância total intragrupo considerando um novo medóide (observação não medóide) em comparação com o medóide atual. A variância total intragrupo é uma medida da dispersão dos pontos dentro de um grupo.\nPara realizar o agrupamento, o algoritmo PAM segue os seguintes passos:\n\nEscolha inicial dos \\(K\\) medóides a partir do conjunto de dados;\nAs observações não selecionadas como medóides são atribuídas ao grupo cujo medóide é o mais próximo;\nSelecionar aleatoriamente uma observação não medóide \\(o_r\\);\nCalcular o custo de se mudar o medóide atual para \\(o_r\\);\nCaso o custo seja menor que 0, realizar a troca de medóide;\nRepetir os passos 2 a 5 até que não haja mais mudanças de agrupamento.\n\nO custo de mudança do medóide atual para outra observação é calculado como a diferença da variância total intragrupo considerando a nova observação como representante em comparação com o medóide atual.\nAlém disso, é comum utilizar a medida de distância absoluta no lugar da distância euclidiana quadrática para calcular a distância entre os pontos e os medóides. O método pode ser visto abaixo:\n\n## escolhendo k pelo metodo do cotovelo\ncotovelo_pam &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::pam,\n                      method = \"wss\") +\n                geom_vline( xintercept = 7, linetype = 2) +\n                labs(x = \"Numero de Grupos\",\n                      y = \"Variancia Total Intragrupo\",\n                      title = \"PAM\")\n\npam &lt;- cluster::pam(dados_norm ,\n                      k = 7)\n\nA função cluster::pam no R retorna os seguintes elementos:\n\nmedoids: Um vetor contendo os índices das observações selecionadas como medóides finais de cada cluster.\nclustering: Um vetor contendo os rótulos dos clusters aos quais cada observação foi atribuída.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, incluindo o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\n\nEsses elementos fornecem informações sobre os medóides finais selecionados, a atribuição de clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento e informações adicionais sobre cada cluster.\nPara lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a variância total intragrupo para cada agrupamento gerado. A partição que apresentar menor variância total intragrupo é selecionada como o resultado final do algoritmo. Observe abaixo a aplicação para o R:\n\ncotovelo_clara &lt;- factoextra::fviz_nbclust(dados_norm ,\n                      cluster::clara ,\n                      method = \"wss\") +\n                  geom_vline( xintercept = 7, linetype = 2) +\n                  labs(x = \"Numero de Grupos\",\n                        y = \"Variancia Total Intragrupo\",\n                        title = \"CLARA\")\nclara &lt;- cluster::clara(dados_norm ,\n                          k = 7, samples = 10)\n\nA função cluster::clara no R retorna os seguintes resultados:\n\nmedoids: Um objeto pamobject contendo os medóides finais de cada cluster.\nclustering: Um vetor com os rótulos dos clusters atribuídos a cada observação.\nobjective: O valor da medida de dissimilaridade total do agrupamento obtido.\nisolation.distance: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.\nclusinfo: Uma lista com informações adicionais sobre os clusters, como o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.\nsamples: Uma lista contendo os índices das observações selecionadas em cada subamostra.\ncall: A chamada original da função cluster::clara que foi utilizada.\n\nEsses resultados fornecem detalhes sobre os medóides finais escolhidos, a atribuição dos clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento, informações adicionais sobre os clusters, as subamostras utilizadas e a chamada original da função.\n\n\n\n9.1.2 Métodos Hierárquicos\nOs métodos hierárquicos são utilizados para agrupar dados em diferentes níveis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.\nNa abordagem aglomerativa, os grupos são construídos a partir do nível mais baixo, onde cada observação forma um cluster separado, até atingir o nível mais alto, onde todos os dados estão em um único grupo. A fusão dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, também conhecidas como linkages, podem ser definidas da seguinte forma:\n\nMétodo do vizinho mais próximo (Single linkages): Considera a menor distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\min_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo do vizinho mais distante (Complete linkages): Utiliza a maior distância entre todas as possíveis combinações de observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\max_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo da média das distâncias (Average linkages): Calcula a média das distâncias entre as observações de dois grupos.\n\n\\[\nd(C_l,C_{l'}) = \\frac{1}{|C_l||C_{l'}|}\\sum_{x_i \\in C_l ;x_k \\in C_{l'}} d(x_i,x_j).\n\\]\n\nMétodo do centróide (Centróide linkages): Considera a distância entre os centróides de cada grupo como medida de dissimilaridade.\n\n\\[\nd(C_l,C_{l'}) = d^2(c_l,c_{l'}) .\n\\]\n\nMétodo de Ward: Minimiza a variância dentro dos grupos ao fundir os clusters que levam à menor variação possível.\n\n\\[\nd(C_l,C_{l'}) = \\frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'}) .\n\\]\nNo contexto dos métodos hierárquicos de agrupamento, a abordagem aglomerativa é amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada iteração é necessário identificar a melhor divisão do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hierárquico aglomerativo consiste em:\n\nCada observação é inicialmente atribuída a um cluster separado.\nCom base no método de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.\nOs dois grupos com a menor dissimilaridade são fundidos em um único grupo.\nRepetem-se os passos 2 e 3 até que todas as observações estejam em um único grupo.\n\nJá na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo único que contém todas as observações e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observações. O algoritmo DIANA segue os seguintes passos:\n\nTodas as observações são agrupadas em um único grupo.\nA observação com a maior dissimilaridade média em relação aos pontos do mesmo grupo é separada em um novo grupo.\nCada observação do grupo inicial é atribuída ao novo grupo se a dissimilaridade média em relação aos objetos desse grupo for menor do que a dissimilaridade média em relação aos demais pontos do grupo inicial.\n\n4.Calcula-se o diâmetro de todos os grupos (a maior dissimilaridade entre duas observações) e seleciona-se o grupo com o maior diâmetro.\n\nRepetem-se os passos 2 a 4 até que cada observação esteja em um grupo separado.\n\nNo agrupamento hierárquico, a visualização dos clusters é feita por meio de um dendrograma, um gráfico ramificado que mostra as junções e divisões dos clusters. A altura do ramo no primeiro nó do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o número de grupos a partir do dendrograma, busca-se uma grande diferença de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma característica dos métodos hierárquicos é que as decisões de agrupamento ou divisão não são desfeitas, ou seja, não há troca de observações entre os clusters. Decisões de união ou divisão mal feitas podem resultar em grupos de baixa qualidade. Além disso, esses métodos não são bem dimensionados, pois cada decisão de mesclagem ou divisão requer a avaliação de muitos objetos ou clusters. Pelo exemplo abaixo, uma possível resposta de número adequado de clusters seria de dois ou três grupos.\n\n\n\n\n\nOs agrupamentos hierárquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das funções hclust e cluster::diana, é obtido um objeto da classe “hclust” e da clase “diana” respectivamente, esses objetos contém informações sobre o agrupamento hierárquico realizado, incluindo a estrutura do dendrograma, as distâncias entre os objetos e outras propriedades relacionadas. Seguido pela seleção, com base no dendrograma, do número K ideal de clusters (Não necessariamente só um K), e o “corte” da árvore aglomerativa no valor ideal identificado. É apresentado abaixo para todos os métodos citados a aplicação para o R, supondo a distância utilizada como a euclidiana calculada no início do capítulo.\n\nlibrary(ggdendro)\n#AGLOMERATIVOS ############\n#METODO WARD -----\n##CRIAR O OBJETO HCLUST\nagl_ward &lt;- hclust(dist_euclidian , method = \"ward.D2\")\n\n# PLOT DO DENDROGRAMA \ndendograma_ward &lt;-  plot(cut(as.dendrogram(agl_ward), h = 20)$upper ,\n main = \"Ward - cortado em H = 20\")\n\n# \"corte\" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS\nagl_ward_res &lt;- cutree(agl_ward , k = 3:8)\n\n# segue o mesmo para todos os outros metodos\n\n#METODO SINGLE LINKAGE -----\nagl_single &lt;- hclust(dist_euclidian , method = \"single\")\n\ndendograma_single &lt;- plot(cut(as.dendrogram(agl_single), h = 4)$upper ,\n                        main = \"Vizinho mais Proximo - cortado em H = 4\",\n                          xlab = \"\")\n\nagl_single_res &lt;- cutree(agl_single , k = 3:8)\n\n#METODO COMPLETE LINKAGE -----\nagl_complete &lt;-  hclust(dist_euclidian , method = \"complete\")\n\ndendograma_complete &lt;- plot(cut(as.dendrogram(agl_complete), h = 10)$upper ,\n                             main = \"Vizinho mais Distante - cortado em H = 10\")\n\nagl_complete_res &lt;- cutree(agl_complete , k = 3:8)\n\n#METODO AVARAGE LINKAGE -----\nagl_ave &lt;- hclust(dist_euclidian , method = \"average\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_ave), h = 15)$upper ,\n                                main = \" Media das Distancias - cortado em H = 15\")\n\nagl_ave_res &lt;- cutree(agl_ave, k = 3:8)\n\n#METODO CENTROIDE LINKAGE -----\nagl_cent &lt;- hclust(dist_euclidian , method = \"centroid\")\n\ndendograma_ward &lt;- plot(cut(as.dendrogram(agl_cent), h = 15)$upper ,\n                            main = \" Centroide - cortado em H = 15\")\n\nagl_cent_res &lt;- cutree(agl_cent , k = 3:8)\n\n####### DIVISIVO (DIANA) #########\ndiana &lt;- cluster::diana(dist_euclidian ,\n                            diss = TRUE ,\n                            metric = \"euclidean\",\n                            keep.diss = FALSE ,\n                            keep.data = FALSE\n                            )\n\ndendrograma_diana &lt;-  plot(cut(as.dendrogram(diana), h = 7)$upper ,\n                                   main = \"Diana - cortado em H = 7\")\n\ndiana_res &lt;- cutree(diana , k = 3:8)\n\n\n\n9.1.3 DBSCAN\nO método DBSCAN (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de agrupamento que foi proposto para encontrar clusters com formas arbitrárias, ou seja, clusters que não necessariamente possuem uma forma esférica ou convexa. Ele busca identificar as regiões mais densas do espaço vetorial separadas por regiões com menos objetos, como é possível ver na figura abaixo sua eficiência em relação a outros métodos.\n\n\n\n\n\nA ideia geral do algoritmo DBSCAN é identificar os clusters de forma que a densidade de pontos ao redor de cada ponto de um grupo seja maior que um limite estabelecido. Ele utiliza dois parâmetros principais: \\(\\epsilon\\) e MinPts, para entendimento do agrupamento DBSCAN observe a definição dos seguintes termos:\n\n\\(\\epsilon\\)-vizinhos: Os \\(\\epsilon\\)-vizinhos de um ponto \\(x_j\\) são os objetos cuja distância para \\(x_i\\) seja menor ou igual a \\(\\epsilon\\), ou seja, a vizinhança de \\(x_i\\) é composta pelos objetos que estão dentro de um raio \\(\\epsilon\\) ao redor de \\(x_i\\).\nPonto de Núcleo: Um ponto \\(x_i\\) é considerado um ponto de núcleo se o número de seus \\(\\epsilon\\)-vizinhos (ou seja, os pontos dentro da vizinhança de \\(x_i\\)) for maior ou igual a um valor mínimo estabelecido chamado MinPts.\nPonto de Borda: Um ponto \\(x_i\\) é considerado um ponto de borda se o número de seus \\(\\epsilon\\) -vizinhos for menor do que MinPts, mas ele está dentro da vizinhança de algum ponto de núcleo.\nDiretamente alcançável por densidade: Um ponto \\(x_j\\) é diretamente alcançável por densidade por um ponto \\(x_i\\) se \\(x_j\\) é um \\(\\epsilon\\)-vizinho de \\(x_i\\) e se \\(x_i\\) é um ponto de núcleo.\nAlcançável por densidade: Um ponto \\(x_i\\) é alcançável por densidade por um ponto \\(x_j\\) se existe uma cadeia de pontos \\(x_{i'}\\) , onde $i’ N \\mathbb{N} $, em que \\(x_1 = x_j, x_N = x_i\\) e \\(x_{i' + 1}\\) é diretamente alcançável por densidade por \\(x_{i'}\\).\nConectado por densidade: Um ponto \\(x_i\\) é conectado por densidade a um ponto \\(x_j\\) se existe um terceiro ponto \\(x_{i'}\\) em que ambos são alcançáveis por densidade por \\(x_{i'}\\).\n\nCom base nesses critérios, o DBSCAN define os grupos da seguinte forma:\n\nSe um ponto \\(x_i\\) é alcançável por densidade por um ponto \\(x_j\\), então ambos pertencem ao mesmo cluster \\(C_l\\).\nTodos os pontos de um cluster são conectados por densidade entre si.\n\nUma das vantagens do DBSCAN é sua capacidade de identificar outliers no conjunto de dados, uma vez que ele não atribui todos os objetos a grupos. Os outliers são definidos como os pontos que não são atribuídos a nenhum cluster. O algoritmo DBSCAN funciona da seguinte maneira:\n\nInicialmente, todas as observações da base de dados são classificadas como “não visitadas”.\nEm seguida, os seguintes passos são executados repetidamente:\n(a). Selecionar aleatoriamente uma observação “não visitada” \\(x_i\\).\n(b). Verificar o número de \\(epsilon\\)-vizinhos da observação \\(x_i\\) selecionada:\n\nSe o número de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for maior ou igual a MinPts, um novo cluster é criado para \\(x_i\\), e todos os objetos na \\(\\epsilon\\)-vizinhança de \\(x_i\\) são adicionados a um conjunto candidato, \\(M\\). O algoritmo adiciona iterativamente a \\(C_l\\) todos os objetos em 𝑀 que não pertencem a nenhum cluster.\nSe o número de \\(\\epsilon\\)-vizinhos \\(|NEps(x_i)|\\) for menor do que MinPts, \\(x_i\\) é marcado como um ponto de ruído (outlier).\n\nRepetir os passos 1 e 2 até que não haja mais observações “não visitadas”.\n\nOs parâmetros \\(\\epsilon\\) e MinPts afetam diretamente o resultado do agrupamento, determinando o tamanho mínimo de cada grupo e a distância entre os clusters. Uma das limitações do DBSCAN é o fato de que esses parâmetros são globais, ou seja, definidos para todos os grupos formados, o que pode não ser ideal quando diferentes clusters possuem densidades de pontos distintas. Para a seleção do valor de MinPts é comum que seja baseado no problema em si ou conhecimento prévio. Porém, uma proposta para seleção é o MinPts = 2×\\(p\\) pode ser um bom valor para o parâmetro. Já o \\(\\epsilon\\) costuma ser baseado no número mínimo de pontos definido, em que é construído o gráfico das distâncias de cada ponto para os MinPts−1 vizinhos mais próximos e escolhido o valor “cotovelo”, ou seja, onde começa a ocorrer um crescimento mais acentuado nas distâncias ordenadas. Abaixo vemos como esse método se dá de forma aplicada para o software de programação tanto da seleção do melhor \\(\\epsilon\\) quanto da aplicação do modelo.\n\n## selecao do melhor eps\n dbscan::kNNdist(dados_diss ,\n k = 3, all = FALSE)\n## ajuste com o eps selecionado\n dbscan &lt;- dbscan::dbscan(dados_norm , eps = 2, minPts = 4)\n\nEssa função retorna os seguintes elementos:\n\ncluster: É um vetor que atribui um número de cluster a cada ponto de dados no conjunto de entrada. Os pontos que não pertencem a nenhum cluster são rotulados como “0” ou como “outlier”.\neps: É o valor do raio (epsilon) usado no algoritmo.\nminPts: É o número mínimo de pontos necessários para formar um cluster.\nborder: É um vetor lógico que indica se cada ponto é um ponto de borda.\nreachability: É um vetor que armazena os valores de reachability distance de cada ponto.\ncore_dist: É um vetor que contém as distâncias de densidade mínima( core distance ) para cada ponto.\n\nUma técnica relacionada ao DBSCAN que lida com a limitação dos parâmetros globais é o algoritmo OPTICS (Ordering Points To Identify the Clustering Structure), proposto por (Ankerst et al. 1999). O OPTICS ordena os dados de forma que as observações em grupos mais densos estarão mais próximas na ordenação. Esse algoritmo pode ser uma opção para explorar diferentes densidades de pontos em diferentes clusters.\n\n\n9.1.4 Agrupamento Espectral\nAgrupar conjuntos de dados com alta dimensionalidade pode ser desafiador para alguns métodos de agrupamento. A grande quantidade de variáveis em alta dimensão pode aumentar significativamente o custo computacional e dificultar a separação adequada dos grupos. Nesse contexto, os métodos de agrupamento espectral surgem como uma abordagem promissora, pois permitem reduzir a dimensionalidade dos dados sem perder a informação contida nas variáveis, além de melhorar a separação entre grupos que podem não estar claramente distintos devido à alta dimensionalidade.\nOs algoritmos de agrupamento espectral usam uma medida de dissimilaridade para representar o conjunto de dados como um grafo. Nessa representação, os vértices do grafo correspondem às observações (\\(V = v_1, ..., v_N\\)), e as arestas (\\(A\\)) são definidas por uma matriz de similaridade, onde \\(a(x_i, x_{i'})\\) representa o peso da aresta que conecta os vértices \\(x_i\\) e \\(x_{i'}\\). O agrupamento em \\(K\\) clusters é então formulado como um problema de corte de arestas, que pode ser computacionalmente complexo. Para contornar essa complexidade, é aplicada a teoria espectral.\nO algoritmo de Ng, Jordan e Weiss (NJW) é um exemplo de algoritmo de agrupamento espectral. Ele requer a definição prévia de um parâmetro, assim como o algoritmo K-médias. Esse parâmetro é o número de grupos \\(K\\) a serem formados. O algoritmo NJW segue os seguintes passos:\n\nCalcula-se a matriz de similaridade \\(A\\), usando um parâmetro escalar \\(\\sigma\\). A similaridade entre cada par de pontos \\(x_i\\) e \\(x_{i'}\\) é medida usando uma função de similaridade, como a função Gaussiana. A matriz \\(A\\) é preenchida com os valores calculados.\nCalcula-se a matriz de graus \\(D\\), onde cada elemento \\(d_{ij}\\) na diagonal principal representa a soma das similaridades da linha \\(i\\) da matriz \\(A\\). A matriz \\(D\\) captura a estrutura de conectividade do conjunto de dados.\nConstrói-se a matriz Laplaciana \\(L\\), que é uma forma de normalização de \\(A\\). No algoritmo NJW, a matriz Laplaciana é definida como \\(L = D^{(-1/2)} 𝐴 𝐷^{(-1/2)}\\). Essa normalização realça as diferenças entre os grupos de dados.\nIdentificam-se os \\(K\\) maiores autovalores de \\(L\\) e seus respectivos autovetores associados, que são armazenados em uma matriz \\(Z\\) de tamanho \\(N \\times K\\).\nDefine-se uma nova matriz \\(Y\\), normalizada a partir de \\(Z\\). Cada elemento \\(y_{ii'}\\) de \\(Y\\) é calculado dividindo-se o elemento \\(z_{ii'}\\) de \\(Z\\) pelo produto da raiz quadrada do elemento diagonal correspondente a \\(z_{ii}\\) e a raiz quadrada do elemento diagonal correspondente a \\(z_{i'}\\). Essa normalização ajusta as magnitudes dos dados.\nCom a redução da dimensionalidade dos dados na matriz \\(Y\\) (de tamanho \\(N \\times K\\)), pode-se aplicar um algoritmo de agrupamento, como o K-médias, para realizar o agrupamento dos dados.\n\n\nspectral_clustering &lt;- function(data , # dados\n                                k) # numero de grupos\n                                {\n    # matriz de dissimilaridade\n    A &lt;- as.matrix(KRLS::gausskernel (data , 2)) # sigma^2 = 1\n    diag(A) &lt;- 0\n    \n    # matriz Laplaciana\n    L = diag(1 / sqrt(rowSums(A))) %*% A %*% diag(1 / sqrt(rowSums(A)))\n    \n    # matriz de autovetores associados aos k maiores autovalores\n    auto &lt;- eigen(L, symmetric = TRUE)\n    X &lt;- auto$vectors[, 1:k]\n    \n    # matriz normalizada\n    Y &lt;- X / sqrt(rowSums(X ^ 2))\n    \n    # agrupamente k medias na matriz redimensionalizada\n    set.seed (1122)\n    k_means &lt;- kmeans(Y, centers = k)\n    \n    return(list(\n    clusters = k_means$cluster , #retornar grupos\n    auto = auto , #retornar autovalores e autovetores\n    autovetores = X, #retornar autovetores dos K maiores autovalores\n    normalizada = Y #retornar matriz normalizada\n    ))\n}\n\nA escolha dos parâmetros \\(K\\) e \\(\\sigma\\) pode ser feita de forma arbitrária, dependendo do objetivo do estudo. No entanto, existem abordagens e técnicas disponíveis para auxiliar na seleção desses parâmetros. No exemplo acima , o parâmetro \\(\\sigma\\) foi fixado em 1, e o número de grupos \\(K\\) foi escolhido com base na análise dos primeiros autovalores da matriz Laplaciana \\(L\\). O valor ótimo de \\(K\\) é aquele em que ocorre uma queda significativa dos autovalores, uma vez que os maiores autovalores da matriz Laplaciana tendem a ser próximos de 1.\n\n\n9.1.5 Validação do Modelo\nNo problema do agrupamento, não é possível verificar o grau de acerto do resultado obtido, uma vez que os verdadeiros grupos não são conhecidos a priori. Portanto, é necessário aplicar algum tipo de validação à partição final.\nQuatro índices de avaliação interna da qualidade do agrupamento são os principais a serem utilizados atualmente: Davies-Bouldin (DB), Dunn (D), Silhueta (S) e Calinski-Harabasz (CH).\n\n9.1.5.1 Davies-Bouldin (DB):\nO índice DB mede a similaridade média entre cada grupo e seu grupo mais similar dentre os demais clusters. A distância média \\(\\delta_l\\) de um grupo \\(l\\) às suas observações é calculada em relação a um valor referencial \\(m_l\\). A fórmula para \\(\\delta_l\\) é:\n\\[\n\\delta_{l} = \\left(\\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} \\left \\| x_{i} - m_{l} \\right \\|^q\\right)^{\\frac{1}{q}}.\n\\]\nonde \\(n_l\\) é o número de observações no grupo \\(l\\), \\(x_i\\) é uma observação, \\(m_l\\) é o valor referencial (centróide ou medóide) e \\(q\\) é um valor pré-definido, onde os valores mais utilizados são \\(q = 1\\) e \\(q= 2\\). Para o índice, é utilizada também distância entre os grupos \\(\\Delta_{ll'}\\), que é obtida como a distância entre os valores referenciais de cada grupo. A fórmula para \\(\\Delta_{ll'}\\) é:\n\\[\n\\Delta_{ll'} = \\left( \\sum_{j=1}^{p} |m_{jl} - m_{jl'}|^{t} \\right)^{\\frac{1}{t}}.\n\\]\nonde \\(t \\in \\mathbb{N}\\) é pré-definido e geralmente adotado como \\(t = 1\\) (distâncias absolutas) ou \\(t=2\\) (distância euclidiana). Assim, o índice de Davies-Bouldin (DB) fica definido por:\n\\[\nDB = \\frac{1}{K} \\sum_{l=1}^{K} \\max_{l \\neq l'} \\left( \\frac{\\delta_{l} + \\delta_{l'}}{\\Delta_{ll'}} \\right).\n\\] Nesse caso, buscamos agrupar observações de forma a minimizar a variância intragrupo e maximizar a diferença entre eles. Portanto, valores menores do índice de Davies-Bouldin são considerados melhores. Observe a métrica aplicada ao conjunto de dados de agrupamento obtido pelo k-médias (As medidas seguintes também serão aplicadas ao mesmo conjunto).\n\n#Metrica DB usando Centroide\nclusterSim::index.DB(dados_norm ,\n                     k_medias$cluster)$DB\n#Metrica DB usando Medoide\nclusterSim::index.DB(dados_norm ,\n                      k_medias$cluster ,\n                      d = dist_euclidian ,\n                      centrotypes = \"medoids\"\n                      )$DB \n\n\n\n9.1.5.2 Dunn (D):\nO índice D mede a razão entre a separação dos grupos e a variância dentro deles. A separação entre dois grupos \\(l\\) e \\(l'\\) é calculada pela distância do vizinho mais próximo, dessa forma buscamos valores grandes para essa medida de avaliação. A variância intragrupo é representada pelo diâmetro \\(diam_l\\) do cluster. A fórmula para o índice D é:\n\\[\nD = \\frac{{\\min_{l,l' \\in \\{1,...,K\\}; l \\neq l'} d(C_{l}, C_{l'})}}{{\\max_{l \\in \\{1,...,K\\}} diam_{l}}}.\n\\]\n\n#Metrica D\n clValid::dunn(distance = dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.3 Silhueta (S):\nO índice de Silhueta(S) mede a qualidade do agrupamento considerando as distâncias de cada ponto em relação às observações do mesmo grupo e aos demais clusters formados. Para cada observação \\(i\\) pertencente ao grupo \\(C_l\\), definimos as medidas \\(a_i\\) e \\(b_i\\) da seguinte maneira:\n$$\n\\[\\begin{split}\na_i = \\frac{1}{n_{l}-1} \\sum_{i' \\neq i, i' \\in C_{l}}^{n_l} d(x_{i}, x_{i'}),\\\\\n\nb_i = \\min_{l \\neq l'} \\left( \\frac{1}{n_{l'}} \\sum_{i' \\in C_{l'}}^{n_{l'}} d(x_{i}, x_{i'}) \\right).\n\\end{split}\\]\n$$\nonde \\(d(x_i,x_{i'})\\) representa a distância entre as observações \\(x_i\\) e \\(x_{i'}\\). A partir dessas medidas, calculamos a silhueta \\(s_i\\) da observação \\(i\\), com \\(i = 1, ..., N\\), usando a fórmula:\n\\[\ns_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}.\n\\]\nO coeficiente de Silhueta S global é obtido por:\n\\[\nS = \\frac{1}{K} \\sum_{l=1}^{K} \\frac{1}{n_{l}} \\sum_{i=1}^{n_{l}} s_i.\n\\] Dessa forma, valores maiores de S indicam agrupamentos mais densos e separados, o que é considerado um cenário adequado para um agrupamento bem-sucedido.\n\n#Metrica S\nclusterSim::index.S(dist_euclidian , k_medias$cluster)\n\n\n\n9.1.5.4 Calinski-Harabasz (CH):\nO índice de Calinski-Harabasz (CH) considera a variância intra-grupo, chamada de \\(WGGS_l\\), de cada cluster \\(C_l\\) gerado, levando em conta a distância quadrática de cada observação em relação ao seu valor de referência \\(m_l\\), que pode ser um centróide ou medóide. A variância intra-grupo total,\\(WGGS\\), é calculada como a soma das variâncias intra-grupo de cada cluster:\n\\[\nWGGS = \\sum_{l=1}^{K} \\sum_{i=1}^{n_{l}} d^2(x_{i}, m_l).\n\\]\nAlém disso, o cálculo do índice CH utiliza uma medida de dispersão \\(BGSS\\) entre os grupos, que é obtida através da soma ponderada das distâncias quadráticas do valor de referência de cada cluster em relação a um valor central global \\(m\\). Essa medida é calculada da seguinte forma:\n\\[\nBGSS = \\sum_{l=1}^{K} n_{l} d^2(m_l, m).\n\\]\nDessa forma, o índice CH de Calinski-Harabasz é dado por:\n\\[\nCH = \\frac{(N - K) }{K - 1}\\times \\frac{BGSS}{WGSS}.\n\\]\nOnde, valores maiores do índice CH indicam grupos com menor variância e bem separados, o que é considerado uma boa característica de um agrupamento.\n\n#Metrica CH usando Medoide\nclusterSim::index.G1(dados_norm , k_medias$cluster , \n                     d = dist_euclidian , centrotypes = \"medoids\")\n\n#Metrica CH usando centroide\nclusterSim::index.G1(dados_norm , k_medias$cluster)\n\n\n\n\n\nAnkerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, e Jörg Sander. 1999. «OPTICS: Ordering points to identify the clustering structure». ACM Sigmod record 28 (2): 49–60.\n\n\nHartigan, John A, e Manchek A Wong. 1979. «Algorithm AS 136: A k-means clustering algorithm». Journal of the royal statistical society. series c (applied statistics) 28 (1): 100–108.\n\n\nKaufman, Leonard, e Peter J Rousseeuw. 2009. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. «Least squares quantization in PCM». IEEE transactions on information theory 28 (2): 129–37."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ciência de Dados Aplicada à Saúde Materno-Infantil",
    "section": "",
    "text": "Prefácio"
  },
  {
    "objectID": "intro.html#bases-de-dados",
    "href": "intro.html#bases-de-dados",
    "title": "1  Introdução",
    "section": "1.1 Bases de dados",
    "text": "1.1 Bases de dados\nOs dados considerados nas aplicações deste livro são provenientes do Sistema de Informação da Vigilância Epidemiológica da Gripe (SIVEP-Gripe), sistema oficial para o registro dos casos e óbitos por Síndrome Respiratória Aguda Grave (SRAG) disponibilizado pelo Ministério da Saúde. Os dados correspondem a registros de gestantes e puérperas de 10 a 55 anos hospitalizadas com SRAG por COVID-19 confirmada por teste de PCR. Também é utilizados o conjunto de dados do Sistema de Informação sobre Nascidos Vivos (SINASC) obtido por meio da Plataforma de Ciência de Dados aplicada à Saúde (PCDaS) da Fundação Oswaldo Cruz (Fiocruz). Os conjuntos de dados são utilizado para ilustrar e demonstrar diversos aspectos dos conceitos abordados no texto, e podem ser baixados em https://github.com/observatorioobstetrico/dados_livro_cd_saude.\n\n1.1.1 Dados de COVID-19 em gestantes e puérperas\nEssa base consiste em 11.523 registros de gestantes e puérperas diagnosticadas com COVID-19 no período de março de 2020 a dezembro de 2021. Alguns estudos conduzidos pelo OOBr usaram esses dados, dentre os quais podem ser citados: Características demográficas e epidemiológicas sobre mulheres grávidas e puérperas que morreram de Síndrome Respiratória Aguda Grave no Brasil, Mortalidade materna associada à COVID-19 no Brasil em 2020 e 2021: comparação com mulheres não grávidas e homens e Desfechos da COVID-19 em puérperas, gestantes e mulheres não gestantes e nem puérperas hospitalizadas.\nO dicionários das variáveis a ser considerado neste livro está na Tabela 1.1.\n\n\nTabela 1.1: Dicionário das variáveis da base de dados de COVID-19 em gestantes e puérperas.\n\n\n\n\n\n\n\nVariável\nDescrição\nValores\n\n\n\n\nDT_NOTIFIC\nData de preenchimento da ficha de notificação\nDia/Mês/Ano\n\n\nDT_SIN_PRI\nData de primeiros sintomas do caso\nDia/Mês/Ano\n\n\nDT_NASC\nData de nascimento da gestante ou puérpera\nDia/Mês/Ano\n\n\nDT_INTERNA\nData em que gestante ou puérpera foi hospitalizada\nDia/Mês/Ano\n\n\nSEM_PRI\nSemana epidemiológica do início dos sintomas\n1 a 52\n\n\nCS_RACA\nRaça da gestante ou puérpera\n1- branca; 2- preta; 3- amarela; 4- parda; 5-indígena; 9- ignorado\n\n\nCS_ESCOL_N\nNível de escolaridade da gestante ou puérpera\n0- sem escolaridade (analfabeto); 1- fundamental 1° ciclo (1ª a 5ª série); 2- fundamental 2 (6ª a 9ª série); 3- medio (1° ao 3° ano); 4- superior; 5- não se aplica; 9- ignorado\n\n\nidade\nIdade, em anos, da gestante ou puérpera\n10 a 55\n\n\nCS_GESTANT\nMomento gestacional ou puerpério\n1- 1° trimestre; 2- 2° trimestre; 3- 3° trimestre; 4- idade gestacional ignorada; 5- não; 9- ignorado\n\n\nPUERPERA\nSe paciente é puérpera ou parturiente (mulher que pariu recentemente - até 45 dias do parto)\n1- sim; 2- não; 9- ignorado\n\n\nSG_UF\nSigla do estado de residência da gestante ou puérpera\nSigla padronizada pelo IBGE\n\n\nID_MN_RESI\nNome do município de residência da gestante ou puérpera\nNomes padronizados pelo IBGE\n\n\nCO_MUN_RES\nCódigo do município de residência da gestante ou puérpera\nCódigo definido pelo IBGE\n\n\nCS_ZONA\nTipo de zona de residência da gestante ou puérpera\n1- urbana; 2- rural; 3- periurbana; 9- ignorado\n\n\nFEBRE\nSe gestante ou puérpera manifestou sintoma de febre\n1- sim; 2- não; 9- ignorado\n\n\nTOSSE\nSe gestante ou puérpera manifestou sintoma de tosse\n1- sim; 2- não; 9- ignorado\n\n\nGARGANTA\nSe gestante ou puérpera manifestou sintoma de dor de garganta\n1- sim; 2- não; 9- ignorado\n\n\nDISPNEIA\nSe gestante ou puérpera manifestou sintoma de dispneia\n1- sim; 2- não; 9- ignorado\n\n\nDESC_RESP\nSe gestante ou puérpera manifestou sintoma de desconforto respiratório\n1- sim; 2- não; 9- ignorado\n\n\nSATURACAO\nSe gestante ou puérpera manifestou sintoma de saturação\n1- sim; 2- não; 9- ignorado\n\n\nDIARREIA\nSe gestante ou puérpera manifestou sintoma de diarreia\n1- sim; 2- não; 9- ignorado\n\n\nVOMITO\nSe gestante ou puérpera manifestou sintoma de vômito\n1- sim; 2- não; 9- ignorado\n\n\nDOR_ABD\nSe gestante ou puérpera manifestou sintoma de dor abdominal\n1- sim; 2- não; 9- ignorado\n\n\nFADIGA\nSe gestante ou puérpera manifestou sintoma de fadiga\n1- sim; 2- não; 9- ignorado\n\n\nPERD_OLFT\nSe gestante ou puérpera manifestou sintoma de perda de olfato\n1- sim; 2- não; 9- ignorado\n\n\nPERD_PALA\nSe gestante ou puérpera manifestou sintoma de perda de paladar\n1- sim; 2- não; 9- ignorado\n\n\nASMA\nSe gestante ou puérpera tem asma\n1- sim; 2- não; 9- ignorado\n\n\nDIABETES\nSe gestante ou puérpera tem diabetes mellitus\n1- sim; 2- não; 9- ignorado\n\n\nNEUROLOGIC\nSe gestante ou puérpera tem doença neurológica\n1- sim; 2- não; 9- ignorado\n\n\nPNEUMOPATI\nSe gestante ou puérpera tem outra pneumopatia crônica\n1- sim; 2- não; 9- ignorado\n\n\nIMUNODEPRE\nSe gestante ou puérpera tem imunodeficiência ou imunodepressão (diminuição da função do sistema imunológico)\n1- sim; 2- não; 9- ignorado\n\n\nRENAL\nSe gestante ou puérpera tem doença renal crônica\n1- sim; 2- não; 9- ignorado\n\n\nOBESIDADE\nSe gestante ou puérpera tem obesidade\n1- sim; 2- não; 9- ignorado\n\n\nCARDIOPATI\nSe gestante ou puérpera tem doença cardiovascular crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEMATOLOGI\nSe gestante ou puérpera tem doença hematológica crônica\n1- sim; 2- não; 9- ignorado\n\n\nHEPATICA\nSe gestante ou puérpera tem doença hepática crônica\n1- sim; 2- não; 9- ignorado\n\n\nVACINA_COV\nSe gestante ou puérpera recebeu vacina COVID-19\n1- sim; 2- não; 9- ignorado\n\n\nDOSE_1_COV\nData em que gestante ou puérpera recebeu a 1ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nDOSE_2_COV\nData em que gestante ou puérpera recebeu a 2ª dose da vacina COVID-19\nDia/Mês/Ano\n\n\nFAB_COV_1\nFabricante da vacina que a gestante ou puérpera recebeu na 1ª dose\n\n\n\nFAB_COV_2\nFabricante da vacina que a gestante ou puérpera recebeu na 2ª dose\n\n\n\nSUPORT_VEN\nSe gestante ou puérpera precisou de ventilação mecânica; se sim, se foi invasiva ou não\n1- sim, invasivo; 2- sim, não invasivo; 3- não; 9- ignorado\n\n\nUTI\nSe gestante ou puérpera foi internada na UTI\n1- sim; 2- não; 9- ignorado\n\n\nDT_ENTUTI\nData de entrada da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nDT_SAIDUTI\nData de saída da gestante ou puérpera na UTI\nDia/Mês/Ano\n\n\nEVOLUCAO\nEvolução do caso da gestante ou puérpera\n1- cura; 2- óbito; 3- óbito por outras causas; 9- ignorado\n\n\n\n\n\n\n1.1.2 INDICADORES OBSTÉTRICOS\nConsiste em um conjunto de dados com 5570 observações sobre indicadores obstétricos a nível municipal do ano de 2019, sendo os seguintes indicadores:\n\nPrematuridade: Refere-se ao parto que ocorre em idade gestacional inferior a 37 semanas.\nGestação Múltipla: Diz respeito a uma gestação que gera dois ou mais fetos simultaneamente.\nParto Cesárea: Trata-se do tipo de parto em que a extração do feto é realizada por meio de intervenção cirúrgica.\nConsultas de Pré-Natal: Consiste no acompanhamento médico realizado pela gestante durante a gravidez. O ideal é que ocorram pelo menos 7 consultas médicas durante o período de pré-natal.\nApgar: É uma escala proposta em 1953 pela médica Virgínia Apgar, que atribui pontuações a 5 sinais do recém-nascido: frequência cardíaca, respiração, tônus muscular, irritabilidade reflexa e cor da pele. A escala varia de 0 a 10, e escores abaixo de 7 requerem atenção dos profissionais obstetras. Essa avaliação é feita no primeiro minuto após o nascimento (Apgar do primeiro minuto) e novamente após 5 minutos (Apgar do quinto minuto).\nAnomalia congênita: Refere-se a alterações estruturais ou funcionais que podem ser causadas por diversos fatores e têm origem durante a vida intrauterina.\n\nA obtenção dos indicadores utilizados foi realizada considerando as frequências relativas ao município de residência dos nascidos vivos no ano da coleta. Os indicadores obstétricos são calculados como percentuais válidos, ou seja, os dados faltantes da variável em questão são desconsiderados. Por exemplo:\n\nPercentual válido de prematuridade = (número de prematuros / número de nascidos vivos com informação de prematuridade) x 100.\n\nO Dicionário para esse conjunto de dados é apresentado em Tabela 1.2 .\n\n\nTabela 1.2: Dicionário das variáveis da base de dados de Indicadores Obstétricos.\n\n\n\n\n\n\nVariável\nDescrição\n\n\n\n\nuf\nUnidade Federativa (Estados + Distrito Federal)\n\n\nmunicipio\nMunicípio\n\n\ncodigo\nCódigo do município do IBGE\n\n\nnascidos_vivos\nNúmero de nascidos vivos\n\n\nporc_premat\nPercentual válido de prematuridade\n\n\nporc_gesta_multipla\nPercentual válido de gestações múltiplas\n\n\nporc_cesarea\nPercentual válido de partos cesáreas\n\n\nporc_0_consulta\nPercentual válido de nascidos com nenhuma consulta de pré-natal\n\n\nporc_7mais_consulta\nPercentual válido de nascidos com 7 ou mais consultas de pré-natal\n\n\nporc_apgar1_menor_7\nPercentual válido de Apgar de 1º minuto menor que 7\n\n\nporc_apgar5_menor_7\nPercentual válido de Apgar de 5º minuto menor que 7\n\n\nporc_anomalia\nPercentual válido de nascidos com anomalia congênita\n\n\nporc_peso_menor_2500\nPercentual válido de nascidos com peso menor que 2.500 gramas\n\n\nporc_fem\nPercentual válido de nascidos do sexo feminino"
  },
  {
    "objectID": "manipulacao_dados.html",
    "href": "manipulacao_dados.html",
    "title": "2  Manipulação de dados",
    "section": "",
    "text": "Neste capítulo falaremos alguns princípios básicos sobre manipulação de dados. Iremos trabalhar em um cenário mais próximo da realidade possível, ou seja, iremos trabalhar em cima de uma base de dados real. O objetivo é manipular a base e torná-la pronta para ser usada nos capítulos seguintes. Será mostrado desde como importar a base até como criar novas variáveis que poderão ser utilizadas em análises. Não será possível cobrir todo o ramo de manipulação em um só capítulo, mas iremos trabalhar com o máximo de ferramentas possíveis. Pacotes ou funções que não forem utilizadas aqui, mas que são interessantes serão mencionados ao longo do capítulo junto a links que contenham explicações de como utilizá-las. Vale ressaltar que estamos em um cenário mais básico e introdutório. Vamos começar.\n\n2.0.1 Importação de dados\nUm dos caminhos mais simples para importar dados no R é utilizando a função read.table(). Está função é simples pois ja vem instalada com o R, faz parte do pacote base utils, e importa arquivos nos formatos cvs e txt.\nA utilização do pacote é bem simples, não preciso carregá-lo na memória usando library().\n\ndados1 &lt;- read.table(file = \"dados.csv\", sep = \";\")\ndados2 &lt;- read.table(file = \"caminho-para-o-arquivo/dados.csv\", sep = \";\")\n\nObserve que na função temos os argumentos file e sep. O file indica o nome do arquivo que será importado e sep indica qual o símbolo separador de colunas, que neste caso é a virgula. Note também que usamos dois exemplos, o primeiro considera que o seu arquivo está no diretório de trabalho (quando criamos o projeto e colocamos os arquivos de dados na pasta criada pelo projeto), não sendo necessário especificar o caminho até do arquivo. O outro exemplo mostra como especificar o local do seu arquivo. A função possui mais argumentos que você pode explorar usando o help, mas no geral, esses dois são os mais utilizados.\n\n2.0.1.1 Extensão .txt ou .csv\nCaso esteja trabalhando com arquivos do tipo cvs ou txt o pacote readr irá servir muito bem. As funções deste pacote são bem rapidas e algumas delas são focadas em tranformar arquivos simples em data.frame. Aglumas funções do pacote são\n\nread_cvs(): para arquivos delimitados por vírgulas.\nread_cvs2(): para arquivos delimitados por ponto e vírgula.\nread_tsv(): para arquivos delimitados por tabulações.\nread_delim(): para aquivos com qualquer delimitador.\nread_fwf(): para arquivos compactos que devem ter a largura de cada coluna especificada.\nread_table(): para arquivos de texto tabulas com colunas separas por espaço.\n\nCaso esta seja a primeira vez que você ira utilizar este pacote, será necessário instalá-lo em seu computar. Você pode fazer isso utilizando a função install.packages(\"readr\") e é claro, antes de usar qualquer pacote que não faça parte do R base, você deve carregá-lo. Como exemplo, consideramos um arquivo chamado dados1 que queremos importar para o R.\n\nlibrary(readr)\ndados_csv &lt;- read_csv(file = \"caminho-para-o-arquivo/dados1.csv\")\ndados_txt &lt;- read_delim(file = \"caminho-para-o-arquivo/dados1.txt\", delim = \" \")\n\nApesar dos argumentos deste pacote serem semelhantes aos da função read.table(), devemos nos atentar a algumas diferenças. Aqui é o argumento delim que indica qual o separador das colunas no arquivo texto.\nVale ressaltar que para cada função read_, existe umas respectiva função write_ para exportar o arquivo no formato de interesse. Como exemplo, queremos salvar a base de dados mtcars na pasta do meu computador com o nome cars:\n\nwrite_csv(x = mtcars, path = \"cars.csv\")\nwrite_delim(x = mtcars, delim = \" \", path = \"cars.txt\")\n\n\n\n2.0.1.2 Arquivos em Excel\nArquivos em formato xlsx são muito utilizados, porém o R não possui uma função nativa para importar este tipo de arquivo. Existem diversos pacotes para importar dados neste e formato e os principais são redxl, xlsx, XLConnect e tydixl. Apesar destes pacotes terem objetivos semelhantes, cada um tem suas peculiaridades, então aconselhamos estudar cada um desses pacotes e assim decidir qual melhor atende às suas necessidades. Aqui vamos mostrar apenas o pacote readxl, pois é um dos mais facéis e diretos de se utilizar. Este pacote serve para importar e ler planilhas do Excel nos formatos xlsx ou xls. A seguir estão listadas algumas funções para importação e leitura de dados:\n\nread_excel(): esta função detecta automaticamente a extensão do arquivo, e importa arquivos do tipo xsl e xlsx.\nread_xsl(): importa arquivos no formato xsl.\nread_xlsx(): importa arquivos no formato xlsx.\n\nNovamente, é necessário à instalação e carregamento do pacote caso não o tenha em seu computador. Para exemplicar consideramos um arquivo chamado dados2 que queremos importar para o R.\n\nlibrary(readxl)\ndados_excel1 &lt;- read_excel(path = \"dados2.xls\")\ndados_excelx1 &lt;- read_excel(path = \"dados2.xlsx\")\n\nPor meio da função read_excel conseguimos importar tanto um arquivo no formato xls quanto no formato xlsx.\nPodemos também exportar um arquivo em excel (.xls e .xlsx) ao considerar a função write_xlsx do pacote writexl. Suponha que temos o interesse em salvar a base de dados dados em excel na pasta do computador (exportar) com o nome de dados_correto:\n\nlibrary(writexl)\nwrite_xlsx(dados, \"dados_correto.xlsx\")\n\n\n\n\n2.0.2 Análise de consistência e tratamento de dados\nO tratamento dos dados toma muitas vezes a maior parte do tempo de uma análise estatística.\nA análise de consistência consiste em realizar uma primeira análise dos dados com o intuito de encontrar inconsistências. São exemplos de inconsistências:\n\nboas práticas para nome das variáveis.\ncomo erros de digitação;\nindivíduos imputados mais de uma vez na planilha de dados de maneira errada;\nidentificar casos missings e avaliar se a observação está ausente de maneira correta ou não;\nidentificar as categorias de variáveis qualitativas.\n\nA partir daqui iremos trabalhar com a nossa base de dados de COVID-19 em gestantes e puérperas.\nImportando os dados\nComo já aprendemos a importar os dados, vamos direto ao ponto. Nos dados estão no forma rds que não foi mencionado anteriormente, mas o pacote readr tem uma função para importar este tipo de arquivo.\n\ndados &lt;- readr::read_rds(\"dados/dados_covid[SUJO].rds\")\nknitr::kable(head(dados))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDT_NOTIFIC\nDT_SIN_PRI\nDT_NASC\nDT_INTERNA\nSEM_PRI\nSG_UF\nID_MN_RESI\nCO_MUN_RES\nCS_ZONA\nCS_RACA\nCS_ESCOL_N\nidade\nCS_GESTANT\nPUERPERA\nFEBRE\nTOSSE\nGARGANTA\nDISPNEIA\nDESC_RESP\nSATURACAO\nDIARREIA\nVOMITO\nFADIGA\nPERD_OLFT\nPERD_PALA\nDOR_ABD\nCARDIOPATI\nHEMATOLOGI\nHEPATICA\nASMA\nDIABETES\nNEUROLOGIC\nPNEUMOPATI\nIMUNODEPRE\nRENAL\nOBESIDADE\nVACINA_COV\nDOSE_1_COV\nDOSE_2_COV\nFAB_COV_1\nFAB_COV_2\nDT_ENTUTI\nDT_SAIDUTI\nUTI\nSUPORT_VEN\nEVOLUCAO\n\n\n\n\n15/05/2020\n06/05/2020\n03/06/2003\n15/05/2020\n19\nCE\nMORRINHOS\n230890\nNA\n4\nNA\n16\n2\nNA\n1\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n18/05/2020\n10/05/2020\n07/07/1996\n15/05/2020\n20\nPR\nCURITIBA\n410690\n1\n1\n2\n23\n2\n2\n2\n2\n2\n1\n2\n2\n1\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n30/04/2020\n20/04/2020\n26/03/1996\n24/04/2020\n17\nSP\nSAO CAETANO DO SUL\n354880\n1\n9\n9\n24\n9\n1\n1\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n11/05/2020\n04/05/2020\n02/06/1986\n09/05/2020\n19\nPA\nMARABA\n150420\n1\n4\n4\n33\n5\n1\n1\n1\n2\n2\n1\n2\n2\n2\nNA\nNA\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n2\n1\n\n\n01/07/2020\n12/06/2020\n11/12/1996\n30/06/2020\n24\nDF\nSANTA MARIA\n530150\n1\n9\nNA\n23\n5\n1\n2\n2\n2\n2\n2\n2\n1\n2\nNA\n1\nNA\nNA\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n09/06/2020\n09/06/2020\n09/12/1984\n09/06/2020\n24\nRO\nPORTO VELHO\n110020\n1\n4\n2\n35\n3\n2\n1\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\nNA\nNA\nNA\nNA\nNA\n\n\n2\n3\n1\n\n\n\n\n\n\n2.0.2.1 Tratamento da base de dados\nInicialmente, vamos verificar os nomes das variáveis na base de dados por meio da função names. Note que os nomes estão, de certa forma, padronizados. Todos maíusculos (com exceção de “idade”), separados por “_”. Este ainda não é o cenário ideal para trabalharmos, mas poderia ser pior, contendo maiúsculas, espaços e acentos. Utilizar os dados com essas características não impossibilita as futuras análises, mas pode atrapalhar quando precisamos selecionar algumas dessas variáveis.\n\nnames(dados)\n\n [1] \"DT_NOTIFIC\" \"DT_SIN_PRI\" \"DT_NASC\"    \"DT_INTERNA\" \"SEM_PRI\"   \n [6] \"SG_UF\"      \"ID_MN_RESI\" \"CO_MUN_RES\" \"CS_ZONA\"    \"CS_RACA\"   \n[11] \"CS_ESCOL_N\" \"idade\"      \"CS_GESTANT\" \"PUERPERA\"   \"FEBRE\"     \n[16] \"TOSSE\"      \"GARGANTA\"   \"DISPNEIA\"   \"DESC_RESP\"  \"SATURACAO\" \n[21] \"DIARREIA\"   \"VOMITO\"     \"FADIGA\"     \"PERD_OLFT\"  \"PERD_PALA\" \n[26] \"DOR_ABD\"    \"CARDIOPATI\" \"HEMATOLOGI\" \"HEPATICA\"   \"ASMA\"      \n[31] \"DIABETES\"   \"NEUROLOGIC\" \"PNEUMOPATI\" \"IMUNODEPRE\" \"RENAL\"     \n[36] \"OBESIDADE\"  \"VACINA_COV\" \"DOSE_1_COV\" \"DOSE_2_COV\" \"FAB_COV_1\" \n[41] \"FAB_COV_2\"  \"DT_ENTUTI\"  \"DT_SAIDUTI\" \"UTI\"        \"SUPORT_VEN\"\n[46] \"EVOLUCAO\"  \n\n\numa boa prática consiste em padronizar os nomes das variáveis, até para facilitar a lembrança deles. Para isso, utilizaremos o pacote janitor para a arrumação da base de dados. Usamos a função clean_names() para primeiro ajuste dos nomes das variáveis.\n\ndados &lt;- janitor::clean_names(dados) \nnames(dados)\n\n [1] \"dt_notific\" \"dt_sin_pri\" \"dt_nasc\"    \"dt_interna\" \"sem_pri\"   \n [6] \"sg_uf\"      \"id_mn_resi\" \"co_mun_res\" \"cs_zona\"    \"cs_raca\"   \n[11] \"cs_escol_n\" \"idade\"      \"cs_gestant\" \"puerpera\"   \"febre\"     \n[16] \"tosse\"      \"garganta\"   \"dispneia\"   \"desc_resp\"  \"saturacao\" \n[21] \"diarreia\"   \"vomito\"     \"fadiga\"     \"perd_olft\"  \"perd_pala\" \n[26] \"dor_abd\"    \"cardiopati\" \"hematologi\" \"hepatica\"   \"asma\"      \n[31] \"diabetes\"   \"neurologic\" \"pneumopati\" \"imunodepre\" \"renal\"     \n[36] \"obesidade\"  \"vacina_cov\" \"dose_1_cov\" \"dose_2_cov\" \"fab_cov_1\" \n[41] \"fab_cov_2\"  \"dt_entuti\"  \"dt_saiduti\" \"uti\"        \"suport_ven\"\n[46] \"evolucao\"  \n\n\nVeja que ele deixou todos os nomes minúsculos. Neste caso não foi feito, mas a função substitui o espaço por “_” e tira acentos. Isso ajuda a evitar problemas futuros em algumas análises que não lidam muito bem com acentos e espaços nos nomes das variáveis.\nOutro problema comum é a presença de linhas e colunas vazias. Na base de dados em questão, não há linhas nem colunas em branco, como pode ser visto na saída abaixo.\n\njanitor::remove_empty(dados,\"rows\")\njanitor::remove_empty(dados,\"cols\")\n\n\n\n2.0.2.2 \nIdentificando casos duplicados\nOutra boa prática é identificar casos duplicados, isto é, identificar se há casos erroneamente repetidos. O ideal é utilizar variável chave do seu banco de dados, ou seja, aquela em que cada observação é única. Por exemplo, em uma base de dados de funcionários de uma empresa, uma variável chave poderia ser o CPF. Uma variável chave também pode ser a combinação de variáveis, gerando assim observações únicas. Para identificar casos duplicados, usamos a função get_dupes do pacote janitor. Em nosso banco de dados não tempos uma varíavel chave, então não vamos especificá-la na função, assim a função irá procurar observações repetidas considerando todas as variáveis, ou seja, linhas repetidas.\n\njanitor::get_dupes(dados)\n\nNo variable names specified - using all columns.\n\n\nNo duplicate combinations found of: dt_notific, dt_sin_pri, dt_nasc, dt_interna, sem_pri, sg_uf, id_mn_resi, co_mun_res, cs_zona, ... and 37 other variables\n\n\n [1] dt_notific dt_sin_pri dt_nasc    dt_interna sem_pri    sg_uf     \n [7] id_mn_resi co_mun_res cs_zona    cs_raca    cs_escol_n idade     \n[13] cs_gestant puerpera   febre      tosse      garganta   dispneia  \n[19] desc_resp  saturacao  diarreia   vomito     fadiga     perd_olft \n[25] perd_pala  dor_abd    cardiopati hematologi hepatica   asma      \n[31] diabetes   neurologic pneumopati imunodepre renal      obesidade \n[37] vacina_cov dose_1_cov dose_2_cov fab_cov_1  fab_cov_2  dt_entuti \n[43] dt_saiduti uti        suport_ven evolucao   dupe_count\n&lt;0 linhas&gt; (ou row.names de comprimento 0)\n\n\nEm nosso caso, não temos casos duplicados. Caso tivesse, seria necessário remover as linhas duplicadas. Isto pode ser feito com o uso da função distinct do pacote dplyr.\n\n\n\n2.0.3 Identificar problemas nas variáveis da base de dados\nOutra etapa importante na análise de consistência é identificar o tipo de variável e ver se o R está interpretando corretamente o tipo de cada variável.\nTemos na nossa base de dados variáveis de data, além de variáveis qualitativas e quantitativas (veja o dicionário das variáveis na em: refenciar parte). Assim, precisamos entender se o R realmente entendeu todas as variáveis da maneira correta. Uma maneira de identificar isso e também de ver algumas descritivas das variáveis que nos auxiliam a ver possíveis inconsistências na base de dados é a a função glimpse do pacote dplyr. A função skim do pacote skimr também pode ajudar nisso.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;chr&gt; \"15/05/2020\", \"18/05/2020\", \"30/04/2020\", \"11/05/2020\", \"01…\n$ dt_sin_pri &lt;chr&gt; \"06/05/2020\", \"10/05/2020\", \"20/04/2020\", \"04/05/2020\", \"12…\n$ dt_nasc    &lt;chr&gt; \"03/06/2003\", \"07/07/1996\", \"26/03/1996\", \"02/06/1986\", \"11…\n$ dt_interna &lt;chr&gt; \"15/05/2020\", \"15/05/2020\", \"24/04/2020\", \"09/05/2020\", \"30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;int&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;int&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;int&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;int&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;int&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;int&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;int&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;int&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;int&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;int&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;int&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"30/06/2020\", \"\", \"12/07/2020\",…\n$ dt_saiduti &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"29/07/2020\", \"\", \"\", \"\", \"\", \"…\n$ uti        &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;int&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nNo R, as variáveis qualititativas são nomeadas “factor”, as variáveis quantitativas são nomeadas “numeric” e as variáveis de data são “date”. Note que na importação dos dados o R não entendeu corretamente os tipos de variáveis. Mas vamos corrigir isso no que segue.\nComeçando pela data, vamos rodar o seguinte código:\n\ndados$dt_notific  &lt;- as.Date(dados$dt_notific, format = \"%d/%m/%Y\")\ndados$dt_sin_pri  &lt;- as.Date(dados$dt_sin_pri, format = \"%d/%m/%Y\")\ndados$dt_nasc  &lt;- as.Date(dados$dt_nasc, format = \"%d/%m/%Y\")\ndados$dt_interna  &lt;- as.Date(dados$dt_interna, format = \"%d/%m/%Y\")\ndados$dt_entuti  &lt;- as.Date(dados$dt_entuti, format = \"%d/%m/%Y\")\ndados$dt_saiduti  &lt;- as.Date(dados$dt_saiduti, format = \"%d/%m/%Y\")\n\nA função as.Date informa para o R que a variável indicada é de data. O argumento format indica o formato que está a data, nesse caso, “dia/mês/ano”. Aqui é possível verificar todos os formatos de datas da função. Vamos ver como ficou:\n\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;int&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;int&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;int&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;int&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;int&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;int&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;int&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;int&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;int&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;int&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;int&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;int&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;int&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;int&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;int&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nAgora vamos lidar com as variáveis qualitativas. Veja que “cs_zona” foi identificada como int. Isso acontece porque ela foi tabulada como número, como posteriormente variáveis deste tipo serão recodificadas de acordo com o dicionário, precisamos tratá-la como fator. Já as demais variáveis qualitativas estão identificadas como numeric, dbl ou chacacter pois na tabulação suas categorias estão codificadas com números ou textos. Para então dizer ao R o verdadeiro tipo dessas variáveis, vamos utilizar os seguintes comandos:\n\ndados$cs_raca &lt;- as.factor(dados$cs_raca)\ndados$cs_escol_n &lt;- as.factor(dados$cs_escol_n)\ndados$cs_gestant &lt;- as.factor(dados$cs_gestant)\ndados$puerpera &lt;- as.factor(dados$puerpera)\ndados$cs_zona &lt;- as.factor(dados$cs_zona)\ndados$febre &lt;- as.factor(dados$febre)\ndados$tosse &lt;- as.factor(dados$tosse)\ndados$suport_ven &lt;- as.factor(dados$suport_ven)\ndados$uti &lt;- as.factor(dados$uti)\ndados$evolucao &lt;- as.factor(dados$evolucao)\n\nUma forma um pouco mais eficiente de fazer isso é selecionar as variáveis por meio de um vetor, por exemplo, quero que as variáveis da coluna 10 até a coluna 20 sejam fatores. Podemos fazer isso com a ajuda a função lapply. Essa função, em resumo, nos possibilita aplicar uma função em uma lista de elementos e retorna uma lista de mesmo tamanho em que o resultado é a aplicação desta função a cada elemento da lista. Neste caso, aplicamos a função as.factor nas colunas selecionadas (lista de elementos). Veja como é feito.\n\n\ndados[,c(17:37)] &lt;- lapply(dados[,c(17:37)], as.factor)\nglimpse(dados)\n\nRows: 11,523\nColumns: 46\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, NA, 1, 1, NA, 2…\n$ cs_raca    &lt;fct&gt; 4, 1, 9, 4, 9, 4, 9, 1, 4, 9, 4, 4, 4, 4, 4, 9, 4, 9, 4, 4,…\n$ cs_escol_n &lt;fct&gt; NA, 2, 9, 4, NA, 2, 4, 2, NA, NA, NA, 9, 9, 3, 3, NA, 3, NA…\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2, 2, 9, 5, 5, 3, 1, 5, 3, 3, 4, 3, 5, 3, 3, 3, 3, 9, 3, 3,…\n$ puerpera   &lt;fct&gt; NA, 2, 1, 1, 1, 2, NA, 1, NA, 1, NA, NA, 1, 2, 1, NA, NA, 1…\n$ febre      &lt;fct&gt; 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1, 1, 1…\n$ tosse      &lt;fct&gt; 1, 2, 2, 1, 2, 1, 1, 1, NA, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1…\n$ garganta   &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, NA, 1, NA, 2, NA, 2, 1, 1, 2, 2, NA, …\n$ dispneia   &lt;fct&gt; NA, 1, 2, 2, 2, 2, 2, 1, NA, NA, 1, NA, 2, 1, 2, 1, 1, NA, …\n$ desc_resp  &lt;fct&gt; NA, 2, 2, 1, 2, 2, 1, 1, NA, 1, 1, NA, 2, 1, 2, 2, 1, NA, 1…\n$ saturacao  &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, 1, NA, NA, 2, NA, 2, 2, 2, 1, 1, NA, …\n$ diarreia   &lt;fct&gt; NA, 1, 2, 2, 1, 2, 2, 1, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ vomito     &lt;fct&gt; NA, 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, NA, 2, 1, 2, 2, 2, NA, …\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, 1, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 1, 2…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 1, 2, 2, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, 2, 2, 2, …\n$ cardiopati &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ hematologi &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ hepatica   &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ asma       &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ diabetes   &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ neurologic &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 1, NA, NA, …\n$ pneumopati &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ imunodepre &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ renal      &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ obesidade  &lt;fct&gt; NA, 2, 2, 2, 2, 2, NA, 2, NA, NA, NA, NA, 2, 2, 2, NA, NA, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, NA, 2…\n$ suport_ven &lt;fct&gt; 2, 3, 3, 2, 3, 3, 9, 1, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, 3, 2,…\n$ evolucao   &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, NA, 1, 1, 1, 1, 1, 1…\n\n\nÓtimo! Corrigimos as inconsistências das variáveis qualitativas. Mas outra questão surge: como faço para usar um rótulo nos números codificados nas categorias das variáveis qualitativas? Para o grupo, por exemplo, ao invés de aparecer 1 quero que apareça “sim”. Para isso, vamos utilizar o pacote forcats que lida com variáveis qualitativas (categóricas). Para renomear as categorias das variáveis, vamos usar a função fct_recode desse pacote:\n\ndados$cs_raca &lt;- forcats::fct_recode(dados$cs_raca,\n                                   branca = \"1\",\n                                   preta = \"2\",\n                                   amarela = \"3\",\n                                   parda = \"4\",\n                                   indigena = \"5\",\n                                   ignorado = \"9\")\n\ndados$cs_escol_n &lt;- forcats::fct_recode(dados$cs_escol_n,\n                                     \"sem escola\"  = \"0\",\n                                     fund1 = \"1\",\n                                     fund2 = \"2\",\n                                     medio = \"3\",\n                                     superior = \"4\",\n                                     ignorado = \"9\")\n\ndados$cs_gestant &lt;- forcats::fct_recode(dados$cs_gestant,\n                                     \"1tri\" = \"1\",\n                                     \"2tri\" = \"2\",\n                                     \"3tri\" = \"3\",\n                                     IG_ig = \"4\",\n                                     nao = \"5\",\n                                     ignorado = \"9\")\n\ndados$puerpera &lt;- forcats::fct_recode(dados$puerpera,\n                                      sim = \"1\",\n                                      nao = \"2\",\n                                      ignorado = \"9\")\n\ndados$cs_zona &lt;- forcats::fct_recode(dados$cs_zona,\n                                  urbana = \"1\",\n                                  rural = \"2\",\n                                  periurbana = \"3\",\n                                  ignorado = \"9\")\n\ndados$febre &lt;- forcats::fct_recode(dados$febre,\n                                   sim = \"1\",\n                                   nao = \"2\",\n                                   ignorado = \"9\")\n\ndados$suport_ven &lt;-forcats::fct_recode(dados$suport_ven,\n                                       \"sim, invasivo\" = \"1\",\n                                       \"sim, nao invasivo\" = \"2\",\n                                       nao = \"3\",\n                                       ignorado = \"9\")\n\ndados$uti &lt;-forcats::fct_recode(dados$uti,\n                                       sim = \"1\",\n                                       nao = \"2\",\n                                       ignorado = \"9\")\n\ndados$evolucao &lt;-forcats::fct_recode(dados$evolucao,\n                                       cura = \"1\",\n                                       obito = \"2\",\n                                       \"obito por outras causas\" = \"3\",\n                                       ignorado = \"9\")\n\nEste tramanto foi feito para todas as variáveis qualitativas da base, mas por conta do tamanho do código, omitimos algumas da saída.\nFinalmente chegamos nas variáveis quantitativas. Uma forma de identificar problemas em variáveis quantitativas é avaliar os valores mínimo e máximo de cada variável e ver se tem algum valor impossível para a mesma. Em nosso caso podemos verificar a variável idade. Seria meio estranho encontrar alguém com valores extremamente altos ou negativos, concorda?! A função summary pode ser uma opção boa aqui, ela nos formece algumas medidas descritivas como, media, mínimo, máximo, entre outros.\n\nsummary(dados$idade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  10.00   25.00   30.00   30.25   35.00   55.00       9 \n\n\nAparentemente nossa variável esta dentro do esperado, sem valores inesperados.\n\n2.0.3.1 Transformação de dados\nÉ possível modificar ou criar novas variáveis na base de dados por meio da função mutate do pacote dplyr, você pode veificar melhor essa função clicando aqui. Também podemos criar categorias com base em alguma condição por meio da função case_when também do pacote dplyr, veja melhor aqui. Para ficar mais claro, vamos a um exemplo combinando as duas funções. Vamos criar a variável “faixa_et”, onde as observações serão as faixas etárias. São essas: “&lt;20”, “20-34” e “&gt;=”. Veja como faz:\n\ndados &lt;- dados |&gt; \n  mutate(faixa_et = case_when(\n    idade &lt; 20 ~ \"&lt;20\",\n    idade &gt;= 20 & idade &lt; 34 ~ \"20-34\",\n    idade &gt;= 34 ~ \"&gt;=34\"\n  ))\n\ntable(dados$faixa_et)#table nos mostra as observações da quela variável e a sua frequência. \n\n\n  &lt;20  &gt;=34 20-34 \n  714  3862  6938 \n\n\nAqui fizemos a utilização da função “pipe” |&gt; que agora está no pacote base do R, mas que antes era necessário carregá-la por meio de pacotes. Essa função é de extrema importância, facilita a programção no R de uma forma inimaginável. É válido dedicar um pouco de seu tempo para entender melhor essa função. Separamos alguns links que pode te ajudar a entender melhor e você pode acessá-los clickando aqui, aqui ou aqui. Como foi mencionado acima, a função foi adicionada ao R base há pouco tempo, então esses links se referem ao pipe “antigo”, mas fique tranquilo, a função é a mesma. Para resumir, o pipe pega a saída de uma função e a passa para outra função como um argumento. Isso nos permite vincular uma sequência de etapas de análise.\n\n\n2.0.3.2 Manipulação de datas\nAlgo interessante também é trabalhar com a varíavel de datas. Podemos calulcar a diferença entre duas datas no R de forma bem simples por meio da função difftime do pacote base do R. Para exemplificar vamos criar a variável “dias_uti” que vai ser ser quantos dias a pessoa ficou internada na uti. Vamos fazer isso calculando a diferença entre a data de saída e a data de entrada na uti e queremos o resultado em dias.\n\ndados$dias_uti &lt;- difftime(dados$dt_saiduti, dados$dt_entuti, units = \"days\")\nglimpse(dados)\n\nRows: 11,523\nColumns: 48\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nNote que não utilizamos a função mutate para criar está nova variável, utilizamos apenas o $ para representar a variável e atribuímos a função. Assim, o R já entende como uma variável.\n\n\n2.0.3.3 Manipulação de dados\nJá temos a nossa base de dados devidamente tratada para prosseguir com a análise descritiva, mas quando falamos de manipulação de dados, um leque de possibilidades aparece. Em diversos cenários precisamos filtrar observações, reordená-las, selecionar variáveis específicas, entre outras coisas. Não poderíamos deixar de mencionar o poderoso tidyverse. O tidyverse é um pacote que contém um coleção de outros pacotes que são utilizados para manipulação, exploração e visualização de dados e que compartilham uma filosofia de design bem parecida, por isso de forma combinada permitem que você consiga fazer inúmeros trabalhos. Os pacotes que fazem parte desse universo são: dplyr, tidyr, ggplot2, forcats, purrr, stringr, tibble e readr. Anteriormente já trabalhamos com alguns destes pacotes, mas agora é válido aprofundarmos um pouco mais em alguns deles. Aqui você irá acessar o site do tidyverse onde podera navegar por cada pacote e aprender mais sobre suas utilidades e aqui você irá acessar um post escrito pelo Laboratório de Data Scinence - UFES (daslab) que contem diversos exemplos práticos de uso de todos os pacotes do universo tidyverse. Neste capítulo iremos trabalhar com algumas funções específicas.\n\n2.0.3.3.1 Pacote dplyr\nO dplyr é extremamente útil e nos ajuda a resolver os desafios mais comuns de manipulação de dados.\nSuas principais funções são:\n\nfilter() - filtra linhas;\ngroup_by() - agrupa pela(s) variável(is) no argumento. Função muito útil quando usada a funçaõ summurise.\nsummarise() - reduz vários valores a um único resumo.\nselect() - seleciona colunas;\narrange() - ordena a base;\nmutate() - cria/modifica colunas.\n\nJá utilizamos algumas funções do pacote, vamos falar sobre outras. Como já avançamos um pouco sobre a utilização de funções, vamos combinar algumas funções, o que geralmente é feito no dia a dia.\n\n#criando um novo banco de dados selecionando 3 variáveis\ndados_tratamento &lt;- dados |&gt; \n  select(sg_uf, cs_zona, idade)\n\nglimpse(dados_tratamento)\n\nRows: 11,523\nColumns: 3\n$ sg_uf   &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\", \"M…\n$ cs_zona &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana, ur…\n$ idade   &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26, 20…\n\n\nAqui criamos a base “dados_tratamento” onde apenas selecionamos algumas colunas da base de dados inicial com a função select.\n\ndados_tratamento2 &lt;- dados_tratamento |&gt; \n  filter(cs_zona == \"urbana\") |&gt; \n  group_by(sg_uf) |&gt; \n  summarise(media = mean(idade, na.rm = TRUE)) |&gt; \n  arrange(desc(media))\n\nknitr::kable(head(dados_tratamento2))\n\n\n\n\nsg_uf\nmedia\n\n\n\n\nAP\n36.66667\n\n\nBA\n31.04152\n\n\nRR\n31.00000\n\n\nSP\n30.94237\n\n\nRJ\n30.93570\n\n\nMG\n30.93257\n\n\n\n\n\nVamos entender o código acima. Primeiro acessamos a base “dados_tratamento” e com função filter selecionamos apenas as observações “urbana”. Após isso utilizamos a função group_by para agrupar nossas observações pela variável “sg_uf” e por últimos, combinamos com a função summarise para criar a variável “media” que será a media da variável idade. Note que nesta função utilizamos o argumento na.rm - TRUE. Este argumento serve para indicar para a função se ela deve ou não remover valores NA's do cálculo, o default é FALSE. Como não é possível calcular a média de valores ausentes e temos variáveis ausentes, foi necessário utilizar este argumento. Caso contrário, Estados com valores faltantes ficariam com NA. Por último, utilizamos a função arrange para ordernar os dados em em ordem descrente pela variavel media. Uma dica para tentar entender melhor o funcionamento das funções é tentar refazer o código utilizando uma função de cada vez e ir vendo como fica. Então, em poucas linhas de códigos conseguimos criar uma base com a idade média dos Estados considerando apenas zonas urbanas, legal né?\n\n\n2.0.3.3.2 Pacote stringr\nUm desafio muito grande na manipulação de dados é extrair informações de caracteres. Em resumo caracteres são letras, símbolos, sinais, números que representem algo escrito, etc.. Essa sequência de caracteres formam o que chamamos de string. Diversas vezes encontramos variáveis com categorias não padronizadas, como, por exemplo, uma variável contendo “São Paulo”, “sao paulo” e “sp”. Apesar de representarem o mesmo estado, elas são diferentes. Nesse sentido, uma parte muito importante no tratamento de dados é “lapidar” esse conjunto de caracteres para que seja possível usá-los nas análises. Essa é a introdução do post do daslab onde é passado de uma maneira muito prática como trabalhar com strings utilizando o pacote stringr, la você vai aprender também sobre expressões regulares, que com certeza serão úteis em vários momentos da sua carreira. Link do post. Como as variáveis de texto do nosso banco de dados já estão bem padronizadas não será necessário realizar nenhum tratamento, mas por ser um pacote de extrema importância e que não havia sido mencionado ainda, deixamos ele aqui para que você possa se aprofundar mais. Como em nossa base dados as variáveis de texto estão padrozinados, não será necessário realizar nenhum tratamento.\n\n\n\n2.0.3.4 Manipulando o formato da base de dados\nEm certos casos é necessário mudar o formato das bases de dados, fazer com que colunas se tornem linhas vice-versa. Vamos utilizar a base “dados_tratamento”. Veja que ela está no formato long, em que as avaliações do mesmo indivíduo (variável de identificação de indivíduo é “registro”) estão nas linhas. Queremos que as zonas fiquem nas colunas, com as três colunas (vamos tirar valores ignorados): urbana, rural e periurbana, ou seja, queremos o formato wide. Um pacote do R que pode nos auxiliar a transformar formato long em wide e vice-versa é o tidyr. A função que usaremos é spread, como segue:\n\nlibrary(tidyr)\n\ndados_formato &lt;- dados_tratamento |&gt; \n  filter(!is.na(cs_zona) & cs_zona != \"ignorado\") |&gt; \n  mutate(id = row_number())\n\nknitr::kable(head(dados_formato))\n\n\n\n\nsg_uf\ncs_zona\nidade\nid\n\n\n\n\nPR\nurbana\n23\n1\n\n\nSP\nurbana\n24\n2\n\n\nPA\nurbana\n33\n3\n\n\nDF\nurbana\n23\n4\n\n\nRO\nurbana\n35\n5\n\n\nPI\nurbana\n31\n6\n\n\n\n\n\nFizemos pequenas alterações na base de dados. Primeiro realizamos um filtro para retirarmos valores faltantes da variável “cs_zona”, pois essa passará a\n\ndados_formato2 &lt;- dados_formato |&gt; \n  pivot_wider(names_from = cs_zona, values_from = idade)  \n\nknitr::kable(head(dados_formato2))\n\n\n\n\nsg_uf\nid\nurbana\nrural\nperiurbana\n\n\n\n\nPR\n1\n23\nNA\nNA\n\n\nSP\n2\n24\nNA\nNA\n\n\nPA\n3\n33\nNA\nNA\n\n\nDF\n4\n23\nNA\nNA\n\n\nRO\n5\n35\nNA\nNA\n\n\nPI\n6\n31\nNA\nNA\n\n\n\n\n\n\ndados_formato3 &lt;- dados_formato2 |&gt; \n  pivot_longer(cols = c(\"urbana\",   \"rural\",    \"periurbana\"), names_to = \"cs_zona\", values_to = \"idade\")\n\nknitr::kable(head(dados_formato3))\n\n\n\n\nsg_uf\nid\ncs_zona\nidade\n\n\n\n\nPR\n1\nurbana\n23\n\n\nPR\n1\nrural\nNA\n\n\nPR\n1\nperiurbana\nNA\n\n\nSP\n2\nurbana\n24\n\n\nSP\n2\nrural\nNA\n\n\nSP\n2\nperiurbana\nNA\n\n\n\n\n\n\n\n2.0.3.5 Combinando bases de dados\nQuando estamos trabalhando com dados, nem sempre uma única base irá conter todas as informações que precisamos, na verdade, isso é mais comum do que se possa imaginar. Assim, saber juntar duas bases de dados é indispensável. Vamos começar então falando sobre chave primária. Em resumo, chave primária se refere a um ou mais campos, onde combinados (no caso de mais de uma chave primária), não se repete na mesma tabela. Em outras palavras, uma chave primária no meu banco dados seria uma variável onde as observações não se repetem ou a combinação de variáveis que tornam as observações únicas. Para exemplicar vamos pegar nossa base de dados e separar em duas, para que posteriormente possamos juntalas. Como em nossa base de dados não temos naturalmente nenhuma chave primária, vamos utilizar a função mutate(id = row_number()) para criarmos um identificar único para este exemplo. Após isso, vamos dividir a nossa base de dados em duas, mantendo em comum entre elas apenas a nossa chave primária, neste caso, a variável “id”\n\ndados &lt;- dados |&gt; \n  mutate(id = row_number()) |&gt; \n  select(id, everything())#selecionar variavel id e todas as outras \n\ndados1 &lt;- dados[, c(1:24)]\n\nglimpse(dados1)\n\nRows: 11,523\nColumns: 24\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n\ndados2 &lt;- dados[, c(1, 25:49)]\n\nglimpse(dados2)\n\nRows: 11,523\nColumns: 26\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nEm “dados1” selecionamos as colunas de 1 até a 24, onde a coluna 1 é a variável “id”. Em dados 2 selecionamos a coluna depois e as colunas de 25 até a 49. Agora temos dois banco de dados e precisamos juntá-los.\nHá algumas funções de combinação de duas bases de dados no pacote dplyr. Elas recebem três argumentos: a primeira base a ser declarada (x=), a segunda base a ser declarada (y=) e a variável de identificação informada no argumento by=. Aqui estão as funções mais úteis:\n\nleft_join() - retorna todas as linhas da base de dados no argumento x e todas as colunas das duas bases de dados. Linhas da base de dados de x sem correspondentes em y receberão NA na base de dados combinada.\nright_join() - retorna todas as linhas da base de dados no argumento y e todas as colunas das duas bases de dados. Linhas da base de dados de y sem correspondentes em x receberão NA na base de dados combinada.\nfull_join() - retorna todas as linhas e todas as colunas de x e de y. As linhas sem correspondência entre as bases receberão NA na base de dados combinada.\ninner_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y e todas as colunas das duas bases de dados.\nsemi_join() - filtra a base de dados no argumento x apenas onde tem valores correspondentes na base de dados no argumento y, mantendo apenas as colunas da base de dados de x.\nanti_join() - filtra a base de dados no argumento x para incluir apenas valores que não possuem correspondências na base de dados no argumento y.\n\nAssim sendo, no nosso exemplo, tanto as funções left_join(), right_join(), full_join() e inner_join() retornarão a mesma combinação, pois “dados1” e “dados2” possuem exatamente os mesmos indivíduos, ou seja, não há nenhuma linha que esteja em uma das bases de dados e que não está na outra. Este cenário foi um pouco mais simples, mas pense que no dia a dia você irá encontrar bases onde você precisará encontrar chaves primarias entre elas. Além disso, varios problemas podem vir acompanhados, por exemplo, imagine que para juntar duas bases você utilizará uma chave formada pela combinação de duas variáveis: UF e Município. Em uma base a sua UF está no formato de sigla e na outra está sendo representada pelo código da UF atribuido pelo IBGE. Já na variável de Município, Em uma base os dados estão todos padronizados, maiúsculos e sem acentuação, já na outra base está no formato “padrão” com a primeira letra maiúscula e acentuação. Veja que será necessário um bom tratamento de dados para pode juntar essas bases. Voltando para o nosso exemplo, vamos a prática.\n\ndados_todos &lt;- full_join(dados1, dados2, by=c(\"id\")) \n\nglimpse(dados_todos)\n\nRows: 11,523\nColumns: 49\n$ id         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dt_notific &lt;date&gt; 2020-05-15, 2020-05-18, 2020-04-30, 2020-05-11, 2020-07-01…\n$ dt_sin_pri &lt;date&gt; 2020-05-06, 2020-05-10, 2020-04-20, 2020-05-04, 2020-06-12…\n$ dt_nasc    &lt;date&gt; 2003-06-03, 1996-07-07, 1996-03-26, 1986-06-02, 1996-12-11…\n$ dt_interna &lt;date&gt; 2020-05-15, 2020-05-15, 2020-04-24, 2020-05-09, 2020-06-30…\n$ sem_pri    &lt;int&gt; 19, 20, 17, 19, 24, 24, 26, 27, 28, 24, 14, 29, 28, 10, 36,…\n$ sg_uf      &lt;chr&gt; \"CE\", \"PR\", \"SP\", \"PA\", \"DF\", \"RO\", \"PI\", \"RS\", \"PE\", \"MA\",…\n$ id_mn_resi &lt;chr&gt; \"MORRINHOS\", \"CURITIBA\", \"SAO CAETANO DO SUL\", \"MARABA\", \"S…\n$ co_mun_res &lt;int&gt; 230890, 410690, 354880, 150420, 530150, 110020, 221100, 431…\n$ cs_zona    &lt;fct&gt; NA, urbana, urbana, urbana, urbana, urbana, urbana, urbana,…\n$ cs_raca    &lt;fct&gt; parda, branca, ignorado, parda, ignorado, parda, ignorado, …\n$ cs_escol_n &lt;fct&gt; NA, fund2, ignorado, superior, NA, fund2, superior, fund2, …\n$ idade      &lt;dbl&gt; 16, 23, 24, 33, 23, 35, 31, 17, 22, 29, 28, 22, 27, 25, 26,…\n$ cs_gestant &lt;fct&gt; 2tri, 2tri, ignorado, nao, nao, 3tri, 1tri, nao, 3tri, 3tri…\n$ puerpera   &lt;fct&gt; NA, nao, sim, sim, sim, nao, NA, sim, NA, sim, NA, NA, sim,…\n$ febre      &lt;fct&gt; sim, nao, sim, sim, nao, sim, sim, sim, sim, sim, sim, NA, …\n$ tosse      &lt;fct&gt; sim, nao, nao, sim, nao, sim, sim, sim, NA, sim, sim, sim, …\n$ garganta   &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, NA, sim, NA, nao, NA, nao…\n$ dispneia   &lt;fct&gt; NA, sim, nao, nao, nao, nao, nao, sim, NA, NA, sim, NA, nao…\n$ desc_resp  &lt;fct&gt; NA, nao, nao, sim, nao, nao, sim, sim, NA, sim, sim, NA, na…\n$ saturacao  &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, sim, NA, NA, nao, NA, nao…\n$ diarreia   &lt;fct&gt; NA, sim, nao, nao, sim, nao, nao, sim, NA, NA, nao, NA, nao…\n$ vomito     &lt;fct&gt; NA, nao, nao, nao, nao, nao, nao, nao, NA, NA, nao, NA, nao…\n$ fadiga     &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ perd_olft  &lt;fct&gt; NA, NA, NA, NA, sim, nao, NA, NA, NA, NA, nao, NA, NA, sim,…\n$ perd_pala  &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, sim, …\n$ dor_abd    &lt;fct&gt; NA, NA, NA, NA, NA, nao, NA, NA, NA, NA, nao, NA, NA, nao, …\n$ cardiopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hematologi &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ hepatica   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ asma       &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ diabetes   &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ neurologic &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ pneumopati &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ imunodepre &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ renal      &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ obesidade  &lt;fct&gt; NA, nao, nao, nao, nao, nao, NA, nao, NA, NA, NA, NA, nao, …\n$ vacina_cov &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_1_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dose_2_cov &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_1  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ fab_cov_2  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ dt_entuti  &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-06-30, NA, 2020-07-12, NA…\n$ dt_saiduti &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, 2020-07-29, NA, NA, NA, NA, NA…\n$ uti        &lt;fct&gt; nao, nao, nao, nao, nao, nao, nao, sim, nao, sim, nao, nao,…\n$ suport_ven &lt;fct&gt; \"sim, nao invasivo\", \"nao\", \"nao\", \"sim, nao invasivo\", \"na…\n$ evolucao   &lt;fct&gt; cura, cura, cura, cura, cura, cura, cura, obito, cura, cura…\n$ faixa_et   &lt;chr&gt; \"&lt;20\", \"20-34\", \"20-34\", \"20-34\", \"20-34\", \"&gt;=34\", \"20-34\",…\n$ dias_uti   &lt;drtn&gt; NA days, NA days, NA days, NA days, NA days, NA days, NA d…\n\n\nPronto, temos nossa base completa e aprendemos um pouco sobre manipular dados. O pacote tidyverse será um grande aliado seu no R de forma geral. Como mencionado anteriormente, não cobrimos tudo o que é necessário saber para trabalhar com manipulação de dados, é necessário entender a demanda e pesquisar soluções. Saber traduzir o seu problema para que consiga pesquisar com mais facilidade é uma habilidade muito importante. Recomendamos que treine, trabalhe com diferentes tipos de dados, pesquise pacotes, funções, etc. Com o tempo fará com tranquilidade coisas que hoje considera difícil. Lembre-se, a pratica leva a perfeição."
  },
  {
    "objectID": "descritiva.html#medidas-resumo",
    "href": "descritiva.html#medidas-resumo",
    "title": "4  Análise exploratória dos dados",
    "section": "4.1 Medidas-resumo",
    "text": "4.1 Medidas-resumo\nUma medida-resumo é uma construção matemático/estatística que tenta capturar em um único número um comportamento presente nos dados. Quatro grandes grupos de medidas podem ser considerados para resumir variáveis quantitativas, são elas: posição, dispersão, assimetria e curtose. Vejamos agora que medidas são essas e como interpretá-las.\n\n4.1.1 Medidas de posição\nAs medidas de posição, como o nome diz, indicam posições de interesse de valores da variável. Por exemplo, se idade é a variável de interesse investigada em um grupo de pessoas, e se quer trazer um informação resumida dela no grupo, apresentar a menor e a maior idade encontrada, valores típicos da idade no grupo, são exemplos de medidas de posição.\nDe maneira geral, vamos explorar aqui as seguintes medidas posição: valor mínimo, valor máximo, percentis e medidas de tendência central, tais como moda, média e mediana. Os valores mínimo e máximo de uma variável quantitativa estão relacionados, respectivamente, com o menor e o maior valor observado da variável analisada.\nMedidas que buscam descrever um valor típico que a variável apresenta são chamados de medidas de tendência ou posição central. Mas o que seria uma valor típico? Como podemos definir isso? A resposta não é única e, por isso, existem diferentes medidas de tendência central. Por exemplo, se o valor típico considerado for aquele que mais se repete no conjunto de dados para variável, o que temos é a moda. Se o valor típico for aquele que ocupa uma posição central no conjunto de dados, de tal forma que 50% dos dados observados estão abaixo desse valor e os demais 50% estão acima, o que temos é a mediana. Agora, se o valor típico considerado for pensado como um ponto de equilíbrio das observações da variável, então temos a média.\nPor definição, a medida estatística moda corresponde aos(s) valor(es) mais frequente(s) do conjunto de dados observados para uma variável. Conjunto de dados que não apresentam valores repetidos são considerados amodais. Um conjunto de dados é bimodal se tiver duas modas, indicando que não apenas um único valor, mas dois valores do conjunto de dados apresentam frequências igualmente mais altas que os demais valores. Usando de mesmo raciocínio, havendo três ou mais valores modais em um conjunto de dados, dizemos que o conjunto de dados é trimodal ou multimodal, respectivamente. Vale citar que a moda também pode ser obtida para variáveis qualitativas.\nA média é a medida obtida ao somar todos os valores da variável e dividí-la pela quantidade de dados observados. Matematicamente, considere \\(x_1\\), \\(x_2\\), …, \\(x_n\\) as observações de uma variável \\(X\\), assim a média é definida como:\n\\[\\begin{equation}\n\\bar{x} = \\frac{\\sum_{i = 1}^n x_i}{n}.\n\\end{equation}\\]\nPara entender essa medida como ponto de equilíbrio, vamos representar cada valor observado como pesos de mesma massa e distribuí-los sobre uma reta de massa desprezível nas posições referentes aos valores da variável em questão. Nosso objetivo agora é encontrar um ponto de apoio nessa reta de tal forma que ela e os pesos corretamente posicionados nela fiquem perfeitamente equilibrados, similar a uma balança. A média é o único local em que se pode localizar o ponto de apoio na reta de forma a obter um perfeito equilíbrio da reta e dos pesos. Para ilustrar essa ideia, apresentamos a seguir uma representação gráfica considerando o subconjunto da variável idade {22, 28, 29, 34, 34, 35, 36, 36, 37, 39}, cuja média é 33.\n\n\n\n\n\n\n\n(a) Ponto de equilíbrio na média\n\n\n\n\n\n\n\n\n\n(b) Ponto de equilíbrio fora da média\n\n\n\n\nFigura 4.1: Apresentando a média como ponto de equilíbrio\n\n\nNo início dessa seção, apresentamos a noção intuitiva do que representa o valor mediano, mas não como obtê-lo formalmente. A construção dessa medida passa por organizar os dados de maneira crescente e calcular a posição central dos dados via \\(\\frac{n+1}{2}\\), em que \\(n\\) representa o tamanho do conjunto de dados relacionada a variável de interesse. O valor mediano é o valor na amostra ordenada que ocupa a posição \\(\\frac{n+1}{2}\\).\nQuando \\(n\\) é ímpar, a expressão \\(\\frac{n+1}{2}\\) vai sempre gerar um valor inteiro, facilitando a obtenção da mediana. Por exemplo, o subconjunto a seguir é formado por nove valores retirados da variável idade, sendo eles: 21, 28, 24, 22, 31, 26, 22, 38, 16.\nOrdenando esse subconjunto, obtemos 16, 21, 22, 22, 24, 26, 28, 31, 38.\nComo \\(n=9\\), a posição em que se encontra a mediana será \\(\\frac{9+1}{2}=5\\). Assim, a mediana será 24, pois é o valor que está na quinta posição do subconjunto ordenado.\nSe \\(n\\) é par, a expressão \\(\\frac{n+1}{2}\\) gerará um valor não inteiro que apresenta apenas uma única casa decimal após a vírgula igual a 5. Por exemplo, se a variável idade apresenta apenas 8 valores então \\(n = 8\\) e a posição em que a mediana está localizada é dada por \\(\\frac{n+1}{2} = \\frac{8+1}{2} = 4,5\\). Como inferir um valor para a mediana quando a posição que ela ocupa é decimal? Note que a posição \\(4,5\\) está exatamente no meio das posições 4 e 5, então o valor mediano será definido como a média entre os valores que ocupam as posições 4 e 5.\nO subconjunto abaixo também consiste de valores retirados da variável idade, porém note que nesse exemplo há 8 valores, ou seja, \\(n=8\\).\n\\[ 27, 16, 31, 43, 26, 42, 17, 40. \\]\nAo ordenarmos, temos:\n\\[ 16, 17, 26, 27, 31, 40, 42, 43. \\]\nA mediana será o valor que está na posição \\(\\frac{8+1}{2} = 4,5\\). Logo, visto que a mediana está entre os valores que ocupam a quarta e quinta posição, corresponde à média entre esses valores, sendo \\(\\frac{27+31}{2}=29\\).\nCom ideia correlata a mediana, podemos apresentar medidas de posição não centrais, as quais denominamos quantis ou percentis. O percentil 20, por exemplo, é o valor da variável em que 20% das observações no conjunto de dados apresentam valores menores ou iguais a ele. Por consequência, as restantes 80% das observações possuem valores acima do percentil 20. De maneira geral, podemos definir o percentil de ordem \\(p\\) como o valor da variável em que \\(100p\\%\\) \\((0 &lt; p &lt; 1)\\) das observações estão à sua esquerda, ou seja, são menores ou iguais que ele.\nAlguns percentis destacam-se por serem muito utilizados na análise de dados, não só numericamente como graficamente. Esses percentis são conhecido como quartis e basicamente dividem o conjunto de dados em 4 partes de mesmo tamanho. O primeiro quartil (\\(Q_1\\)) é o percentil 25, o segundo quartil (\\(Q_2\\)) é o percentil 50 e o terceiro quartil (\\(Q_3\\)) é o percentil 75. Vale notar que o segundo quartil é a mediana. De posse desses valores, como veremos mais a frente nesse capítulo, iremos construir o gráfico do tipo \\(boxplot\\), bastante utilizado na análise de dados da saúde.\nAinda com respeito aos percentis, outro termo comum na literatura é o decil que refere-se a divisão em 10 partes de mesmo tamanho do conjunto de dados associado a variável analisada. O primeiro decil, por exemplo, é o percentil 10 e o sexto decil é o percentil 60.\nTodas as medidas de posição aqui apresentadas tem em comum terem a mesma unidade de medida dos valores da variável observada, o que traz bastante interpretabilidade.\n\n\n4.1.2 Medidas de dispersão\nPor mais que as medidas de posição apresentadas sejam muito úteis na análise dados, elas por si só não se bastam como medidas resumo das observações de uma variável em um conjunto de dados. É possível construir diferentes conjuntos de dados para uma mesma variável que apresentam os mesmo valores de medida central (média, mediana e moda), mas tem comportamentos absolutamente diferentes. Por exemplo, veja a figura a seguir.\n\n\n\n\n\n\n\n(a) Dados: 2 ,3, 5 , 5, 7, 8.\n\n\n\n\n\n\n\n(b) Dados: 5,5,5,5.\n\n\n\n\nFigura 4.2: Exemplos de conjuntos de dados com mesma média, moda e mediana.\n\n\nOs dois conjuntos de dados apresentam os mesmos valores de média, mediana e moda. O que diferencia os dois conjuntos? O quão diferentes ou parecidos são as observações entre si em cada conjunto da variável. Na Figura Figura 4.2 (b), notamos que os quatro valores observados são iguais entre si e que, portanto, as observações nesse conjunto não variam, diferente do que ocorre para os dados que geraram o gráfico da Figura Figura 4.2 (a). Medidas de dispersão ou variabilidade são as medidas responsáveis por quantificar o quão diferente são os dados entre si. De forma bastante intuitiva temos que se os dados observados da variável não variam, então a medida de dispersão dela é zero e, caso haja diferenças entre os valores observados, então essa medida vai ser um valor positivo. Quanto maior a medida de variabilidade, mais diferente são os dados observados da variável entre si.\nNão existe uma única medida de dispersão na literatura, aqui vamos considerar as seguintes medidas: amplitude, intervalo interquartil, variância, desvio padrão e coeficiente de variação.\nDe fácil obtenção e interpretabilidade, a amplitude é a diferença entre o valor máximo e o valor mínimo da variável analisada no conjunto de dados e nos dá uma ideia do intervalo de variação dos dados. Uma desvantagem é que essa medida é absolutamente influenciada pela presença de valores discrepantes ou \\(outliers\\). O intervalo interquartil é uma medida mais robusta do que a amplitude intervalar e é calculada como a diferença entre o terceiro e o primeiro quartil, ou seja, é a amplitude entre os 50% dos dados centrais.\nPor mais informativas que sejam as medidas de amplitude e intervalo interquartil, queremos uma medida de dispersão que não considere apenas dois valores da amostra (mínimo e máximo ou primeiro e terceiro quartis) e sim todos os dados. Uma medida bastante intuitiva seria considerar a soma dos desvios de cada uma das observações em torno da média. Mas aí temos um problema: a soma dos desvios da média é sempre zero! Isso acontece porque sempre há desvios positivos e negativos que quando somados se anulam. Uma solução para essa questão é considerar alguma função que considere apenas o valor do desvio e não o seu sinal. Uma função candidata é a função quadrática (lembre que, por exemplo, \\((−2)^2=4\\)). Nessa construção surge a variância: soma dos desvios quadrados dividida pelo total de observações (\\(n\\)), ou seja, a média dos desvios quadrados. Assim, a variância quantifica o quanto os dados estão dispersos da média, em média.\nMatematicamente, considere \\(x_1\\), \\(x_2\\), …, \\(x_n\\) as observações de uma variável \\(X\\) e \\(\\bar{x}\\) a média observada dessa variável. A variância seria calculada como:\n\\[\n\\mbox{Var(X)} = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^2}{n}.\n\\tag{4.1}\\]\nPor mais intuitiva que seja essa construção, programas como o R e similares utilizam em sua análise uma versão modificada do cálculo da variância acima apresentado, em que a soma dos desvios quadrados é dividida por \\(n-1\\), não por \\(n\\). Justificativas para isso se devem a propriedades inferenciais. A maioria dos conjuntos de dados considerados nos estudos referem-se a análise de amostras de uma população e não a análise de todos os elementos de uma população. Ao mesmo tempo, um dos principais objetivos da análise estatística é fazer análises para a população e não apenas para a amostra considerada no estudo. Basicamente, se temos interesse de conhecer o valor médio de uma variável na população (\\(\\mu\\)), na impossibilidade de analisar todos os elementos dela e obter a medida, o fazemos de forma aproximada investigando o valor médio dessa variável na amostra (\\(\\bar{x}\\)). Esse mesmo raciocínio ocorre para a variância, na impossibilidade de obter a variância da variável para todos os elementos da população (\\(\\sigma^2\\)), analisamos essa medida via amostra, o caso é que é possível mostrar que para amostras de tamanho pequeno, a variância apresentada em (Equação 4.1) não aproxima-se bem do valor de \\(\\sigma^2\\). Matematicamente, é possível mostrar que tal dificuldade é contornada fazendo uso do divisor igual a \\(n-1\\) em (Equação 4.1). Na literatura esse cálculo muitas vezes é denominado como variância amostral e representado pelo símbolo \\(S^2\\) de tal forma que\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^2}{n-1}.\n\\tag{4.2}\\]\nVale ressaltar também que a medida que se considera tamanhos de amostra maiores, calcular a variância com divisor \\(n\\) ou \\(n-1\\) torna-se indiferente.\nComo a unidade de medida da variância é o quadrado da unidade de medida da variável correspondente, convém definir outra medida de dispersão que mantenha a unidade de medida original. Uma medida com essa propriedade é a raiz quadrada da variância, conhecida por desvio padrão.\nCaso o interesse seja calcular e comparar a dispersão entre variáveis com unidades dimensionais de natureza diferente, por exemplo, comprimento (em metros) e massa (em kg), não convém utilizar as medidas de dispersão apresentadas anteriormente pois todas as medidas apresentadas carregam consigo a unidade de medida considerada para a variável. Nesse caso, podemos fazer uso do coeficiente de variação (CV) para cada uma das variáveis analisadas, já que o CV é uma medida de dispersão relativa adimensional, calculada via razão entre o desvio-padrão e a média observada para a variável e quanto maior o seu valor, maior a dispersão dos dados em termos relativos a média.\n\n\n4.1.3 Medidas de assimetria e curtose\nAlém das medidas de posição e variabilidade, existe um conjunto de medidas dedicadas a explorar a forma da distribuição de frequências dos dados. Especificamente aqui estudaremos algumas: coeficientes de assimetria e de curtose e variações destas.\nComo boa parte dos estudos na área de saúde é realizado através de amostras da variável de interesse na população, vamos precisar definir os momentos amostrais centrais que serão ferramenta fundamental para a construção dos coeficientes de assimetria, curtose e seus derivados. Por definição, o momento amostral centrado (na média) de ordem \\(r\\) é dado por\n\\[\nm_r = \\frac{\\sum_{i = 1}^n (x_i - \\bar{x})^r}{n}, \\; r = 1, 2, \\cdots.\n\\]\n\nA versão populacional do momento centrado de ordem \\(r\\) é expressa por \\(\\mu_r = \\frac{\\sum_{i = 1}^n (x_i - \\mu)^r}{N}\\), em que \\(r = 1, 2, \\cdots \\;\\) e \\(\\mu\\) e \\(N\\) referem-se, respectivamente, a média da variável de interesse e a quantidade de elementos investigados na população.\n\nDessa forma, o coeficiente de assimetria amostral é dado por \\(\\frac{m_3}{m_2^{3/2}}\\). Populações cuja a distribuição da variável é simétrica apresentam coeficiente de assimetria igual a zero. Distribuições assimétricas à direita apresentam valores positivos de coeficiente de assimetria para a variável analisada populacionalmente, assim como distribuições assimétricas à esquerda apresentam coeficiente de assimetria negativo.\n\nAnalogamente, \\(\\frac{\\mu_3}{\\mu_2^{3/2}}\\) é o coeficiente de correlação populacional.\nPopulações cuja a distribuição da variável é simétrica apresentam coeficiente de assimetria igual a zero. Distribuições assimétricas à direita apresentam valores positivos de coeficiente de assimetria para a variável analisada populacionalmente, assim como distribuições assimétricas à esquerda apresentam coeficiente de assimetria negativo.\n\n\n\n\n\n\n\n\n(a) Simétrico\n\n\n\n\n\n\n\n(b) Simétrico\n\n\n\n\n\n\n\n\n\n(c) Assimétrico à esquerda\n\n\n\n\n\n\n\n(d) Assimétrico à direita\n\n\n\n\nFigura 4.3: Histogramas e funções de densidade\n\n\nCoeficientes de assimetria amostrais diferentes de zero devem ser interpretados com cautela, uma vez que por se tratar de uma amostra não significa que necessariamente o comportamento da variável na população seja assimétrico. Testes estatísticos devem ser realizados para avaliar a hipotese de simetria da variável na população.\nAinda com respeito a forma da distribuição da variável, podemos avaliar o comportamento em suas caudas através do coeficiente de curtose amostral que se define por \\(\\frac{m_4}{m_2^{2}}\\). Distribuições de variáveis com valor de curtose igual a 3 são denominadas mesocúrticas. Tomada muitas vezes como referência, a distribuição normal apresenta coeficiente de curtose igual a três. Distribuições com coeficiente de curtose menores que 3 são denominadas platicúrticas e apresentam caudas mais leves (“finas”) do que a da distribuição normal. Distribuições com coeficiente de curtose maiores que 3 são denominadas leptocúrticas e apresentam caudas mais pesadas (“grossas”) do que a da distribuição normal. A distribuição t-Student é um exemplo de distribuição leptocúrtica.\nNa literatura é muito comum ser apresentado uma variante do coeficiente de curtose denominada excesso de curtose. Esse excesso é avaliado em relação a curtose do modelo normal por isso seu valor é calculado fazendo o coeficiente de curtose subtraído de 3. Dessa forma, o excesso de curtose em distribuições mesocúrticas é igual a zero, em distribuições leptocúrticas é maior que zero e em distribuições platicúrticas é menor que zero.\n\n\n\nFigura 4.4: Exemplo de funções de densidade com diferentes medidas de curtose.\n\n\nRessalva similar feita ao coeficiente de assimetria deve ser considerado para o coeficiente de curtose ou de excesso de curtose. Um coeficiente de curtose amostral diferente de três ou, de forma equivalente, com excesso de curtose amostral diferente de zero, não implica necessariamente que a distribuição da variável na população possui caudas mais leves ou mais pesadas do que a da distribuição normal. Para que se possa fazer tal afirmação é necessária a realização de testes estatísticos inferenciais. Os coeficientes amostrais de assimetria e curtose tão somente nos dão uma medida da forma da distribuição de frequências e \\(insights\\) do comportamento da variável na população, que devem ser verificados via análise inferencial estatística.\nNo R, para obter essas medidas resumo vamos utilizar a função descr também do pacote summarytools. No comando abaixo pedimos ao R as medidas descritivas da variável quantitativa “idade”.\n\ndescr(dados$idade)\n\nDescriptive Statistics  \ndados$idade  \nN: 11523  \n\n                       idade\n----------------- ----------\n             Mean      30.25\n          Std.Dev       7.04\n              Min      10.00\n               Q1      25.00\n           Median      30.00\n               Q3      35.00\n              Max      55.00\n              MAD       7.41\n              IQR      10.00\n               CV       0.23\n         Skewness       0.17\n      SE.Skewness       0.02\n         Kurtosis      -0.10\n          N.Valid   11514.00\n        Pct.Valid      99.92\n\n\nNote que os nomes de algumas das estatísticas apresentadas pela função descr estão em inglês. \\(Mean\\) refere-se ao valor médio da variável analisada, \\(Std.Dev\\) corresponde ao desvio-padrão, IQR é o símbolo para o intervalo interquartil, MAD é o desvio-médio absoluto, \\(Skewness\\) é o coeficiente de assimetria, \\(SE.Skewness\\) é o erro-padrão do coeficiente de assimetria, \\(Kurtosis\\) é o coeficiente de excesso de curtose e \\(N.Valid\\) e \\(Pct.Valid\\) correspondem, respectivamente, ao número de observações válidas e seu percentual no conjunto de dados considerado para a variável.\nEspecificamente para a variável idade, podemos notar que das 11523 observações, apenas 11514 (99.92 %) foram consideradas válidas. Isso acontece porque nesse conjunto de dados, 9 pessoas não declararam a idade, ficando com a casela vazia (NA). Todas as medidas-resumo foram calculadas considerando apenas as observações válidas. Sendo assim, algumas análises que podem ser realizadas para a variável idade através da função descr são que a idade média dentre as observações válidas foi de 30,25 anos, com desvio-padrão de 7,04 anos. A menor idade observada foi de 10 anos e a máxima foi de 55 anos. 50% das mulheres analisadas tem idade inferior a 30 anos (mediana) e 25% delas tem idade superior a 35 anos (Q3). O coeficiente de assimetria foi 0.17 (com erro-padrão de 0.02), indicando que a distribuição de frequências da variável idade é levemente assimétrica à direita. O coeficiente de excesso de curtose foi -0.10 e o intervalo interquartil (Q3 - Q1) foi 10.\nSe quiser que a tabela apresente apenas algumas medidas-resumo pré-selecionadas, podemos informar ao R por meio do argumento stats. Ainda, se quisermos que na tabela as medidas resumo fiquem na coluna, usamos o argumento transpose = TRUE, como segue:\n\ndescr(dados$idade,stats = c(\"min\", \"mean\", \"med\",\"sd\",\"max\"), transpose = TRUE) #sd é o desvio padrão (standard deviation)\n\nDescriptive Statistics  \ndados$idade  \nN: 11523  \n\n                Min    Mean   Median   Std.Dev     Max\n----------- ------- ------- -------- --------- -------\n      idade   10.00   30.25    30.00      7.04   55.00\n\n\nCaso se tenha interesse em apresentar a natureza da variável e um \\(preview\\) gráfico da distribuição de frequências, podemos fazer uso da função dfSummary. O argumento method = \"render\" para a função print permite uma melhor apresentação visual dos gráficos em documentos do tipo R \\(Markdown\\).\nprint(dfSummary(dados$idade), method = \"render\")\n\n\nData Frame Summary\ndados\nDimensions: 11523 x 1\n  Duplicates: 11476\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n\n1\nidade [numeric]\n\n\n\nMean (sd) : 30.2 (7)\n\n\nmin ≤ med ≤ max:\n\n\n10 ≤ 30 ≤ 55\n\n\nIQR (CV) : 10 (0.2)\n\n\n\n46 distinct values\n\n11514 (99.9%)\n9 (0.1%)\n\n\n\n\nGenerated by summarytools 1.0.1 (R version 4.3.0)2023-06-18\n\n\nAlém do gráfico, uma informação adicional apresentada é que as 11514 observações válidas da variável idade estão distribuídas em 46 distintos valores.\nOutro pacote bastante interessante para medidas descritivas é o modelsummary. Destacamos algumas funções desse pacote:\n\ndatasummary_skim: retorna as medidas descritivas das variáveis do banco de dados a depender do tipo identificado no argumento type= (categorical ou numeric);\ndatasummary: retorna as medidas descritivas das variáveis a depender de como monta os argumentos da função, permitindo retornar as medidas descritivas das variáveis quantitativas de interesse por categorias de outra(s) variável(is).\n\nPara explorar a funcionalidade desse pacote e suas funções, vamos filtrar o banco de dados original considerando apenas as informações das mulheres gestantes e puérperas internadas em UTI.\n\ndados_uti &lt;- dados[!is.na(dados$dias_uti),]\n\nVamos selecionar algumas variáveis do banco de dados dados_uti e organizá-las em um novo \\(data.frame\\).\nlibrary(dplyr)\ndados_uti_res &lt;- select(dados_uti,idade,cardiopati, faixa_et, evolucao, dias_uti)\nPara esses dados, vamos fazer algumas análises via pacote modelsummary. Assim,\nlibrary(modelsummary)\nAo usar a função datasummary_skim, vamos obter as medidas descritivas das variáveis quantitativas (argumento type = \"numeric\") e das variáveis qualitativas (argumento type = \"categorical\"), respectivamente:\ndatasummary_skim(dados_uti_res,\n  type = \"numeric\",\n  histogram = FALSE)\n\n\n\n\n\nUnique (#)\nMissing (%)\nMean\nSD\nMin\nMedian\nMax\n\n\n\n\nidade\n42\n0\n31.2\n6.6\n10.0\n31.0\n55.0\n\n\ndias_uti\n75\n0\n12.0\n13.7\n0.0\n8.0\n200.0\n\n\n\n\n\n\ndatasummary_skim(dados_uti_res,\n  type = \"categorical\", na.rm = FALSE)\n\n\n\n\n\n\nN\n%\n\n\n\n\ncardiopati\nsim\n182\n7.8\n\n\n\nnao\n791\n34.0\n\n\n\nignorado\n17\n0.7\n\n\nfaixa_et\n&lt;20\n98\n4.2\n\n\n\n&gt;=34\n908\n39.1\n\n\n\n20-34\n1317\n56.7\n\n\n\nNA\n1\n0.0\n\n\nevolucao\ncura\n1645\n70.8\n\n\n\nobito\n623\n26.8\n\n\n\nobito por outras causas\n7\n0.3\n\n\n\nignorado\n31\n1.3\n\n\n\n\n\n\nComo a variável faixa etária (faixa_et) foi declarada como fator, a função datasummary_skim apresenta 1 valor NA, indicando que apenas uma mulher que esteve em UTI não teve determinada sua faixa etária/idade. Para que as demais variáveis categóricas apresentem essa informação e não as deixe omitida, como no caso da variável cardiopatia, vamos precisar declarar essas variáveis como caracter e não como fator. Esse procedimento também será adotado para as demais variáveis categóricas.\ndados_uti_res$cardiopati &lt;- as.character(dados_uti_res$cardiopati)\ndados_uti_res$evolucao &lt;- as.character(dados_uti_res$evolucao)\n\ndatasummary_skim(dados_uti_res,\n  type = \"categorical\", na.rm = FALSE)\n\n\n\n\n\n\nN\n%\n\n\n\n\ncardiopati\nignorado\n17\n0.7\n\n\n\nnao\n791\n34.0\n\n\n\nsim\n182\n7.8\n\n\n\nNA\n1334\n57.4\n\n\nfaixa_et\n&lt;20\n98\n4.2\n\n\n\n&gt;=34\n908\n39.1\n\n\n\n20-34\n1317\n56.7\n\n\n\nNA\n1\n0.0\n\n\nevolucao\ncura\n1645\n70.8\n\n\n\nignorado\n31\n1.3\n\n\n\nobito\n623\n26.8\n\n\n\nobito por outras causas\n7\n0.3\n\n\n\nNA\n18\n0.8\n\n\n\n\n\n\nUma das funções mais interessantes do pacote modelsummaryé a datasummary, pois ela nos permite analisar variáveis quantitativas separada pelas categorias (grupos) de uma variável qualitativa. Por exemplo, suponha que tenhamos interesse em analisar o tempo de internação em UTI, estratificado pelos grupos faixa-etária e evolução do caso, fazendo uso das seguintes medidas descritivas: média, mediana, desvio padrão, mínimo, máximo e tamanho da amostra válido (sem considerar observações faltantes para a variável em questão). O primeiro passo é declarar as medidas-resumo de interesse como funções. O argumento na.rm = TRUE indica que o cálculo da função deve ser realizado excluindo os valores faltantes da variável.\nmedia &lt;- function(x)   mean(x, na.rm = TRUE)\nmediana &lt;- function(x) median(x, na.rm = TRUE)\ndp &lt;- function(x) sd(x, na.rm = TRUE)\nminimo &lt;- function(x) min(x, na.rm = TRUE)\nmaximo &lt;- function(x) max(x, na.rm = TRUE)\nn &lt;- function(x) sum(!is.na(x))\ndatasummary( (evolucao + faixa_et) ~\n              dias_uti*(n+media+dp+minimo+mediana+maximo), data = dados_uti_res)\n\n\n\n\n\n\nn\nmedia\ndp\nminimo\nmediana\nmaximo\n\n\n\n\nevolucao\ncura\n1645.00\n10.78\n12.01\n0.00\n6.00\n107.00\n\n\n\nignorado\n31.00\n7.94\n10.19\n0.00\n4.00\n40.00\n\n\n\nobito\n623.00\n15.18\n15.46\n0.00\n12.00\n200.00\n\n\n\nobito por outras causas\n7.00\n51.71\n64.62\n2.00\n25.00\n183.00\n\n\nfaixa_et\n&lt;20\n98.00\n10.55\n11.07\n0.00\n6.00\n58.00\n\n\n\n&gt;=34\n908.00\n12.91\n14.66\n0.00\n8.00\n200.00\n\n\n\n20-34\n1317.00\n11.55\n13.15\n0.00\n8.00\n183.00\n\n\n\n\n\n\nAgora veja como fica se eu considerar as medidas descritivas de mais de uma variável quantitativas por duas variáveis qualitativas, selecionando apenas as medidas descritivas, média, desvio-padrão e número de casos observados:\ndatasummary((evolucao + cardiopati)  ~\n              (dias_uti + idade)*(n+media+dp), data = dados_uti_res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndias_uti\n\n\nidade\n\n\n\n\n\nn\nmedia\ndp\nn\nmedia\ndp\n\n\n\n\nevolucao\ncura\n1645.00\n10.78\n12.01\n1645.00\n31.05\n6.57\n\n\n\nignorado\n31.00\n7.94\n10.19\n31.00\n31.29\n7.66\n\n\n\nobito\n623.00\n15.18\n15.46\n622.00\n31.61\n6.70\n\n\n\nobito por outras causas\n7.00\n51.71\n64.62\n7.00\n32.14\n7.65\n\n\ncardiopati\nignorado\n17.00\n8.76\n9.62\n17.00\n34.00\n6.20\n\n\n\nnao\n791.00\n12.74\n13.30\n791.00\n30.96\n6.82\n\n\n\nsim\n182.00\n14.12\n14.28\n182.00\n33.66\n6.86"
  },
  {
    "objectID": "descritiva.html#tabelas-cruzadas---duas-variáveis-qualitativas",
    "href": "descritiva.html#tabelas-cruzadas---duas-variáveis-qualitativas",
    "title": "4  Análise exploratória dos dados",
    "section": "4.2 Tabelas cruzadas - duas variáveis qualitativas",
    "text": "4.2 Tabelas cruzadas - duas variáveis qualitativas\nTabelas cruzadas ou tabelas de contingência são tabelas que apresentam frequências de duas ou mais variáveis qualitativas conjuntamente.\nNo R, para obter tabelas cruzadas, vamos utilizar a função ´ctable´ também do pacote ´summarytools´. No comando abaixo, pedimos ao R uma tabela cruzada entre as variáveis qualitativas evolução (evolucao) e faixa etária (faixa_et) no banco de dados otiginal.\nctable(dados$evolucao,y=dados$obesidade,prop=\"t\")\nCross-Tabulation, Total Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 (4.82%)\n2897 (25.1%)\n95 (0.82%)\n5943 (51.58%)\n9490 ( 82.4%)\n\n\nobito\n\n199 (1.73%)\n446 ( 3.9%)\n14 (0.12%)\n587 ( 5.09%)\n1246 ( 10.8%)\n\n\nobito por outras causas\n\n3 (0.03%)\n13 ( 0.1%)\n0 (0.00%)\n6 ( 0.05%)\n22 ( 0.2%)\n\n\nignorado\n\n10 (0.09%)\n71 ( 0.6%)\n0 (0.00%)\n206 ( 1.79%)\n287 ( 2.5%)\n\n\n\n\n23 (0.20%)\n129 ( 1.1%)\n4 (0.03%)\n322 ( 2.79%)\n478 ( 4.1%)\n\n\nTotal\n\n790 (6.86%)\n3556 (30.9%)\n113 (0.98%)\n7064 (61.30%)\n11523 (100.0%)\n\n\n\nO argumento prop indica a forma como deve ser calculada a proporção. Por padrão, a proporção é sempre calculada tendo-se como referencial o total em linha, ou seja, prop = \"r\". Outras opções são prop = \"t\", indicando que a proporção é em relação ao número total de observações e prop = \"c\" se o referencial for o total por coluna.\nctable(dados$evolucao,y=dados$obesidade,prop=\"r\")\nCross-Tabulation, Row Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 ( 5.8%)\n2897 (30.5%)\n95 (1.0%)\n5943 (62.6%)\n9490 (100.0%)\n\n\nobito\n\n199 (16.0%)\n446 (35.8%)\n14 (1.1%)\n587 (47.1%)\n1246 (100.0%)\n\n\nobito por outras causas\n\n3 (13.6%)\n13 (59.1%)\n0 (0.0%)\n6 (27.3%)\n22 (100.0%)\n\n\nignorado\n\n10 ( 3.5%)\n71 (24.7%)\n0 (0.0%)\n206 (71.8%)\n287 (100.0%)\n\n\n\n\n23 ( 4.8%)\n129 (27.0%)\n4 (0.8%)\n322 (67.4%)\n478 (100.0%)\n\n\nTotal\n\n790 ( 6.9%)\n3556 (30.9%)\n113 (1.0%)\n7064 (61.3%)\n11523 (100.0%)\n\n\n\nctable(dados$evolucao,y=dados$obesidade,prop=\"c\")\nCross-Tabulation, Column Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555 ( 70.3%)\n2897 ( 81.5%)\n95 ( 84.1%)\n5943 ( 84.13%)\n9490 ( 82.4%)\n\n\nobito\n\n199 ( 25.2%)\n446 ( 12.5%)\n14 ( 12.4%)\n587 ( 8.31%)\n1246 ( 10.8%)\n\n\nobito por outras causas\n\n3 ( 0.4%)\n13 ( 0.4%)\n0 ( 0.0%)\n6 ( 0.08%)\n22 ( 0.2%)\n\n\nignorado\n\n10 ( 1.3%)\n71 ( 2.0%)\n0 ( 0.0%)\n206 ( 2.92%)\n287 ( 2.5%)\n\n\n\n\n23 ( 2.9%)\n129 ( 3.6%)\n4 ( 3.5%)\n322 ( 4.56%)\n478 ( 4.1%)\n\n\nTotal\n\n790 (100.0%)\n3556 (100.0%)\n113 (100.0%)\n7064 (100.00%)\n11523 (100.0%)\n\n\n\nNote que em todas as tabelas de contingência há a existência de linha e coluna sem nome, isso acontece pois esta linha e/ou coluna está resumindo os valores faltantes (NA). Por exemplo, a distribuição de frequências da variável evolução para os que não preencheram o \\(status\\) de obesidade nos diz que 5943 (84.13%) foram curados, 587 (8.31%) faleceram, 6 (0.08%) viram a óbito por motivos outros que não COVID-19, 206 (2.92%) ignoraram essa informação (preencheram com 9) e 322 (4.56%) deixaram em branco não só a informação da obesidade, mas também o desfecho final da evolução. Para obter a tabela de contingência apenas dos casos válidos simultâneos em ambas as variáveis, insira o argumento useNA = \"no\"\".\nctable(dados$evolucao,y=dados$obesidade, prop=\"c\", useNA = \"no\")\nCross-Tabulation, Column Proportions\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\ncura\n\n555 ( 72.4%)\n2897 ( 84.5%)\n95 ( 87.2%)\n3547 ( 82.4%)\n\n\nobito\n\n199 ( 25.9%)\n446 ( 13.0%)\n14 ( 12.8%)\n659 ( 15.3%)\n\n\nobito por outras causas\n\n3 ( 0.4%)\n13 ( 0.4%)\n0 ( 0.0%)\n16 ( 0.4%)\n\n\nignorado\n\n10 ( 1.3%)\n71 ( 2.1%)\n0 ( 0.0%)\n81 ( 1.9%)\n\n\nTotal\n\n767 (100.0%)\n3427 (100.0%)\n109 (100.0%)\n4303 (100.0%)\n\n\n\nCaso não haja interesse em se apresentar as proporções, basta considerar o argumento prop=\"none\", da seguinte forma:\nctable(dados$evolucao,y=dados$obesidade,prop=\"none\")\nCross-Tabulation\nevolucao * obesidade\nData Frame: dados\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n555\n2897\n95\n5943\n9490\n\n\nobito\n\n199\n446\n14\n587\n1246\n\n\nobito por outras causas\n\n3\n13\n0\n6\n22\n\n\nignorado\n\n10\n71\n0\n206\n287\n\n\n\n\n23\n129\n4\n322\n478\n\n\nTotal\n\n790\n3556\n113\n7064\n11523\n\n\n\nPara tabelas de contingência com mais de duas variáveis, podemos adotar o seguinte procedimento:\nwith(dados, stby(data = list(x = evolucao, y = obesidade), \n                   INDICES = faixa_et, FUN = ctable))\nCross-Tabulation, Row Proportions\nevolucao * obesidade\nData Frame: dados\nGroup: faixa_et = &lt;20\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n12 (2.0%)\n214 (35.0%)\n2 (0.3%)\n383 (62.7%)\n611 (100.0%)\n\n\nobito\n\n3 (6.5%)\n22 (47.8%)\n0 (0.0%)\n21 (45.7%)\n46 (100.0%)\n\n\nobito por outras causas\n\n0 (0.0%)\n2 (66.7%)\n0 (0.0%)\n1 (33.3%)\n3 (100.0%)\n\n\nignorado\n\n0 (0.0%)\n7 (20.0%)\n0 (0.0%)\n28 (80.0%)\n35 (100.0%)\n\n\n\n\n0 (0.0%)\n2 (10.5%)\n0 (0.0%)\n17 (89.5%)\n19 (100.0%)\n\n\nTotal\n\n15 (2.1%)\n247 (34.6%)\n2 (0.3%)\n450 (63.0%)\n714 (100.0%)\n\n\n\nGroup: faixa_et = &gt;=34\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n231 ( 7.5%)\n972 (31.5%)\n37 (1.2%)\n1842 (59.8%)\n3082 (100.0%)\n\n\nobito\n\n73 (13.9%)\n182 (34.7%)\n6 (1.1%)\n263 (50.2%)\n524 (100.0%)\n\n\nobito por outras causas\n\n1 (12.5%)\n6 (75.0%)\n0 (0.0%)\n1 (12.5%)\n8 (100.0%)\n\n\nignorado\n\n1 ( 1.2%)\n22 (25.9%)\n0 (0.0%)\n62 (72.9%)\n85 (100.0%)\n\n\n\n\n12 ( 7.4%)\n44 (27.0%)\n0 (0.0%)\n107 (65.6%)\n163 (100.0%)\n\n\nTotal\n\n318 ( 8.2%)\n1226 (31.7%)\n43 (1.1%)\n2275 (58.9%)\n3862 (100.0%)\n\n\n\nGroup: faixa_et = 20-34\n\n\n\n\nobesidade\nsim\nnao\nignorado\n\nTotal\n\n\nevolucao\n\n\n\n\n\n\n\n\ncura\n\n310 ( 5.4%)\n1710 (29.5%)\n56 (1.0%)\n3715 (64.2%)\n5791 (100.0%)\n\n\nobito\n\n123 (18.2%)\n242 (35.9%)\n8 (1.2%)\n302 (44.7%)\n675 (100.0%)\n\n\nobito por outras causas\n\n2 (18.2%)\n5 (45.5%)\n0 (0.0%)\n4 (36.4%)\n11 (100.0%)\n\n\nignorado\n\n9 ( 5.4%)\n42 (25.1%)\n0 (0.0%)\n116 (69.5%)\n167 (100.0%)\n\n\n\n\n11 ( 3.7%)\n82 (27.9%)\n4 (1.4%)\n197 (67.0%)\n294 (100.0%)\n\n\nTotal\n\n455 ( 6.6%)\n2081 (30.0%)\n68 (1.0%)\n4334 (62.5%)\n6938 (100.0%)"
  },
  {
    "objectID": "descritiva.html#gráficos",
    "href": "descritiva.html#gráficos",
    "title": "4  Análise exploratória dos dados",
    "section": "4.3 Gráficos",
    "text": "4.3 Gráficos\nUm gráfico pode ser a maneira mais adequada para resumir e apresentar um conjunto de dados. Tem a vantagem de facilitar a compreensão de uma determinada situação que queira ser descrita, permitindo uma interpretação rápida e visual das suas principais características.\nA visualização dos dados é uma etapa importantíssima da análise estatística, pois é também a partir dela que criamos a intuição necessária para escolher o teste ou modelo mais adequado para o nosso problema.\n\n4.3.1 Pacote ggplot2\nUm pacote maravilhoso para gráficos no R é o ggplot2. A ideia por trás desse pacote é um gráfico pode ser entendido como um mapeamento dos dados a partir de atributos estéticos (cores, formas, tamanho) de formas geométricas (pontos, linhas, barras).\nlibrary(ggplot2)\n\n4.3.1.1 Atributos estéticos\nA função aes descreve como as variáveis são mapeadas em aspectos visuais. Para isso, vamos precisar indicar qual variável será representada no eixo x, qual será representada no eixo y, a cor e o tamanho dos componentes geométricos, etc. de formas geométricas a serem pré-definidas pelos geoms. A escolha da forma geométrica vai depender da natureza das variáveis a serem analisadas e será discutido na sequencia. Além disso, os aspectos que podem ou devem ser mapeados vão depender do tipo de gráfico que estamos querendo construir. Basicamente, no pacote ´ggplot2´ temos as seguintes formas geométricas:\n\ngeom_point() gera gráficos de dispersão transformando pares (x,y) em pontos.\ngeom_line: para retas definidas por pares (x,y);\ngeom_abline: para retas definidas por um intercepto e uma inclinação;\ngeom_hline: para retas horizontais;\ngeom_bar: para barras;\ngeom_histogram: para histogramas;\ngeom_boxplot: para boxplots;\ngeom_density: para densidades;\ngeom_area: para áreas.\n\nPara cada uma das formas geométricas podemos estabelecer aspectos visuais que podem melhorar a visualização dos dados. Aspectos visuais mais utilizados:\n\ncolor: altera a cor de formas que não têm área (pontos e retas);\nfill: altera a cor de formas com área (barras, caixas, densidades, áreas);\nsize: altera o tamanho de formas;\ntype: altera o tipo da forma, geralmente usada para pontos;\nlinetype: altera o tipo da linha.\n\nPara exemplificar os diferentes tipos de gráficos associando-os a variáveis de diferentes naturezas, vamos considerar o banco de dados de COVID-19 em gestantes e puérperas. De maneira geral, é necessário seguir alguns passos gerais para a construção de qualquer gráfico via ggplot2, a saber:\n\nPasso 1: sempre iniciar a construção chamando a função ggplot.\nPasso 2: especificar na função ggplot o objeto que acomoda o banco de dados e apresenta a variável de interesse para a qual se quer fazer o gráfico. Esse objeto deve ser do tipo \\(dataframe\\).\nPasso 3: informar as variáveis a serem consideradas no eixo horizontal e vertical via função aes e demais funções estéticas dependentes das variáveis.\n\nPasso 4: informar o tipo de gráfico que se quer fazer (barra, histograma, \\(boxplot\\), etc).\n\n\n\n4.3.1.2 Gráficos para variáveis qualitativas e quantitativas discretas com poucos valores diferentes\nUm dos gráficos mais utilizados para a apresentação visual de variáveis qualitativas e quantitativas discretas com poucas observações diferentes é o gráfico de barras. Para construí-lo, é necessário utilizar no Passo 4 a função geom_bar.\nA seguir apresentamos um exemplo de gráfico de barras para a variável qualitativa evolução dos casos relacionado a gestantes e puérperas hospitalizadas por COVID-19.\nggplot(dados, aes(x = evolucao)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\")\n\nNote que a função labs é responsável não só por inserir os títulos nos eixos, como poder visto no gráfico anterior, mas também títulos e subtítulos. Essa função pode ser sempre utilizada na construção de gráficos via pacote ggplot2, independente do tipo de gráfico a ser apresentado. Por exemplo,\nggplot(dados, aes(x = evolucao)) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nNo código a seguir apresentamos como apresentar as barras organizadas de maneira decrescente. Note que há uma mudança na ordem das barras referente a óbitos e óbitos por outras causas. Além disso, independente da disposição escolhida para as barras, a categoria que representa as mulheres que não tiveram sua evolução preenchida na notificação (NA) sempre é apresentada como última barra, ainda que tenha uma alta frequência em relação as outras categorias.\nggplot(dados, aes(x = reorder(evolucao, evolucao, function(x)-length(x)))) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nCaso o interesse seja nas barras dispostas de maneira crescente, com exceção do NA, basta reordenar a variável da seguinte forma:\nggplot(dados, aes(x = reorder(evolucao, evolucao, function(x) length(x)))) +\n  geom_bar(fill = \"blue\") +\n  labs(x = \"Tipos de evolução\", y = \"Número de casos\", title = \"Evolução dos casos hospitalizados por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nA variável evolução (evolucao) pode ser classificada como qualitativa nominal e, portanto, estamos livres para escolher a ordem com que as categorias são apresentadas. Vamos ver agora o caso da variável faixa-etária (faixa_et) que, intrinsecamente, é uma variável qualitativa ordinal. Se utilizássemos o mesmo código considerado inicialmente para a variável evolução, obteríamos o seguinte gráfico:\nggplot(dados, aes(x = faixa_et)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Faixa etária\", y = \"Número de casos\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nNote que a barra para as mulheres com pelo menos 34 anos e a barra que representa as mulheres com idade de 20 (incluso) a 34 anos estão em posições trocadas, não respeitando a ordenação natural da variável.\n# Especificando a ordem dos níveis do fator \ndados$faixa_et = factor(dados$faixa_et, levels = c('&lt;20', '20-34', '&gt;=34'))\n\n# Gerando o gráfico\nggplot(dados, aes(x = faixa_et)) +\n  geom_bar(fill = \"purple\") +\n  labs(x = \"Faixa etária\", y = \"Número de casos\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nTodos os gráficos de barra apresentados anteriormente apresentaram no eixo vertical a contagem de indivíduos por categoria. vejamos agora o código para apresentar o gráfico de barras com as frequências relativas.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  labs(x = \"Faixa etária\", y = \"Frequência relativa\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nSe o interesse é apresentar o gráfico em termos percentuais, basta acrescentar a função scale_y_continuous com o argumento labels=scales::percent.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nPara inserir o percentual de cada categoria no gráfico, incluímos no código anterior a função geom_text, com o argumento lab declarando o cálculo da frequência arredondado a duas casas decimais. O argumento vjust indica a que altura se quer que o texto com o percentual apareça no gráfico. Quanto mais negativo for o valor, mais acima da barra estará o texto com o percentual. Se vjust = 0 apresenta o texto encima da barra e, quanto mais positivo for o valor de vjust escolhido, mais interno da barra ficará o texto.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", vjust = -0.1)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nO argumento width da função geom_bar permite controlar a espessura da barra através de valores variando de 0 a 1, sendo que o tamanho 1 representa o maior tamanho.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\", width=0.2) + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", vjust = -0.1)+\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nPara mudar a disposição das barras de vertical para horizontal, basta acrescentar basta inserir a função coord_flip(). Para diminuir o tamanho da letra, ajustamos na função geom_text alterando os valores do argumento size.\nggplot(dados, aes(x = faixa_et, y = (..count..)/sum(..count..))) +  \n  geom_bar(fill=\"purple\") + \n  geom_text(aes(label = round((((..count..)/sum(..count..))*100), 2)), stat= \"count\", hjust = 0, size  = 3) +\n  scale_y_continuous(labels=scales::percent) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")+\n  coord_flip()\n\nVejamos agora como construir um gráfico de barras com duas variáveis qualitativas conjuntamente. Para exemplificar, vamos considerar as variáveis faixa etária (faixa_et) e cardiopatia (cardiopat). Nosso interesse é analisar a distribuição da faixa-etária estratificada pelas categorias da variável cardiopatia que, como vimos antes, podem assumir os resultados “sim”, “não”, “ignorado” e “NA”.\nggplot(dados, aes(x = faixa_et, group = cardiopati)) +  \n  geom_bar(aes(y = ..prop..), stat = \"count\", fill=\"green4\") + \n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\nO próximo código apresenta o gráfico anterior com as porcentagens inseridas na figura.\nggplot(dados, aes(x = faixa_et, group = cardiopati)) +  \n  geom_bar(aes(y = ..prop..), stat = \"count\", fill=\"green4\") + \n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat = \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\nVamos agora colocar cada categoria com a sua própria cor, a fim de facilitar a comparação. Para isso, vamos considerar o argumento fill = factor(..x..) na função geom_bar. Como as categorias da variável faixa-etária são autoexplicativas, o termo theme(legend.position=\"none\") for inserido na construção do gráfico para se evitar a criação de uma nova legenda para as cores relacionadas as faixas de idade, evitando assim repetição de informação.\nggplot(dados, aes(x=faixa_et, group = cardiopati))  + \n  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=\"count\") +\n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat= \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  theme(legend.position=\"none\")+\n  scale_y_continuous(labels=scales::percent) +\n  facet_grid(~cardiopati)\n\nA escala de cores utilizada por padrão pela função geom_bar nem sempre atende a todos os públicos. O pacote viridis do R apresenta escalas de cores projetadas para melhorar a legibilidade dos gráficos para leitores com formas comuns de daltonismo e/ou deficiência em visão de cores. Para usá-lo, devemos inserir a função scale_fill_manual(values = c(viridis(4))), com o valor 4 representando as 4 categorias da faixa etária.\nlibrary(viridis)\n\nggplot(dados, aes(x=faixa_et, group = cardiopati))  + \n  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=\"count\") +\n  geom_text(aes(label = scales::percent(..prop.., accuracy = 0.1), y= ..prop..), stat= \"count\", vjust = -.1) +\n  labs(x = \"Faixa etária\", y = \"Porcentagem\", title = \"Faixa etária das hospitalizadas por COVID-19 pela presença de cardiopatia\", subtitle = \"Mulheres gestantes e puérperas\")+\n  theme(legend.position=\"none\")+\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_manual(values = c(viridis(4))) +\n  facet_grid(~cardiopati)\n\nVários dos gráficos aqui mencionados podem construídos fazendo usos de outras funções e código do pacote do ggplot2. Especificamente sobre o gráfico de barras, na página (http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization#barplot-of-counts) podemos encontrar outras interessantes customizações gráficas que podem ser implementadas via pacote ggplot2.\n\n\n4.3.1.3 Gráficos para variáveis quantitativas\nTodos os códigos apresentados anteriormente podem ser usados quando a variável de interesse é quantitativa discreta com poucos valores diferentes. No caso de haver muitos diferentes valores para a variável quantitativa discreta, gráficos do tipo histograma costumam ser mais informativos. Vamos apresentar agora ferramentas para a construção de gráficos para esse tipo de variável e para variáveis quantitativas contínuas. Todos os gráficos a serem construídos daqui para frente farão uso da paleta viridis. Vejamos a construção do histograma de densidades para a variável idade.\nA função considerada agora é geom_histogram. O argumento y = ..density.. indica que queremos apresentar o histograma de densidades, bins = 15 refere-se ao número de barras contíguas que queremos que o gráfico apresente, ´fill´ designa a cor a preencher o gráfico e color é a cor da linha das barras.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density..), bins = 15, fill = viridis(1), color = \"black\") +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histograma de densidades das idades de hospitalizadas por COVID-19\", subtitle = \"Mulheres gestantes e puérperas\")\n\nEnquanto medida, densidade corresponde a razão entre o número de casos contabilizados em intervalo e a amplitude do intervalo. Uma das vantagens em considerar o histograma de densidades e não o histograma de frequências é que a área total sob o gráfico corresponde a 1, deixando o gráfico na mesma escala de funções de densidade. Por exemplo, na figura a seguir inserimos uma versão alisada empírica do histograma. O argumento alpha na função geom_density controla o nível de transparência das cores preenchidas no histograma alisado e varia de 0 a 1, com 1 representando a cor sólida.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density..), bins = 15, fill = viridis(1), color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histograma de densidades das idades de hospitalizadas por COVID-19 \", subtitle = \"Mulheres gestantes e puérperas\")\n\nNo código a seguir apresentamos uma maneira de construir o histograma de densidades de uma variável quantitativa nas diferentes categorias de uma variável qualitativa. Aqui a variável qualitativa refere-se a informações sobre o \\(status\\) de cardiopatia e a variável quantitativa considerada foi a idade. Para facilitar a comparação das diferentes faixas de valores da variável quantitativa entre as categorias, vamos inserir uma escala inserir uma escala de cores entre as faixas de valores, deixando o gráfico com um aspecto bastante atrativo e informativo. O argumento considerado na função geom_density é fill = ..x.., informando que o preenchimento do histograma será realizado nas faixas de valores da variável quantitativa. Para a escala de cores, inserimos a função scale_fill_gradientn(colours = c(viridis(15))), em que o valor 15 corresponde ao número de barras contíguas (bins = 15) considerado no histograma.\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density.., fill=..x..), bins = 15, color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas de densidades das idades fixado status de cardiopatia \", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  scale_fill_gradientn(colours = c(viridis(15)))+\n  theme(legend.position=\"none\")+\n  facet_grid(~cardiopati)\n\nPara que os histogramas sejam dispostos um abaixo do outro, basta substituir a função facet_grid(~cardiopati) por facet_wrap(~cardiopati, ncol=1).\nggplot(dados, aes(x=idade))  + \n  geom_histogram(aes(y = ..density.., fill=..x..), bins = 15, color = \"black\") +\n  geom_density(fill = \"grey55\", color = \"grey80\", alpha = 0.2) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas de densidades das idades fixado status de cardiopatia \", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  scale_fill_gradientn(colours = c(viridis(15)))+\n  theme(legend.position=\"none\")+\n  facet_wrap(~cardiopati, ncol=1)\n\nCaso se queira apresentar os histogramas alisados sobrepostos por categoria, pode-se usar o código proposto na sequência.\nggplot(dados, aes(x = idade, fill = cardiopati)) +\n  geom_density(alpha = 0.5) +\n  labs(x = \"Idade\", y = \"Densidade\", title = \"Histogramas alisados das idades pelo status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\") +\n  scale_fill_manual(values = c(viridis(3)), name = \"Cardiopatia\")\n\nUm outro gráfico muito utilizado na apresentação de uma variável quantitativa é o boxplot que apresenta, visualmente, os valores mínimo, primeiro quartil (Q1), mediana ou segundo quartil (Q2), terceiro quartil (Q3), máximo e possíveis \\(outliers\\). Baseado nessas medidas, temos uma ideia do comportamento da variável quantitativa em termos de posição, dispersão, assimetria e dados discrepantes. A posição central é dada pela mediana e a dispersão pelo intervalo interquartil. As posições relativas entre Q1, Q2 e Q3 nos dão uma ideia da simetria ou assimetria da distribuição.\n\n\n\n\n\nOs valores \\(Q_1\\), \\(Q_2\\) e \\(Q_3\\) já foram apresentados anteriormente. No gráfico, o segundo quartil (ou seja, a mediana) é representada pela linha que corta a caixa do boxplot. Já o primeiro quartil (\\(Q_1\\)) é a base da caixa e o terceiro quartil (\\(Q_3\\)), o topo. O intervalo interquartil está representado na altura da caixa. A escolha do tamanho da base da caixa é arbitrária, devendo-se tão somente garantir que as linhas \\(Q_1\\), \\(Q_2\\) e \\(Q3\\) estão localizadas na altura em que os valores dos quartis foram obtidos.\nAs linhas em azul são linhas imaginárias (normalmente não aparecem graficadas nos gráficos) e representam valores que distinguem valores \\(outliers\\) dos demais. Valores \\(outliers\\) são valores considerados discrepantes dentro do conjunto de dados de uma variável. Para a representação no boxplot, são considerados \\(outliers\\), observações com valores maiores que o Limite Superior (LS) ou com valores menores que o Limite Inferior (LI), tal que \\(LI = Q_1 - 1.5 (Q_3 - Q_1)\\) e \\(LS = Q_3 + 1.5 (Q_3 - Q_1)\\). Ainda na figura, observamos a presença de um \\(\\mbox{Mínimo}^*\\) e \\(\\mbox{Máximo}^*\\) que nem sempre representam o menor e a maior, respectivamente, observações na amostra. Por definição, \\(\\mbox{Mínimo}^*\\) é o menor valor maior que o Limite Inferior (LI), ou seja, é o menor valor observado no conjunto de dados desconsiderado os valores \\(outliers\\). Analogamente, \\(\\mbox{Máximo}^*\\) é o maior valor menor que o Limite Superior (LS), ou seja, é o maior valor observado no conjunto de dados desconsiderado os valores \\(outliers\\).\nNo pacote ggplot2, esse gráfico é construído com a função geom_boxplot que apresenta argumentos similares aos gráficos de barras e histograma.\nggplot(dados, aes(y=idade))  + \n  geom_boxplot(fill = viridis(1), color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades de hospitalizadas por COVID-19 \", subtitle = \"Mulheres gestantes e puérperas\")\n\nO boxplot, assim como o histograma, também pode ser utilizado para apresentar o comportamento de variáveis quantitativas em função das categorias de variáveis qualitativas.\nggplot(dados, aes(y=idade, x = cardiopati))  + \n  geom_boxplot(fill = viridis(1), color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nPara que cada categoria apresente sua própria cor, basta declarar qual a variável que será considerada para colorir os boxplots em suas categorias via argumento fill em ggplot e indicar quais as cores serão utilizadas. Como a variável cardiopatia (cardiopati) tem 4 categorias, informamos as cores na função geom_boxplot usando o termo fill = viridis(4). Além disso, caso se tenha interesse em destacar as observações \\(outliers\\) com outras cores, pode ser usado o argumento outlier.color na função geom_boxplot. Por exemplo, na figura abaixo, vamos destacar os \\(outliers\\) em vermelho.\nggplot(dados, aes(y=idade, x = cardiopati, fill = cardiopati))  + \n  geom_boxplot(fill = viridis(4), color = \"black\", outlier.color = \"red\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nNo código a seguir, vamos apresentar o boxplot da variável quantitativa idade pela variável qualitativa \\(status\\) de cardiopatia, estratificado nas diferentes categorias de evolução do caso. Para clareza do texto no gráfico, a legenda da variável cardiopatia foi colocada abaixo dele, assim como o texto no eixo horizontal foi disposto de forma inclinada, através da função theme(legend.position=\"bottom\", axis.text.x=element_text(angle=30, hjust=0.8)).\nggplot(dados, aes(y=idade, x = cardiopati, fill = cardiopati))  + \n  geom_boxplot(color = \"black\") +\n  labs(x = \"\", y = \"Idade\", title = \"Boxplot das idades pelo status de cardiopatia, fixada a evolução\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  theme(legend.position=\"bottom\", axis.text.x=element_text(angle=30, hjust=0.8)) +\n  scale_fill_manual(values = c(viridis(4)), name = \"Cardiopatia\")+\n  facet_grid(~evolucao)\n\nNosso objetivo agora é explorar a construção de gráficos quando temos duas variáveis quantitativas através do diagrama de dispersão. Para exemplificar a construção, vamos considerar as variáveis do banco de dados dados_uti_res que contém as variáveis idade e quantidade de dias em uti (dias_uti).\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(colour = viridis(1)) +\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nO pacote ggExtra consegue inserir no gráfico de dispersão gerado pelo ggplot2 histogramas, histogramas alisados e boxplot das variáveis marginais. Para isso, basta utilizar a função ggMarginal informando o nome do objeto em que está guardado o gráfico de dispersão e o tipo de gráfico marginal a ser inserido.\np &lt;- ggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(colour = viridis(1)) +\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nlibrary(ggExtra)\n\n# histograma marginal\nggMarginal(p, type=\"histogram\")\n\n# histograma alisado marginal\nggMarginal(p, type=\"density\")\n\n# Boxplot marginal\nggMarginal(p, type=\"boxplot\")\n\nPodemos construir gráficos de dispersão estratificados por categorias de uma variável qualitativa. No código abaixo, vamos utilizar o \\(status\\) de cardiopatia e, para isso, usamos o argumento color = cardiopati na função geom_point.\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(aes(color = cardiopati)) +\n  scale_colour_viridis_d(\"Cardiopatia\", na.value = \"grey50\")+\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI, fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")\n\nComo pontos de diferentes cores estão sobrepondo, uma visualização possível para facilitar a análise pode ser feita separando os diagramas de dispersão em diferentes planos cartesianos de mesma escala.\nggplot(dados_uti_res, aes(x=idade, y = dias_uti))  + \n  geom_point(aes(color = cardiopati)) +\n  scale_colour_viridis_d(\"Cardiopatia\", na.value = \"grey50\")+\n  labs(x = \"Idade\", y = \"Dias em UTI \", title = \"Gráfico de dispersão da idade pelo tempo em UTI, fixado status de cardiopatia\", subtitle = \"Gestantes e puérperas hospitalizadas por COVID-19\")+\n  facet_wrap(~cardiopati, ncol=1)\n\n\n\n\n4.3.2 Pacote esquisse\nO pacote esquisse disponibiliza um \\(dashboard\\) interativo para criação de gráficos por meio do pacote ggplot2.\nlibrary(esquisse)\nAo rodar a função esquisser(), um janela é aberta (veja Figura 4.5), em que usuário deve escolher a base de dados a trabalhar. Feito isso, uma outra janela será aberta apresentando todas as variáveis presentes no banco de dados escolhido (veja Figura 4.6), permitindo assim fazer os gráficos. Preparamos um tutorial para a utilização do pacote esquisse que pode ser acessado aqui (https://www.youtube.com/watch?XXXXXXXXXXXXX) .\n\n\n\nFigura 4.5: Primeira tela do esquisser.\n\n\n\n\n\nFigura 4.6: Segunda tela do esquisser.\n\n\n\n\n4.3.3 Materiais complementares\nLivros e Artigos:\n\nMercier F, Consalvo N, Frey N, Phipps A, Ribba B. From waterfall plots to spaghetti plots in early oncology clinical development. Pharm Stat. 2019;18(5):526-532. doi:10.1002/pst.1944\nGillespie TW. Understanding waterfall plots. J Adv Pract Oncol. 2012;3(2):106-111.\nSonnad SS. Describing data: statistical and graphical methods. Radiology. 2002;225(3):622-628. doi:10.1148/radiol.2253012154\nIn J, Lee S. Statistical data presentation. Korean J Anesthesiol. 2017;70(3):267-276. doi:10.4097/kjae.2017.70.3.267\nMorettin P, Singer J. Estatística e Ciência de Dados. 1nd ed. LTC; 2022.\n\n\\(Sites\\):\n\nhttps://r-graph-gallery.com/index.html\n\nhttp://www.sthda.com/english/wiki/data-visualization\nhttps://www.cedricscherer.com/2019/08/05/a-ggplot2-tutorial-for-beautiful-plotting-in-r/"
  },
  {
    "objectID": "tabulacao.html#variáveis",
    "href": "tabulacao.html#variáveis",
    "title": "3  Tabulação de dados",
    "section": "3.1 Variáveis",
    "text": "3.1 Variáveis\nOs objetos apresentados, ou variáveis, podem ser denotados como o armazenamento de informações sobre a característica de interesse a respeito de cada unidade amostral, variáveis socioeconômicas como raça, renda e escolaridade são um ótimo exemplo. As variáveis podem ser divididas em dois tipos:\n\nVariáveis Qualitativas: cujos valores podem ser separados por categorias não numéricas. Sendo chamadas de variáveis qualitativas ordinais quando há presença de uma ordenação entre as categorias (Ex.: Escolaridade), e variáveis qualitativas nominais caso contrário (Ex.: Raça, Sexo)\nVariáveis Quantitativas: onde os valores são expressos em números resultantes de uma contagem ou mensuração. Podendo ser quantitativas discretas, quando resultam de um conjunto finito ou enumerável de possíveis valores (Ex.: Número de vitórias ou de filhos), ou ainda variáveis quantitativas continuas quando assumem valores em uma escala continua (Ex.: Peso, Altura).\n\nObserve as 10 unidades amostrais para as variáveis da base de dados COVID-19 para melhor compreensão, onde a idade representa variável quantitativa discreta, a raça represeta qualitativa nominal e a escolaridade é relativa a qualitativa ordinal.\n\n\n\n\n\n\nidade_anos\nraca\nescol\n\n\n\n\n5\n39\nparda\nsuperior\n\n\n6\n34\nbranca\nsuperior\n\n\n8\n29\nbranca\nmedio\n\n\n11\n28\nbranca\nmedio\n\n\n13\n37\nparda\nfund2\n\n\n16\n27\nbranca\nmedio\n\n\n17\n44\nbranca\nmedio\n\n\n23\n31\nbranca\nmedio\n\n\n24\n33\namarela\nmedio\n\n\n25\n25\nparda\nmedio\n\n\n\n\n\nPodemos olhar uma variável por outra perspectiva, assumindo um outro tipo de classificação. Isso pode soar um pouco estranho a princípio, mas olher o exemplo a seguir para melhor compreensão, considere a variável idade, podemos transformar em faixas de idade para classificação em criança, jovem, adulto e idoso. Observe:\n\n#criacao da variavel classificacao\nclassificacao &lt;- idade_anos |&gt;\n  lapply(function(x) ifelse(x &lt; 12, 'crianca',\n                            ifelse(x &lt; 25, 'jovem',\n                                   ifelse( x &lt; 60 ,'adulto','idoso'))))\n#tabela concatenando idade e classificacao\nclassificacao |&gt; \n  unlist() |&gt; \n  cbind(idade_anos) |&gt; \n  head(10) |&gt; knitr::kable()\n\n\n\n\n\nidade_anos\n\n\n\n\njovem\n24\n\n\nadulto\n31\n\n\nadulto\n27\n\n\njovem\n20\n\n\nadulto\n39\n\n\nadulto\n34\n\n\nadulto\n34\n\n\nadulto\n29\n\n\nadulto\n44\n\n\nadulto\n27\n\n\n\n\n\nAgora, temos uma variável categórica ordinal."
  },
  {
    "objectID": "tabulacao.html#como-tabular",
    "href": "tabulacao.html#como-tabular",
    "title": "3  Tabulação de dados",
    "section": "3.2 Como tabular",
    "text": "3.2 Como tabular\nÉ perceptível, até mesmo quando trabalhamos com DataFrames e matrizes, a forma proposta de visualização e armazenamento dessas variáveis. Por colunas onde cada coluna representa uma das características (no nosso exemplo, idade, raça e escolaridade).\nFazemos isso de forma a facilitar nossa análise, sendo cada linha um indíviduo e, cada uma das observações dentro dessa linha, suas características.\nAssim como discutido, podemos obter nossas bases de dados de diversas fontes, como planilhas excel, arquivos .csv, bases SQL, ou até mesmo criá-las no nosso próprio R script com a função data.frame() como já apresentado. Por ser mais intuitivo e mais utilizado no dia a dia, vamos tomar o excel para exemplificar todo o processo. Você irá notar que o processo é realizado de forma bem simples.\n\n\n\n\n\nTabulação das variáveis no excel\n\n\n\n\nCada uma das células receberá um valor x referente a alguma característica indicada pela coluna e um indivíduo representado pela linha, em nosso caso temos 3 características para cada uma das 4 observações."
  },
  {
    "objectID": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "href": "tabulacao.html#alguns-problemas-no-meio-do-caminho",
    "title": "3  Tabulação de dados",
    "section": "3.3 Alguns problemas no meio do caminho",
    "text": "3.3 Alguns problemas no meio do caminho\nÉ valido ressaltar que é possível se deparar com alguns problemas que talvez possam vir a ser solucionados da maneira errada.\nA forma como tabulamos nossos dados pode vir a ser um facilitar ou empecilho em nossas análises, um belo exemplo é a forma citada anteriormente de classificação dos dados ou transformação para que sejam salvos em alguma outra categoria, como faixa etária ou idade.\nOutro problema é quando trabalhamos com dados que possas vir a ter mais de uma resposta. Por exemplo: Quais sintomas estava sentindo? O melhor a se fazer nesse caso é criar uma coluna para cada um dos possíveis sintomas.\n\n\n\n\n\nMais de uma opção de escolha na variável\n\n\n\n\nUma outra forma seria:\n\n\n\n\n\nMais de uma opção de escolha na variável, outra forma\n\n\n\n\nDevemos lembrar sempre de anexar um ID ou forma de identificação única para cada uma das observações. É possível criar uma ou trabalhar com alguma já existente, um exemplo de uma já existente é o próprio CPF ou RG quando trabalhamos com pessoas.\nVale ressaltar outras boas práticas ao realizar a tabulação:\n\nSe trabalhando com Excel ou Softwares parecidos, deixe a planilha apenas com a tabela de dados, evite armazenar na mesma planilha várias informações avulssas que não façam parte da sua tabela;\nNo R conseguimos especificar qual planilha de um arquivo .xlsx queremos transferir, porém pode vir a ser um pouco confuso as vezes, então é sugerido deixar todas as suas informações em uma única tabela em uma única planilha;\nPadronização é extremamente importante, salve todos os dados para cada coluna em apenas um determinado formato (Ex.: Coluna Idade - Integer, Coluna Raça - Character), lembrando sempre de manter um padrão de medida (cm, L), variáveis do tipo categórico tambem precisam de padronização (Evite coisas como: Não, nao, n, N, não);\nCuidado ao classificar dados faltantes, uma prática errada é preencher esses dados com 0, isso pode vir a atrapalhar toda sua análise\nFoi citado CPF como forma de identificação, mas pode haver casos em que teremos mais de uma linha contendo um mesmo indivíduo dependendo do nosso tipo de dados. Ou seja, esteja atento para que não haja duplicidade de variável identificadora ou ID."
  },
  {
    "objectID": "estimacao.html#estimadores-e-estimativas",
    "href": "estimacao.html#estimadores-e-estimativas",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.1 Estimadores e estimativas",
    "text": "5.1 Estimadores e estimativas\nAgora que estamos a parte de algumas definições que serão importantes para o entendimento do capítulo, podemos iniciar com o seu conteúdo propriamente dito. Em primeiro lugar, chamamos de estimador qualquer estatística cujos valores são utilizados para se estimar um parâmetro ou uma função de um parâmetro. Dessa forma, temos, portanto, que todo estimador será, também, uma variável aleatória, uma vez que eles são funções das variáveis aleatórias que compõem nossa amostra. Quando coletamos a amostra, observamos os valores das variáveis aleatórias que a compõem e os substituímos na expressão do estimador, obtemos o que chamamos de estimativa. Estimativas não são valores aleatórios, mas sim realizações de variáveis aleatórias (dos estimadores).\nDiversos métodos foram desenvolvidos ao longo dos anos para se encontrar estatísticas que possam ser utilizadas como estimadores. Entre os mais conhecidos, podemos citar: o método dos momentos, que encontra estimadores relacionando os momentos amostrais e populacionais; o método dos mínimos quadrados, a partir do qual encontramos o estimador que minimiza a chamada soma de quadrado dos erros; e o método da máxima verossimilhança, provavelmente o mais conhecido e utilizado, por meio do qual encontramos o estimador que maximiza a probabilidade de a amostra coletada ter sido observada, através da maximização da função de verossimilhança. Não entraremos em maiores detalhes sobre nenhum desses métodos, uma vez que nosso objetivo aqui é introduzir o conceito de estimação de forma mais intuitiva. Entretanto, caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o método da máxima verossimilhança e tudo aquilo que está por trás dele. O post está disponível  neste link .\nO problema da estimação não se resume somente a encontrar estimadores. De fato, existem infinitos estimadores para qualquer que seja o parâmetro que tenhamos interesse. A questão agora é, então, estabelecer critérios que nos permitam determinar o melhor estimador em um certo conjunto. Nesse contexto, podemos definir uma série de propriedades que os estimadores possuem ou podem possuir. São elas:\n\nVício: dizemos que um estimador é não viciado (ou não viesado) se o seu valor esperado coincide com o verdadeiro valor do parâmetro em questão. Em outras palavras, estimadores não viciados acertam, em média, o valor do parâmetro que estão estimando. Caso essa afirmação não seja verdadeira, dizemos que o estimador apresenta vício. O vício é a diferença entre o verdadeiro valor do parâmetro e o valor esperado de seu estimador.\nConsistência: dizemos que um estimador é consistente se, à medida que o tamanho da amostra aumenta, o seu valor esperado converge para o verdadeiro valor do parâmetro em questão e sua variância converge para zero. Dessa forma, estimadores consistentes não necessariamente são não viciados para tamanhos pequenos de amostra: eles só precisam ser não viciados quando esse tamanho é muito grande.\nErro quadrático médio: definimos o erro quadrático médio como sendo o valor esperado da diferença quadrática entre o estimador e o verdadeiro valor do parâmetro estimado. Tal como a variância de uma variável aleatória é uma medida da dispersão de seus valores em torno de sua média, o erro quadrático médio é uma medida da dispersão dos valores do estimador em torno do verdadeiro valor do parâmetro. Dessa forma, estimadores com erros quadráticos médios pequenos são preferíveis. Algo a se notar é que o erro quadrático médio pode ser reescrito como sendo a soma entre o quadrado do vício do estimador e sua variância. Com isso, para estimadores não viciados, o erro quadrático médio se reduz à variância do estimador.\n\nEm um mundo ideal, parece ser intuitivo que nossa busca pelo melhor estimador se dê através do erro quadrático médio, tentando encontrar aquele para o qual essa medida seja a menor possível. Essa tarefa é, entretanto, raramente possível. Em geral, o erro quadrático médio de um estimador é uma função do valor desconhecido do parâmetro a ser estimado, e é muito comum que, para dois estimadores de um parâmetro, seus erros quadráticos médios se entrelassem: para certos valores do parâmetro, o primeiro estimador pode ter o menor erro quadrático médio, enquanto para outros valores o segundo estimador é o que o tem. Com isso, em nossa busca pelo melhor estimador, é comum restringirmos o conjunto de todos os estimadores possíveis à classe dos estimadores não viciados, fazendo com que o erro quadrático médio seja apenas uma função da variância do estimador. Essa restrição é feita porque existem técnicas que nos permitem encontrar, entre os estimadores não viciados, aqueles que possuem a menor variância possível. Não trataremos dessas técnicas ao longo deste livro, mas essa é uma importante consideração para se entender o motivo de, em geral, trabalharmos com estimadores não viciados. De toda forma, passemos, então, para a próxima seção, na qual veremos alguns exemplos de estimadores pontuais utilizando os dados apresentados e tratados em capítulos anteriores."
  },
  {
    "objectID": "estimacao.html#estimação-pontual",
    "href": "estimacao.html#estimação-pontual",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.2 Estimação pontual",
    "text": "5.2 Estimação pontual\nPara iniciar esta seção, comecemos com uma definição. Chamamos de estimação pontual a técnica de estimação na qual utilizamos um único valor de uma estatística para representarmos, ou estimarmos, o valor desconhecido de um parâmetro de interesse. Chamamos essa estatística de estimador pontual, enquanto ao seu valor observado damos o nome de estimativa pontual. [completar]\n\n5.2.1 Estimação pontual da média de uma população\nVoltando aos exemplos apresentados na introdução do capítulo, suponha, primeiramente, que nosso parâmetro de interesse seja a idade média das gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período de março de 2020 a dezembro de 2021, a qual denotaremos por \\(\\mu\\). Como temos acesso a todos os registros dessa população, o valor desse parâmetro não é desconhecido, mas isso servirá de auxílio para exemplificar os métodos que aqui serão empregados. Criando um vetor contendo todos os elementos da população, temos:\n\ndados &lt;- readr::read_rds(\"dados/dados_covid[LIMPO].rds\")\npopulacao1 &lt;- dados$idade_anos[which(dados$evolucao == \"óbito\")]\nlength(populacao1)\n\n[1] 1266\n\n\nO processo envolvido na criação do vetor populacao1 é o seguinte: dentro do data frame dados, que contém todos os registros de nossa população, estamos selecionando o valor da variável idade_anos de todas as pacientes para as quais o valor da variável evolucao é “óbito”. O tamanho desse vetor, obtido por meio da função length(), do pacote básico {base}, é de 1266. Ou seja, a população de gestantes e puérperas hospitalizadas por COVID-19 que vieram a óbito por conta dessa doença no período considerado é composta por 1266 elementos. Para calcular o valor da idade média dessas mulheres, podemos utilizar a função mean(), também do pacote básico {base}, que calcula a média aritmética de um dado vetor.\n\nmean(populacao1)\n\n[1] 31.80806\n\n\nA saída do código nos revela o valor de \\(31.81\\) anos. Note que esse valor não representa uma estimativa; ele é, de fato, o verdadeiro valor do parâmetro \\(\\mu\\). O que aconteceria, entretanto, se tivéssemos acesso apenas a uma amostra da população em questão? Poderíamos garantir que os resultados obtidos seriam válidos para todas as gestantes e puérperas desse grupo? É o que começaremos a ver na subseção seguinte.\n\n5.2.1.1 Trabalhando com amostras da população\nPara exemplificar os conceitos de estimação definidos anteriormente, simularemos a retirada de amostras da população de gestantes e puérperas com a qual estamos trabalhando. Dentre as várias maneiras de se obter amostras de uma população, utilizaremos, aqui, a amostragem aleatória simples (AAS) com reposição, uma técnica de amostragem probabilística (ou seja, que atribui a cada elemento da população uma probabilidade, conhecida a priori, de pertencer à amostra), na qual todos os elementos da população possuem a mesma probabilidade de serem sorteados. Utilizaremos a AAS com reposição, que admite a possibilidade de um elemento ser selecionado mais de uma vez, por sua maior simplicidade teórica e por algumas implicações matemáticas e estatísticas que ela carrega, como a independência entre as unidades sorteadas.\nAntes da realização da amostragem, denotamos as variáveis a serem selecionadas por \\(X_1, X_2, ..., X_{n}\\), sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera hospitalizada pela COVID-19 e que faleceu em decorrência dessa doença, com \\(i = 1, 2, ..., n\\). Dizemos que essa sequência de variáveis aleatórias forma uma amostra aleatória de tamanho \\(n\\). Sendo, novamente, \\(\\mu\\) o parâmetro que representa a idade média da população em questão, e denotando por \\(\\sigma^2\\) o parâmetro que representa a variância populacional das idades dessas gestantes e puérperas, temos, ainda, que \\(E(X_i) = \\mu\\) e que \\(Var(X_i) = \\sigma^2\\).\nDentro do R, podemos obter uma amostra aleatória de tamanho, digamos, \\(n = 30\\), a partir da função sample(), do pacote básico {base}. Utilizaremos três argumentos dessa função: o primeiro, x, recebe o vetor de elementos do qual a amostra será retirada; o segundo, size, recebe o número de itens a serem sorteados; por fim, o terceiro argumento, replace, receberá o valor TRUE, indicando que a amostragem deve ser realizada com reposição. O código utilizado para a realização desse processo, bem como a amostra obtida, podem ser vistos abaixo.\n\nset.seed(43)\namostra1 &lt;- sample(x = populacao1, size = 30, replace = TRUE)\namostra1\n\n [1] 32 27 37 38 39 32 30 24 29 30 34 40 33 17 34 35 32 38 23 24 33 38 18 25 26\n[26] 32 25 36 29 32\n\n\nÉ importante ressaltar que, enquanto \\(X_1, X_2, ..., X_{30}\\) são variáveis aleatórias, os valores guardados no vetor amostra1 representam realizações das mesmas. Dessa forma, temos, para a amostra sorteada, que \\(x_1 = 32\\), \\(x_2 = 27\\), \\(x_3 = 37\\) e assim por diante. Essas realizações seriam, muito provavalmente, diferentes caso desempenhássemos o procedimento de retirada da amostra novamente. No presente caso, o código acima sempre resultará nos mesmos elementos, uma vez que estamos fixando a semente inicial do gerador de números pseudo-aleatórios do R por meio da função set.seed(), do pacote básico {base}. Desfixando a semente inicial do sorteio, entretanto, o resultado obtido através da função sample() seria diferente a cada vez que rodássemos o bloco de código. Observe.\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 33 27 17 27 24 28 29 30 32 32 31 41 25 30 26 41 26 27 33 27 33 24 39 36\n[26] 30 40 32 28 23\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 33 42 32 29 32 30 30 39 35 48 32 26 27 42 23 29 31 37 27 32 25 41 43 22 21\n[26] 30 34 22 30 37\n\n\n\nsample(x = populacao1, size = 30, replace = TRUE)\n\n [1] 22 33 32 30 30 31 36 37 37 23 28 33 38 40 21 23 38 33 23 37 27 31 43 39 41\n[26] 36 35 39 39 45\n\n\nCom a distinção entre variáveis aleatórias e suas realizações em mente, precisamos, nesse próximo caso, definir qual estimador utilizaremos para estimarmos o parâmetro em questão. Como temos interesse na idade média da população, uma escolha muito intuitiva seria utilizar a média aritmética dos valores amostrados como uma estimativa do valor desse parâmetro. Assim, considerando que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória de tamanho \\(n\\) dessa população, definimos o estimador da média amostral como sendo a estatística dada por\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{n}}{n} = \\frac{\\sum_{i = 1}^{n} X_i}{n}.\n\\]\nAlém de muito intuitivo, esse estimador é, também, não viciado e consistente. Podemos facilmente demonstrar essas propriedades de forma matemática, utilizando para isso propriedades de valores esperados. Quanto a ser não viciado, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da esperança o multiplicando:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nSendo a esperança da soma de variáveis aleatórias equivalente à soma das esperanças marginais de cada variável, temos:\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right)\n\\]\nPor fim, como \\(E(X_i) = \\mu\\),\n\\[\nE(\\bar{X}) = E\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n}E\\left(\\sum_{i = 1}^{n} X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n}  E\\left(X_i\\right) = \\frac{1}{n} \\sum_{i = 1}^{n} \\mu = \\frac{1}{n} (n\\mu) = \\mu.\n\\]\nLogo, como o valor esperado do estimador é igual ao parâmetro que ele estima, concluímos que \\(\\bar{X}\\) é um estimador não viciado. Para demonstrarmos que ele é, também, consistente, precisamos calcular sua variância. Assim,\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right).\n\\]\nComo \\(\\frac{1}{n}\\) é um valor constante que está multiplicando a variável aleatória \\(\\sum_{i = 1}^{n} X_i\\), podemos retirá-lo da variância elevando-o ao quadrado:\n\\[\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right).\n\\]\nComo utilizamos a AAS com reposição para a retirada da amostra, garantimos que as variáveis aleatórias que a compõem são independentes. Assim, sendo a variância da soma de variáveis aleatórias independentes dada pela soma das variâncias marginais de cada variável, e como \\(Var(X_i) = \\sigma^2\\), temos:\n\\[\n\\begin{align}\nVar(\\bar{X}) = Var\\left(\\frac{\\sum_{i = 1}^{n} X_i}{n}\\right) = \\frac{1}{n^2} Var\\left(\\sum_{i = 1}^{n} X_i\\right) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(X_i) \\\\ & = \\frac{1}{n^2} \\sum_{i = 1}^{n} \\sigma^2 \\\\ & = \\frac{1}{n^2} (n \\sigma^2) \\\\ & = \\frac{\\sigma^2}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\bar{X}\\). Essa informação, aliada ao fato de \\(\\bar{X}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Voltando ao exemplo em que estávamos, como definimos que \\(n = 30\\), a expressão do estimador da média amostral se torna\n\\[\n\\bar{X} = \\frac{X_1 + X_2 + ... + X_{30}}{30} = \\frac{\\sum_{i = 1}^{30} X_i}{30}.\n\\]\nSubstituindo os valores da amostra coletada e calculando sua média aritmética, obtemos:\n\nmean(amostra1)\n\n[1] 30.73333\n\n\nCom isso, concluímos que uma estimativa pontual para a idade média das gestantes e puérperas que faleceram em decorrência da COVID-19 é de \\(30.73\\). Esse valor é relativamente próximo do verdadeiro valor do parâmetro, o qual sabemos ser 31.81. Lembre-se, entretanto, que a estimativa obtida depende diretamente da amostra que foi coletada, uma vez que nosso estimador é uma função da amostra e, portanto, é uma variável aleatória. A cada vez que realizássemos um novo sorteio, o valor de nossa estimativa seria, muito provavelmente, diferente do anterior. Como nosso objetivo é fazer uma afirmação sobre o parâmetro \\(\\mu\\) a partir da amostra coletada, é interessante considerar que a validade dessa afirmação seria melhor compreendida se soubéssemos o que acontece com nosso estimador quando retiramos todas as amostras de mesmo tamanho possíveis de nossa população. Retomaremos essa discussão posteriormente. Buscaremos, agora, estimar um outro tipo de parâmetro: a proporção populacional.\n\n\n\n5.2.2 Estimação pontual da proporção populacional\nPara o segundo exemplo, suponha que o parâmetro no qual temos interesse seja a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas. Representaremos esse parâmetro por \\(p\\). Assim como no exemplo anterior, podemos calcular seu valor, uma vez que temos acesso a todos os registros dessa população. Note que, como estamos tratando da proporção válida, precisamos que nossa população seja composta apenas pelas mulheres para as quais o valor da variável diarreia foi preenchido de forma válida (ou seja, com sim ou não). Assim, temos:\n\npopulacao2 &lt;- dados$diarreia[which(!is.na(dados$diarreia) & dados$diarreia != \"ignorado\")]\nlength(populacao2)\n\n[1] 8472\n\nhead(populacao2, 20)\n\n [1] \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n[13] \"não\" \"sim\" \"não\" \"não\" \"não\" \"não\" \"não\" \"não\"\n\n\nObservando as saídas acima, podemos notar que nossa população é formada por 8.472 elementos, que assumem valor sim, quando a gestante ou puérpera apresentou diarreia como um dos sintomas da COVID-19, e não, quando esse sintoma não foi apresentado. Para facilitar nosso trabalho a partir daqui, transformaremos os valores sim em 1 e os valores não em 0, utilizando para isso a função ifelse(), do pacote básico {base}. Essa função recebe três argumentos: o primeiro, test, recebe um vetor lógico; o segundo, yes, recebe o valor que a função deve retornar quando o dado elemento desse vetor lógico for verdadeiro; por fim, o terceiro argumento, no, recebe o valor que a função deve retornar quando o dado elemento do vetor lógico for falso. Observe o código e a saída abaixo.\n\npopulacao2_transformada &lt;- ifelse(populacao2 == \"sim\", yes = 1, no = 0)\nhead(populacao2_transformada, 20)\n\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n\n\nCalculando, por fim, a proporção desejada, que nada mais será do que a média aritmética do vetor populacao2_transformada, uma vez que ele é formado por zeros e uns, temos:\n\nmean(populacao2_transformada)\n\n[1] 0.128305\n\n\nLogo, o valor do parâmetro \\(p\\) - a proporção válida de gestantes e puérperas hospitalizadas por COVID-19 no período de março de 2020 a dezembro de 2021 que apresentaram diarreia como um de seus sintomas - é de 0,128, ou 12,8%. Vamos, agora, fingir que não tínhamos acesso a métodos de se calcular o valor desse parâmetro, tentando novamente estimá-lo por meio da coleta de amostras da população.\n\n5.2.2.1 Trabalhando com amostras de uma população com distribuição Bernoulli\nDiferentemente do que ocorria com o exemplo anterior, a amostra aleatória que agora coletaremos será composta por variáveis aleatórias para as quais sabemos a “forma” de sua distribuição de probabilidade. Podemos definir \\(Y_1, Y_2, ..., Y_{n}\\) como sendo uma amostra aleatória da distribuição Bernoulli com parâmetro \\(p\\), na qual \\(Y_i\\) recebe o valor 1, caso a \\(i\\)-ésima gestante ou puérpera sorteada tenha apresentado diarreia como um dos sintomas da COVID-19 (sucesso), e 0, caso contrário (fracasso), com \\(i = 1, 2, ..., n\\). O parâmetro \\(p\\) representa a probabilidade de sucesso (que sabemos ser de 0,128, apesar de estarmos fingindo que não temos essa informação). Como queremos estimar uma proporção, é intuitivo considerarmos como estimador a proporção das gestantes ou puérperas da amostra que apresentaram diarreia como sintoma. Assim, definimos o estimador da proporção amostral, denotado por \\(\\hat{p}\\), como sendo\n\\[\n\\hat{p} = \\frac{Y_1 + Y_2 + ... + Y_n}{n} = \\frac{\\sum_{i = 1}^{n} Y_i}{n}.\n\\]\nDe maneira similar ao que fizemos com o estimador da média amostral, \\(\\bar{X}\\), podemos demonstrar que o estimador da proporção amostral é, também, não viciado e consistente. Quanto à primeira propriedade, sabendo que \\(E(Y_i) = p\\), para \\(i = 1, 2, ..., n\\), temos:\n\\[\nE\\left(\\hat{p}\\right) = E \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n} \\sum_{i = 1}^{n} E\\left(Y_i \\right) =  \\frac{1}{n}  \\sum_{i = 1}^{n} p = \\frac{1}{n} (np) = p.\n\\]\nAssim, como \\(E(\\hat{p}) = p\\), podemos concluir que o estimador da proporção amostral é não viesado. Em outras palavras, esse estimador “acerta”, em média, o verdadeiro valor do parâmetro \\(p\\). Para a segunda propriedade, precisamos, primeiramente, calcular a variância de \\(\\hat{p}\\). Como \\(Y_i\\) segue distribuição \\(Bernoulli(p)\\), sabemos que \\(Var(Y_i) = p(1 - p)\\). Dessa forma, temos que\n\\[\n\\begin{align}\nVar(\\hat{p}) = Var \\left( \\frac{\\sum_{i = 1}^{n} Y_i}{n} \\right) = \\frac{1}{n^2} \\sum_{i = 1}^{n} Var(Y_i) & = \\frac{1}{n^2} \\sum_{i = 1}^{n} p(1 - p) \\\\ &  = \\frac{1}{n^2} \\left[np(1 - p)\\right] \\\\ &  = \\frac{p(1 - p)}{n}.\n\\end{align}\n\\]\nObserve que, quanto maior o valor do tamanho de amostra \\(n\\), menor é o valor da variânica de \\(\\hat{p}\\). Essa informação, aliada ao fato de \\(\\hat{p}\\) ser não viciado, nos permite concluir que o estimador em questão é consistente. Investigadas as propriedades do estimador, podemos partir para a retirada da amostra, utilizando novamente a função sample() para simular uma amostra de tamanho \\(n = 30\\) obtida por meio da AAS com reposição. A amostra coletada pode ser vista abaixo.\n\nset.seed(43)\namostra2 &lt;- sample(x = populacao2_transformada, size = 30, replace = TRUE)\namostra2\n\n [1] 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n\n\nAplicando os valores obtidos no estimador da proporção amostral, que nada mais é do que a média aritmética da amostra, temos:\n\nmean(amostra2)\n\n[1] 0.1333333\n\n\nCom isso, concluímos que uma estimativa pontual para a proporção válida de gestantes e puérperas hospitalizadas pela COVID-19 no período em estudo e que apresentaram diarreia como um dos sintomas da doença é de 0,133, ou de 13,3%. Novamente, essa estimativa depende diretamente da amostra obtida; novas amostragens quase certamente resultariam em estimativas diferentes para o parâmetro. Com isso, volta à tona a reflexão levantada no final do exemplo anterior, de que a validade de nossa afirmação sobre o verdadeiro valor de \\(p\\) seria melhor compreendida caso levássemos em consideração a distribuição de nosso estimador, \\(\\hat{p}\\). Conseguiríamos estudar o comportamento probabilístico de \\(\\hat{p}\\) caso aumentássemos o tamanho da amostra? A resposta, já adiantando, é sim. O que utilizamos para realizar esse estudo, entretanto, será visto na próxima seção.\n\n\n\n5.2.3 A distribuição amostral de estimadores\nComo vimos ao longo das seções anteriores, o problema da Inferência Estatística que queremos resolver consiste em fazer uma afirmação sobre um certo parâmetro de uma determinada população por meio de uma amostra. Para encará-lo, decidimos que nossa afirmação será baseada em uma certa estatística \\(T\\), para a qual demos o nome de estimador, que será uma função da amostra (\\(X_1, X_2, ..., X_n\\)). Quando coletamos a amostra, podemos obter um valor particular de \\(T\\), digamos \\(t_0\\), para o qual demos o nome de estimativa. E é com base nesse valor \\(t_0\\) que faremos a afirmação sobre o parâmetro de interesse. Para entendermos melhor a incerteza por trás de nossa afirmação, entretanto, seria de nosso interesse determinar qual é a distribuição de \\(T\\) quando a amostra, \\(X_1, X_2, ..., X_n\\), assume todos os valores possíveis. Chamamos essa distribuição de distribuição amostral da estatística T. Bussab e Moretin (referência) esquematizam o procedimento para a obtenção da distribuição amostral da seguinte maneira:\n\nA partir de uma determinada população \\(X\\), com certo parâmetro de interesse \\(\\theta\\), obtemos todas as amostras possíveis com um mesmo tamanho amostral \\(n\\), de acordo com uma certa técnica de amostragem;\nPara cada amostra obtida, calculamos o valor \\(t\\) da estatística \\(T\\);\nOs valores \\(t\\) formam uma nova população, cuja distribuição recebe o nome de distribuição amostral de \\(T\\).\n\nÉ muito comum, no entanto, que não sejamos capazes de coletar todas as amostras possíveis de uma população. Com isso, acabamos tendo que nos contentar em simular um número grande de amostras, para assim termos uma ideia do que acontece com a estatística de interesse. Para melhor entendermos as ideias apresentadas, consideremos os estimadores \\(\\bar{X}\\), a média amostral, e \\(\\hat{p}\\), a proporção amostral. Nos exemplos antecedentes, acabamos determinando, talvez sem perceber, a média e a variância das distribuições amostrais de ambos os estimadores quando estávamos demonstrando duas de suas propriedades - a falta de vício e a consistência. Retomando os resultados obtidos, encontramos que\n\n\\(E(\\bar{X} = \\mu)\\) e \\(Var(\\bar{X}) = \\displaystyle \\frac{\\sigma^2}{n}\\);\n\\(E(\\hat{p} = p)\\) e \\(Var(\\hat{p}) = \\displaystyle \\frac{p(1 - p)}{n}\\).\n\nMédias e variâncias não são, todavia, tudo aquilo que precisamos para determinar a distribuição amostral de estimadores. Precisamos, também, determinar sua “forma”. Para isso, coletaremos várias amostras e construiremos histogramas das distribuições de \\(\\bar{X}\\) e \\(\\hat{p}\\) para diferentes tamanhos de amostra. Comecemos simulando \\(M\\) = 100 amostras, cada uma com tamanho \\(n\\) = 15, da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo, a qual chamamos de populacao1. Utilizaremos para isso a função replicate(), do pacote básico {base}. Essa função recebe dois argumentos: o primeiro, n, recebe o número de replicações a serem feitas, enquanto o segundo, expr, recebe a expressão que será replicada. O resultado, guardado no objeto amostras_pop1, é uma matriz na qual o elemento \\([a_{ij}]\\) representa o \\(i\\)-ésimo elemento da \\(j\\)-ésima amostra, com \\(i = 1, 2, ..., 15\\) e \\(j = 1, 2, ..., 100\\). As cinco primeiras colunas dessa matriz podem ser vistas abaixo.\n\nset.seed(43)\nM &lt;- 100\nn &lt;- 15\namostras_pop1 &lt;- replicate(M, expr = sample(x = populacao1, size = n, replace = TRUE))\namostras_pop1[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]   32   35   33   26   33\n [2,]   27   32   33   41   42\n [3,]   37   38   27   26   32\n [4,]   38   23   17   27   29\n [5,]   39   24   27   33   32\n [6,]   32   33   24   27   30\n [7,]   30   38   28   33   30\n [8,]   24   18   29   24   39\n [9,]   29   25   30   39   35\n[10,]   30   26   32   36   48\n[11,]   34   32   32   30   32\n[12,]   40   25   31   40   26\n[13,]   33   36   41   32   27\n[14,]   17   29   25   28   42\n[15,]   34   32   30   23   23\n\n\nRepetimos o mesmo processo para a população de gestantes e puérperas com preenchimento válido da variável diarreia, que chamamos de populacao2_transformada. Novamente, as cinco primeiras colunas da matriz de amostras, que agora denominamos amostras_pop2, podem ser vistas abaixo.\n\nset.seed(43)\namostras_pop2 &lt;- replicate(M, expr = sample(x = populacao2_transformada, size = n, replace = TRUE))\namostras_pop2[, 1:5]\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,]    0    0    0    0    0\n [2,]    0    0    0    0    0\n [3,]    0    1    0    0    0\n [4,]    0    0    0    0    0\n [5,]    0    0    0    1    1\n [6,]    0    0    0    0    0\n [7,]    0    0    0    0    0\n [8,]    1    0    0    0    0\n [9,]    1    0    0    0    0\n[10,]    0    0    0    1    0\n[11,]    0    0    0    0    0\n[12,]    0    0    0    1    1\n[13,]    0    0    0    0    0\n[14,]    0    1    0    0    0\n[15,]    0    0    0    1    0\n\n\nCom as amostras em mãos, o próximo passo é calcular o valor do respectivo estimador em cada uma delas. Realizaremos esse processo com a função apply(), também do pacote básico {base}, a qual permite que apliquemos qualquer função em todas as linhas ou colunas de uma matriz. Utilizaremos três de seus argumentos: o primeiro, X, recebe a matriz na qual queremos aplicar a função; o segundo, MARGIN, recebe a direção em que a função será aplicada (1 caso queiramos que a função seja aplicada nas linhas da matriz, ou 2 caso queiramos a aplicar em suas colunas); por fim, o terceiro, FUN, recebe a função que queremos aplicar. O código utilizado nesse processo, bem como parte dos vetores obtidos, podem ser vistos abaixo.\n\nx_barras &lt;- apply(X = amostras_pop1, MARGIN = 2, FUN = mean)\nhead(x_barras)\n\n[1] 31.73333 29.73333 29.26667 31.00000 33.33333 30.73333\n\np_chapeus &lt;- apply(X = amostras_pop2, MARGIN = 2, FUN = mean)\nhead(p_chapeus)\n\n[1] 0.1333333 0.1333333 0.0000000 0.2666667 0.1333333 0.2000000\n\n\nPor fim, criemos os histogramas da distribuição de cada estimador. Como já discutimos sobre a criação de histogramas no capítulo de Estatística Descritiva, o código abaixo deve ser familiar.\n\nlibrary(ggplot2)\n\nggplot(data.frame(x_barra = x_barras), aes(x = x_barra))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"lightblue\",\n    bins = 15,\n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(26, 36)) +\n  labs(\n    x = \"Idade média das gestantes e puérperas\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral da médias amostrais para n = 15\"\n    ) +\n  geom_vline(xintercept = mean(x_barras), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 34, \n    y = 0.3,\n    label = paste(\"Valor médio das estimativas:\", round(mean(x_barras), 3))\n    )\n\n\n\nggplot(data.frame(p_chapeu = p_chapeus), aes(x = p_chapeu))  + \n  geom_histogram(\n    aes(y = after_stat(density)),\n    fill = \"steelblue\", \n    bins = 15, \n    color = \"black\"\n    ) +\n  coord_cartesian(xlim = c(0, 0.5)) +\n  labs(\n    x = \"Proporção de gestantes ou puérperas com diarreia como sintoma\", \n    y = \"Densidade\",\n    title = \"Distribuição amostral das proporções amostrais para n = 15\"\n    ) + \n  geom_vline(xintercept = mean(p_chapeus), linetype = 2) +\n  annotate(\n    geom = \"text\",\n    x = 0.23, \n    y = 9,\n    label = paste(\"Valor médio das estimativas:\", round(mean(p_chapeus), 3))\n    )\n\n\n\n\nObservando os histogramas, podemos notar que, mesmo para um tamanho de amostra pequeno como \\(n\\) = 15, a distribuição de \\(\\bar{X}\\) se assemelha à distribuição normal, visto que apresenta o característico formato aproximado de sino e uma quase simetria em torno de sua média. Essa combinação de fatores nos sugere que \\(X_1, X_2, ..., X_{15}\\), as variáveis aleatórias que compõem as amostras da população de idades, seguem, também, uma distribuição simétrica em torno da média. Além disso, o valor médio das estimativas \\(\\bar{x}\\), de 31,843, está muito próximo do verdadeiro valor do parâmetro populacional \\(\\mu\\), que sabemos ser de 31,81 anos. Esse resultado já era esperado, uma vez que a distribuição de \\(\\bar{X}\\) está centrada em \\(\\mu\\). Pouco podemos dizer, entretanto, do histograma da distribuição amostral de \\(\\hat{p}\\) até o momento; apenas que sua média está muito próxima do verdadeiro valor de \\(p\\), que sabemos ser de 0,128, como já era esperado pelo mesmo motivo. Aumentemos, então, o tamanho das amostras, e observemos os resultados obtidos. Como a única modificação será o valor da variável n, ocultaremos os códigos utilizados para evitar uma maior poluição visual. Dessa forma, para \\(n\\) = 30, temos:\n\n\n\n\n\n\n\n\nPara \\(n = 50\\),\n\n\n\n\n\n\n\n\nPara \\(n = 100\\),\n\n\n\n\n\n\n\n\nPor fim, para \\(n = 200\\),\n\n\n\n\n\n\n\n\nObserve que, conforme aumentamos o tamanho das amostras, os histogramas de ambos os estimadores tendem a se concentrar cada vez mais em torno de suas respectivas médias, uma vez que as variância das estimativas se torna cada vez menor. Dessa forma, podemos concluir que estimativas obtidas a partir de tamanhos de amostra maiores têm uma maior probabilidade de “acertarem” o verdadeiro valor do parâmetro que estão estimando. É também notável que mesmo os histogramas das proporções amostrais aparentam convergir para o formato da distribuição normal conforme o valor de \\(n\\) aumenta. Esse fato, por incrível que pareça, não é coincidência: ele é decorrência direta do Teorema Central do Limite (TCL), o qual afirma que, independente da distribuição da população, quanto maior o tamanho amostral, mais próxima será a distribuição amostral da média de uma distribuição normal. Vale lembrar que a proporção amostral nada mais é do que um caso particular da média amostral em que os valores observados na amostra contém apenas zeros e uns, o que explica a aplicação do TCL nesse caso. Para sermos mais precisos, podemos dizer de forma aproximada que, para tamanhos suficientemente grandes de amostra,\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right) \\text{ e } \\hat{p}\\sim N\\left(p, \\frac{p(1-p)}{n} \\right).\n\\]\nCom isso, aqui terminamos o conteúdo referente à estimação pontual."
  },
  {
    "objectID": "estimacao.html#estimação-intervalar",
    "href": "estimacao.html#estimação-intervalar",
    "title": "5  Conceitos básicos de probabilidade",
    "section": "5.3 Estimação intervalar",
    "text": "5.3 Estimação intervalar\nAté este ponto, todos os estimadores que apresentamos e discutimos são pontuais, uma vez que fornecem um único valor como estimativa para o parâmetro de interesse. Estimativas pontuais, por mais úteis que sejam, acabam fornecendo uma informação incompleta sobre o valor estimado do parâmetro em questão. Como estimadores são variáveis aleatórias e, portanto, possuem uma distribuição de probabilidade, seria de nosso interesse que a estimativa a ser apresentada levasse em consideração uma medida de seu possível erro. Essa essa medida pode ser, por exemplo, um intervalo relacionado à dimensão da confiança que temos de que o verdadeiro valor do parâmetro está sendo captado. Dessa forma, a partir daqui, entramos no campo da estimação intervalar. Dentro da Inferência Clássica, que estamos estudando neste capítulo, estimativas intervalares se dão a partir dos chamados intervalos de confiança. Intervalos de confiança incorporam, à estimativa pontual do parâmetro, informações a respeito da variabilidade do estimador. Além disso, eles são obtidos através da distribuição amostral de seus estimadores, o que justifica ainda mais o conteúdo que vimos na última subseção de estimação pontual.\nComo o intuito deste livro não é conter uma porção pesada de teoria, introduziremos o conceito de intervalos de confiança a partir de exemplos, realizando explicações sobre os elementos envolvidos em sua construção conforme seja necessário. Caso seja de seu interesse, já publicamos, no site do Observatório, um texto que pode te ajudar a entender o melhor a teoria por trás dos intervalos de confiança, que conta também com o detalhamento de um dos principais métodos utilizados para a construção desses intervalos: o método da quantidade pivotal. O post está disponível  neste link . Com isso em mente, prossigamos para nosso primeiro exemplo: a criação de intervalos de confiança para a média amostral.\n\n5.3.1 Intervalos de confiança para a média amostral quando a variância populacional é conhecida\nUtilizando o exemplo já apresentado na seção anterior, considere que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Denotando, novamente, por \\(\\mu\\) a média populacional das idades dessas mulheres, e por \\(\\sigma^2\\) a variância populacional dessas idades, temos, ainda que \\(E(X_i) = \\mu\\) e \\(Var(X_i) = \\sigma^2\\). Suponha que queiramos estimar o valor de \\(\\mu\\), utilizando para isso o estimador \\(\\bar{X}\\). Suponha também, neste primeiro exemplo, que o valor de \\(\\sigma^2\\) é conhecido. Note que não estamos fazendo nenhuma suposição sobre a distribuição de probabilidade dessas variáveis. Dessa forma, precisaremos, a partir deste ponto, impor uma restrição: o tamanho da amostra deve ser grande o suficiente para que possamos aplicar o Teorema Central do Limite. Caso essa restrição seja cumprida, sabemos, por meio do TCL e de forma aproximada, que\n\\[\n\\bar{X} \\sim N \\left(\\mu, \\frac{\\sigma^2}{n} \\right).\n\\]\nSubtraindo de uma variável aleatória a sua média e dividindo o resultado por seu desvio padrão, obtemos o que chamamos de variável aleatória padronizadas. Uma variável aleatória padronizada tem média igual a zero e variância igual a um. Aplicando esse resultado em nosso estimador, \\(\\bar{X}\\), obtemos uma nova variável, a qual chamaremos de \\(Z\\), cuja distribuição estará totalmente definida, o que será de grande utilidade na construção de nosso intervalo. Observe.\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\] Como conhecemos a distribuição de probabilidade de \\(Z\\), podemos, para um certo valor \\(\\alpha\\), com \\(0 &lt; \\alpha &lt; 1\\), encontrar valores \\(z_1\\) e \\(z_2\\), com \\(z_1 &lt; z_2\\), tais que\n\\[\nP(z_1 &lt; Z &lt; z_2) = 1 - \\alpha.  \\qquad \\qquad \\text{(I)}\n\\]\nChamamos o valor \\(1 - \\alpha\\) de coeficiente de confiança. Sua interpretação será feita posteriormente. Quanto à probabilidade acima, note que existem infinitos valores de \\(z_1\\) e \\(z_2\\) que a satisfazem. Como queremos encontrar um intervalo que contenha os valores mais plausíveis do parâmetro em estudo, é de nosso interesse que a amplitude desse intervalo seja a menor possível, sendo a amplitude de um intervalo definida como a diferença entre seus extremos superior e inferior. E, para que esse interesse seja cumprido, é necessário que os valores de \\(z_1\\) e \\(z_2\\) sejam os mais próximos possíveis. Para distribuições simétricas em torno do zero, como é o caso da distribuição normal padrão, podemos mostrar que a amplitude do intervalo será mínima se os valores de \\(z_1\\) e \\(z_2\\) forem opostos, ou seja, se \\(z_1 = -z_2\\). Com isso, precisamos apenas encontrar um valor \\(z\\) tal que\n\\[\nP(Z \\leqslant z) = 1 - \\frac{\\alpha}{2}\n\\]\nA este valor, o qual denotamos por \\(z_{1 - \\alpha/2}\\), damos o nome de quantil de ordem \\(1 - \\alpha/2\\). Um quantil de ordem \\(k\\) de uma variável aleatória, com \\(0 &lt; k &lt; 1\\), nada mais é que o ponto tal que, quando nele aplicada a função de distribuição acumulada da variável, a probabilidade obtida é igual a \\(k\\) (a ordem que o quantil representa). Em uma situação prática, na qual teríamos um valor definido de \\(\\alpha\\), poderíamos utilizar uma tabela da distribuição normal padrão para encontrar o valor de \\(z_{1 - \\alpha/2}\\), ou mesmo utilizar a função qnorm(), do pacote básico {stats}, para realizar esse processo. A função qnorm(), bem como a família de funções do R que seguem a estrutura “qnome_da_distribuição()”, representa a função quantílica: para uma dada probabilidade e para dados valores dos parâmetros da distribuição, a função retorna o quantil cuja ordem é a probabilidade estipulada em seus arguementos. Com isso em mente, podemos reescrever \\(z_1\\) e \\(z_2\\) como sendo\n\\[\nz_1 = -z_{1 - \\alpha/2} \\text{ e } z_2 = z_{1 - \\alpha/2}.\n\\]\nPara que a explicação acima seja melhor absorvida, observe o gráfico a seguir, que representa a curva da densidade de probabilidade da distribuição normal padrão.\n\n\n\n\n\nFigura 1: Para uma confiança de (100 - \\(\\alpha\\))%, a área em cada cauda da distribuição deverá ser de \\(\\alpha\\)/2 para que o intervalo seja o menor possível.\n\n\n\n\nVoltando à probabilidade definida em \\(\\text{(I)}\\), a atualizando com os resultados obtidos e reescrevendo \\(Z\\), temos:\n\\[\nP(z_1 &lt; Z &lt; z_2) = P \\left(-z_{1 - \\alpha/2} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma}  &lt; z_{1 - \\alpha/2} \\right)  = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-z_{1 - \\alpha/2} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} &lt; z_{1 - \\alpha/2} \\right) \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\sigma &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; z_{1 - \\alpha/2}\\sigma \\right)  \\\\ &\n= P\\left(-z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\  &\n= P\\left(-\\bar{X} + -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) \\\\ & = P\\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - -z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\] Portanto, quando a variância populacional é conhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}};\\; \\bar{X} + z_{1 - \\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right).\n\\]\nA interpretação do resultado acima deve ser feita com cuidado. É preciso entender que a expressão \\(IC(\\mu,\\ 1 - \\alpha)\\) envolve uma variável aleatória, \\(\\bar{X}\\), fazendo com que o intervalo obtido também seja aleatório. Para o intervalo aleatório encontrado acima, podemos dizer que a probabilidade aproximada de ele conter o verdadeiro valor do parâmetro \\(\\mu\\) é de \\(1 - \\alpha\\). Aproximada, nesse caso, porque estamos utilizando o TCL para fazer uma aproximação da distribuição de probabilidade de \\(\\bar{X}\\); caso a população seguisse distribuição normal, essa probabilidade seria exata. De qualquer forma, quando coletamos a amostra e observamos uma estimativa \\(\\bar{x}\\), obtemos um intervalo numérico, que chamamos de intervalo de confiança observado. A partir desse ponto, não existem mais quantidades aleatórias na expressão do intervalo, uma vez que, na Inferência Clássica, os parâmetros, por mais que possam ser desconhecidos, são quantidades fixas. Dessa forma, não podemos mais afirmar que um intervalo de confiança observado possui probabilidade \\(1 - \\alpha\\) de conter o verdadeiro valor do parâmetro. Podemos apenas dizer que temos uma confiança considerável de que esse intervalo contém o verdadeiro valor do parâmetro. A medida da nossa confiança é de \\(1 - \\alpha\\) porque, antes de colhermos a amostra, \\(1 - \\alpha\\) era a probabilidade aproximada de que o intervalo aleatório contivesse o verdadeiro valor de \\(\\mu\\).\nComo a distinção entre confiança e probabilidade pode ser difícil de se entender, uma interpretação conveniente para intervalos de confiança é a seguinte: se obtivéssemos várias amostras de mesmo tamanho e, para cada uma delas, calculássemos os correspondentes intervalos de confiança com coeficiente de confiança \\(1 - \\alpha\\), esperaríamos que a proporção de intervalos que contivessem o verdadeiro valor do parâmetro fosse igual a \\(1 - \\alpha\\).\nPor fim, antes de realizarmos um exemplo numérico, podemos fazer algumas considerações a respeito da escolha do valor de \\(\\alpha\\). É possível mostrar que, conforme aumentamos o coeficiente de confiança, a amplitude do intervalo aumenta. Isso, claro, é algo que deveríamos esperar, visto que intervalos maiores possuem naturalmente uma maior chance de conterem um valor desconhecido. Com isso, para que os intervalos sejam o mais informativo possível, mantendo uma confiança elevada, é necessário que selecionemos \\(\\alpha\\) de forma balanceada, sendo uma escolha muito comum o valor 0,05 (nesse caso, temos que o coeficiente de confiança é de 0,95). É muito mais interessante, por exemplo, um intervalo de confiança que diz que o verdadeiro valor do salário médio de um estatístico está entre 3,5 a 6 salários mínimos do que um intervalo que diz que esse valor está entre 2 a 7,5 salários mínimos. Apesar de, com o segundo intervalo, termos uma maior confiança de que o verdeiro valor do salário médio está sendo captado, a qualidade da informação que extraímos dele é consideravelmente pior do que aquela obtida com o primeiro intervalo.\n\n5.3.1.1 Um exemplo numérico\nSubstituindo as letras por números, e continuando o exemplo em que estávamos, vamos, agora, obter, através do R, um intervalo de 95% confiança para \\(\\mu\\). Lembre-se que aqui \\(\\mu\\) é a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021. Como precisamos que nossa amostra seja suficientemente grande para que possamos aplicar o TCL, utilizaremos \\(n = 50\\), uma vez que, observando os histogramas criados na seção de Distribuição Amostral, a distribuição de \\(bar{X}\\) já se aproxima satisfatóriamente bem da distribuição normal a partir desse ponto. Além disso, como a expressão do intervalo de confiança obtido acima leva em consideração que a variância populacional é conhecida, precisaremos, também, dessa informação. Para calculá-la, teremos de multiplicar o resultado da função var() por \\(\\frac{N - 1}{N}\\), sendo \\(N\\) o tamanho da população, uma vez que essa função utiliza \\(N - 1\\) como denominador para o cálculo da variância. Observe o código abaixo.\n\nsigma2 &lt;- var(populacao1) * (length(populacao1) - 1)/length(populacao1)\nsigma2\n\n[1] 45.97659\n\n\nObtendo uma amostra de tamanho \\(n = 50\\) por meio da função sample(), e a armazenando no vetor amostra_ic_media, temos:\n\nset.seed(34)\namostra_ic_media &lt;- sample(x = populacao1, size = 50, replace = TRUE)\nhead(amostra_ic_media, 20)\n\n [1] 26 39 32 35 35 35 31 21 25 40 33 32 19 22 19 30 40 31 38 36\n\n\nNesse próximo passo, criaremos uma função, a qual chamaremos de ic_media_caso1(), que calculará intervalos de confiança para o parâmetro \\(\\mu\\) quando a variância populacional é conhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função possuirá três argumentos: dados, que receberá o vetor de valores observados na amostra; sigma, que receberá o valor do desvio padrão populacional; e alpha, que receberá o valor necessário para se obter o coeficiente de confiança associado ao intervalo, que nesse caso será de 0,05. Utilizaremos a já explicada função qnorm() para encontrar o valor do quantil de ordem \\(1 - \\alpha\\) da normal padrão (não utilizaremos os argumentos mean e sigma dessa função, uma vez que seus valores padrões são, respectivamente, 1 e 0). O limite inferior do intervalo será guardado no vetor limite_inferior, enquanto o superior será guardado em limite_superior. A função retornará um data frame contendo algumas medidas referentes à amostra e o intervalo de confiança propriamente dito. O resultado do processo pode ser visto abaixo.\n\nic_media_caso1 &lt;- function(amostra, sigma, alfa) {\n  media &lt;- mean(amostra)\n  n &lt;- length(amostra)\n  z &lt;- round(qnorm(p = 1 - alfa/2), 3)\n  limite_inferior &lt;- round(media - z * sigma/sqrt(n), 3)\n  limite_superior &lt;- round(media + z * sigma/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n, \n      estimativa_pontual = media,\n      limite_inferior, \n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso1(amostra = amostra_ic_media, sigma = sqrt(sigma2), alfa = 0.05)\n\n   n estimativa_pontual limite_inferior limite_superior amplitude\n1 50              32.16          30.281          34.039     3.758\n\n\nPortanto, um intervalo de 95% de confiança para a média populacional das idades das gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período em estudo é de (28,401; 32,159). Intervalos de confiança podem, inclusive, ser utilizados como substitutos para testes de hipóteses, dos quais falaremos no próximo capítulo. Caso quiséssemos, por exemplo, testar a hipótese de que o valor de \\(\\mu\\) é igual a 27, rejeitaríamos a hipótese nula sob um nível de significância de 0,05, uma vez que 27 não está contido no intervalo de 95% de confiança para \\(\\mu\\) obtido acima.\n\n\n\n5.3.2 Intervalos de confiança para a média amostral quando a variância populacional é desconhecida\nConsidere, assim como no exemplo anterior, que \\(X_1, X_2, ..., X_n\\) formam uma amostra aleatória da população de idades de gestantes e puérperas hospitalizadas e falecidas em decorrência da COVID-19 no período de março de 2020 a dezembro de 2021, sendo \\(X_i\\) a variável aleatória que representa a idade da \\(i\\)-ésima gestante ou puérpera sorteada, com \\(i = 1, 2, ..., n\\). Suponha que queiramos, novamente, estimar o valor de \\(\\mu\\), mas que agora o valor da variância populacional \\(\\sigma^2\\) é desconhecido. Ainda com a restrição de que o tamanho da amostra deve ser suficientemente grande para que possamos aplicar o Teorema Central do Limite, sabemos, como visto anteriormente, que\n\\[\nZ  = \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{\\sigma} \\sim N(0, 1).\n\\]\nLembre-se, entretanto, que estamos no caso em que \\(\\sigma^2\\) é desconhecido, e por isso não podemos utilizar a variável acima para construirmos o intervalo de confiança, uma vez que o intervalo encontrado seria função de \\(\\sigma\\). Dessa forma, é intuitivo que em algum momento utilizemos um estimador da variância populacional na expressão do intervalo. Por sorte, existe um conhecido resultado dentro da Estatística que fornecerá uma função que nos seja conveniente. Definindo, primeiramente, o estimador da variância amostral, denotado por \\(S^2\\), como sendo\n\\[\nS^2 = \\frac{\\sum_{i = 1}^n (X_i - \\bar{X})}{n - 1},\n\\] sabemos ser válido que\n\\[\nT = \\frac{\\sqrt{n} \\left(\\bar{X} - \\mu \\right)}{S} \\sim t_{n - 1},\n\\] onde \\(S\\) é a raiz quadrada de \\(S^2\\) e \\(t_{n - 1}\\) representa a distribuição de probabilidade T de Student com \\(n - 1\\) graus de liberdade. A demonstração desse resultado pode ser consultada no post do Observatório que referenciamos no início da seção. A partir daqui, o processo para obtermos a expressão do intervalo de confiança será muito semelhante ao que realizamos no exemplo anterior. Como a distribuição da variável aleatória \\(T\\) é totalmente conhecida, uma vez que, em uma situação prática, o valor do tamanho da amostra \\(n\\) estaria definido, podemos encontrar valores \\(t_{1_{(n-1)}}\\) e \\(t_{2_{(n-1)}}\\), com \\(t_{1_{(n-1)}} &lt; t_{2_{(n-1)}}\\), tais que\n\\[\nP\\left(t_{1_{(n-1)}} &lt; T &lt; t_{2_{(n-1)}} \\right) = 1 - \\alpha. \\qquad \\qquad \\text{(II)}\n\\]\nComo a distribuição T de Student, assim como a distribuição normal padrão, é simétrica em torno de zero, os valores de \\(t_1\\) e \\(t_2\\) que geram a menor amplitude possível para o intervalo de confiança serão dados por\n\\[\nt_{1_{(n-1)}} = -t_{(n - 1;\\;1 -\\alpha/2)} \\text{ e } t_{2_{(n-1)}} = t_{(n - 1;\\;1 -\\alpha/2)}.\n\\]\nEm outras palavras, para encontrarmos os valores desses pontos, basta que calculemos o quantil de ordem \\(1 - \\alpha/2\\) da distribuição T de Student com \\(n - 1\\) graus de liberdade. Voltando à probabilidade definida em \\(\\text{(II)}\\), a atualizando com os resultados obtidos e reescrevendo \\(T\\), temos:\n\\[\nP\\left(t_{1_{(n-1)}} &lt; T &lt; t_{2_{(n-1)}} \\right) = P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; -t_{(n - 1;\\;1 -\\alpha/2)} \\right) = 1 - \\alpha.\n\\]\nComo queremos obter um intervalo de confiança para \\(\\mu\\), precisamos isolá-lo na expressão acima, a saber:\n\\[\n\\begin{align}\n& P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} &lt; \\frac{\\sqrt{n}\\left(\\bar{X} - \\mu\\right)}{S} &lt; t_{(n - 1;\\;1 -\\alpha/2)} \\right) \\\\ &\n= P\\left(-t_{(n - 1;\\;1 -\\alpha/2)} S &lt; \\sqrt{n}\\left(\\bar{X} - \\mu\\right) &lt; t_{(n - 1;\\;1 -\\alpha/2)} S\\right) \\\\ & =\nP\\left(-t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; \\bar{X} - \\mu &lt; t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ & =\nP\\left(-\\bar{X} + -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; - \\mu &lt; -\\bar{X} + t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) \\\\ &\n= P\\left(\\bar{X} - t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} &lt; \\mu &lt; \\bar{X} - -t_{(n - 1;\\;1 -\\alpha/2)} \\frac{S}{\\sqrt{n}} \\right) = 1 - \\alpha\n\\end{align}\n\\]\nPortanto, quando a variância populacional é desconhecida, um intervalo de confiança para \\(\\mu\\), com coeficiente de confiança de \\(1 - \\alpha\\), é dado por\n\\[\nIC(\\mu,\\ 1 - \\alpha) = \\left(\\bar{X} - t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}}; \\bar{X} + t_{(n-1;\\;1 - \\alpha/2)} \\frac{S}{\\sqrt{n}} \\right).\n\\]\nAlgo interessante a se observar é que, tanto o intervalo aleatório acima, quanto o intervalo aleatório encontrado no exemplo anterior, possuem probabilidade 1 - \\(\\alpha\\) de conterem o verdadeiro valor do parâmetro \\(\\mu\\), mesmo que suas estruturas sejam diferentes. Entretanto, como na construção do intervalo apresentado no presente exemplo foi utilizado um estimador de \\(\\sigma\\), a amplitude média dos intervalos de confiança obtidos por esse método será maior do que a amplitude média dos intervalos obtidos quando utilizamos o método apresentado no exemplo anterior. Com isso, dizemos que o intervalo de confiança acima é menos informativo, uma vez que o intervalo de valores plausíveis para \\(\\mu\\), nesse caso, será maior que o do intervalo de confiança construído anteriormente. Esse problema é mais evidente para tamanhos pequenos de amostra, e torna-se menos relevante conforme o valor de \\(n\\) aumenta.\n\n5.3.2.1 Um exemplo numérico\nPara demonstrar a diferença entre o intervalo de confiança derivado acima e aquele derivado no exemplo anterior, podemos aproveitar o mesmo exemplo sobre a média das idades das gestantes e puérperas previamente discutido. Novamente, criaremos uma função, a qual chamaremos de ic_media_caso2(), que retornará o intervalo de confiança para a média populacional quando a variância populacional é desconhecida e \\(\\bar{X}\\) segue distribuição normal (ou aproximadamente normal). A função se dará de forma similar àquela criada anteriormente, sendo as únicas diferenças o cálculo do desvio padrão amostral, feito com a função sd(), do pacote básico {stats} e a obtenção do quantil da distribuição T de Student através da função qt(), do mesmo pacote. O resultado do processo pode ser visto abaixo. Lembre-se que o vetor amostra_ic_media foi criado no exemplo anterior.\n\nic_media_caso2 &lt;- function(dados, alfa) {\n  media &lt;- mean(dados)\n  n &lt;- length(dados)\n  S &lt;- round(sd(dados), 3)\n  t &lt;- qt(1 - alfa/2, n - 1)\n  limite_inferior &lt;- round(media - t * S/sqrt(n), 3)\n  limite_superior &lt;- round(media + t * S/sqrt(n), 3)\n  amplitude &lt;- limite_superior - limite_inferior\n  return(\n    data.frame(\n      n,\n      S, \n      estimativa_pontual = media,\n      limite_inferior,\n      limite_superior,\n      amplitude)\n    )\n}\n\nic_media_caso2(dados = amostra_ic_media, alfa = 0.05)\n\n   n     S estimativa_pontual limite_inferior limite_superior amplitude\n1 50 6.634              32.16          30.275          34.045      3.77\n\n\nComo podemos perceber, o intervalo de confiança obtido acima apresenta uma amplitude levemente maior do que o intervalo obtido anteriormente (3,77 contra 3,758). Dessa forma, apesar de a diferença não ser gritante, podemos perceber que a utilização de uma estimativa para o valor da variância populacional acarretou, como já explicado anteriormente, em um intervalo levemente menos informativo a respeito do verdadeiro valor da média populacional. Essa diferença na amplitude, entretanto, não é regra: poderíamos ter coletado uma amostra em que a amplitude do intervalo de confiança construído pelo presente método fosse menor. Além disso, como o tamanho da amostra é grande o suficiente, a diferença nas amplitudes dos dois intervalos passa a ser praticamente desconsiderável. Com isso, aqui encerramos a construção de intervalos de confiança para a média de uma população. Partamos, agora, para a última seção desse capítulo, na qual trataremos da obtenção de intervalos de confiança para a proporção populacional.\n\n\n\n5.3.3 Intervalos de confiança para a proporção populacional\nA fazer"
  },
  {
    "objectID": "testes_np.html#grupos-não-pareados",
    "href": "testes_np.html#grupos-não-pareados",
    "title": "6  Testes não-paramétricos",
    "section": "6.1 Grupos não-pareados",
    "text": "6.1 Grupos não-pareados\nPara grupos não-pareados ou independentes, dentre os testes mais conhecidos e utilizados temos os testes de Mann-Whitney e Kolmogorov-Smirnov para dois grupos, e o de Kruskal-Wallis para três ou mais grupos.\n\n6.1.1 Teste de Mann-Whitney\nO teste de Mann-Whitney é um teste estatístico não paramétrico usado para comparar dois grupos independentes, sendo o grau de medida pelo menos ordinal para variável de interesse e nominal para variável independente. Além disso, o teste é geralmente usado quando a variável resposta ou de interesse não é normalmente distribuída, sendo assim, uma alternativa não-paramétrica ao teste t para amostras independentes.\nQuando o resultado do teste for significativo, significa que ambos os grupos representam populações com distribuições diferentes. Caso a suposição de que ambas as distribuições são iguais seja feita, então podemos interpretar o teste como uma comparação de medianas, que ao ser significativo dizemos que as medianas são significativamente diferentes. Assim, considerando dois grupos independentes, as hipóteses podem ser definidas como:\n\\(H_0:\\) distribuição\\(_1=\\) distribuição\\(_2\\)\n\\(H_{\\mathrm{a}}:\\) distribuição\\(_1 \\neq\\) distribuição\\({ }_2\\)\nSuponha que queremos comparar as idades (anos) de gestantes hospitalizadas com Covid-19 de dois hospitais diferentes. Considere as idades coletadas nos seguintes grupos:\nHospital A: \\(\\quad 46,48,34,36,22,44,50\\)\nHospital B: \\(\\quad 25,21,39,14,37\\)\nConfirmado que o teste de Mann-Whitney é apropriado, o primeiro passo que devemos tomar, é ordenar os valores de ambos os grupos de forma crescente em um único grupo, e então, identificar o posto de cada valor desse grupo. Para o exemplo abordado, teremos o seguinte:\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & 14 & 21 & \\color{red}{22} & 25 & \\color{red}{34} & \\color{red}{36} & 37 & 39 & \\color{red}{44} & \\color{red}{46} & \\color{red}{48} & \\color{red}{50}\\\\\n\\hline \\text { Posto } & 1 & 2 & \\color{red}{3} & 4 & \\color{red}{5} & \\color{red}{6} & 7 & 8 & \\color{red}{9} & \\color{red}{10} & \\color{red}{11} & \\color{red}{12}\\\\\n\\hline\n\\end{array}\\]\nOnde está destacado em vermelho as idades das gestantes hospitalizadas no Hospital A e seus respectivos postos. Além disso, destacado em preto essa mesma informação para as gestantes hospitalizadas no Hospital B.\n\n6.1.1.1 Estatística \\(U\\)\nO teste de Mann-Whitney também é chamado de teste \\(U\\) de Mann-Whitney, pois calculamos o que chamamos de estatística \\(U\\), a qual é uma quantidade baseada na soma dos postos identificados de cada grupo calculada para fazer a avaliação das hipóteses.\nAgora, considere \\(n_1\\) e \\(n_2\\) como o número de observações do grupo 1 e grupo 2 respectivamente, em que, o grupo 1 será aquele com o menor número de observações (caso ambos os grupos tenham a mesma quantidade de observações, então \\(n_1 = n_2\\)). Além disso, considere \\(R_1\\) e \\(R_2\\) como a soma dos postos identificados nos grupos 1 e 2 respectivamente, e \\(N = n_1 + n_2\\) como o total de observações considerando ambos os grupos.\nDefinido os termos necessários e considerando o exemplo apresentado, calculamos as seguintes quantidades:\n\\(U_1=n_1n_2+\\displaystyle \\frac{n_1\\left(n_1+1\\right)}{2}-R_1=5\\cdot7+\\displaystyle \\frac{5 \\cdot\\left(5+1\\right)}{2}-22 = 28\\),\n\\(U_2=n_1 n_2+\\displaystyle \\frac{n_2\\left(n_2+1\\right)}{2}-R_2=5 \\cdot 7+\\displaystyle \\frac{7 \\cdot\\left(7+1\\right)}{2}-56 = 7\\).\nTendo calculado \\(U_1\\) e \\(U_2\\) a estatística \\(U\\) será o menor valor entre ambas, neste caso, \\(U = U_2\\). Podemos confirmar se todo procedimento foi feito corretamente ao verificar a relação \\(n_1n_2 = U_1 + U_2\\), a qual será \\(5\\cdot 7 = 28 + 7 = 35\\) no exemplo considerado.\n\n\n6.1.1.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos calcular a quantidade \\(U_c\\), o qual é o valor crítico de \\(U\\) necessário para decidir em relação às hipóteses. Este valor pode ser encontrado em tabelas de valores críticos da estatística U bilateral (ou unilateral a depender das hipóteses), ou podemos utilizar a função qwilcox() do R e subtrair o resultado por 1, para obtê-lo no caso bilateral. Para o exemplo dos hospitais temos \\(n_1 = 5\\) e \\(n_2 = 7\\), considerando nível de 5% de significância, podemos utilizar a função qwilcox() para encontrar \\(U_c\\) da seguinte forma:\n\nqwilcox(0.05/2,5,7) - 1\n\n[1] 5\n\n\nOnde \\(0.05/2\\) é o nível de significância dividido por dois devido ao teste ser bilateral, e subtraímos por \\(1\\) como uma correção matemática por \\(U\\) ser discreto e qwilcox() calcular quantis, e não valores críticos de \\(U\\). Assim, \\(U_c = 5\\) no exemplo considerado.\nAssim, rejeitamos a hipótese nula quando o valor da estatística \\(U\\) calculada for menor ou igual ao valor crítico de \\(U\\), ou seja, quando \\(U \\leq U_c\\). Com os valores calculados de \\(U = 7\\) e \\(U_c = 5\\), podemos concluir ao nível de 5% de significância que não há diferença significativa entre as distribuições das idades das gestantes hospitalizadas em ambos hospitais.\n\n\n6.1.1.3 Empates\nDependendo do problema, pode existir valores repetidos observados, e ao ordená-los para identificar os postos, esses valores são considerados empates.\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & 14 & 21 & \\color{red}{25} & \\color{red}{25} & \\color{red}{34} & \\color{red}{36} & 37 & 39 & \\color{red}{44} & \\color{red}{46} & \\color{red}{48} & \\color{red}{50}\\\\\n\\hline \\text { Posto } & 1 & 2 & \\color{red}{3,5} & \\color{red}{3,5} & \\color{red}{5} & \\color{red}{6} & 7 & 8 & \\color{red}{9} & \\color{red}{10} & \\color{red}{11} & \\color{red}{12}\\\\\n\\hline\n\\end{array}\\]\nObserve nesse exemplo que há um empate para idade de 25 anos. Nesse caso, ao ordenar os valores teríamos os postos 3 e 4 para esses valores repetidos, mas devemos atribuir a média desses postos para dar continuidade ao teste. Assim, temos \\(\\displaystyle \\frac{3 + 4}{2}= 3,5\\).\n\n\n6.1.1.4 Estatística \\(U\\) normalizada\nÉ tido como um consenso a utilização dos procedimentos apresentados até então sobre o teste Mann-Whitney no cenário em que \\(n_1\\) e \\(n_2\\) são menores ou iguais que 20. Por outro lado, no cenário em que \\(n_1\\) e \\(n_2\\) são grandes (\\(&gt;20\\)), podemos aproximar a distribuição de \\(U\\) para uma normal. Assim, calculamos a estatística \\(Z_u\\) para avaliar as hipóteses da seguinte forma:\n\\[Z_u = \\displaystyle \\frac{U-\\displaystyle\\frac{n_1  n_2}{2}}{\\sqrt{\\displaystyle\\frac{n_1 n_2\\left(n_1+n_2+1\\right)}{12}}}.\\]\nEsse procedimento nada mais é do que subtrair a média de \\(U\\) e dividir pelo seu desvio padrão.\nAssim, para avaliar as hipóteses, é necessário obter o valor crítico de \\(Z_u\\), o qual pode ser obtido a partir de tabelas de valores críticos da distribuição normal padrão, ou podemos recorrer à função qnorm() no R com os seguintes argumentos: qnorm(0.05/2, lower.tail = FALSE) para teste bilateral e nível de 5% de significância; qnorm(0.05, lower.tail = FALSE) para teste unilateral e nível de 5% de significância. Os valores críticos de \\(Z_u\\) para o teste bilateral e unilateral ao nível de 5% de significância são 1,96 e 1,65 respectivamente. Assim, rejeitamos a hipótese nula quando \\(|Z_u|\\geq1,96\\) e \\(|Z_u|\\geq1,65\\) no teste bilateral e unilateral respectivamente.\nCaso haver empates, a aproximação deve ser feita da seguinte forma:\n\\[Z_u = \\displaystyle \\frac{U-\\displaystyle\\frac{n_1  n_2}{2}}{\\sqrt{\\displaystyle\\frac{n_1n_2\\left(n_1+n_2+1\\right)}{12}- \\displaystyle\\frac{n_1n_2\\left[\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\right]}{12  \\left(n_1+n_2\\right) \\left(n_1+n_2-1\\right)}}},\\]\nOnde, na quantidade \\(\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) o \\(s\\) indica quantos grupos de empates ocorreram, ex: idade 25 se repete duas vezes e idade 45 se repete cinco vezes, totalizando 2 grupos de empates ou de repetições. Além disso, o termo \\(t\\) é o número total de empates no grupo de repetições \\(i\\). Assim, para as repetições hipotéticas da idade 25 e 45 teremos:\n\\[\\sum_{i=1}^2\\left(t_i^3-t_i\\right) = (2^3 - 2) + (5^3 - 5) = 126.\\]\nDessa forma, podemos calcular \\(Z_u\\) e então avaliar as hipóteses da mesma forma que avaliamos no cenário sem empates.\n\n\n6.1.1.5 Como aplicar o teste no R\nNa prática, tendo os dados em um formato de planilha, podemos carregá-los no software R e então aplicar o teste recorrendo a uma função específica, a qual irá efetuar os procedimentos formalmente explicados ao longo do texto e retornar um valor-p, o qual podemos utilizar para avaliar as hipóteses, onde, rejeitamos a hipótese nula caso valor-p \\(\\leq 0,05\\), sendo 5% de nível de significância escolhido. Podemos usar a seguinte estrutura:\n\nstats::wilcox.test(x, y, correct = FALSE,  alternative = \"two.sided\")\n\nOs argumentos x e y são os vetores dos valores dos grupos a serem comparados. O argumento correct indica se o usuário quer que seja feito uma correção de continuidade do valor-p no caso de aproximação normal da estatística \\(U\\), o que geralmente não é feito, a menos que a estatística \\(Z_u\\) esteja muito próxima do valor crítico utilizado. E por fim, o argumento alternative permite escolher o formato do teste de hipóteses desejado.\n\n\n\n6.1.2 Teste de Kolmogorov-Smirnov\nAssim como o teste de Mann-Whitney, o teste de Kolmogorov-Smirnov é uma alternativa não paramétrica ao teste t para grupos independentes quando há a suspeita de não normalidade da variável de interesse. O teste possui grau de medida pelo menos ordinal, e é sensível a qualquer diferença de tendência central, dispersão, simetria ou curtose das distribuições.\nVisa verificar se ambos os grupos originaram-se de populações com a mesma distribuição. Para isso, é calculado a estatística de teste \\(D\\) a partir das distribuições de frequências acumuladas dos grupos, e então, utiliza-se essa estatística \\(D\\) para avaliar as hipóteses. Quanto maior a estatística, maior será a evidência para rejeitar a hipótese nula.\nRetornando ao exemplo das idades de gestantes hospitalizadas com Covid-19 em dois diferentes hospitais, as hipóteses para o teste bilateral terão a seguinte forma:\n\\(H_0:\\) distribuição\\(_1=\\) distribuição\\(_2\\)\n\\(H_{\\mathrm{a}}:\\) distribuição\\(_1 \\neq\\) distribuição\\({ }_2\\)\nSão hipóteses com formas idênticas as do teste de Mann-Whitney. Porém, embora tenham o mesmo objetivo de comparar as distribuições para ambos os grupos, o método difere consideravelmente. Além disso, o teste de Mann-Whitney é sensível em relação à mediana, enquanto o teste de Kolmogorov-Smirnov é sensível em relação aos aspectos gerais das distribuições sendo testadas como foi discutido anteriormente.\nEmbora seja o mesmo exemplo, serão considerados valores diferentes por motivos didáticos:\nHospital A: \\(\\quad 22,22,25,27,29\\)\nHospital B: \\(\\quad 27,31,34,37,44\\)\nConsiderando as notações introduzidas no desenvolvimento do teste de Mann-Whitney, os tamanhos dos grupos são \\(n_1 = 5\\) e \\(n_2 = 5\\). Vale lembrar que, usualmente, denotamos \\(n_1\\) como sendo o tamanho do menor grupo, chamado de grupo 1. Como neste exemplo ambos possuem o mesmo tamanho, o grupo 1 sera composto dos valores das idades referentes as gestantes hospitalizadas no Hospital A.\n\n6.1.2.1 Estatística \\(D\\)\nPrecisamos calcular a estatística \\(D\\) para avaliar as hipóteses. Essa estatística nada mais é do que o maior valor que encontrarmos ao realizar a diferença entre as proporções de frequências relativas acumuladas dos dois grupos. Para isso, construímos a seguinte tabela:\n\\[\\begin{array}{ccccc}\n\\hline\n\\text{Idades}_1 & S_1 & \\text{Idades}_2 & S_2 & S_1-S_2 \\\\\n\\hline 22;22 & 2 / 5=0,40 & - & 0 & 0,40-0=0,40 \\\\\n25 & 3 / 5=0,60 & - & 0 & 0,60-0=0,60 \\\\\n27 & 4 / 5=0,80 & 27 & 1 / 5=0,20 & 0,80-0,20=0,60 \\\\\n29 & 5 / 5=1,00 & - & 1 / 5=0,20 & 1,00-0,20=\\color{red}{0,80} \\\\\n- & 5 / 5=1,00 & 31 & 2 / 5=0,40 & 1,00-0,40=0,60 \\\\\n- & 5 / 5=1,00 & 34 & 3 / 5=0,60 & 1,00-0,60=0,40 \\\\\n- & 5 / 5=1,00 & 37 & 4 / 5=0,80 & 1,00-0,80=0,20 \\\\\n- & 5 / 5=1,00 & 44 & 5 / 5=1.00 & 1,00-1,00=0,00 \\\\\n\\hline\n\\end{array}\\]\nAs colunas Idades1 e Idades2 são referentes aos valores do grupo 1 (usualmente o menor grupo, o que não é o caso, pois \\(n_1 = n_2\\)) e grupo 2 respectivamente. Porém, note que existe um valor repetido (Idades1 \\(= 22\\)) na primeira coluna, quando isso ocorre todas as repetições desse mesmo valor permanecem na mesma linha. Além disso, observe que na primeira linha e coluna Idades2 foi atribuído um símbolo de \\(-\\), isso ocorre, pois não há um valor de Idades2 \\(= 22\\) assim como na coluna Idades1. O mesmo ocorre na quinta linha e coluna Idades1, pois nessa coluna não há um valor de Idades1 \\(= 31\\) assim como ocorre na coluna Idades2, por exemplo. Para mais, a tabela é construída com os valores de ambos os grupos ordenados de forma crescente no decorrer das linhas.\nAs colunas \\(S_1\\) e \\(S_2\\) são referentes as proporções das frequências relativas acumuladas das colunas Idades1 e Idades2 respectivamente. Note que, na primeira linha e coluna Idades1 da tabela, temos uma frequência relativa de \\(\\displaystyle \\frac{2}{5}\\) para Idades1 \\(= 22\\) devido a esse valor repetir uma vez. Na segunda linha temos uma frequência relativa de \\(\\displaystyle \\frac{1}{5}\\) para Idades1 \\(= 25\\), pois esse valor aparece uma única vez, que ao acumular com a frequência da linha anterior obtemos \\(\\displaystyle \\frac{2}{5} + \\displaystyle \\frac{1}{5} = \\displaystyle \\frac{3}{5}\\). Na terceira e quarta linha da tabela os valores da coluna Idades1 também não se repetem, o que faz com que tenham uma frequência relativa de \\(\\displaystyle \\frac{1}{5}\\) que ao acumular com as frequências anteriores resultam em \\(\\displaystyle \\frac{4}{5}\\) e \\(\\displaystyle \\frac{5}{5}\\). Após a quarta linha não há mais valores para a coluna Idades1, portanto, a proporção das frequências relativas se mantêm em \\(\\displaystyle \\frac{5}{5} = 1\\). Além disso, na primeira e segunda linha as frequências relativas referentes a coluna Idades2 são zeradas, pois não há valores para essa coluna nessas linhas, e por serem as primeiras linhas não há nenhuma frequência a ser acumulada. Isso muda a partir da terceira linha, onde a mesma lógica utilizada para construir as frequências relativas \\(S_1\\) pode ser usada na construção de \\(S_2\\).\nPor fim, basta calcular os valores de \\(S_1 - S_2\\) para todas as linhas da tabela. A estatística \\(D\\) será o maior dentre esses valores, sendo \\(D = 0,80\\) no exemplo abordado.\n\n\n6.1.2.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos obter o valor crítico \\(D_c\\), o qual é um valor tabelado que pode ser verificado em Sheskin (2003) fazendo uso apenas das quantidades \\(n_1\\), \\(n_2\\) e o nível de significância escolhido. Para \\(n_1 = 5\\), \\(n_2 = 5\\) e o nível de significância de 5%, temos que \\(D_c = 0,80\\). Assim, em um teste de hipóteses bilateral rejeitamos a hipótese nula quando o valor absoluto da estatística \\(D\\) for maior ou igual que o valor crítico, ou seja, quando \\(|D| \\geq D_c\\). No exemplo, como \\(D = 0,80\\) e \\(D_c = 0,80\\), poderíamos concluir que há evidências de que ambos os grupos se originaram de populações com distribuições diferentes.\nEmbora não seja muito comum, testes de hipóteses unilaterais podem ser empregados:\n\nCaso a hipótese alternativa seja \\(H_{\\mathrm{a}}:\\) distribuição\\(_1 &gt;\\) distribuição\\({ }_2\\), então rejeita-se a hipótese nula quando \\(|D| \\geq D_c\\), onde \\(D_c\\) é o valor crítico tabelado para o teste de Kolmogorov-Smirnov unilateral, que para o exemplo temos \\(D_c = 0,60\\) ao nível de 5% de significância. Além disso, para os dados serem consistentes com o teste, a proporção da frequência acumulada referente ao grupo 1 deve ser maior que a do grupo 2 no ponto em que a estatística de teste \\(D\\) foi identificada, ou seja, precisamos ter \\(S_1 &gt; S_2\\) na quarta linha da tabela do exemplo considerado para este teste unilateral ser utilizado. Assim, esse formato de hipótese é aplicável ao problema dado que as condições são satisfeitas, e poderíamos chegar nas mesmas conclusões que no teste bilateral.\nCaso a hipótese alternativa seja \\(H_{\\mathrm{a}}:\\) distribuição\\(_1 &lt;\\) distribuição\\({ }_2\\), o procedimento de avaliação da hipótese é o mesmo dos casos anteriores. Porém, para ser utilizado temos que respeitar a condição \\(S_1 &lt; S_2\\) na linha da tabela referente a estatística \\(D\\), o que não ocorre no exemplo considerado, o que faz com que esse formato de hipótese seja inapropriado.\n\n\n\n6.1.2.3 Como aplicar o teste no R\nÉ fácil notar que com valores grandes de \\(n_1\\) e \\(n_2\\) a construção da tabela para encontrar a estatística \\(D\\) se torna trabalhosa sem o auxílio computacional. Agora que a ideia de como o teste é construído foi formalmente explicada, podemos verificar como utilizá-lo no software R através de uma função específica. Segue abaixo a estrutura para efetuar o teste de Kolmogorov-Smirnov no R:\n\nstats::ks.test(x, y, alternative = \"two.sided\")\n\nOs argumentos x e y são os vetores dos valores dos grupos a serem comparados, enquanto o argumento alternative permite escolher o formato do teste de hipóteses desejado. Ao executar a função, ela irá retornar a estatística de teste e o valor-p, os quais podemos utilizar para avaliar as hipóteses, onde, rejeitamos a hipótese nula caso valor-p \\(\\leq 0,05\\), sendo 5% de nível de significância escolhido.\nAlém disso, podemos utilizar a mesma função para efetuar o teste de Kolmogorov-Smirnov para um grupo. A lógica de construção do teste para um grupo é similar a construção do teste para dois, a principal diferença é de que em vez de comparar as distribuições para dois grupos, estaremos comparando a distribuição de um único grupo com uma distribuição hipotética ou teórica. A estrutura de aplicação no R será a seguinte:\n\nstats::ks.test(x, \"pnorm\", alternative = \"two.sided\")\n\nOs argumentos x e alternative são os mesmos, a diferença é que em vez de utilizar o argumento y utilizamos o argumento \"pnorm\", o qual estamos especificando que queremos comparar a distribuição da população que deu origem ao grupo com a distribuição empírica normal.\n\n\n\n6.1.3 Teste de Kruskal-Wallis\nO teste de Kruskal-Wallis é uma extensão do teste de Mann-Whitney em que é possível contruí-lo utilizando a lógica de postos dos valores ordenados dos grupos do estudo, e comparar a distribuição desses grupos, os quais, nesse caso, podem ser mais do que dois. Além disso, é um teste sensível à mediana como o teste de Mann-Whitney, e requer que a variável de interesse seja no mínimo ordinal. Como os grupos precisam ser independentes e não é necessário assumir normalidade, o teste de Kruskal-Wallis é uma alternativa não-paramétrica a análise de variância de um fator (One-way ANOVA).\nConsidere \\(k\\) como sendo o número de grupos no estudo. Quando \\(k = 2\\), o teste de Kruskal-Wallis terá resultados equivalentes aos do teste de Mann-Whitney. Além disso, o teste verifica se as distribuições das populações das quais os grupos foram amostrados são iguais, mas caso a suposição de que a forma dessas distribuições é a mesma for feita, então, o teste pode ser interpretado com base na mediana. Assim, considerando \\(k\\) grupos independentes, as hipóteses podem ser definidas como:\n\\(H_0:\\) Todos os \\(k\\) grupos originam-se de populações com distribuições idênticas.\n\\(H_{\\mathrm{a}}:\\) Pelo menos 2 grupos originam-se de populações com distribuições diferentes.\nRetornaremos ao exemplo que considera a idade(anos) das gestantes hospitalizadas com Covid-19. Considere \\(n_1 = 5\\), \\(n_2 = 5\\) e \\(n_3 = 5\\) como a quantidade de gestantes de três hospitais diferentes dos quais foram registradas as idades para o estudo. Os grupos podem ser vistos abaixo:\nHospital A: \\(\\quad 27,24,18,18,25\\)\nHospital B: \\(\\quad 43,39,31,36,41\\)\nHospital C: \\(\\quad 25,27,29,24,24\\)\nAssim como feito na construção do teste de Mann-Whitney, deve-se considerar todas as idades registradas como se fossem de um único grupo e então ordená-las para que os postos de cada registro seja identificado. No caso de empates, o posto atribuído a cada valor repetido será a média dos postos desses valores inicialmente identificados, assim como feito para o teste de Mann-Whitney.\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Idades } & \\color{red}{18} & \\color{red}{18} & \\color{red}{24} & 24 & 24 & \\color{red}{25} & 25 & \\color{red}{27} & 27 & 29 & \\color{blue}{31} & \\color{blue}{36} & \\color{blue}{39} & \\color{blue}{41} & \\color{blue}{43}\\\\\n\\hline \\text { Posição} & \\color{red}{1} & \\color{red}{2} & \\color{red}{3} & 4 & 5 & \\color{red}{6} & 7 & \\color{red}{8} & 9 & 10 & \\color{blue}{11} & \\color{blue}{12} & \\color{blue}{13} & \\color{blue}{14} & \\color{blue}{15}\\\\\n\\hline \\text { Posto } & \\color{red}{1,5} & \\color{red}{1,5} & \\color{red}{4} & 4 & 4 & \\color{red}{6,5} & 6,5 & \\color{red}{8,5} & 8,5 & 10 & \\color{blue}{11} & \\color{blue}{12} & \\color{blue}{13} & \\color{blue}{14} & \\color{blue}{15}\\\\\n\\hline\n\\end{array}\\]\nIdentificado os postos de cada observação, é possível obter algumas quantidades de interesse para a construção do teste. Considere \\(R_1 = 22\\), \\(R_2 = 65\\) e \\(R_3 = 33\\) como sendo a soma dos postos do grupo 1 (verde), grupo 2 (vermelho) e grupo 3 (preto) respectivamente. Além disso, considere \\(N = n_1 + n_2 + n_3\\) como o total de observações registradas, que para o exemplo abordado é de \\(N = 15\\).\n\n6.1.3.1 Estatística \\(H\\)\nPara avaliar as hipóteses é necessário calcular a estatística de teste \\(H\\), a qual é obtida a partir de uma aproximação pela distribuição qui-quadrado e possui a seguinte forma:\n\\[H=\\displaystyle \\frac{12}{N(N+1)} \\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right]-3(N+1).\\]\nNote que a quantidade \\(\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right]\\) indica, para cada grupo, a soma da razão entre o quadrado da soma dos postos e o número de observações, ou seja:\n\\[\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right] = \\displaystyle \\frac{\\left(R_1\\right)^2}{n_1} + \\displaystyle \\frac{\\left(R_2\\right)^2}{n_2} + \\displaystyle \\frac{\\left(R_3\\right)^2}{n_3} = \\displaystyle \\frac{\\left(22\\right)^2}{5} + \\displaystyle \\frac{\\left(65\\right)^2}{5} + \\displaystyle \\frac{\\left(33\\right)^2}{5} = 1159,6.\\]\nAssim, sabendo que \\(N = 15\\) e \\(\\sum_{j=1}^k\\left[\\displaystyle \\frac{\\left(R_j\\right)^2}{n_j}\\right] = 1159,6\\) obtemos a estatística \\(H = 9,98\\).\n\n\n6.1.3.2 Avaliação das hipóteses\nTendo calculado a estatística \\(H\\), basta obtermos o valor crítico \\(H_c\\) para avaliar as hipóteses. O valor crítico \\(H_c\\) pode ser obtido a partir da distribuição qui-quadrado com \\(df = K-1\\) graus de liberdade quando há pelo menos cinco observações em cada grupo. Os valores críticos de uma qui-quadrado são quantidades tabeladas, assim, podemos acessá-los diretamente de uma tabela de pontos críticos dessa distribuição, ou, a partir da função qchisq() no r, sendo necessário especificar o grau de liberdade df e o nível de significância p com a estrutura qchisq(p = 0.05, df = 2, lower.tail = FALSE), considerando o exemplo abordado. Quando existem menos que cinco observações nos grupos, o valor de \\(H_c\\) pode ser obtido a partir de tabelas de valores exatos. Esses valores tabelados podem ser verificados em Sheskin (2003) ou pelo endereço www.dataanalytics.org.uk/critical-values.\nConsiderando o nível de 5% de significância e grau de liberdade \\(df = 2\\), o valor crítico para o exemplo é de \\(H_c = 5,99\\). Além disso, rejeitamos a hipótese nula quando \\(H \\geq H_c\\), como o valor da estatística de teste \\(H\\) é de fato maior que o valor crítico \\(H_c\\), podemos concluir que há evidências de que pelo menos dois dos três grupos originam-se de populações com distribuições diferentes.\nCaso haja um número excessivo de empates, é recomendado utilizar uma correção para a estatística \\(H\\) que considere essas repetições. A seguinte quantidade deve ser calculada:\n\\[C=1-\\displaystyle \\frac{\\sum_{i=1}^s\\left(t_i^3-t_i\\right)}{N^3-N}.\\]\nNote que \\(\\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) é a mesma quantidade utilizada para incluir a informação de empates ao calcular a estatística \\(U\\) normalizada no teste de Mann-Whitney, em que \\(s\\) indica quantos grupos de empates ocorreram e \\(t\\) o número total de empates no grupo. No exemplo sendo considerado temos \\(s = 4\\) grupos de empates, sendo duas repetições do valor 18, três do valor 24, duas do valor 25 e duas do valor 27, ou seja, \\(t_1 = 2\\), \\(t_2 = 3\\), \\(t_3 = 2\\) e \\(t_4 = 2\\) respectivamente. Assim, para calcular \\(C\\) teremos:\n\\[\\sum_{i=1}^s\\left(t_i^3-t_i\\right) = (2^3-2) + (3^3-3) + (2^3-2) + (2^3-2) = 42.\\]\nSabendo que \\(N = 15\\), o valor do fator de correção será \\(C = 0.987\\). Para obter a estatística \\(H\\) corrigida, basta dividir pelo valor de \\(C\\), onde teremos \\(\\displaystyle \\frac{H}{C} = 10,11\\). Assim, para o exemplo abordado, as conclusões serão as mesmas feitas anteriormente ao avaliar as hipóteses com a estatística sem correção.\n\n\n6.1.3.3 Como aplicar o teste no R\nO teste pode ser facilmente aplicado no R ao utilizar a função kruskal.test(), a qual irá retornar o valor da estatística \\(H\\) aproximada pela qui-quadrado, os graus de liberdade considerados e o valor-p. Assim, rejeita-se a hipótese nula quando valor-p \\(\\leq 0,05\\) considerando 5% de nível de significância. A função possui a seguinte estrutura:\n\nstats::kruskal.test(resposta ~ grupos, data = dados)\n\nO argumento resposta ~ grupos é a fórmula de entrada da função para aplicação do teste, em que resposta é a variável de interesse e grupos é a variável independente onde os grupos são especificados, sendo grupos uma variável que precisa ser tratada como factor dentro do R (ver apêndice). No exemplo abordado, teríamos idade ~ hospitais, por exemplo. O argumento data é onde precisamos especificar o nome da base de dados utilizada que possui as variáveis sendo incluídas na fórmula.\nCaso o teste de Kruskal-Wallis seja realizado, e como resultado, a hipótese nula seja rejeitada, podemos identificar quais pares de grupos apresentam diferentes distribuições de suas populações. O teste de Dunn, uma alternativa não-paramétrica ao teste de Tukey, pode ser usado para realizar comparações múltiplas entre os grupos. Para aplicá-lo no R, podemos instalar o pacote FSA e recorrer à função dunnTest(), a qual possui a seguinte estrutura:\n\nFSA::dunnTest(resposta ~ grupos, data = dados, method = \"bonferroni\")\n\nOs argumentos resposta ~ grupos e data são os mesmos utilizados na função kruskal.test(). O argumento method é utilizado para especificar o método de controle da taxa de erro experimental, sendo o bonferroni o mais usual. A função retorna uma matriz com valores-p ajustados pelo método bonferroni organizados em uma coluna chamada P.adj para o usuário avaliar cada par de grupos, sendo cada linha equivalente a cada par sendo comparado. Caso o valor-p ajustado for menor ou igual ao nível de significância (geralmente 5%), então, as distribuições das populações dos grupos sendo comparados são significativamente diferentes."
  },
  {
    "objectID": "testes_np.html#grupos-pareados",
    "href": "testes_np.html#grupos-pareados",
    "title": "6  Testes não-paramétricos",
    "section": "6.2 Grupos pareados",
    "text": "6.2 Grupos pareados\nGrupos ou amostras são considerados pareados quando as observações em um grupo estão relacionadas ou emparelhadas com as observações de outro grupo. Isso significa que cada observação em um grupo é emparelhada ou relacionada com uma observação correspondente no outro grupo.\nUm exemplo comum de grupos pareados é quando o mesmo grupo de indivíduos é medido duas ou mais vezes em diferentes momentos, o que chamamos de amostras antes e depois. Outro cenário, é quando temos grupos não necessariamente com os mesmos indivíduos avaliados em diferentes momentos, mas com indivíduos diferentes em que conseguimos pareá-los de acordo com características em comum. Um exemplo de pareamento entre indivíduos de dois grupos é quando há o interesse em estudar a eficácia de um novo tratamento médico, onde cada paciente no grupo de tratamento é emparelhado com um paciente no grupo de controle com características semelhantes, formando assim, pares de observações pareadas.\nDentre os testes não-paramétricos mais conhecidos e utilizados nestes cenários, temos os testes de Wilcoxon e McNemar para dois grupos, e o de Friedman para três ou mais grupos.\n\n6.2.1 Teste de Wilcoxon\nO teste de Wilcoxon é utilizado em problemas onde a variável de interesse pode ser ordenada (medida pelo menos ordinal) e há dois grupos dependentes a serem comparados. A hipótese a ser testada é se a mediana das diferenças dos valores pareados das populações que deram origem aos grupos é zero. O teste é usualmente aplicado quando as suposições sobre o teste t para dois grupos dependentes são violadas, principalmente em relação à normalidade dos dados, pois, pelo teste de Wilcoxon ser uma alternativa não-paramétrica ao teste t para duas amostras dependentes, as suposições requeridas são mais leves.\nO teste consiste em identificar as diferenças entre os valores pareados dos grupos, identificar os postos das diferenças absolutas, e então, atribuir os sinais originais das diferenças em cada posto (sinalização dos postos). Além disso, diferenças que resultaram em zero, não são consideradas na construção do teste.\nSejam as seguintes hipóteses:\n\\(H_0:\\) A mediana das diferenças dos valores pareados é igual a zero.\n\\(H_{\\mathrm{a}}:\\) A mediana das diferenças dos valores pareados difere de zero.\nConsidere que em um determinado hospital estava sendo reportado altos níveis de ansiedade em gestantes no terceiro trimestre de gravidez. Foi medido o nível de ansiedade de 10 gestantes antes e após participarem de um programa de atividades em grupo que durou duas semanas, onde, 0 indica sem ansiedade e 10 ansiedade severa. Suponha que as 10 gestantes foram selecionadas de forma aleatória, e que as diferenças dos pares de níveis de ansiedade (antes e depois) seguem uma distribuição simétrica em torno da mediana. Considere os seguintes dados:\n\\[\\begin{array}{llllllllllll}\n\\hline \\text { Indivíduo } & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\\\n\\hline \\text { Antes} & 8 & 9 & 8 & 7 & 6 & 4 & 6 & 6 & 7 & 3 \\\\\n\\hline \\text { Depois } & 2 & 6 & 7 & 5 & 4 & 5 & 6 & 3 & 2 & 5 \\\\\n\\hline\n\\end{array}\\]\nCada indivíduo possui um nível de ansiedade registrado antes e depois do programa de atividades em grupos. O grupo 1 é composto pelos níveis de ansiedade registrados antes das atividades, e o grupo 2 composto pelos níveis de ansiedade registrados depois das atividades.\nConsidere \\(D\\) como uma representação das diferenças. Assim, calculamos \\(D\\), o valor absoluto das diferenças \\(|D|\\), os postos de \\(|D|\\), e então, os postos de \\(|D|\\) sinalizados:\n\\[\\begin{array}{cccccc}\n\\hline \\text { Indivíduo } & \\text{Grupo} 1 & \\text{Grupo} 2 & D & |D| & \\text { Postos de }|D| & \\text { Postos sinalizados de }|D|\\\\\n\\hline\n\\mathbf{7} & 6 & 6 & 0 & - & - & -\\\\\n\\mathbf{6} & 4 & 5 & -1\\phantom{-} & 1 & 1,5 & -1,5\\phantom{-}\\\\\n\\mathbf{3} & 8 & 7 & 1 & 1 & 1,5 & 1,5\\\\\n\\mathbf{10} & 3 & 5 & -2\\phantom{-} & 2 & 4,0 & -4,0\\phantom{-}\\\\\n\\mathbf{4} & 7 & 5 & 2 & 2 & 4,0 & 4,0\\\\\n\\mathbf{5} & 6 & 4 & 2 & 2 & 4,0 & 4,0\\\\\n\\mathbf{2} & 9 & 6 & 3 & 3 & 6,5 & 6,5\\\\\n\\mathbf{8} & 6 & 3 & 3 & 3 & 6,5 & 6,5\\\\\n\\mathbf{9} & 7 & 2 & 5 & 5 & 8,0 & 8,0\\\\\n\\mathbf{1} & 8 & 2 & 6 & 6 & 9,0 & 9,0\\\\\n\\hline\n\\end{array}\\]\nNote que a tabela é construída ordenando a coluna \\(D\\) de forma crescente desconsiderando o sinal dos valores. Além disso, a primeira linha gerou um valor de \\(D = 0\\), o qual deve ser desconsiderado no teste, e então, os postos são identificados a partir da segunda linha. Para mais, note que houve empates nos valores de \\(|D|\\), para os quais os postos são identificados como a média dos postos caso não fossem empates, assim como é feito para os testes de Mann-Whitney e Kruskal-Wallis.\nO próximo passo é calcular a soma dos postos em que a sinalização de \\(|D|\\) foi negativa, o qual denotaremos de \\(R_{neg}\\). Além disso, devemos calcular a soma dos postos em que a sinalização de \\(|D|\\) foi positiva, denotada por \\(R_{pos}\\).\nPara o exemplo, temos \\(R_{neg} = 5,5\\) e \\(R_{pos} = 39,5\\). Além disso, podemos verificar a seguinte relação:\n\\[R_{neg} + R_{pos} =\\displaystyle \\frac{n(n+1)}{2}.\\]\nCaso essa relação não for verdadeira, é um indicativo de que houve erro nos cálculos. Como houve um par de valores entre os grupos cuja diferença foi zero, o número de pares considerados no teste para o exemplo abordado será de \\(n = 9\\). Dessa forma, teremos \\(\\displaystyle \\frac{n(n+1)}{2} = \\displaystyle \\frac{9(9+1)}{2} = 45\\) que corresponde ao valor de \\(R_{neg} + R_{pos} = 5,5 + 39,5 = 45.\\)\n\n6.2.1.1 Avaliação das hipóteses\nCaso os valores das diferenças originaram-se de uma população cuja mediana é zero, espera-se que os valores de \\(R_{neg}\\) e \\(R_{pos}\\) sejam equivalentes ao valor esperado da estatística de teste de Wilcoxon, a qual denotaremos de estatística \\(V\\). O valor esperado possui a seguinte forma:\n\\[\\displaystyle \\frac{n(n+1)}{4}.\\]\nAssim, para o exemplo, o valor esperado da estatística de teste \\(V\\) é de \\(\\displaystyle \\frac{9(9+1)}{4} = 22,5\\). Note que o valor esperado da estatística de teste \\(V\\) parece ser razoavelmente diferente de \\(R_{neg}\\) e \\(R_{pos}.\\)\nCaso \\(R_{pos}\\) for significativamente maior que \\(R_{neg}\\), significa haver uma alta probabilidade de que o grupo 1 originou-se de uma população com valores maiores do que os da população da qual o grupo 2 originou-se, o contrário também é verdadeiro caso \\(R_{neg}\\) for significativamente maior. Como \\(R_{pos} &gt; R_{neg}\\) no exemplo considerado, podemos interpretar a hipótese alternativa como sendo:\n\\(H_{\\mathrm{a}}:\\) A mediana das diferenças dos valores pareados é maior que zero.\nAssim, resta identificar se \\(R_{pos}\\) é de fato significativamente maior que \\(R_{neg}\\).\n\n\n6.2.1.2 Estatística \\(V\\)\nA estatística \\(V\\) de Wilcoxon nada mais é do que o menor dos valores entre \\(R_{neg}\\) e \\(R_{pos}\\). Assim, para o exemplo em questão, a estatística de teste será \\(V = R_{neg} = 5,5\\).\nPara avaliar as hipóteses precisamos obter o valor crítico de \\(V\\), denotado por \\(V_c\\). Valores críticos de \\(V\\) são quantidades tabeladas identificadas de acordo com a quantidade de postos utilizados no teste ou pares válidos, esses valores podem ser encontrados em Sheskin (2003). Assim, para \\(n = 9\\) e nível de 5% de significância, o valor crítico de \\(V\\) é de \\(V_c = 8\\) para o teste unilateral.\nRejeitamos a hipótese nula quando \\(V \\leq V_c\\). Como \\(R_{pos} &gt; R_{neg}\\), \\(V = 5,5\\) e \\(V_c = 8\\), podemos concluir, ao nível de 5% de significância, que os níveis de ansiedade registrados das gestantes antes do programa de atividades em grupo foram maiores do que após as atividades, ou seja, houve uma redução significativa nos níveis de ansiedade das gestantes em decorrência das atividades em grupo.\n\n\n6.2.1.3 Estatística \\(V\\) normalizada\nNa prática, quando \\(n &gt; 30\\) podemos normalizar a estatística de teste \\(V\\), e então, avaliar as hipóteses baseado na distribuição normal padrão. A estatística de teste \\(V\\) normalizada será denotada por \\(Z_V\\), e possui a seguinte forma:\n\\[Z_T=\\displaystyle \\frac{T-\\displaystyle\\frac{n(n+1)}{4}}{\\sqrt{\\displaystyle\\frac{n(n+1)(2 n+1)}{24}}}.\\]\nComo \\(n = 9\\), temos que o valor da estatística \\(V\\) normalizada é de \\(Z_V = -2,01\\). O valor crítico da estatística de teste normalizada é um valor tabelado o qual pode ser verificado em tabelas de valores críticos da distribuição normal padrão. Pode ser encontrada em Sheskin (2003), ou, obtido computacionalmente no R através da função qnorm(). Para o nível de 5% de significância, os valores críticos para normal padrão são muito conhecidos, sendo 1,96 e 1,65 para os testes bilaterais e unilaterais respectivamente.\nRejeita-se a hipótese nula quando \\(|Z_V|\\geq Z_{crítico}\\). Como \\(R_{pos} &gt; R_{neg}\\), \\(Z_V = -2,01\\) e \\(Z_{crítico} = 1,65\\), então, podemos concluir que há evidências para a rejeição da hipótese nula, e a mesma interpretação feita anteriormente para o teste sem normalização permanece.\n\n\n6.2.1.4 Como aplicar o teste no R\nPara aplicar o teste de Wilcoxon no R podemos utilizar a mesma função e estrutura utilizada para o teste de Mann-Whitney. Utilizamos a seguinte estrutura:\n\nstats::wilcox.test(x, y, correct = FALSE,  alternative = \"two.sided\", paired = TRUE)\n\nOs argumentos x e y são referentes aos grupos 1 e 2 respectivamente. O argumento correct indica se o usuário quer que seja feito uma correção de continuidade do valor-p no caso de aproximação normal da estatística \\(V\\). O argumento alternative especifica a forma do teste a ser aplicado (unilateral ou bilateral). E por fim, o mais importante no caso do teste de Wilcoxon, o argumento paired que indica que o teste considere grupos pareados.\nA função wilcox.test() neste caso irá retornar o valor da estatística de teste \\(V\\) e o valor-p. A hipótese nula pode ser rejeitada quando valor-p \\(\\leq 0,05\\), quando o nível de significância desejado for de 5%.\n\n\n\n6.2.2 Teste de McNemar\nO teste de McNemar é usualmente utilizado em cenários onde a variável de interesse é categórica, dicotômica (duas categorias) e os grupos são dependentes (geralmente experimentos do tipo “antes-depois” de um evento de intervenção). O objetivo do teste é verificar a existência de diferenças significativas entre os grupos em relação à variável de interesse, e sua construção é feita tendo como base tabelas de contingência 2x2.\nConsidere o exemplo abordado na construção do teste de Wilcoxon, onde, foi coletado os dados do nível de ansiedade de gestantes no terceiro trimestre gestacional, antes e depois, de um programa de atividades em grupo, sendo 0 indicando sem ansiedade e 10 indicando ansiedade severa. Porém, considere que o número de gestantes que participaram do programa foi de 100, e que o nível de ansiedade foi classificado como baixo ou alto (baixo: 0 a 5; alto: 6 a 10). Além disso, considere que as gestantes foram selecionadas de forma aleatória e que as categorias baixa e alta são mutualmente exclusivas. Os dados são apresentados na seguinte forma:\n\\[\\begin{array}{c|cc|c}\n\\hline \\text { Antes/Depois } & \\text { Baixo } & \\text { Alto } & {\\text { Soma das linhas }} \\\\\n\\hline \\text { Baixo } & \\mathrm{a = 17} & \\mathrm{b = 10} & \\mathrm{n}_1=\\mathrm{27} \\\\\n\\text { Alto } & \\mathrm{c = 59} & \\mathrm{d = 14} & \\mathrm{n}_2=\\mathrm{73} \\\\\n\\hline \\text { Soma das colunas } & \\mathrm{76} & \\mathrm{24} & \\mathrm{n} =\\mathrm{100} \\\\\n\\hline\n\\end{array}\\]\nPara a construção do teste, as caselas de interesse são aquelas onde houve mudança na avaliação, ou seja, a casela \\(c\\), em que indica que a gestante apresentava alto nível de ansiedade, e então, passou a apresentar baixo nível de ansiedade após as atividades em grupo, e a casela \\(b\\), em que indica que a gestante apresentava baixo nível de ansiedade, e passou a apresentar alto nível após as atividades em grupo. As caselas \\(a\\) e \\(d\\) não são consideradas na construção do teste, pois não houve alterações em decorrência da intervenção (programa de atividades em grupo). Assim, considerando as populações as quais os grupos (antes; depois) representam, as hipóteses terão a seguinte forma bilateral:\n\\(H_0:\\) A proporção das observações da casela \\(b\\) é igual a proporção das observações da casela \\(c\\), ou seja, \\(\\pi_b = \\pi_c\\).\n\\(H_{\\mathrm{a}}:\\) A proporção das observações da casela \\(b\\) difere da proporção das observações da casela \\(c\\), ou seja, \\(\\pi_b \\neq \\pi_c\\).\nOs valores \\(\\pi_b\\) e \\(\\pi_c\\) podem ser estimados por \\(p_b = \\displaystyle \\frac{b}{b + c}\\) e \\(p_c = \\displaystyle \\frac{c}{b + c}\\) respectivamente. Para o exemplo considerado, temos \\(p_b = 0,14\\) e \\(p_c = 0,85\\). Assim, como \\(p_c &gt; p_b\\), a hipótese unilateral \\(H_{\\mathrm{a}}: \\pi_b &lt; \\pi_c\\) é consistente com os dados e pode ser considerada.\n\n6.2.2.1 Estatística \\(M\\)\nA estatística de teste utilizada no teste de McNemar é baseada na distribuição qui-quadrado e será denotada por \\(M\\). O cálculo da estatística \\(M\\) pode ser feito da seguinte maneira:\n\\[M = \\displaystyle \\frac{(b-c)^2}{b+c}.\\]\nNote que essa estatística sempre será não-negativa, e caso o número de observações na casela \\(b\\) e \\(c\\) seja o mesmo, então, a estatística \\(M\\) será zero. Para o exemplo considerado, teremos que o valor da estatística de teste será \\(M = 34,79\\).\n\n\n6.2.2.2 Avaliação das hipóteses\nPara avaliar as hipóteses precisamos obter o valor crítico da estatística \\(M\\), que será denotado por \\(M_c\\). Valores críticos de \\(M\\) são os mesmos da distribuição qui-quadrado, os quais são valores tabelados podendo ser encontrados em Sheskin (2003), ou, podemos utilizar a função qchisq() no R para encontrá-los. A função qchisq() terá a seguinte forma:\n\nstats::qchisq(p = 0.10, df = 1, lower.tail = FALSE)\n\n[1] 2.705543\n\n\nO argumento p indica o nível de significância considerado a depender do formato das hipóteses, como estamos considerando um teste unilateral, para o teste ter o nível de significância de 5% devemos especificar \\(p = 0,10\\). Caso o teste fosse bilateral, então, teríamos \\(p = 0,05\\). O argumento df indica o grau de liberdade considerado, que será o número de caselas avaliadas menos 1, como por construção o teste avalia apenas duas caselas, especificamos \\(df = 1\\). O argumento lower.tail deve ser especificado como FALSE para o formato de p apresentado, caso esse argumento for especificado como TRUE, o valor utilizado no argumento p precisa ser indicado como seu complementar. No exemplo, caso lower.tail = TRUE, então, teríamos p = 1 - 0,10 = 0,90.\nAssim, rejeitamos a hipótese nula quando \\(M \\geq M_c\\). Como \\(M = 34,79\\) e \\(M_c = 2,71\\), então, ao nível de 5% de significância, há evidências de uma mudança significativa nos níveis de ansiedade das gestantes, onde a direção dessa mudança é de níveis altos de ansiedade para níveis baixos. É importante notar, que essa hipótese apenas é aceita, pois a relação \\(p_c &gt; p_b\\) é consistente com os dados.\nEmbora o exemplo utilizado seja de um experimento em que os grupos são formados a partir de dados obtidos das mesmas pessoas duas vezes (antes e depois da intervenção), em experimentos em que os grupos são formados de indivíduos diferentes, mas pareados por características em comum, o teste de McNemar pode ser aplicado normalmente.\n\n\n6.2.2.3 Como aplicar o teste no R\nPodemos aplicar o teste de McNemar facilmente no R ao utilizar a função mcnemar.test(), a qual irá retornar a estatística de teste, os graus de liberdades considerados e o valor-p calculado. Essa função possui a seguinte estrutura:\n\nstats::mcnemar.test(x, correct = TRUE)\n\nO argumento x é a tabela de contingência 2x2 considerada, e deve ser passada como um objeto do tipo matriz (ver apêndice). O argumento correct indica se o usuário quer que seja aplicado uma correção de continuidade na estatística de teste, o que é sempre recomendado na prática dado que o teste de McNemar utiliza uma distribuição contínua para aproximar uma distribuição de probabilidade discreta. Assim, aplicando o teste com a função mcnemar.test(), podemos rejeitar a hipótese nula quando valor-p \\(\\leq 0,05\\) considerando nível de 5% de significância, ou podemos obter o valor crítico da estatística de teste e avaliar \\(M \\geq M_c\\).\n\n\n\n6.2.3 Teste de Friedman\nO teste de Friedman é utilizado para comparar mais de dois grupos dependentes, onde, sua construção de baseia em postos. Assim, a variável de interesse precisar ser pelo menos ordinal para ser possível identificar os postos. Além disso, é usado quando os pressupostos de normalidade não são atendidos, sendo uma alternativa não-paramétrica a análise de variância de um fator (One-way ANOVA) de medidas repetidas. O teste tem por objetivo verificar se as distribuições das populações as quais os grupos representam são iguais em relação à mediana, logo, teremos a seguinte estrutura de hipóteses:\n\\(H_0:\\) Todos os \\(k\\) grupos originam-se de populações com medianas idênticas.\n\\(H_{\\mathrm{a}}:\\) Pelo menos 2 grupos originam-se de populações com medianas diferentes.\nConsidere o exemplo abordado na construção dos testes de Wilcoxon e de McNemar, onde, foi coletado os dados do nível de ansiedade de gestantes no terceiro trimestre gestacional, antes e depois, de um programa de atividades em grupo, sendo 0 indicando sem ansiedade e 10 indicando ansiedade severa. Porém, agora considere que os níveis de ansiedade foram medidos antes do início do programa e, posteriormente, após duas atividades em grupo diferentes (atividades A e B), onde, foram escolhidas baseado em estudos anteriores que indicaram que eram atividades capazes de reduzir a ansiedade. Além disso, considere que foram selecionadas de forma aleatória 5 gestantes no terceiro trimestre gestacional para participarem do experimento, e que os tempos entre as atividades foram determinados apropriadamente. Os dados e os postos foram identificados da seguinte forma:\n\\[\\begin{array}{ccccccc}\n\\hline \\text { Indivíduos } & \\text{Grupo}_1 & \\text{Postos}_1 & \\text{Grupo}_2 & \\text{Postos}_2 & \\text{Grupo}_3 & \\text{Postos}_3 \\\\\n\\hline 1 & 5 & 3 & 3 & 1 & 4 & 2 \\\\\n2 & 7 & 3 & 3 & 1 & 5 & 2 \\\\\n3 & 6 & 3 & 5 & 2 & 4 & 1 \\\\\n4 & 9 & 3 & 2 & 1 & 5 & 2 \\\\\n5 & 6 & 2,5 & 6 & 2,5 & 3 & 1 \\\\\n\\hline\n\\end{array}\\]\nA coluna Grupo1 indica os níveis de ansiedade das gestantes (identificadas na coluna Indivíduos) registrados antes das atividades, sendo os valores na coluna Postos1 seus respectivos postos. Além disso, a coluna Grupo2 indica os níveis de ansiedade registrados após a atividade A, cujos postos são identificados na coluna Postos2. Para mais, a coluna Grupo3 indica os níveis de ansiedade registrados das gestantes após a atividade B, sendo seus postos identificados na coluna Postos3. Por fim, detonamos a soma dos postos dos grupos por \\(R_1\\), \\(R_2\\) e \\(R_3\\), onde aplicamos a soma de todos os valores para cada coluna Postos1, Postos2 e Postos3. Assim, teremos \\(R_1 = 14,5\\), \\(R_2 = 7,5\\) e \\(R_3 = 8,0\\).\nNote que o procedimento de identificação dos postos é feito para cada indivíduo em específico, ou seja, é identificado os postos dos valores (níveis de ansiedade) dos grupos por linha separadamente, de forma que os postos de cada linha não interferem os postos das outras. Caso houver empates dos valores, então, a média dos postos identificados caso não fossem empates será aplicado, da mesma forma em que é feito nos demais testes não-paramétricos abordados nesta seção.\nEmbora o exemplo utilizado seja de um experimento onde os mesmos indivíduos passam por condições diferentes, e então, para cada condição é formado um novo grupo, cenários onde os grupos são formados por indivíduos diferentes, porém, pareados por características em comum, também são válidos para aplicação do teste de Friedman.\n\n6.2.3.1 Estatística \\(Q\\)\nPara avaliar as hipóteses devemos calcular a estatística de teste, a qual denotaremos de estatística \\(Q\\). A estatística \\(Q\\) é aproximada a partir da distribuição qui-quadrado da seguinte forma:\n\\[Q=\\displaystyle \\frac{12}{n k(k+1)}\\left[\\sum_{i = 1}^k\\left(R_i\\right)^2\\right]-3 n(k+1).\\]\nExperimentos do tipo “antes-depois” possuem a característica de serem geralmente avaliados os mesmos indivíduos mais de uma vez, assim, os grupos formados serão do mesmo tamanho, onde, teremos \\(n_1 = n_2 = n_3 = n\\), sendo \\(n = 5\\) no exemplo considerado. O número de grupos é denotado por \\(k\\), assim, podemos calcular a seguinte quantidade:\n\\[\\displaystyle \\sum_{i = 1}^k(R_i)^2 = (R_1)^2 + (R_2)^2 + (R_3)^2 = (14,5)^2 + (7,5)^2 + (8,0)^2 = 330,5.\\]\nCom todos os termos identificados, a estatística para o teste de Friedman será \\(Q = 6,1\\).\n\n\n6.2.3.2 Avaliação das hipóteses\nTendo calculado a estatística de teste, precisamos obter o valor crítico de \\(Q\\) para avaliar as hipóteses, o qual será denotado por \\(Q_c\\). Como a estatística \\(Q\\) é aproximada pela qui-quadrado, então, os valores críticos da distribuição qui-quadrado podem ser utilizados. Tais valores podem ser obtidos de forma tabelada, onde, tais tabelas podem ser encontradas em Sheskin (2003). Outra forma de obtê-los é através da função qchisq(), da mesma forma em que foi explicado na construção do teste de McNemar. Considerando que os graus de liberdade são \\(k - 1\\), então, ao nível de 5% de significância, teremos o seguinte valor crítico para o exemplo abordado:\n\nstats::qchisq(p = 0.05, df = 2, lower.tail = FALSE)\n\n[1] 5.991465\n\n\nRejeitamos a hipótese nula quando \\(Q \\geq Q_c\\), logo, dado que \\(Q = 6,1\\) e \\(Q_c = 5,99\\), há evidências de que pelo menos dois grupos (condições as quais o nível de ansiedade foi registrado) diferem significativamente.\n\n\n6.2.3.3 Correção de empates\nCaso houver um número excessivo de indivíduos onde ocorreu empates (muitas linhas da tabela em que tenham empates ou muitos empates em uma mesma linha), é recomendado utilizar um fator de correção para aplicar na estatística de teste, o qual, terá a seguinte forma:\n\\[C=1- \\displaystyle\\frac{\\sum_{i=1}^s\\left(t_i^3-t_i\\right)}{n\\left(k^3-k\\right)}.\\]\nNote que a quantidade \\(\\displaystyle \\sum_{i=1}^s\\left(t_i^3-t_i\\right)\\) é a mesma utilizada no fator de correção de empates para o teste de Kruskal-Wallis, onde, \\(s\\) indica quantos conjuntos de empates ocorreram e \\(t\\) o número total de empates no conjunto. Logo, para o exemplo considerado teremos:\n\\[\\displaystyle \\sum_{i=1}^s\\left(t_i^3-t_i\\right) = \\left(2^3-2\\right) = 6.\\]\nAssim, sabendo que \\(n = 5\\) e \\(k = 3\\), o valor do fator de correção será \\(C = 0,95\\). Então, a estatística de teste \\(Q\\) corrigida será \\(Q_{corrigida} = \\displaystyle \\frac{Q}{C} = 6,42\\).\nA avaliação das hipóteses é feita da mesma forma, onde, rejeitamos a hipótese nula quando \\(Q_{corrigida} \\geq Q_c\\).\n\n\n6.2.3.4 Como aplicar o teste no R\nPodemos aplicar o teste de Friedman no R utilizando a função friedman.test(), a qual, possui a seguinte estrutura:\n\nstats::friedman.test(y, groups, blocks)\n\nÉ recomendável criar um objeto do tipo data.frame (ver apêndice) com cada coluna necessária para realização do teste, sendo: uma coluna com as observações da variável de interesse para o argumento y da função; uma coluna com os grupos (variável independente) para o argumento groups; uma coluna com a identificação numérica dos indivíduos para o argumento blocks. Tendo criado o objeto, digamos, objeto dados, basta passar as colunas para os argumentos, sendo y = dados$variavel, groups = dados$grupos e blocks = dados$individuos. O teste irá retornar a estatística de teste, os graus de liberdade utilizados e o valor-p, em que rejeitamos a hipótese nula quando \\(Q \\geq Q_c\\) ou quando valor-p \\(\\leq 0,05\\) ao nível de 5% de significância.\nPodemos realizar comparações múltiplas após a confirmação da rejeição da hipótese nula, para isso, devemos utilizar métodos específicos para o teste de Friedman. Um dos métodos mais utilizados é o teste de Conover, que pode ser utilizado para comparações múltiplas tanto para o teste de Friedman quanto para o teste de Kruskal-Wallis, como também, em outros cenários. Podemos aplicar o teste de Conover no R com a função frdAllPairsConoverTest(), a qual, pertence ao pacote PMCMRplus e possui a seguinte estrutura:\n\nPMCMRplus::frdAllPairsConoverTest(y, groups, blocks, p.adjust.method = \"bonf\")\n\nNote que a estrutura é a mesma da função friedman.test(), a única diferença é o argumento p.adjust.method, o qual, indica o método de ajuste do valor-p a ser utilizado, onde, usualmente especificamos o método de Bonferroni por “bonf”. A função irá retornar uma pequena matriz de valores-p para cada comparação feita, onde, identificamos os pares de grupos que diferem de forma significativa ao nível de 5% quando valor-p \\(\\leq 0,05\\)."
  },
  {
    "objectID": "testes_np.html#materiais-complementares",
    "href": "testes_np.html#materiais-complementares",
    "title": "6  Testes não-paramétricos",
    "section": "6.3 Materiais complementares",
    "text": "6.3 Materiais complementares\nLivros e Artigos:\n\nCorder, Gregory W., and Dale I. Foreman. “Nonparametric statistics for non‐statisticians.” (2011).\nPett, Marjorie A. Nonparametric statistics for health care research: Statistics for small samples and unusual distributions. Sage Publications, 2015.\nSheskin, David J. Handbook of parametric and nonparametric statistical procedures. Chapman and hall/CRC, 2003.\nTomkins, C. C., and C. Hall. “An introduction to non-parametric statistics for health scientists.” University of Alberta Health Sciences Journal 3.1 (2006): 20-26.\n\n\\(Sites\\):\n\nhttps://www.graphpad.com/guides/prism/latest/statistics/index.htm\nhttps://www.dataanalytics.org.uk/critical-values-for-the-kruskal-wallis-test/"
  },
  {
    "objectID": "supervisionada.html#conceitos-iniciais",
    "href": "supervisionada.html#conceitos-iniciais",
    "title": "7  Aprendizado supervisionado",
    "section": "7.1 Conceitos iniciais",
    "text": "7.1 Conceitos iniciais\nAprendizado supervisionado pode ser definido como a tarefa de aprender uma função que mapeia uma entrada em uma saída e isso é feito com base em exemplos e treinos. Em outras palavras, uma máquina é treinada para encontrar soluções chamadas rótulos, onde esses rótulos identificam alguma característica. Apesar de também poder ser usada para regressão, o aprendizado supervisionado tem como tarefa típica a classificação. Um exemplo bem simples de classificação é: suponha que eu queira classificar imagens de animais, nesse caso possuo um banco de dados com imagens de cachorros e gatos. Quero que meu algoritmo classifique as imagens identificando o tipo do animal na imagem. Para isso o algoritmo é treinado utilizando vários exemplos para que ele consiga classificar novas imagens posteriormente. Outra tarefa é predizer um valor com base em características, por exemplo, prever o valor de um carro dado um conjunto de características (quilometragem, idade, marca, etc.) chamadas preditores. Este tipo de tarefa é chamada regressão. Para treinar o sistema é preciso incluir diversos exemplos, assim o banco de dados é separado em treino e teste, onde o é feito o treinamento na base treino para posteriormente serem feitos os testes de predição e avaliação da qualidade do ajuste na base teste.\n\n7.1.1 Dificuldades gerais do machine learnig\nComo dito anteriormente, a idéia geral do aprendizado de máquina é construir um algoritmo para solucionar os meus problemas, onde esse algoritmo será treinado com dados. Mas, o que acontece se o meu algoritmo for ruim ou os dados estiverem ruins?\n\n7.1.1.1 Quantidade insuficiente de dados\nFalando sobre dados ruins, o primeiro problema é a quantidade de dados. Já parou pra pensar em quão difícil é treinar uma máquina? Voltando ao exemplo anterior, para você aprender a diferenciar um cachorro de um gato quando era criança, bastou alguém lhe apontar qual era qual algumas vezes você se tornou capaz de diferenciar cães de gatos independente das características. Uma máquina não consegue fazer isso facilmente, é necessário uma quantidade grande de dados para a maioria dos algoritmos, até mesmo para problemas simples como o do exemplo citado e para problemas complexos, como reconhecimento de imagem ou fala você pode precisar de milhões de exemplos.\n\n\n7.1.1.2 Dados de treino não representativos\nComo mencionado anteriormente, o treinamento de um algoritmo é feito por meio de uma base de dados, onde está é separada em dados de treinamento e de teste, para que eu possa usá-lo e generalizá-lo em dados futuros. Dados de treinamento que não representem bem os dados que serão usados no futuro podem ser um modelo que não funcionará bem. Utilizando o exemplo do algoritmo de regressão onde o objetivo era prever os valores dos carros com base em suas características. Digamos que meu algoritmo foi treinado com uma base de dados de carros apenas do estado de São Paulo, mas meu algoritmo será utilizado para prever carros de todo o país, pode ser que não funcione tão bem. Os estados podem alterar significativamente os preços dos carros por meio de impostos, por exemplo. É de extrema importância utilizar um conjunto de dados de treino que represente bem os dados que você deseja generalizar. Isso pode não ser uma tarefa fácil, pode encontrar problemas com amostras, principalmente se ela for muito pequena e até mesmo uma amostra grande pode não ser representativa.\n\n\n7.1.1.3 Qualidade dos dados\nComo pode ter imaginado, a qualidade dos dados também é de extrema importância. Dados com discrepâncias, vários erros, e gerados a partir de medições de baixa qualidade fará com que fique mais difícil o seu algoritmo identificar padrões e tomar decisões. Se você convive com pessoas do ramo da ciência de dados em geral, é bem provável que já tenha ouvido alguém dizer algo do tipo: “gastamos a maior parte do nosso tempo para limpar os dados”. Isso não é em vão. Na maioria dos casos, principalmente no ramo de aprendizagem de máquinas é gasto um enorme tempo para limpar os dados pois pode influenciar muito na qualidade do modelo. Por exemplo, se algumas informações forem muito discrepantes, é preciso decidir entre tentar corrigir ou excluí-las. Se uma variável tiver uma quantidade significativa de valores faltantes, deverá ser decidido se essas observações serão excluídas ou se será possível utilizar métodos de imputação de dados. Treinar mais de um modelo com diferentes decisões tomadas sobre os dados também pode ser efetivo.\n\n\n7.1.1.4 Sobreajustamento dos dados (Overfitting)\nO sobreajustamento é um conceito que ocorre quando nosso modelo (não só um modelo de aprendizado de máquinas), se ajusta exatamente aos nossos dados de treinamento. Ouvir isso uma primeira vez pode parecer excelente, ou até mesmo o cenário ideal, afinal, queremos que o nosso modelo se ajuste o máximo possível, certo? bom.. não exatamente. O que acontece neste caso, é que o modelo mostra-se adequado apenas para os dados de treino, como se o modelo tivesse apenas decorado os dados de treino e não fosse capaz de generalizar para outros dados nunca vistos antes. Assim, o desempenho do nosso modelo quando usado em novos dados cai drasticamente. Algumas razões que podem levar a um sobreajustamento: base de treino muito pequena, não contendo dados suficientes para representar bem todos valores de entrada possíveis; grande quantidade de informações irrelevantes (dados ruidosos); treinamento excessivo em um único conjunto de amostra; modelo muito complexo, fazendo com que ele aprenda os ruídos nos dados de treinamento. Agora que sabemos o problema que é um sobreajustamento e as razões que podem levar a isso, precisamos falar sobre como evitar que isso aconteça. Existem algumas técnicas comumente utilizadas.\n\nRegularização: Foi dito anteriormente que uma razão para o sobreajustamento é a complexidade do modelo, então, faz sentido diminuirmos sua complexidade. Isso pode ser feito removendo ou diminuindo o número de parâmetros.\nParada antecipada: Quando um modelo está sendo treinado por rodadas de repetição, é possível avaliar cara uma dessa repetição. Normalmente o desempenho de um modelo melhora a cada repetição, mas chega um momento em que começa a acontecer o sobreajustamento. A ideia da parada antecipada é pausar o treinamento antes que chegue a esse ponto.\nAumento de dados: Essa técnica consiste em aumentar ligeiramente os dados da amostra toda vez que o modelo os processa, ou seja, injetar dados limpos e relevantes nos dados de treino. Isso faz com que os conjuntos de treino pareçam “exclusivos” do modelo, impedindo que ele aprenda suas características. Mas isso deve ser feito com moderação, pode injetar dados que não estão limpos pode fazer mais mal do que bem. Além disso, não é um método garantido.\n\nExistem outras técnicas que podem ser utilizadas para evitar o sobreajustamento. Mas precisamos falar também sobre como detectá-los.\nUma forma “não técnica” e que não deve ser a sua única forma de tentar identificar o sobreajustamento é por meio da visualização gráfica. A visualização gráfica pode ser usada apenas para levantar hipóteses, nunca para tomar uma decisão final. Até mesmo porque nem sempre é possível verificar esse problema visualmente. Talvez a técnica mais eficiente para isso é a Validação Cruzada k-fold (k-fold Cross Validation). Vamos falar sobre posteriormente.\n\n\n7.1.1.5 Subajustamento dos dados (Underfitting)\nComo pode ter imaginado, subajustamento é o oposto do sobreajustamento. Ocorre quando seu modelo é muito simples para aprender a estrutura dos dados. O subajustamento leva a um erro elevado tanto nos dados de treino quanto nos dados de teste. Pode ocorrer quando o modelo não foi treinado por tempo suficiente ou as variáveis ​​de entrada não são significativas o suficiente para determinar uma relação significativa entre as variáveis ​​de entrada e saída. Aqui também estamos em um cenário a ser evitado e apesar de ser contrário ao sobreajustamento, as técnicas tanto para identificar quanto para evitar o problema são semelhantes. Um adendo, geralmente, identificar um subajustamento é mais fácil que identificar um sobreajustamento.\n\n\n\n7.1.2 Modelo de Regressão Linear\nJá temos uma breve noção sobre o que é aprendizado supervisionado, agora vamos introduzir o funcionamento básico de um modelo. Como foi mencionado, aprendizado supervisionado é usado principalmente para métodos de classificação e regressão. Um modelo de regressão linear, como o próprio nome já diz, se enquadra nos métodos de regressão. A regressão consiste em modelar um valor de previsão com base em variáveis independentes. De forma mais geral, o modelo consiste em fazer uma previsão “simples” calculando uma ponderação entre as somas dos recursos de entrada e uma constante chamada intercepto. Assim, obtemos uma relação linear entre a variável de saída e as variáveis de entrada. A linha de regressão é a linha de melhor ajuste para o modelo.\n\n\n\n\n\nFonte: https://is.gd/Z9yiHp\n\n\n\n\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nonde:\n\n\\(\\hat y\\) é o valor predito\n\\(n\\) o número de características\n\\(x_i\\) é a \\(i^{th}\\) característica\n\\(\\beta_j\\) é o \\(j^{th}\\) parâmetro do modelo\n\nCerto, temos uma definição matemática do nosso modelo, mas como posso treiná-lo? Treinar um modelo significa também definir os parâmetros para que o modelo se ajuste melhor aos meus dados. Em outras palavras, um modelo treinado irá se ajustar à melhor linha para prever o valor de \\(y\\) para um dado valor de \\(x\\). Assim, ao encontrar os melhores valores de \\(\\beta 's\\) obtemos a melhor linha de ajuste. Para isso, primeiro precisamos de uma medida de quão bem (ou mal) o modelo se ajusta aos meus dados. Posteriormente será discutido quais as medidas mais comuns para avaliação de modelos de regressão.\nExistem algumas suposições importantes que devem ser feitas para utilizar um modelo de regressão linear. Estas são algumas verificações formais durante a construção de um modelo de regressão linear, o que garante a obtenção do melhor resultado possível do conjunto de dados fornecido.\n\nSuposição de linearidade: A regressão linear assume que a relação entre a entrada e saída é linear. Pode parecer um pouco óbvio, mas em alguns casos onde, em um primeiro olhar, faça sentido usar uma regressão linear, nossos dados não permitam isso. Pode ser necessário transformar os dados.\nHomocedasticidade: Homocedasticidade é uma situação em que o termo de erro é o mesmo para todos os valores de variáveis ​​independentes. Com homocedasticidade, não deve haver uma distribuição padrão clara de dados no gráfico de dispersão.\nErros normalmente distribuídos: A regressão linear assume que o termo de erro deve seguir o padrão de distribuição normal. Se os termos de erro não forem normalmente distribuídos, os intervalos de confiança se tornarão muito amplos ou muito estreitos, o que pode causar dificuldades em encontrar coeficientes. Você pode obter algum benefício usando transformações (por exemplo, log ou BoxCox) em suas variáveis ​​para tornar sua distribuição mais gaussiana.\nMulticolinearidade: O modelo de regressão linear não assume nenhuma autocorrelação em termos de erro. Se houver alguma correlação no termo de erro, isso reduzirá drasticamente a precisão do modelo. A autocorrelação geralmente ocorre se houver uma dependência entre os erros residuais. Considere calcular correlações pareadas para seus dados de entrada e remover os mais correlacionados.\n\n\n\n7.1.3 Modelo de Regressão logística\nAlguns algoritmos de regressão podem ser usados para classificação (o contrário também é válido). A regressão logística é um dos algoritmos mais populares do machine learning e geralmente é usada para estimar a probabilidade de que uma instância pertença a uma classe. Por exemplo, qual a probabilidade de que o objeto de uma imagem seja um cachorro? ou um gato? Neste caso, se a probabilidade estimada for maior que 50%, então o modelo pode prever que naquela imagem tem um cachorro (classe rotulada como “1”), se for menor, prevê que é um gato (classe rotulada como “0”). Este tipo de regressão pode retornar valores categóricos ou discretos, como: Sim ou Não, 0 ou 1, verdadeiro ou falso, entre outros. Mas aqui, ela fornece os valores probabilísticos que estão entre 0 e 1. Apesar de ser semelhante a regressão linear, aqui não ajustamos uma linha de regressão, mas sim uma função logística em forma de “S” que prevê os dois valores máximos (0 ou 1).\n\n\n\n\n\nFonte: https://is.gd/87NSTU\n\n\n\n\nA equação de regressão Logística pode ser obtida a partir da equação de Regressão Linear.\n\\[\n\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_nx_n\n\\]\nO problema de usar essa abordagem é que podemos prever probabilidades negativas em alguns casos e valores maiores que 1 em outros. Essas previsões não são sensatas, pois sabemos que a verdadeira probabilidade deve ser um número entre 0 e 1. Para resolver esse problema, devemos modelar \\(\\hat y\\) usando uma função que forneça saídas entre 0 e 1 para todos os valores de \\(\\hat y\\). Na regressão logística usamos a função logística como sendo:\n\\[\n\\hat y = \\frac{e^{\\beta_0+\\beta_1X}}{1 + e^{\\beta_0+\\beta_1X}}\n\\]\nDepois de algumas manipulações, chegamos que\n\\[\n\\frac{\\hat y}{1- \\hat y} = e^{\\beta_0+\\beta_1X}\n\\]\nMas precisamos variar de \\(-\\infty\\) até \\(\\infty\\), então pegue o logaritmo da equação e temos:\n\\[\n\\log\\bigg[\\frac{\\hat y}{1- \\hat y} \\bigg ] = {\\beta_0+\\beta_1X}\n\\]\nExistem alguns tipos de regressão logística:\n\nBinomial: Aqui deve haver apenas dois tipos de possíveis variáveis, como 0 ou 1, Falso ou Verdadeiro, etc.\nMultinomial: Pode também haver 3 ou mais tipos não ordenados possíveis da variável dependente, como, cachorro, gato ou tigre.\nOrdinal: Na regressão logística ordinal, pode haver 3 ou mais tipos ordenados possíveis de variáveis ​​dependentes, como “baixo”, “médio” ou “alto”.\n\n\n\n7.1.4 Medidas de desempenho\nAo desenvolver projetos de Aprendizado de Máquinas no geral, é preciso sempre verificar a qualidade do modelo. Assim como não existe um modelo padrão para resolver todos os problemas, não existe uma única métrica para avaliar a qualidade do modelo. Saber qual métrica é mais apropriada para determinado cenário é crucial, pois escolhas erradas podem gerar modelos problemáticos.\n\n7.1.4.1 Modelos de regressão\nExistem algumas métricas importantes para medirmos a qualidade de um modelo de regressão.\n\nRoot Mean Squared Error (RMSE): Uma medida muito comum usada em um modelo de regressão é a Root Mean Squared Error (REQM). Em resumo, o REQM é uma medida que mostra o quão espalhados estão esses resíduos. A métrica possui valor mínimo 0 e sem valor máximo. Quanto maior esse número, pior o modelo.\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n}  (\\hat y_i - y_i)^2}\n\\]\nUma vantagem dessa métrica é que predições muito distantes do real aumentam o valor da medida com facilidade, o que torna a métrica bem vinda em problemas onde erros grandes não são tolerados.\nMean Absolute Error (MAE): Esta medida é bem simples de entender. Nada mais é que a média do erro que cada ponto tem em relação a linha de regressão. É um pouco parecido com o que RMSE faz, a diferença é que aqui, erros grandes não afetam tanto a medida. A sua interpretação é: quanto maior o MAE, maior é o erro do modelo. Apesar de ser simples, não deve ser usado em todos os modelos. Esta medida é bastante sensível a valores discrepantes (outliers). Portanto, deve-se avaliar os dados antes de utilizar esta métrica.\n\\[\nMAE = \\frac{1}{N}\\sum|y_i - \\hat{y_i}|\n\\]\n\\(R^2\\): R-quadrado é uma medida que visa explicar qual a porcentagem de variância que pôde ser prevista pelo modelo, ou seja, o qua “próximo” as medidas reais estão dos nossos dados. Segue a fórmula:\n\\[\nR^2 = 1 - \\frac{\\sum(y_i - {\\hat{y_i}})^2} {\\sum(y_i - {\\bar{y}})^2}\n\\]\nonde, \\(\\hat{y_i}\\) é o valor predito, \\(\\bar{y}\\) é o valor médio das amostras e \\(y_i\\) é o valor observado. O valor resultante varia de 0 a 1. Quanto maior o valor, melhor o modelo. Por exemplo, caso tivéssemos avaliando um modelo e tivéssemos \\(R^2 = 0.87\\), entende-se então que 87% da variância de nossos dados podem ser explicadas pelo modelo. Esta medida possui algumas limitações importantes: só pode ser aplicada perfeitamente para modelos univariados; é uma medida enviesada, pode definição e em casos de overfitting, o valor da métrica ainda continua alto.\n\n\n\n7.1.4.2 Modelos de classificação\n\nAcurácia: A acurácia nos diz quantos de nossos exemplos foram classificados corretamente e é dada pela seguinte fórmula:\n\\[\n\\mbox{acurácia}   =  \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nOnde TP = True Positive (Verdadeiro Positivo), TN = True Negative (Verdadeiro Negativo), FP = False Negative (Falso Negativo) e FN = False Negative (Falso Negativo).\nA métrica então é definida pela razão entre os acertos e o total (erros + acertos). É uma métrica extremamente fácil de ser usada e interpretada. Por exemplo, se tenho 100 observações e meu modelo classificou 80 corretamente, então a acurácia do meu modelo é 80%. Apesar de simples, essa métrica pode não ser adequada em alguns casos.\nUma desvantagem é que podemos obter uma acurácia alta, mas o nosso modelo pode ter uma performance inadequada. Por exemplo, pense que temos um conjunto de dados de animais com 100 observações, sendo 90 cachorros e 10 gatos. Se tivermos um modelo que sempre classificará todas as observações como cachorro, ainda teríamos um modelo com acurácia de 90%. É uma métrica boa, mas não estamos avaliando o nosso modelo de uma boa forma. Se introduzirmos novos dados que venham de uma forma mais equilibrada, ou seja, com quantidade de cachorros e gatos parecidas, o modelo se comportaria de forma ruim. Portanto, para conjuntos de dados desbalanceados, outras métricas são mais eficientes.\nOutra desvantagem é que esta medida atribui o mesmo peso para ambos os erros. Por exemplo, em um modelo que classifica exames de câncer entre positivo e negativo para a doença. Um modelo com acurácia 90% aparentemente é um bom modelo onde os 10% de erro podem ter sido falsos negativos ou falsos positivos. Porém, o erro por falso negativo aqui é bem mais grave.\nPrecisão: Essa métrica é definida pela razão entre a quantidade de observações classificadas corretamente como positivos e o total de classificados como positivos, segue a fórmula:\n\n\\[\n\\mbox{precisão}  =  \\frac{TP}{TP + FP}\n\\]\nEsta medida atribui um peso maior para os erros por falso positivo. Pode ser entendida como a resposta para a pergunta: das observações classificadas como positivos, quantos são verdadeiramente positivos? então, neste caso, se a precisão fosse de 90%, é esperado que a cada 100 observações classificadas como positivos, apenas 90 são de fato positivos.\n\nSensibilidade: Essa métrica avalia então a capacidade do modelo detectar com sucesso resultados classificados como positivos em relação a todos os pontos de dados positivos.\n\\[\n\\mbox{sensibilidade} = \\frac{TP}{TP + FN}\n\\]\nEspecificidade: Ao contrário da sensibilidade, a especificidade avalia a capacidade do modelo detectar resultados negativos. Segue a fórmula:\n\n\\[\n\\mbox{especificidade} = \\frac{TN}{TN + FP}\n\\]\n\n\n7.1.4.3 Matriz de confusão\nMatriz de confusão é uma matriz usada para avaliar o desempenho de um modelo de classificação. A matriz compara os valores de destino reais com os previstos pelo modelo. A matriz é N x N onde é N é o número de classes. Para um problema de classificação binária teríamos uma matriz 2 x 2. As matrizes de confusão revelam quando um modelo confunde consistentemente duas classes, simplificando a determinação da probabilidade de os resultados de um modelo serem confiáveis.\n\n\n\n\n\n\n\n\n\n\n\n\nObservado\n\n\n\nPredito\nNegativo\nPositivo\n\n\n\n\nNegativo\nVerdadeiro Negativo (TN)\nFalso Negativo (FN)\n\n\nPositivo\nFalso Positivo (FP)\nVerdadeiro Positivo (TP)\n\n\n\n\n\n\n\nNa matriz de confusão colocamos os valores reais nas colunas e os valores preditos nas linhas. Assim o cruzamento das linhas e das colunas passam a ser nossas métricas. (não é incomum vermos versões invertidas)\n\n\n\n7.1.5 Curva AUC-ROC\nQuando queremos verificar ou visualizar o desempenho de um modelo de classificação binária (0, 1), podemos utilzar a AUC (Area Under The Curve) ROC (Receiver Operating Characteristics). Está é uma das métricas de avaliação de desempenho de modelos de classificação mais importantes.\n\n7.1.5.1 O que é a curva AUC-ROC\nA curva ROC é uma métrica de avaliação para problemas de classificação binária. É uma curva de probabilidade que plota o TPR (sensibilidade) contra o FPR (1 - especificidade). A AUC (Area Under the Curve) é a medida da capacidade de um classificador para distinguir entre classes e é usada com um resumo da curva ROC. Quanto maior a AUC, melhor o modelo está em distinguir entre classes positivas e negativas. Por analogia, quanto maior a AUC, melhor o modelo está em distinguir entre pacientes com a doença e sem a doença.\nQuando AUC = 1, o modelo é capaz de distinguir perfeitamente os pontos positivos e negativos corretamente. Porém, se AUC = 0, o modelo estaria predizendo todos valores negativos como positivos e vice e versa. Quando AUC= 0,5, o modelo não é capaz de distinguir os valores. Neste caso o modelo está prevendo cada classe de forma aleatória ou de forma constante para todos os dados. Já quando 0,5 &lt; AUC &lt; 1, há uma grande chance de que o modelo consiga distinguir os valores negativos das classes positivas. Aqui o modelo é capaz de detectar mais números de verdadeiros positivos e verdadeiros negativos do que falsos negativos e falsos positivos.\nA curva ROC é produzida calculando e plotando a taxa de verdadeiros positivos em relação à taxa de falsos positivos para um único classificador em vários limites . Por exemplo, na regressão logística, o limiar seria a probabilidade prevista de uma observação pertencente à classe positiva. Normalmente, na regressão logística, se uma observação é prevista como positiva com probabilidade &gt; 0,5, ela é rotulada como positiva. No entanto, poderíamos realmente escolher qualquer limite entre 0 e 1 (0,1, 0,3, 0,6, 0,99, etc.) — e as curvas ROC nos ajudam a visualizar como essas escolhas afetam o desempenho do classificador.\nA figura abaixo demonstra como alguns modelos teóricos podem plotar a curva ROC. A linha cinza pontilhada representa um classificador que não é melhor do que a adivinhação aleatória, seria a linha diagonal. Um modelo com uma taxa de verdadeiros positivos de 100% e falsos negativos de 0% seria plotado sobre a linha da esquerda e a de cima. Quase todos os exemplos do mundo real cairão em algum lugar sobre essas duas linhas tendo o cenário 0,5 &lt; AUC &lt; 1, como exemplo, a linha azul da figura. Normalmente procuramos um classificador que mantenha uma alta taxa de verdadeiros positivos e, ao mesmo tempo, uma baixa taxa de falsos positivos. Embora seja útil visualizar a curva ROC, em muitos casos podemos reduzir essas informações a uma única métrica, a AUC.\n\n\n\n\n\n\n\nFonte: https://is.gd/iyWHe8\n\n\n\n\nUm grande ponto positivo da curva ROC é que ela permita que encontremos um limite de classificação adequado ao nosso problema específico\nPor exemplo, se estivéssemos avaliando um classificador de spam de e-mail, gostaríamos que a taxa de falsos positivos fosse muito, muito baixa. Não queremos que alguém perca um e-mail importante para o filtro de spam só porque nosso algoritmo foi muito agressivo. Provavelmente até permitiríamos uma boa quantidade de e-mails de spam reais (verdadeiros positivos) através do filtro apenas para garantir que nenhum e-mail importante fosse perdido.\nPor outro lado, se nosso classificador está prevendo se alguém tem uma doença terminal, podemos aceitar um número maior de falsos positivos (diagnosticado incorretamente a doença), apenas para garantir que não perderemos nenhum verdadeiro positivo (pessoas que realmente têm a doença).\nAlém disso, também podemos comparar o desempenho de diferentes classificadores para o mesmo problema.\nExistem outras métricas de desempenho que não foram apresentadas aqui, mas devemos ressaltar alguns pontos. Não existe uma métrica certa ou errada, devemos apenas nos atentar e buscar a que melhor atende o nosso problema. É possível também utilizar mais de uma métrica para o mesmo modelo.\n\n\n\n7.1.6 Validação Cruzada (Cross-Validation)\nAté aqui falamos um pouco sobre alguns problemas que podem ser encontrados no aprendizado de máquinas e superficialmente sobre dois modelos de regressão. Vamos falar agora sobre um método que é bem utilizado para validar a estabilidade do seu modelo. Como mencionamos anteriormente, não podemos simplesmente ajustar um modelo aos meus dados de treino e esperar que ele funcione perfeitamente, ou até mesmo esperar que aquele seja o melhor modelo possível para fazer alguma validação. Falamos um pouco sobre isso quando discutimos sobre sobreajustamento e subajustamento. Então, vamos nos aprofundar sobre um método que nos garanta que o nosso modelo obteve a maioria dos padrões dos dados corretos sem captar muitos ruídos.\nO que é validação cruzada?\nValidação cruzada é uma técnica para avaliar um modelo de aprendizado de máquina e testar o seu desempenho. Pode ajudar a comparar e selecionar um modelo mais apropriado para o nosso problema. É bem fácil de entender, de implementar e tende a ter um viés menor do que outros métodos usados para o mesmo objetivo. Por isso é uma ferramenta tão utilizada. Tanto a validação cruzada quanto outros algoritmos funcionam de maneira semelhante, consiste em: dividir o conjunto de dados em treino e teste; treinar o modelo no conjunto treino; validar o modelo no conjunto teste e repetir as etapas anteriores algumas vezes. Dentro da validação cruzada existem diversas técnicas onde umas são mais utilizadas. Já mencionamos anteriormente o método\nk-fold, mas exite também os métodos, hold-out, leave-p-out, k-fold stratified, entre outros. Vamos falar sobre alguns deles.\n\nHold-Out Cross Validation: Esta é a técnica mais simples e comum. Ele consiste em remover uma parte dos dados de treinamento e usá-la para obter previsões do modelo treinado no restante dos dados. A estimativa de erro informa como nosso modelo está se saindo em dados não vistos ou no conjunto de validação. A implementação é extremamente fácil e existem pacotes que podem ajudar nisso. Mas apesar disto, esse método tem um grande desvantagem. Se estivermos trabalhando com um conjunto de dados que não é completamente uniforme, podemos acabar em uma situação difícil após a separação. O conjunto de treino pode não representar muito bem o conjunto de teste, ou seja, os conjuntos podem ser bem diferentes, onde um é mais fácil do que o outro.\nK-Fold Cross Validation: O K-Fold pode se apresentar como um técnica que minimiza as desvantagens do método Hold-Out apresentando uma nova maneira de dividir o banco de dados. Neste método os dados são divididos em k subconjuntos (daí o nome). O método de validação é repetido k vezes, onde, a cada vez, um dos k subconjuntos é usado como conjunto de teste e os outros k-1 conjuntos são unidos para formar o conjunto de treinamento. A estimativa de erro é a média de todas as k tentativas. Como cada ponto de dados chega a um conjunto de validação exatamente uma vez e a um conjunto de treinamento k-1 vezes, isso reduz significativamente o viés. Como “regra geral”, k=5 ou k=10 é escolhido, mas não existe nada fixo. Comparando diretamente ao método Hold-Out, o método K-Fold tende a ser melhor, mas também possui uma desvantagem. Aumentar o k resultado no treinamento de mais modelos e o processo de treinamento pode ser custoso e demorado.\nLeave-P-Out Cross Validation: Este método consiste em criar todos os conjuntos de treinamento e testes possíveis usando p amostras como conjunto de teste. Em outras palavras, deixa p pontos de dados fora dos dados de treino, ou seja, se houver n pontos de dados na amostra original, np amostras são usadas para treinar o modelo p pontos são usadas como conjunto teste. Como se pode imaginar, este método é extremamente exaustivo, tendo em vista que é preciso validar o modelo para todas as combinações possíveis e para um p demasiadamente grande, pode ser computacionalmente inviável.\n\nO método de validação cruzada também pode nos ajudar a ajustar hiperparâmetros, falaremos sobre isso posteriormente.\n\n\n7.1.7 Hiperparâmetros\nJá discutimos anteriormente sobre treinamento de um modelo e mencionamos os Hiperparâmetros, vamos agora discutir de um modo geral o que eles são e qual a sua importância. Hiperparâmetros são atributos que controlam o treinamento de um modelo. Eles ajudam a direcionar um modelo, melhorando seu desempenho e evitando que ele aprenda somente com os dados de treino, ou seja, evitar o overfitting e underfitting. Por exemplo, no processo de configuração de uma rede neural, precisamos decidir quantas camadas ocultas de nós precisam ser usadas entre a camada de entrada e a camada de saída, assim como quantos nós cada camada vai usar. Esses parâmetros não estão diretamente ligados aos dados de treino e não são aprendidos diretamente pelos modelos. Parâmetros são as variáveis ​​que o algoritmo de Machine Learning usa para prever resultados com base na entrada de dados históricos. Já os hiperparâmetros são variáveis ​​que são especificadas ao longo do processo de construção do modelo. Com isso os hiperparâmetros são fornecidos antes dos parâmetros, ou podemos dizer que os hiperparâmetros são utilizados para avaliar os parâmetros ideais do modelo. Encontrar a melhor combinação de hiperparâmetros pode fazer a diferença no seu modelo.\nExemplos comuns de hiperparâmetros\n\nNúmero de árvores em um algoritmo de Random Forest.\nA escolha da função custo ou perda que o modelo usará.\nNúmero de cadamadas ocultas em redes neurais.\nQuantidade mínima de observações dentro de um nó.\nTaxa de divisão de treino e teste.\nProporção de linhas para sortear por árvore.\n\n\n7.1.7.1 Ajuste de hiperparâmetro\nO ajuste de hiperparâmetro consiste em encontrar a configuração de hiperparâmetro que irá resultar no melhor desempenho do modelo. A busca manual pode ser utilizada para localizar os hiperparâmetros ótimos, utilizando uma abordagem de acerto e erro, mas obviamente isso levaria muito tempo. As técnicas mais utilizadas para isso são o Grid Search ou Random Search.\n\nGrid Search: Essa é uma técnica de ajuste que tenta calcular os valores ótimos dos hiperparâmetros. É calculado exaustivamente o desempenho do modelo para cada combinação de todos os hiperparâmetros fornecidos previamente e escolhe, a partir dai, o valor ideal para os hiperparâmetros. Apesar de ser uma técnica que foge da abordagem manual, é um processo demorado e computacionalmente caro.\nRandom Search: Random Search é um método que usa combinações aleatórias de hiperparâmetros para treinar o modelo. As melhores combinações são usadas. A diferença para o Grid Search é que aqui não é especificado um conjunto de valores para os hiperparâmetros. Em vez disso, os valores de cada hiperparâmetro são amostrados a partir de uma distribuição. Essa técnica permite controlar o número de tentativas de combinações de hiperparâmetros, diferente da Grid Search, onde todas as combinações possíveis são tentadas."
  },
  {
    "objectID": "supervisionada.html#agoritmos-de-aprendizado-de-máquina",
    "href": "supervisionada.html#agoritmos-de-aprendizado-de-máquina",
    "title": "7  Aprendizado supervisionado",
    "section": "7.2 Agoritmos de Aprendizado de máquina",
    "text": "7.2 Agoritmos de Aprendizado de máquina\n\n7.2.1 Árvores de Decisão\nAs Árvores de Decisão são algoritmos de aprendizado supervisionado que podem ser utilizados tanto para classificação quanto para regressão. Possui uma estrutura hierárquica em árvore, tendo um nó raiz, ramificações, nós internos e nós folhas. Na análise de decisão, uma árvore de decisão pode ser usada para representar visualmente e explicitamente decisões e tomadas de decisão.\n\n\n\n\n\nÁrvore de Decisçao\n\n\n\n\nNa imagem acima podemos ver uma representação de uma árvore de decisão, que começa com um nó raiz, que não possui ramificações de entrada, a partir da qual a árvore se divide em ramos. Os ramos alimentam os nós internos ou nós decisão. Cada ramificação contém um conjunto de atributos ou regras de classificação, associado a um determinado rótulo de classe, que pode ser encontrado na extremidade da ramificação. O final dos ramos que não se dividem mais são os nós folha. Os nós folha representam todos os resultados possíveis dentro do conjunto de dados.\nO tipo de estrutura de fluxograma é fácil de ser interpretado e cria uma representação que permite que diferentes grupos entendam melhor porque uma determinada decisão foi tomada.\nO algoritmo de Árvore de Decisão possui uma estratégia que busca identificar os pontos de divisão ideias dentro de uma árvore. O processo de divisão é repetido até que todos ou a maioria dos registros tenham sido classificados em um rótulo de classe específico. Classificar ou não todos os pontos de dados como conjuntos depende principalmente da complexidade do algoritmo. Árvores menores ou mais simples, são mais fáceis de atingir nós de folhas puros, onde pontos de dados caem em uma única classe, além de serem mais fáceis de visualizar e interpretar as decisões. Em contrapartida, aumentar o tamanho de uma árvore dificulta manter essa pureza, podendo resultar em poucos dados caindo em uma determinada subárvore, como consequência, isso pode levar ao overfitting. Portanto, esses algoritmos têm preferência por árvores pequenas e simples, adicionando complexidade apenas quando realmente necessário. Árvores de decisão geralmente crescem de forma arbitrária, portanto é preciso decidir quais recursos escolher e quais condições usar para dividir uma árvore, além de saber quando parar essa divisão.\n\n7.2.1.1 Medidas de seleção de atributo\nComo mencionado anteriormente, uma Árvore de decisão é feita a partir de atributos e decidir qual atributo colocar na raiz ou em diferentes níveis da Árvore pode ser complicado. Aqui a aleatoriedade na escolha não é uma boa opção, podendo gerar resultados com baixa precisão. Para o problema de seleção de atributos, temos alguns critérios como: Entropia e ganho de informação, índice de Gini , taxa de ganho, entre outros.\n\nEntropia: Entropia é uma medida de aleatoriedade (impureza) dos valores de uma amostra. Quanto maior a entropia, mais difícil é tirar conclusões dessa informação. É definida pela seguinte fórmula:\n\n\\[\nEntropia(S) = - \\sum_{i=1}^{c} p_{i}\\,log_2\\,pi\n\\]\nOnde, S representa o Estado atual e \\(p_i\\) é a probabilidade de um evento \\(i\\) do estado S.\nOs valores variam entre 0 e 1. Para selecionar o melhor recurso para dividir e encontrar a Árvore ideal, deve-se buscar o atributo com menor entropia.\n\nGanho de Informação: Ganho é usado para medir o quão bem um atributo separa os exemplos de treino de acordo com sua classificação alvo. Quanto maior o ganho de informação, melhor. Ou seja, na construção de uma Árvore de Decisão buscamos um atributo que retorne o maior ganho de informação e a menor entropia. Ganho de informação é calculado a diferença entre a entropia antes da divisão e a entropia média após a divisão do conjunto de dados. Segue a fórmula:\n\n\\[\nIG(Y,X) = Entropia(Y) - Entropia (Y|X)\n\\]\nPodemos visualizar a fórmula como: subtraímos a entropia de \\(Y|X\\) da entropia apenas de \\(Y\\) para calcular a redução da incerteza sobre \\(Y\\) dada uma informação adicional \\(X\\) sobre \\(Y\\).\n\nÍndice de Gini: Também conhecido como impureza de Gini, o Índice de Gini calcula a quantidade de probabilidade de um recurso específico que é classificado incorretamente quando selecionado aleatoriamente. Se todos os elementos estiverem vinculados a uma única classe, ela pode ser chamada de pura. É calculado subtraindo a soma das probabilidades ao quadrado de cada classe de um. Segue a fórmula:\n\n\\[\nGN = 1- \\sum_{i=1}^{n} (p_i)^2\n\\]\nOnde \\(pi\\) denota a probabilidade de um elemento ser classificado para uma classe distinta. O Índice de Gini é semelhante a entropia. Varia entre 0 e 1, onde 0 expressa a pureza da classificação e 1 indica a distribuição aleatória dos elementos em várias classes.\n\nTaxa de Ganho: Taxa de ganho tenta diminuir o viés do ganho de informação. Leva em consideração o número de ramificações que resultariam antes de fazer a divisão. Ele corrige o ganho de informação levando em consideração as informações intrínsecas de uma divisão. A Informação Intrínseca (II) é definida como a entropia das proporções do subconjunto de dados. Segue a fórmula: \\[\nII = -\\sum_{i=1}^n \\frac{N(t_i)}{N(t)} *log{_2}  \\frac{N(t_i)}{N(t)}\n\\]\nOnde \\(N(t_i)\\) é o número de vezes que \\(t_i\\) ocorre dividido pela contagem total de eventos \\(N(t)\\) onde \\(t\\) é o conjunto de eventos.\nA Taxa de Ganho então é dada por:\n\\[\nGR = \\frac{Ganho\\ de\\ Informação}{Informação\\ intrínseca}\n\\]\n\nPara todas as variáveis ​​preditoras, aquela que fornece a maior taxa de ganho é escolhida para a divisão.\n\n\n7.2.1.2 Tipos de Árvores de Decisão\nMencionamos anteriormente que o algoritmo de Árvores de Decisão pode ser usado tanto para classificação quanto para regressão, e é claro, existem vários tipos de algoritmos. Vamos falar um pouco sobre os mais famosos.\n\nID3 (Iterative Dichotomiser 3): Esse algoritmo usa uma abordagem gananciosa de cima para baixo para construir a Árvore. Em outras palavras, ele começa a construir a Árvore de cima e de para cada iteração seleciona o melhor recurso no momento para criar um nó. Geralmente é usado apenas para problemas de classificação com recursos nominais. O algoritmo utiliza o Ganho de Informação para encontrar o melhor recurso.\nC4.5: Este algoritmo é uma melhoria em relação ao ID3. Ele pode usar a Taxa de Ganho como função para encontrar o melhor recurso.\nCART: CART é uma abreviação de “classification and regression trees” (Árvores de classificação e Regressão). Como o próprio nome já diz, pode ser usado tanto para classificação quanto para regressão. O algoritmo busca o melhor critério através do Índice de Gini.\n\n\n\n7.2.1.3 Podando Árvores de Decisão\nComo mencionamos anteriormente, Árvores complexas podem causar overfitting, cabe dizer que as Árvores são os algoritmos mais suscetíveis ao overfitting. Em alguns casos, a Árvore poderá se ramificar inúmeras vezes, gerando uma folha para cada observação, ou seja, fornecendo 100% de precisão no conjunto de treino, logo, é preciso ter um limite.\nPodar uma Árvore de Decisão consiste em remover partes da Árvore que não fornecem poder para classificar instâncias. A poda pode ser distinguida em:\n\nPré-poda: A Árvore é interrompida antes de concluir a classificação do conjunto de treinamento. É um método alternativo que tenta interromper o processo de construção da árvore antes que ele produza folhas com amostras muito pequenas. Essa heurística é conhecida como parada antecipada. Em cada estágio de divisão da árvore, é verificado o erro de validação cruzada. Se o erro não diminuir significativamente, então paramos. A parada antecipada pode prejudicar o ajuste, parando muito cedo.\nPós-poda: Normalmente a técnica mais utilizada, permite que a Árvore classifique o conjunto de treinamento antes de podá-la. Este método consiste em, a partir da Árvore não podada, pega uma sequência de subárvores (podadas) e escolher a melhor por meio da validação cruzada. É escolhida a que tem maior precisão no conjunto de treinamento validado cruzadamente.\n\n\n\n7.2.1.4 Floresta Aleatória (Random Forest)\nA Floresta Aleatória é um algoritmo que funciona a partir de Árvores de Decisão. Ele constrói diferentes Árvores em diferentes amostras e leva combina as saídas para alcançar um único resultado, levando em consideração a maioria de votos para classificação e média em caso de regressão. O algoritmo serve tanto para regressão quanto para regressão, mas aqui temos uma novidade, ele pode lidar com os dois, apesar de apresentar melhores resultados para problemas de classificação.\n\n\n7.2.1.5 Funcionamento do algoritmo\nAntes de falarmos de fato sobre o funcionamento do algoritmo, vamos entender de forma básica o método dos conjuntos. O método dos conjuntos, também conhecido como ensemble methods, pode ser definido resumidamente como, combinar vários modelos. No caso da Floresta Aleatória, uma combinação de Árvores de decisão. Os modelos são combinados e suas previsões são agregadas para identificar o resultado mais popular. Os métodos de conjunto mais populares são bagging e boosting. Em resumo o método bagging, que é usado em Floresta Aleatória, cria um subconjunto de treinamento diferente a partir dos dados de treinamento de amostra com substituição, o que significa que os pontos de dados individuais podem ser escolhidos mais de uma vez. Depos, dependendo do tipo da tarefa, classificação ou regressão, a saída final é baseada na votação da maioria ou a média. O método boosting é utilizado para outros algoritmos como, por exemplo, XGBOOST, portanto, falaremos sobre ele mais pra frente.\nCerto, entendido o método dos conjuntos podemos entrar no algoritmo em si. É basicamente uma extensão do método bagging, já que utiliza o método bagging e a aleatoriedade para criar uma floresta não correlacionada de Árvores de Decisão. A aleatoriedade se dá ao fato que o modelo utiliza uma amostragem aleatória do conjunto de dados de treinamento ao construir árvores e subconjuntos aleatórios de recursos considerados ao dividir os nós, o que garante baixa correlação entre as Árvores.\nOs algoritmos possuem alguns hiperparâmetros, que como explicado anteriormente, precisam ser definidos antes do treinamento do modelo. Os três principais são: o tamanho do nó, o número de Árvores e o número de recursos amostrados. A partir daí o algoritmo pode ser usado para resolver problemas de regressão ou classificação.\n\n\n\n7.2.2 Algoritmos de boosting\nComo falamos anteriormente, boosting é uma técnica de método dos conjuntos. Diferente de bagging que trabalha de forma paralela, considerando os modelos independentes uns dos outros, boosting trabalha de forma diferente. Boosting se refere a algoritmos que convertem alunos fracos em alunos fortes. Trabalha de maneira sequencial, ajustando iterativamente o peso da observação de acordo com a última classificação, assim, diminui o erro do viés e constrói preditivos fortes. Em outras palavras, a técnica consiste em, primeiramente, construir um modelo inicial com os dados de treinamento. Depois, um outro modelo é construído visando corrigir os erros do modelo anterior, atribuindo pesos caso uma entrada seja fornecida erroneamente. O processo continua e adiciona modelos até que todo o conjunto de dados de treino seja previsto corretamente ou o número máximo de modelos seja adicionado. Boosting é um algoritmo genérico e não um modelo, vamos então apresentar os principais modelos baseados no método boosting: Adaptative Bossting (AdaBoost), Gradient Boosting e XGBoost.\n\n7.2.2.1 AdaBoost\nAntes de entrarmos no algoritmo AdaBoost de fato, vamos relembrar o Algoritmo de Floresta Aleatória. Em Floresta Aleatória, o algoritmo cria diversas Árvores que consistem em um nó inicial com várias nós folhas. Não existem regras quanto ao tamanho de cada Árvore, assim, pode haver umas maiores que outras. No modelo AdaBoost, as Árvores possuem um nível, ou seja, apenas 1 divisão conhecido como Stump (toco).\nReforçando, o algoritmo tem apenas um nó com duas folhas. Tocos são aprendizes fracos, pois as técnicas de reforço preferem isso. Como em AdaBoost o erro do primeiro toco influência nos outros, a ordem dos tocos é muito importante. Reforçando, o algoritmo irá atribuir pesos mais altos aos pontos classificados erroneamente. Todos os pontos que têm pesos maiores recebem mais importância no próximo modelo.\n\n\n7.2.2.2 Funcionamento do algoritmo\nPara entendermos de uma forma um pouco mais clara, vamos fazer um passo a passo. Antes do primeiro passo de fato, todos os pontos de dados irão receber algum peso. Inicialmente, os pesos serão iguais para todos. O peso é calculado na forma \\(W = \\frac{1}{N}\\), onde \\(N\\) é o número de observações.\n\nPasso 1: Em primeiro lugar, o algoritmo pega o primeiro recurso e cria o primeiro tronco. Ele irá criar o mesmo número de tocos que o número de recursos, a partir daí, temos as primeiras Árvores de Decisão. O modelo então calcula o índice de Gini para cada Árovre e seleciona aquela com o índice mais baixo para ser o primeiro aprendiz base.\nPasso 2: O segundo passo é calcular o desempenho do toco com um método conhecido como “Importance” ou “Influence”. É calculado usando a seguinte fórmula:\n\\[\n\\mbox{Performance} (\\alpha) = \\frac{1}{2} log(\\frac{1- Erro\\ total }{Erro\\ total})\n\\]O valor sempre será um valor entre 0 e 1, onde 0 indica um toco perfeito e 1 um toco horrível. Calcular o desempenho de um toco é importante pois é preciso atualizar o peso da amostra antes de prosseguir para o próximo modelo. Se o mesmo peso for apicado, a saída recebida será a do primeiro modelo. Previsões erradas receberão mais peso, enquanto os pesos das previsões erradas serão diminuídos. Assim, no próximo modelo com os pesos atualizados, sera dada mais preferência aos pontos com pesos maiores.\nPasso 3: O próximo passo é atualizar os pesos. A atualização é feita utilizando a seguinte fórmula:\n\\[\n\\mbox{Novo peso} = \\mbox{peso antigo} * e^{\\pm Performance (\\alpha)}\n\\] A quantidade \\(\\alpha\\) será negativa quando a amostra for classificada corretamente e positiva quando classificada incorretamente. Vale ressaltar que a soma total de todos os pesos deve ser igual a 1. Em muitos casos os pesos atualizados não irão somar 1, então será necessário normalizar os pesos. A normalização é feita dividindo cada peso pela soma total dos pesos atualizados. Após a normalização dso pesos da amostra, a soma será 1.\n\nPasso 4: Agora, para verificar se os erros diminuíram ou não, a próxima etapa é criar um novo conjunto de dados. Para isso, com base nos novos pesos amostrais, nossas observações serão divididas em baldes, basicamente são intervalos de valores de \\(\\alpha\\). A partir daí, o algoritmo seleciona números aleatórios entre 0 e 1. Ele verifica em qual balde o valor selecionado pertence e seleciona esse registro no novo conjunto de dados. Como registros classificados incorretamente têm pesos amostrais maiores, existe mais probabilidade de serem selecionados várias vezes.\n\nPasso 5: Por fim, com o novo conjunto de dados, o algoritmo cria uma nova Árvore de decisão (toco) e repete o processo desde o primeiro passo até passar por todos os tocos. O processo é repetido até que um erro de treinamento baixo seja alcançado. Suponha um algoritmo simples que tenha construído 3 Árvores de Decisão de maneira sequencial. Assim, o conjunto de dados teste passará por todas as Árvores e, da mesma forma que em Florestas Aleatórias, a classe será selecionada com base na maioria. A partir daí é possível fazer previsões para o conjunto de teste.\n\n\n\n7.2.2.3 Gradient Boosting (Aumento de Gradiente)\nO modelo Gradient Boosting, também é um modelo de reforço, ou seja, utiliza técnicas de conjunto que pode ser usado tanto para regressão quanto para classificação. Este modelo possui algumas semelhanças com o Ada Boost e como já vimos sobre ele, aqui vamos focar um pouco mais em suas diferenças.\nA principal diferença deste modelo para o Ada Boost está no que ele faz com os valores subajustados do seu antecessor. Enquanto Ada Boost ajusta os pesos a cada interação, Gradient Boosting tenta ajustar o novo preditor aos erros residuais cometidos pelo preditor anterior.\nOutra diferença é que, enquanto Ada Boost começa construindo um toco, Gradient Boost começa fazendo uma única folha. Esta folha representa uma estimativa inicial para os pesos das amostras. Em caso de um problema de regressão, ao tentar prever um valor contínuo, o primeiro palpite é o valor médio. A partir daí, o modelo constrói uma Árvore. Aqui, a Árvore é maior que um toco, mas o modelo ainda restringe o tamanho dessa Árvore.\n\n\n7.2.2.4 Funcionamento do modelo\nVamos entender um pouco melhor o funcionamento do modelo fazendo um passo a passo resumido, assim como foi feito em Ada Boost. Vamos exemplificar também focando no modelo de Gradient Boosting para regressão.\n\nPasso 1: Como falamos anteriormente, o modelo começa criando uma única folha. Como estamos em um cenário de dados contínuos, essa folha então é a média das observações. Essa seria nossa previsão para o modelo base.\nPasso 2: O próximo passo é calcular os pseudo-resíduos, que basicamente é o valor observado - valor previsto. O termo pseudo-resíduo é baseado na regressão linear, onde a diferença entre os valores observados e os valores previstos resulta em resíduos. Utilizando os resíduos como alvo é possível gerar novas previsões. Neste caso, as previsões serão os valores de erros.\nPasso 3: Agora é combinado a folha original com a nova Árvore para fazer uma nova predição a partir dos dados de treinamento.\n\nEm resumo, o algoritmo começa com uma única folha. Em seguida é adicionada uma Árvore com base nos resíduos e dimensiona a contribuição das árvores para a previsão final com a taxa de aprendizado. A partir daí continua adicionando Árvores com base nos erros cometidos pela Árvore anterior.\nO modelo possui alguns hiperparâmetros importantes como, o número de estimadores, a profundidade máxima da Árvore, a divisão mínima das amostras, entre outros. Para obter valores precisos desses hiperparâmetros é possível utilizar as técnicas mencionadas anteriormente.\nResumimos bastante os passos e o funcionamento do modelo, mas deve-se ter em mente que existe uma matemática carregada por trás desses modelos. Cada passo, cada função perdida é utilizada por um motivo comprovado.\n\n\n\n7.2.3 Support Vector Machine (SVM)\nPara finalizar, vamos introduzir um último algoritmo muito conhecido no ramo de aprendizado de máquinas. Support Vector Machine, ou SVM, é um algoritmo de aprendizagem supervisionada muito popular que pode ser usado tanto para classificação quanto para regressão. Entretando, esse algoritmo é usado principalmente para classificação, então, focaremos mais nessa parte.\nA ideia basica do algoritmo é relativamente simples: criar uma linha ou um hiperplano que separa os dados em classe para que possamos colcoar facilmente o novo ponto de dados na categoria correta futuramente.\n\n7.2.3.1 Como o modelo funciona\nA ideia inicial do SVM é encontrar uma linha ou hiperplano entre os dados de duas classes. O algoritmo então escolhe pontos ou vetores extremos que ajudam a criar o hiperplano. Esses casos extremos são os vetores de suporte (daí o nome). O SVM recebe os dados de entrada e gera uma linha que separa essas classes da melhor forma possível.\nUm exemplo bem simples para entender o modelo é: suponha que tenhamos um conjunto de dados com duas classes, bolas vermelhas e quadrados azuis. precisamos então encontrar uma linha que separe esse conjunto de dados em duas classes (vermelho e azul).\nComo estamos em um espaço 2-d, uma única linha reta pode separar essa margem. Mas existe uma infinidade de linhas que podem separar essas classes, o SVM então busca encontrar a ideal. Mas como?\nO SVM encontra o ponto mais próximo das linhas de ambas as classes. Esses pontos são chamados de vetores de suporte. Esses pontos são chamados de vetores de suporte e a distância entre os vetores e o hiperplano é chamada de margem. O SVM então busca maximizar essa margem, onde, o hiperplano com margem máxima é chamado de hiperplano ótimo.\nCerto, mas nem sempre estamos um cenário onde nossos dados estão linearmente organizados. Neste caso, não podemos apenas desenhar uma única linha reta. Neste cenário, precisamos adicionar mais uma dimensão. Se para dados lineares trabalhos com duas dimensões (x e y), para dados lineares teremos uma terceira dimensão (z). Deixamos as coordenadas no eixo z serem governadas pela restrição,\n\\[z =x^2 + y^2\\]\n\nAssim, a coordenada z é basicamente o quadrado da distância do ponto à origem.\nFazendo isso, os dados se tornam linearmente separáveis. Seja z = k, onde k é uma constante. Como \\(z =x^2 + y^2\\), temos \\(k =x^2 + y^2\\); que é uma equação de um círculo. Com isso, é possível projetar o separador linar em dimensão superior de volta as dimensões originais usando esta transformação. Concluindo, podemos então classificar os dados adicionando uma dimensão extra a eles, fazendo com que se tornem linearmente separáveis. Encontrar a transformação correta para qualquer conjunto de dados não é tão fácil, mas existem recursos que podem ser usados na implementação do algoritmo para ajudar neste trabalho."
  },
  {
    "objectID": "supervisionada.html#considerações-finais",
    "href": "supervisionada.html#considerações-finais",
    "title": "7  Aprendizado supervisionado",
    "section": "7.3 Considerações finais",
    "text": "7.3 Considerações finais\nNeste capítulo buscamos abordar os principais tópicos do aprendizado supervisionado. O objetivo aqui foi dar uma breve noção sobre o funcionamento dos algoritmos, diferenças nos métodos de funciomanetos, possíveis problemas e dificuldades. Aqui foi apenas o pontapé inicial para que você consiga entender e implementar algoritmos supervisionados."
  },
  {
    "objectID": "tutorialr.html#sobre-o-software-r",
    "href": "tutorialr.html#sobre-o-software-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.1 Sobre o software R",
    "text": "A.1 Sobre o software R\nR é um ambiente computacional e uma linguagem de programação para manipulação, análise e visualização de dados. Para essas finalidades, ele é considerado um dos melhores e um dos mais utilizados dentre os ambientes computacionais disponíveis. O R é mantido pela R Development Core Team e está disponível para diferentes sistemas operacionais: Linux, Mac e Windows.\nO software é livre, ou seja, gratuito, com código aberto em uma linguagem acessível. Nele, estão implementadas muitas metodologias estatísticas. Muitas dessas fazem parte do ambiente base do R e outras acompanham o ambiente sob a forma de pacotes, o que o torna altamente flexível. Os pacotes são bibliotecas com funções extras devidamente documentadas criadas para ajudar a resolver problemas de diferentes áreas do conhecimento.\nO R possui uma comunidade extremamente ativa, engajada desde o aprimoramento de ferramentas e desenvolvimento de novas bibliotecas, até o suporte aos usuários. Sobre o desenvolvimento de novas bibliotecas, um pesquisador em Estatística que desenvolve um novo modelo estatístico pode disponibilizá-lo em um pacote acessível aos usuários que se interessem pelo modelo, por exemplo. Além disso, a disponibilidade e compartilhamento da pesquisa em um pacote no R é uma boa prática quando falamos de reprodutibilidade na ciência. Ainda nesse ponto, realizar as análises de uma pesquisa aplicada em um programa livre e acessível a todos é um dos principais pontos para permitir reprodutibilidade.\nOptar por programar em R também implica na escolha de uma IDE (Integrated Development Environment). Uma IDE é um ambiente de desenvolvimento integrado onde podem ser combinadas ferramentas utilizadas no desenvolvimento de aplicações, como um editor de código ou uma ferramenta de preenchimento inteligente de código. Para o R, a IDE mais popular entre os usuários é o RStudio. O RStudio é um conjunto de ferramentas integradas projetadas para editar e executar os códigos em R. Assim, quando for o interesse utilizar o R, basta abrir o RStudio (R é automaticamente carregado)."
  },
  {
    "objectID": "tutorialr.html#instalação-do-r",
    "href": "tutorialr.html#instalação-do-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.2 Instalação do R",
    "text": "A.2 Instalação do R\nA seguir, será apresentado o passo a passo de como instalar o R e o RStudio para os três sistemas operacionais: Windows, MAC e Linux, respectivamente.\n\nA.2.1 R no Windows\nA forma mais simples de instalar o R consiste em primeiramente acessar a página do software pelo endereço https://cloud.r-project.org/. Ao acessar a página haverão três opções para download, sendo cada uma referente a um sistema operacional em específico. Assim, para conseguir instalar o software em um sistema operacional Windows basta primeiramente clicar no link Download R for Windows.\n\n\n\n\n\nPasso 1\n\n\n\n\nQuatro subdiretórios irão surgir, dentre eles é necessário clicar na base, pois este contém a distribuição base do R para instalação.\n\n\n\n\n\nPasso 2\n\n\n\n\nO subdiretório base irá redirecionar para uma página que contém o link de download do arquivo de instalação do software. Este por sua vez, pode ser identificado como Download + versão atual do R + for Windows.\n\n\n\n\n\nPasso 3\n\n\n\n\nFeito isso, um arquivo executável será baixado no computador, o qual, ao abri-lo, deverá escolher o idioma (português brasileiro) e simplesmente clicar em Avançar toda vez que o cliente de instalação requerer.\n\n\n\n\n\nPasso 4\n\n\n\n\n\n\n\n\n\nPasso 5\n\n\n\n\nAssim, uma instalação padrão do software será instalada no computador.\n\n\nA.2.2 R no MAC\nDa mesma forma a qual iniciamos a instalação do R no Windows também iniciaremos no MAC, onde é necessário acessar o endereço https://cloud.r-project.org/ e clicar no link Download R for macOS.\n\n\n\n\n\nPasso 1\n\n\n\n\nO link irá redirecionar para uma página com arquivos de extensão .pkg típicos de macOS. É importante verificar qual versão disponível é a ideal para seu sistema. A versão do tipo arm64.pkg é referente a versão mais recente do macOS na data deste material.\n\n\n\n\n\nPasso 2\n\n\n\n\nTendo feito o download do arquivo, basta abri-lo para um cliente de instalação ficar disponível, e então, para efetuar uma instalação padrão deve-se seguir as instruções do cliente sem customizações aditivas assim como foi feito para o Windows.\n\n\nA.2.3 R no Linux\nA instalação do R no Linux depende da distribuição sendo utilizada. Basta acessar o mesmo endereço https://cloud.r-project.org/ utilizado na instalação dos outros sistemas, e clicar no link Download R for Linux.\n\n\n\n\n\nPasso 1\n\n\n\n\nFeito isso irá aparecer as opções de distribuições para Linux em que o software está disponível para download, basta selecionar a distribuição compatível. Caso sua distribuição for Ubuntu por exemplo, clicamos nela no respectivo link.\n\n\n\n\n\nPasso 2\n\n\n\n\nAssim, irá ser redirecionado para uma página com as devidas instruções de instalação do R para a distribuição escolhida. Basta seguir as instruções para efetuar uma instalação padrão do software."
  },
  {
    "objectID": "tutorialr.html#instalação-do-rstudio",
    "href": "tutorialr.html#instalação-do-rstudio",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.3 Instalação do RStudio",
    "text": "A.3 Instalação do RStudio\nO RStudio é um conjunto de ferramentas integradas projetadas (IDE - Integrated Development Environment) da linguagem R para auxiliar na produtividade ao utilizar o R. Embora não seja obrigatório o seu uso, é um consenso na comunidade de que o uso do RStudio facilita o aprendizado enquanto acelera a produtividade do usuário, tornando-o indispensável principalmente para iniciantes.\nNo ano de 2022, RStudio iniciou um processo de transição de nome onde passou a se chamar Posit. O objetivo por de trás desse processo se dá na inclusão da comunidade de Python ao R, dado o crescimento notório do Python na área de análise de dados nos últimos anos e que ambas as linguagens se complementam.\nO primeiro passo para instalar o RStudio é acessar o site da Posit e ir até a página de download que pode ser acessada pelo endereço https://posit.co/download/rstudio-desktop/. Feito isso, a página irá apresentar algumas opções, dentre elas uma breve tabela com arquivos executáveis mais recentes disponíveis de instalação do RStudio.\n\n\n\n\n\nArquivos executáveis de instalação\n\n\n\n\nDentre os arquivos executáveis está a versão mais recente para Windows (retângulo vermelho), macOS (retângulo azul) e para diferentes distribuições do Linux (retângulo verde). É preciso fazer o download conforme o seu sistema operacional.\nApós o download basta abrir o arquivo executável baixado e seguir as instruções do cliente para que a instalação seja feita.\n\n\n\n\n\nRStudio aberto pela primeira vez"
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-rstudio",
    "href": "tutorialr.html#primeiros-passos-no-rstudio",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.4 Primeiros passos no RStudio",
    "text": "A.4 Primeiros passos no RStudio\nO RStudio é uma ferramenta que por padrão é dividida em quatro painéis, sendo que cada um deles contêm abas com diferentes utilidades.\n\n\n\n\n\nPainéis do RStudio\n\n\n\n\nA seguir descrevemos melhor os painéis e algumas abas comumente utilizadas do RStudio:\n Editor/Scripts: local para escrever códigos (principalmente arquivos em formato .R).\n Console: onde se executa os códigos e visualiza resultados.\n Aqui, é possível acessar todos os objetos criados em Environment e o histórico de códigos executados em History e conectar fonte de dados em Connections.\n Nessa área, temos diversas utilidades frequentemente utilizadas:\n\npodemos acessar arquivos e pastas do computador pela aba Files;\nna aba Plots, visualizamos resultados em que são gerados figuras (como gráficos e tabelas), caso um comando desse tipo tenha sido executado;\nem Packages, podemos manusear pacotes (instalar, atualizar ou deletar);\nna aba Help temos acesso à documentação de uma determinada função quando utilizado o comando help() ou ?. Uma função nada mais é do que uma estrutura de código pronta com a forma de acesso nome(argumento) que recebe argumentos de entrada e retorna uma resposta. O próprio comando help() é uma função.\n\nO usuário pode alterar as configurações padrões do RStudio ao acessar as opções globais.\n\n\n\n\n\nOpções globais\n\n\n\n\nPara usuários iniciantes, é recomendável configurar a aparência e estrutura (layout) dos painéis conforme a própria preferência para tornar a experiência de uso mais confortável.\n\n\n\n\n\nMenu de aparência\n\n\n\n\nPodemos alterar o layout pelo menu Panel Layout. Usualmente, os painéis são estruturados de forma que o painel Console fique ao lado do painel de Script (Source/Editor), facilitando a visualização dos comandos rodados.\n\n\n\n\n\nMenu de estruturação dos painéis\n\n\n\n\n\nA.4.1 Projetos\nUma funcionalidade importante é a criação de projetos, permitindo dividir o trabalho em múltiplos ambientes, cada um com o seu diretório, documentos e workspace.\nPara criar um projeto, os seguintes passos podem ser seguidos:\n\nClique na opção File do menu, e então em New Project.\nClique em New Directory.\nClique em New Project.\nEscreva o nome do diretório (pasta) onde deseja manter seu projeto, exemplo: “my_project”.\nClique no botão Create Project.\n\nPara criar um novo script para escrever os códigos, vá em File -&gt; New File -&gt; R Script.\n\n\nA.4.2 Boas práticas\nComente bem o seu código: é possível fazer comentários usando o símbolo #. É sempre bom explicar o que uma variável armazena, o que uma função faz, por que alguns parâmetros são passados para uma determinada função, qual é o objetivo de um trecho de código, etc.\nEvite linhas de código muito longas: usar linhas de código mais curtas ajuda na leitura do código.\nEscreva um código organizado. Por exemplo, adote um padrão no uso de minúsculas e maiúsculas, uma lógica única na organização de pastas e arquivos, pode ser adotada uma breve descrição (como comentário) indicando o que um determinado script faz.\nCarregue todos os pacotes que irá usar sempre no início do arquivo: quando alguém abrir o seu código será fácil identificar quais são os pacotes que devem ser instalados e quais dependências podem existir."
  },
  {
    "objectID": "tutorialr.html#primeiros-passos-no-r",
    "href": "tutorialr.html#primeiros-passos-no-r",
    "title": "Apêndice A — Tutorial de R",
    "section": "A.5 Primeiros passos no R",
    "text": "A.5 Primeiros passos no R\nO código pode ser escrito no Script e então ser executado ao apertar o botão Run (localizado no painel de Script) ou com o atalho no teclado Ctrl + Enter. É importante salientar que, apenas a linha em que o símbolo de inserção de código (barra vertical do cursor) estiver é que será executada. Para executar múltiplas linhas simultaneamente, é necessário selecionar as linhas desejadas e então utilizar o comando de execução mencionado.\nOutra forma de escrever e executar códigos é através do painel Console. Normalmente, o Console é utilizado para executar códigos sem muitas linhas de estruturação ou para fazer testes rápidos (ex: uso do R como calculadora). Para rodar o código diretamente pelo painel Console, basta escrevê-lo na linha em que contém o símbolo &gt;, o qual indica que o R está pronto para receber comandos, e então pressionar a tecla Enter.\n\nA.5.1 R como calculadora\nUma das utilidades do R é utilizá-lo como uma calculadora, onde podemos realizar contas matemáticas simples até as mais complexas.\nPor padrão, o R entende as linhas de códigos da esquerda para a direita e de cima para baixo. No entanto, ao se deparar com operações matemáticas, ele respeita algumas prioridades. A operação com maior para a menor prioridade é: potenciação &gt; multiplicação ou divisão &gt; adição ou subtração. Caso haja a necessidade de alterar essa ordem, isso pode ser feito utilizando parênteses.\n\n# Adição.\n10 + 15\n\n[1] 25\n\n# Subtração.\n10 - 2\n\n[1] 8\n\n# Multiplicação.\n2 * 10\n\n[1] 20\n\n# Divisão.\n30/2\n\n[1] 15\n\n# Raiz quadrada.\nsqrt(4)\n\n[1] 2\n\n# Potência.\n2^2\n\n[1] 4\n\n# Potência &gt; Multiplicação &gt; Soma.\n2^2 + 5 * 2\n\n[1] 14\n\n# Multiplicação &gt; Potência &gt; Soma.\n2^2 + (5 * 2)\n\n[1] 14\n\n# Potência &gt; Soma &gt; Multiplicação.\n2 * (2^2 + 5) \n\n[1] 18\n\n\nCaso um comando incompleto seja dado, como 10 ^, o R mostrará um +. Isso não tem a ver com a soma e apenas que o R está esperando que o comando que estava sendo escrito seja finalizado. Para recomeçar, basta terminar a escrita do comando ou apenas pressionar Esc.\nVale também ressaltar que se um comando que o R não reconhece for dado, ele retornará uma mensagem de erro.\n\n\nA.5.2 Atribuição\nOs objetos (também chamados de variáveis) são “locais” onde são guardadas informações (números, textos etc). O ato de “guardar” informações dentro de objetos é chamado de atribuição, e pode ser feito com &lt;- ou =. Embora ambas as formas funcionem, na prática, o sinal &lt;- é usualmente utilizado para atribuições enquanto que o sinal = é utilizado para configurar argumentos de funções.\n\n# Variável x recebe o número 5 de diferentes formas.\nx &lt;- 5 \n\nx = 5\n\ny = (2^2 + 6) - 4\nx &lt;- y - 1\n\nUm ponto importante a se atentar é que o R é case sensitive, isto é, faz a diferenciação entre as letras minúsculas e maiúsculas. Portanto, x é diferente de X.\n\n# Dica: Podemos obter o output do comando ao colocá-lo em volta de ().\n(x &lt;- 10/2)\n\n[1] 5\n\n# Ao chamar X obteremos um erro, pois a variável criada era minúscula.\nX\n\nError in eval(expr, envir, enclos): objeto 'X' não encontrado\n\n\n\n\nA.5.3 Objetos em R\nExistem cinco classes básicas de objetos no R:\n\nCharacter: “UAH!”\nNumeric: 0.95 (números reais)\nInteger: 100515 (inteiros)\nComplex: 2 + 5i (números complexos, a + bi)\nLogical: TRUE (booleanos, TRUE/FALSE)\n\nApós realizar a atribuição, podemos verificar a classe do objeto com a função class().\n\n# Character/texto, deve estar entre aspas \"\".\nx &lt;- \"gestante\"; \nclass(x) \n\n[1] \"character\"\n\n# Numeric/números reais.\nx &lt;- 0.9 \nclass(x) \n\n[1] \"numeric\"\n\n# Integer/números inteiros, tem que ser atribuído com o valor acompanhado de um ‘L’.\nx &lt;- 5L\nclass(x)\n\n[1] \"integer\"\n\n# Complex/números complexos.\nx &lt;- 2 + 5i\nclass(x)\n\n[1] \"complex\"\n\n# logical/valores lógicos.\nx &lt;- TRUE\nclass(x)\n\n[1] \"logical\"\n\n\nOs valores lógicos são apresentados em letra maiúscula. Isso é muito importante, pois o R diferencia letras maiúsculas de minúsculas. Então, valores lógicos só são reconhecidos se escritos como TRUE ou FALSE. Além disso, cada valor lógico assume um valor numérico, sendo TRUE referente ao valor 1 e FALSE referente ao valor 0.\n\n# Operações matemáticas com valores lógicos.\n(TRUE*2)^2 + TRUE + FALSE + 2*TRUE\n\n[1] 7\n\n\nMuitas vezes é do interesse do usuário apagar objetos que foram criados, principalmente se for rodar códigos prontos em um ambiente que outra pessoa estava trabalhando, pois pode haver objetos já criados com os mesmos nomes dos que se encontram no código/script de interesse, o que poderá levar a erros e dificuldades de execução. A remoção de objetos pode ser feito com a função rm() ou remove().\n\n# Criando o objeto x.\nx &lt;- 20\nx\n\n[1] 20\n\n# Removendo o objeto x.\nrm(x)\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\n# Removendo todos os objetos criados.\n(x &lt;- 1)\n\n[1] 1\n\n(y &lt;- 2)\n\n[1] 2\n\nrm(list=ls())\n\nx\n\nError in eval(expr, envir, enclos): objeto 'x' não encontrado\n\ny\n\nError in eval(expr, envir, enclos): objeto 'y' não encontrado\n\n\nVale notar que ao utilizar a função rm() ou a função remove() para remover todos os objetos criados, é necessário incluir um argumento chamado list onde utilizamos o sinal de = para especificar os objetos a serem deletados. A função ls() lista todos os objetos criados até o momento.\n\n\nA.5.4 Vetores\nNo R a estrutura mais básica de dados é chamada de Vector (vetor), podendo aparecer no formado Atomic (atômico) ou no formado de list (lista). Dentre os vetores atômicos existem quatro tipos, sendo eles: Character, Integer, Double e Logical.\n\nCom vetores podemos atribuir vários valores a um mesmo objeto. Para entrar com vários números (ou nomes, ou qualquer outro grupo de coisas), precisamos usar uma função para dizer ao programa que os valores serão combinados em um único vetor. Para criar vetores atômicos a função c() é a mais usual por podermos criar vetores atômicos de todos os tipos diretamente. Também podemos utilizar a função seq() e o símbolo : para criar vetores do tipo Integer, e a função rep() que é capaz de criar vetores Double, por exemplo. Além disso, podemos verificar o tipo do vetor com a função typeof().\n\n# Vetor Double com a função c().\n(vetor1 &lt;- c(2.5, 3, 4/5))\n\n[1] 2.5 3.0 0.8\n\ntypeof(vetor1)\n\n[1] \"double\"\n\n# Vetor Integer com a função c().\n(vetor2 &lt;- c(5L, 7L, 9L))\n\n[1] 5 7 9\n\ntypeof(vetor2)\n\n[1] \"integer\"\n\n# Vetor Character com a função c().\n(vetor3 &lt;- c(\"hospital1\", \"hospital2\"))\n\n[1] \"hospital1\" \"hospital2\"\n\ntypeof(vetor3)\n\n[1] \"character\"\n\n# Vetor Logical com a função c().\n(vetor4 &lt;- c(TRUE, FALSE, FALSE, TRUE))\n\n[1]  TRUE FALSE FALSE  TRUE\n\ntypeof(vetor4)\n\n[1] \"logical\"\n\n# Vetor Integer com a função seq().\n(vetor5 &lt;- seq(1, 5))\n\n[1] 1 2 3 4 5\n\ntypeof(vetor5)\n\n[1] \"integer\"\n\n# Vetor Integer com o símbolo :.\n(vetor6 &lt;- 1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ntypeof(vetor6)\n\n[1] \"integer\"\n\n# Vetor Double com a função rep(). \n(vetor7 &lt;- rep(1,10))\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\ntypeof(vetor7)\n\n[1] \"double\"\n\n\nÉ comum o usuário querer saber o tamanho do vetor que ele está trabalhando, isso pode ser feito com a função length(). Além disso, é importante ter certeza de que estamos trabalhando com um vetor atômico, o que pode ser verificado com a função is.vector().\n\n# Podemos construir um vetor com vetores dentro da função c().\n(vetor &lt;- c(c(1, 2), rep(1, 2), seq(1, 2), 1:2))\n\n[1] 1 2 1 1 1 2 1 2\n\nis.vector(vetor)\n\n[1] TRUE\n\ntypeof(vetor)\n\n[1] \"double\"\n\nlength(vetor)\n\n[1] 8\n\n\nObserve que é possível criar um vetor com elementos de diferentes tipos. Sabemos que a função rep() gera um vetor de tipo Double e a seq() gera um vetor de tipo Integer, e ao criar um vetor utilizando a função c() em conjunto com estas obtemos um vetor de tipo Double, de forma que o R priorizou este tipo ao invés do Integer. No R isso é chamado de coerção, onde o vetor sendo criado irá manter o tipo de maior prioridade dentre os seus elementos, e os elementos de tipos com menor prioridade serão convertidos para o tipo prioritário. Isso ocorre, pois todos os elementos de um vetor atômico devem ter o mesmo tipo. Para os tipos apresentados temos como o de menor prioridade para o maior: Logical &lt; Integer &lt; Double &lt; Character. Além disso, se considerarmos Complex e List, teremos List com maior prioridade seguido de Character e Complex.\nPode ser do interesse do usuário visualizar elementos específicos que existem dentro de um vetor, isso pode ser feito ao especificar a posição do elemento dentro do vetor entre os símbolos [].\n\n# vetor com varios elementos.\nvet &lt;- c(TRUE, 5, 7L, \"hospital\")\ntypeof(vet)\n\n[1] \"character\"\n\n# elemento de posição 3.\nvet[3]\n\n[1] \"7\"\n\n# elementos das posições 2, 3 e 4.\nvet[2:4]\n\n[1] \"5\"        \"7\"        \"hospital\"\n\n\nAs operações vetoriais podem ser realizadas de maneira bastante intuitiva, pois em vetores atômicos as operações são realizadas elemento a elemento.\n\n# Operações com vetores.\nvetor1 &lt;- c(4, 9, 16)\n(vetor1_menos1 &lt;- vetor1 - 1)\n\n[1]  3  8 15\n\n(vetor1_vezes2 &lt;- vetor1 * 2)\n\n[1]  8 18 32\n\n(vetor1_dividido2 &lt;- vetor1/2)\n\n[1] 2.0 4.5 8.0\n\n(vetor1_raiz &lt;- sqrt(vetor1))\n\n[1] 2 3 4\n\nvetor2 &lt;- c(1, 2, 3)\n(vetor1_mais_vetor2 &lt;- vetor1 + vetor2)\n\n[1]  5 11 19\n\n\nVamos agora considerar vetores de pesos (quilos) e alturas (metros) de 6 pessoas.\n\n# Vetores de peso e de quilo.\n(peso &lt;- c(62, 70, 52, 98, 90, 70))\n\n[1] 62 70 52 98 90 70\n\n(altura &lt;- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61))\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Obs: note que o separador decimal do R é um . (ponto).\n\nPodemos a partir dessas informações calcular o IMC. Vale lembrar que o IMC é dado pelo peso (em kg) dividido pela altura (em metros) ao quadrado.\n\n(imc &lt;- peso/(altura^2))\n\n[1] 21.45329 21.13271 16.97959 26.03890 26.58318 27.00513\n\n\nÉ importante saber que, no R, vetores são a base dos demais objetos. Objetos com apenas um elemento, por exemplo, não são considerados escalares, mas vetores de tamanho um. Em outras palavras, os próprios elementos de um vetor são também vetores.\n\nelemento1 &lt;- \"\"\nis.vector(elemento1)\n\n[1] TRUE\n\nlength(elemento1)\n\n[1] 1\n\nelemento2 &lt;- 5\nis.vector(elemento2)\n\n[1] TRUE\n\nlength(elemento2)\n\n[1] 1\n\nelemento3 &lt;- TRUE\nis.vector(elemento3)\n\n[1] TRUE\n\nlength(elemento3)\n\n[1] 1\n\n\nAlém dos vetores de formato atômico também existem os de formado lista, que diferente dos atômicos, as listas podem ter elementos de tipos diferentes de forma que não há necessidade do R efetuar coerções. Para criar listas no R podemos utilizar a função list().\n\n# Lista com vários tipos de elementos (inclusive listas).\n(lista &lt;- list(5, \"hospital\", list(1:5), c(rep(1, 2)), seq(1, 2)))\n\n[[1]]\n[1] 5\n\n[[2]]\n[1] \"hospital\"\n\n[[3]]\n[[3]][[1]]\n[1] 1 2 3 4 5\n\n\n[[4]]\n[1] 1 1\n\n[[5]]\n[1] 1 2\n\nis.vector(lista)\n\n[1] TRUE\n\ntypeof(lista)\n\n[1] \"list\"\n\nlength(lista)\n\n[1] 5\n\n# Dica: podemos verificar a estrutura de qualquer objeto com a função str().\nstr(lista)\n\nList of 5\n $ : num 5\n $ : chr \"hospital\"\n $ :List of 1\n  ..$ : int [1:5] 1 2 3 4 5\n $ : num [1:2] 1 1\n $ : int [1:2] 1 2\n\n# Dica: podemos retornar uma lista para vetor atômico com a função unlist().\nunlist(lista)\n\n [1] \"5\"        \"hospital\" \"1\"        \"2\"        \"3\"        \"4\"       \n [7] \"5\"        \"1\"        \"1\"        \"1\"        \"2\"       \n\n\n\n\nA.5.5 Matrizes\nMatrizes são vetores numéricos com duas dimensões, sendo estas a linha e a coluna às quais o elemento pertence. No R podemos criar matrizes com a função matrix().\n\n# Criando uma matriz de 16 elementos com 4 linhas e 4 colunas.\n(matri &lt;- matrix(seq(1,16), nrow = 4, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nstr(matri)\n\n int [1:4, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n# Podemos verificar se é uma matriz com a função is.matrix().\nis.matrix(matri)\n\n[1] TRUE\n\n\nNote que os números de 1 a 16 foram dispostos na matriz coluna por coluna, ou seja, preenchendo de cima para baixo e depois da esquerda para a direita. Isso ocorre por padrão, pois a função matrix() possui um argumento chamado byrow = FALSE em que, para criar uma matriz em que é preenchida de elementos por linha, basta alterar o argumento para byrow = TRUE. Além disso, a função seq() está gerando os elementos da matriz enquanto o argumento nrow indica o número de linhas e ncol o número de colunas.\nPara visualizar um elemento específico de uma matriz podemos utilizar o mesmo método que usamos com vetores. Lembrando que matrizes ainda são vetores, porém, com uma dimensão a mais. Então, para visualizar um elemento específico devemos indicar a posição do elemento para todas as dimensões existentes, no caso das matrizes, para linha e coluna.\n\n# Obtendo linhas, colunas e elementos específicos.\nmatri[3,  ]   # seleciona a 3ª linha.\n\n[1]  3  7 11 15\n\nmatri[ , 2]   # seleciona a 2ª coluna.\n\n[1] 5 6 7 8\n\nmatri[1, 2]   # seleciona o elemento da primeira linha e segunda coluna.\n\n[1] 5\n\n\nPerceba que cada linha e cada coluna de uma matriz é um vetor (uma dimensão). Assim, podemos alterar uma linha ou uma coluna atribuindo um vetor de interesse, por exemplo.\n\n# substituindo a primeira linha e quarta coluna da matriz.\nmatri[1, ] &lt;- c(9, 9, 9, 9)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    9\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nmatri[, 4] &lt;- rep(1, 4)\nmatri\n\n     [,1] [,2] [,3] [,4]\n[1,]    9    9    9    1\n[2,]    2    6   10    1\n[3,]    3    7   11    1\n[4,]    4    8   12    1\n\n\nÉ de importância para o usuário verificar o tamanho (número de elementos) quando se trata de vetores. Porém, quando se trata de matrizes, é importante conhecer as dimensões além do número de elementos. Para verificar as dimensões de uma matriz podemos utilizar a função dim(), enquanto para o tamanho (número de elementos) ainda podemos utilizar a função length().\n\n# Verificando o tamanho e dimensões da matriz.\nlength(matri)\n\n[1] 16\n\ndim(matri)\n\n[1] 4 4\n\n\nComo sabemos que as linhas e colunas de uma matriz são vetores, podemos adicionar mais linhas e colunas a ela com os elementos que queremos. Para concatenar linhas e colunas em uma matriz podemos utilizar as funções rbind() e cbind() respectivamente.\n\nvet1 &lt;- c(99, 98, 97, 95)\nvet2 &lt;- c(0, 5, 7, 9, 99) \n(matri &lt;- rbind(matri, vet1))\n\n     [,1] [,2] [,3] [,4]\n        9    9    9    1\n        2    6   10    1\n        3    7   11    1\n        4    8   12    1\nvet1   99   98   97   95\n\n(matri &lt;- cbind(matri, vet2))\n\n                 vet2\n      9  9  9  1    0\n      2  6 10  1    5\n      3  7 11  1    7\n      4  8 12  1    9\nvet1 99 98 97 95   99\n\n\nOperações matemáticas entre matrizes e elementos são realizadas elemento a elemento assim como vetores. Porém, quando se trata de matrizes, é de interesse efetuar a multiplicação matricial clássica, o que pode ser feito com a operação %*% respeitando a equidade do número de colunas da matriz que pré-multiplica e o número de linhas da matriz que pós-multiplica.\n\n# Criando duas matrizes 2x2 (duas linhas e duas colunas).\n(matriz1 &lt;- matrix(c(rep(1, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    1    2\n\n(matriz2 &lt;- matrix(c(rep(2, 2), rep(2, 2)), nrow = 2))\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n# Soma duas matrizes (elemento a elemento).\nmatriz1 + matriz2\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    3    4\n\n# Subtrai duas matrizes (elemento a elemento).\nmatriz1 - matriz2\n\n     [,1] [,2]\n[1,]   -1    0\n[2,]   -1    0\n\n# Divide duas matrizes (elemento a elemento).\nmatriz1/matriz2\n\n     [,1] [,2]\n[1,]  0.5    1\n[2,]  0.5    1\n\n# Multiplica duas matrizes (elemento a elemento).\nmatriz1 * matriz2\n\n     [,1] [,2]\n[1,]    2    4\n[2,]    2    4\n\n# Multiplicação matricial clássica.\nmatriz1 %*% matriz2\n\n     [,1] [,2]\n[1,]    6    6\n[2,]    6    6\n\n# Potência de uma matriz (elemento a elemento).\n(matriz3 &lt;- matriz2^2)\n\n     [,1] [,2]\n[1,]    4    4\n[2,]    4    4\n\n# Raiz quadrada de uma matriz (elemento a elemento).\nsqrt(matriz3)\n\n     [,1] [,2]\n[1,]    2    2\n[2,]    2    2\n\n\n\n\nA.5.6 Fatores\nÉ muito comum termos que lidar com variáveis categóricas, ou seja, variáveis que possuem categorias intrínsecas em sua natureza. No R, existe uma classe de objetos chamada Fatores especificamente para representar esse tipo de variável (nominal e ordinal). Os fatores podem ser vistos como vetores de elementos numéricos inteiros (pois são assim internamente representados no R) e possuem rótulos (labels). Consequentemente, são vetores do tipo Double.\n\n# Criando um vetor/variável com a informação do sexo de 7 pessoas. \n(sexo1 &lt;- c(\"Mulher\", \"Homem\", \"Homem\", \"Mulher\", \"Mulher\", \"Mulher\", \"Homem\"))\n\n[1] \"Mulher\" \"Homem\"  \"Homem\"  \"Mulher\" \"Mulher\" \"Mulher\" \"Homem\" \n\n# Verificando a classe da variável sexo1.\nclass(sexo1)\n\n[1] \"character\"\n\n# Transformando em fator.\n(sexo2 &lt;- as.factor(sexo1))\n\n[1] Mulher Homem  Homem  Mulher Mulher Mulher Homem \nLevels: Homem Mulher\n\nclass(sexo2)\n\n[1] \"factor\"\n\n# Verificando os levels da variável de classe factor (sexo2).\nlevels(sexo2)\n\n[1] \"Homem\"  \"Mulher\"\n\n\nPodemos verificar que a variável é representada internamente por elementos numéricos inteiros ao tentar transformá-la em um vetor numérico com a função as.numeric().\n\n# Ao transformar sexo1 obteremos um vetor de dados faltantes (NA) por coerção.\nas.numeric(sexo1)\n\nWarning: NAs introduzidos por coerção\n\n\n[1] NA NA NA NA NA NA NA\n\n# Ao transformar sexo2 obteremos um vetor double com valores inteiros.\n(sexo2_num &lt;- as.numeric(sexo2))\n\n[1] 2 1 1 2 2 2 1\n\ntypeof(sexo2_num)\n\n[1] \"double\"\n\n\nFatores possuem levels em ordem alfabética, e isso pode influenciar diretamente na hora de construir gráficos e realizar aplicações de modelos.\n\n\nA.5.7 Data Frame\nTrata-se de uma “tabela de dados” onde as colunas são as variáveis e as linhas são os registros, e as colunas podem ser de classes diferentes. Logo, a principal diferença entre data frame e matriz é que matrizes só podem conter elementos da mesma classe.\nPara criar data frame no R é utilizado a função data.frame().\n\n# Colunas/variáveis para o data frame.\nID &lt;- seq(1,6)\npes &lt;- c(62, 70, 52, 98, 90, 70)\nalt &lt;- c(1.70, 1.82, 1.75, 1.94, 1.84, 1.61)\nimc &lt;- pes/(alt^2)\n\n# Criando o data frame.\n(dados &lt;- data.frame(ID = ID, peso = pes, altura = alt, imc = imc))\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nPodemos pensar na estrutura de um data frame da mesma forma que de uma matriz. Se por acaso for do interesse olhar os dados de altura, por exemplo, basta acessar a coluna três do data frame.\n\n# Selecionando a variável \"altura\".\ndados[, 3]\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n\nEmbora possamos usar os mesmos métodos discutidos na seção de matrizes, quando se trata de data frames, usualmente selecionamos as variáveis de interesse sem ter que saber em qual coluna ela está. Isso pode ser feito ao utilizar o símbolo $, dessa forma a coluna será selecionada em forma de vetor.\n\n# Selecionando a variável \"altura\".\ndados$altura\n\n[1] 1.70 1.82 1.75 1.94 1.84 1.61\n\n# Dica: também é possível fazer a seleção de colunas da seguinte forma:\ndados[, c(\"altura\", \"peso\")]\n\n  altura peso\n1   1.70   62\n2   1.82   70\n3   1.75   52\n4   1.94   98\n5   1.84   90\n6   1.61   70\n\n\nUtilizando o mesmo símbolo podemos adicionar ou deletar colunas.\n\n# Adicionando a variável \"grupo\".\ngr &lt;- c(rep(1,3),rep(2,3))\ndados$grupo &lt;- gr\ndados\n\n  ID peso altura      imc grupo\n1  1   62   1.70 21.45329     1\n2  2   70   1.82 21.13271     1\n3  3   52   1.75 16.97959     1\n4  4   98   1.94 26.03890     2\n5  5   90   1.84 26.58318     2\n6  6   70   1.61 27.00513     2\n\n# Deletando a variável \"grupo\".\ndados$grupo &lt;- NULL\ndados\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\n\nNote que ao adicionar variáveis a um data frame essa variável tem que ter o mesmo número de elementos que as demais variáveis, caso isso não seja respeitado o R ira retornar um erro.\nA estrutura de data frame é provavelmente a mais utilizada no dia a dia de quem analisa dados. Sabendo disso, existem algumas funções que são importantes de um usuário de R ter em mente.\n\nhead() - Mostra as primeiras 6 linhas.\ntail() - Mostra as últimas 6 linhas.\ndim() - Número de linhas e de colunas.\nnames() - Os nomes das colunas (variáveis).\nstr() - Estrutura do data frame. Mostra, entre outras coisas, a classe de cada coluna.\n\nAlgumas dessas funções já foram abordadas ao longo do texto. As funções de visualização head() e tail() possuem um argumento chamado n o qual podemos customizar o número de linhas que queremos visualizar.\n\nhead(dados, n = 4)\n\n  ID peso altura      imc\n1  1   62   1.70 21.45329\n2  2   70   1.82 21.13271\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n\ntail(dados, n = 4)\n\n  ID peso altura      imc\n3  3   52   1.75 16.97959\n4  4   98   1.94 26.03890\n5  5   90   1.84 26.58318\n6  6   70   1.61 27.00513\n\ndim(dados)\n\n[1] 6 4\n\nnames(dados)\n\n[1] \"ID\"     \"peso\"   \"altura\" \"imc\"   \n\nstr(dados)\n\n'data.frame':   6 obs. of  4 variables:\n $ ID    : int  1 2 3 4 5 6\n $ peso  : num  62 70 52 98 90 70\n $ altura: num  1.7 1.82 1.75 1.94 1.84 1.61\n $ imc   : num  21.5 21.1 17 26 26.6 ...\n\n\nCada coluna do data frame pode ser interpretada como um vetor. Dessa forma, as operações de vetores discutidas anteriormente são válidas.\n\n# Cria uma coluna do produto de peso por altura.\ndados$pesovezesaltura &lt;- dados$peso * dados$altura\ndados\n\n  ID peso altura      imc pesovezesaltura\n1  1   62   1.70 21.45329          105.40\n2  2   70   1.82 21.13271          127.40\n3  3   52   1.75 16.97959           91.00\n4  4   98   1.94 26.03890          190.12\n5  5   90   1.84 26.58318          165.60\n6  6   70   1.61 27.00513          112.70\n\n# Cria uma coluna de peso + cinco.\ndados$peso5 &lt;- dados$peso + 5\ndados\n\n  ID peso altura      imc pesovezesaltura peso5\n1  1   62   1.70 21.45329          105.40    67\n2  2   70   1.82 21.13271          127.40    75\n3  3   52   1.75 16.97959           91.00    57\n4  4   98   1.94 26.03890          190.12   103\n5  5   90   1.84 26.58318          165.60    95\n6  6   70   1.61 27.00513          112.70    75\n\n# Cria uma coluna da metade do peso original.\ndados$pesometade &lt;-  dados$peso/2\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n\n\n\nA.5.8 Operadores lógicos\nSabemos que TRUE e FALSE são objetos que pertencem à classe logical, além de terem representação numérica de 1 e 0 respectivamente.\nA operação lógica nada mais é do que um teste que retorna verdadeiro (TRUE) ou falso (FALSE). Assim, podemos realizar comparações entre valores utilizando alguns operadores específicos.\n\n# Verifica se 9 é igual a 12.\n9 == 12\n\n[1] FALSE\n\n# Verifica se 12 é igual a 12.\n12 == 12\n\n[1] TRUE\n\n# Verifica se 9 é diferente de 12.\n9 != 12\n\n[1] TRUE\n\n# Verifica se 9 é maior que 5.\n9 &gt; 5\n\n[1] TRUE\n\n# Verifica se 9 é maior ou igual a 9.\n9 &gt;= 9\n\n[1] TRUE\n\n# Verifica se 4 é menor que 4.\n4 &lt; 4\n\n[1] FALSE\n\n# Verifica se 4 é menor ou igual que 4.\n4 &lt;= 4\n\n[1] TRUE\n\n\nPodemos notar que estes operadores funcionam bem com números, mas isso não é verdade quando se trata de objetos do tipo character (texto). Dentre esses, o operador == apenas funciona com números e o != funciona normalmente tanto com números quanto para textos. Os operadores &gt;, &gt;=, &lt; e &lt;= funcionam com textos pensando na ordem alfabética destes.\nPodemos utilizar operadores de comparação múltipla mais usuais em conjunto com estes discutidos para tornar as comparações ainda mais dinâmicas.\n\nE: & - será verdadeiro se todas operações forem TRUE.\n\n\nx &lt;- 17\n\n# Verifica se x &gt; 9 é verdadeiro E x &lt; 50 é verdadeiro.\n(x &gt; 9) & (x &lt; 50)\n\n[1] TRUE\n\n# Verifica se x &lt; 9 é verdadeiro E x &lt; 50 é verdadeiro E x &gt; 17 é verdadeiro.\n(x &gt; 9) & (x &lt; 50) & (x &gt; 17)\n\n[1] FALSE\n\n\n\nOU: | - será verdadeiro se pelomenos uma operação for TRUE.\n\n\nx &lt;- 17\n\n# Verifica se x &lt; 9 é verdadeiro OU x &lt; 50 é verdadeiro.\n(x &lt; 9) | (x &lt; 50)\n\n[1] TRUE\n\n# Verifica se x &lt; 9 é verdadeiro OU x &gt; 50 é verdadeiro OU x &lt;= 17 é verdadeiro.\n(x &lt; 9) | (x &gt; 50) | (x &lt;= 17)\n\n[1] TRUE\n\n\n\nNegação: ! - nega a resposta lógica da comparação.\n\n\nx &lt;- 17\n\n# Retorna TRUE se x &lt; 50 for FALSE, e FALSE caso contrário. \n!(x &lt; 50)\n\n[1] FALSE\n\n\nPodemos verificar se um valor (ou conjunto de valores) está contido em um vetor utilizando o operador %in%.\n\nex &lt;- 1:15\n\n# Verifica se os valores 3 e 5 fazem parte dos elementos do vetor ex.\nc(3, 5) %in% ex\n\n[1] TRUE TRUE\n\n# Dica: o operador %in% também funciona com character:\ntexto &lt;- c(\"hospital1\", \"hospital2\", \"hospital3\", \"hospital4\", \"hospital5\")\nc(\"hospital5\", \"UTI\") %in% texto\n\n[1]  TRUE FALSE\n\n\nTodos esses operadores podem ser utilizados ao manipular data frames. Iremos aproveitar o data frame criado anteriormente e adicionar mais duas colunas de textos para realizar alguns testes.\n\n# Visualizando o data frame criado anteriormente\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade\n1  1   62   1.70 21.45329          105.40    67         31\n2  2   70   1.82 21.13271          127.40    75         35\n3  3   52   1.75 16.97959           91.00    57         26\n4  4   98   1.94 26.03890          190.12   103         49\n5  5   90   1.84 26.58318          165.60    95         45\n6  6   70   1.61 27.00513          112.70    75         35\n\n# Adicionando a coluna sexo.\ndados$sexo &lt;- c(\"M\", \"F\", \"M\", \"F\", \"F\", \"M\")\n\n# Adicionando a coluna olhos (preenchimento impreciso = F).\ndados$olhos &lt;- c(\"preto\", \"castanho\", \"F\", \"preto\", \"azul\", \"F\")\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Utilizando o operador %in% para obter as linhas com a cor dos olhos imprecisa.\ndados[dados$olhos %in% dados$sexo, ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo olhos\n3  3   52   1.75 16.97959            91.0    57         26    M     F\n6  6   70   1.61 27.00513           112.7    75         35    M     F\n\n# %in% com ! para obter as linhas com a cor dos olhos correta.\ndados[!(dados$olhos %in% dados$sexo), ]\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n\n# Linhas onde o peso é menor que o imc + 40. Retorna apenas colunas peso e imc.\ndados[(dados$peso &lt; (dados$imc + 40)), c(\"peso\", \"imc\")]\n\n  peso      imc\n3   52 16.97959\n\n\n\n\nA.5.9 Dados faltantes, infinitos e indefinições matemáticas\nDados faltantes é uma das coisas mais comuns em bases de dados, podendo surgir por diferentes fatores. No R, dados faltantes são representados por NA e é um símbolo que todo usuário deve conhecer e saber lidar. Além do NA, símbolos como NaN e Inf também são muito comuns no dia a dia.\n\nNA (Not Available): dado faltante/indisponível.\nNaN (Not a Number): indefinições matemáticas. Como 0/0 e log(-1).\nInf (Infinito): número muito grande ou o limite matemático. Aceita sinal negativo (-Inf).\n\n\nx &lt;- c(1, 6, 9)\n\n# Retorna NA\nx[4]\n\n[1] NA\n\n# Retorna NaN\nlog(-10)\n\nWarning in log(-10): NaNs produzidos\n\n\n[1] NaN\n\n# Retorna Inf\n10^14321\n\n[1] Inf\n\n\nAo lidar com bases de dados é necessário saber verificar se ela apresenta dados faltantes.\n\n# Base de dados que estamos usando.\ndados\n\n  ID peso altura      imc pesovezesaltura peso5 pesometade sexo    olhos\n1  1   62   1.70 21.45329          105.40    67         31    M    preto\n2  2   70   1.82 21.13271          127.40    75         35    F castanho\n3  3   52   1.75 16.97959           91.00    57         26    M        F\n4  4   98   1.94 26.03890          190.12   103         49    F    preto\n5  5   90   1.84 26.58318          165.60    95         45    F     azul\n6  6   70   1.61 27.00513          112.70    75         35    M        F\n\n# Adiciona linhas com dados faltantes.\ndados &lt;- rbind(dados, c(6, NA, 1.75, NA, 125, 99, 50, \"M\", \"castanho\"))\ndados &lt;- rbind(dados, c(9, 50, NA, 50, 127, 97, 55, \"F\", \"azul\"))\n\n# Deleta colunas que não iremos mais usar.\ndados[, c(\"pesovezesaltura\", \"peso5\", \"pesometade\")] &lt;- NULL\ndados\n\n  ID peso altura              imc sexo    olhos\n1  1   62    1.7 21.4532871972318    M    preto\n2  2   70   1.82 21.1327134404057    F castanho\n3  3   52   1.75 16.9795918367347    M        F\n4  4   98   1.94 26.0388989265597    F    preto\n5  5   90   1.84 26.5831758034026    F     azul\n6  6   70   1.61 27.0051309748852    M        F\n7  6 &lt;NA&gt;   1.75             &lt;NA&gt;    M castanho\n8  9   50   &lt;NA&gt;               50    F     azul\n\n# Ao incluir NA a variável imc passou a apresentar mais casas decimais.\n# Dica: podemos arredondar os valores do vetor alterados com a função round().\ndados[1:6, \"imc\"] &lt;- round(as.numeric(dados[1:6, \"imc\"]), digits = 2) \ndados\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n7  6 &lt;NA&gt;   1.75  &lt;NA&gt;    M castanho\n8  9   50   &lt;NA&gt;    50    F     azul\n\n# Avalia se os elementos da coluna peso são NA ou não.\nis.na(dados$peso)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n\n# Verifica se existe pelomenos 1 dado faltante no data frame.\nany(is.na(dados))\n\n[1] TRUE\n\n# Filtra apenas as linhas com NA na variável peso.\ndados[is.na(dados$peso),]\n\n  ID peso altura  imc sexo    olhos\n7  6 &lt;NA&gt;   1.75 &lt;NA&gt;    M castanho\n\n# Dica: as funções na.omit() e complete.cases() podem remover linhas com NA.\nna.omit(dados)\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\ndados[complete.cases(dados), ]\n\n  ID peso altura   imc sexo    olhos\n1  1   62    1.7 21.45    M    preto\n2  2   70   1.82 21.13    F castanho\n3  3   52   1.75 16.98    M        F\n4  4   98   1.94 26.04    F    preto\n5  5   90   1.84 26.58    F     azul\n6  6   70   1.61 27.01    M        F\n\n\nPara lidar com dados faltantes é importante ter pelo menos uma ideia do motivo para eles existirem na base de dados sendo analisada. Muitas vezes não temos ideia desse motivo, e a melhor estratégia acaba sendo analisar os dados, incluindo e reportando com transparência os dados faltantes. Ao analisar dados sem excluir os casos faltantes, muitas vezes nos deparamos com erros inesperados que ocorrem por tentarmos usar funções que não estão considerando esses casos. Situações como essas exigem uma atenção a mais do usuário, tendo que pesquisar e ler documentações de funções para ter certeza do que a função sendo usada está fazendo.\n\n# Criando um vetor com dados faltante.\nvetor1 &lt;- c(NA, 1, 1, 1, 5)\n\n# mean() calcula a média do vetor.\nmean(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nmean(vetor1, na.rm = TRUE)\n\n[1] 2\n\n# sum() calcula a soma dos elementos do vetor.\nsum(vetor1)\n\n[1] NA\n\n# Inclui argumento que desconsidera dado faltante caso existir.\nsum(vetor1, na.rm = TRUE)\n\n[1] 8\n\n\n\n\nA.5.10 Condicionamento: If e else\nAs estruturas if e else, também chamadas de condicionais, servem para executar códigos apenas se uma condição (teste lógico) for satisfeita.\n\nvalor1 &lt;- 224\nvalor2 &lt;- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta &lt;- 10 # resposta é 10.\n} else { # caso contrário.\n  resposta &lt;- 15 # respota é 15.\n  }\nresposta\n\n[1] 15\n\n\nVeja que o R só executa o conteúdo das chaves {} se a expressão dentro dos parênteses () retornar TRUE. Além disso, note que a condição de igualdade é representada por dois iguais (==). Como dito anteriormente, apenas um igual (=) é símbolo de atribuição (preferível &lt;-), em argumentos de estruturas condicionais queremos realizar comparações.\nPara utilizar mais condições podemos utilizar o else if ().\n\nvalor1 &lt;- 224\nvalor2 &lt;- 225\n\n# Cria objeto \"resposta\" de acordo com uma condição.\nif (valor1 == valor2) { # se valor1 for igual ao valor2.\n  resposta &lt;- 10 # resposta é 10.\n} else if (valor1 &gt; valor2) { # Se não, então valor1 é maior que valor2 ?\n  resposta &lt;- 15 # então a resposta é 15.\n  } else { # caso contrário.\n    resposta &lt;- 25 # respota é 25.\n    }\nresposta\n\n[1] 25\n\n\n\n\nA.5.11 Iterador for\nO for serve para repetir uma mesma tarefa para um conjunto de valores diferentes (realiza um loop). Cada repetição é chamada de iteração.\nComo exemplo, considere o vetor atribuído ao objeto vetor1 como segue:\n\nvetor1 &lt;- c(1,20,50,60,100)\n\nPodemos criar um novo vetor que seja formado por cada elemento do vetor1 dividido por sua posição.\n\nvetor2 &lt;- NULL\nfor (i in 1: length(vetor1)){\n  vetor2[i] &lt;- vetor1[i]/i\n}\nvetor2\n\n[1]  1.00000 10.00000 16.66667 15.00000 20.00000\n\n\nNote que primeiro definimos o objeto vetor2, recebendo NULL. O NULL representa a ausência de um objeto e serve para já declarar algum objeto que receberá valor na sequência. Ao rodar o for, o vetor2 passa a ser um vetor de tamanho 5 (tamanho do vetor1).\nNo exemplo, temos 5 iterações e para cada valor de i, correndo de 1 até 5 (tamanho do vetor1), pegamos o valor do vetor1 na posição i e dividimos por i. Assim, formamos o vetor2.\n\n\nA.5.12 Funções\nFunções no R são nomes que guardam um código de R. A ideia é que sempre que rodar a função com os seus argumentos, o código que ela guarda será executado e o resultado será retornado.\nJá usamos anteriormente algumas funções que estão na base do R. Por exemplo, quando usamos class() para entender a classe do objeto que o R está entendendo. Colocamos um argumento dentro do parêntese e o R retornou qual a classe do objeto em questão.\nImportantes:\n\nSe a função tiver mais de um argumento, eles são sempre separados por vírgulas;\nCada função tem os seus próprios argumentos. Para saber quais são e como usar os argumentos de uma função, basta acessar a sua documentação. Uma forma de fazer isso é pela função help(), cujo argumento é o nome da função em questão.\n\n\nhelp(mean)\n\nVeja que abrirá a documentação sobre a função mean no menu “Help” do RStudio. Assim, é possível ver os argumentos e exemplos de uso da função.\nAinda sobre funções já presentes no R, vamos considerar agora a função sample. Veja a documentação dessa função para ver o que ela faz.\n\nhelp(sample)\n\nA função sample retorna uma amostra de um vetor com tamanho especificado em um de seus argumentos com ou sem reposição. Ela apresenta quatro argumentos na forma sample(x, size, replace = FALSE, prob = NULL), em que: x é o vetor do qual será amostrado o número de elementos especificado no argumento size, replace indica se é com ou sem reposição e prob é para especificar probabilidades de seleção.\nPodemos usar essa função para amostrar de um objeto dois elementos (size = 2) em uma seleção com reposição (replace = TRUE) e que a probabilidade de seleção seja a mesma para todos os elementos do vetor. No caso da probabilidade, como podemos ver na documentação da função sample, o default (padrão se o usuário não mudar o argumento) é ser a mesma probabilidade de seleção para todos os elementos. Assim, se o usuário nada especificar para esse argumento, o R entenderá o seu default. O mesmo vale para o argumento replace: caso fosse o interesse fazer a seleção sem reposição, não precisaríamos colocar esse argumento por seu default ser FALSE.\n\n\n[1] 1 1\n\n\nTambém poderíamos usar a mesma função sem colocar o nome dos argumentos, desde que o usuário entenda o que ela está fazendo.\n\nsample(vetor_am, 2 , TRUE) \n\n[1] 10  2\n\n\nNesse caso, é importante que se respeite a ordem dos argumentos: o vetor tem que ser o primeiro, o segundo argumento é size e assim por diante.\nVale ressaltar que as duas últimas saídas não necessariamente serão as mesmas, porque é feito um sorteio aleatório de dois elementos de vetor_am em cada uma delas.\nAlém de usar funções já prontas, podemos criar novas funções. Suponha que queremos criar uma função de dois argumentos que retorna o primeiro mais três vezes o segundo argumento.\n\nf_conta &lt;- function(x, y) {\n  out &lt;- x + 3 * y\n  return(out)\n}\n\nA função acima possui:\n\nnome: f_conta.\nargumentos: x e y.\no corpo out: &lt;- x + 3 * y.\no que retorna: return(out).\n\nPara chamar a função e utilizá-la basta chamar pelo nome com os devidos argumentos, assim como temos feito até então.\n\nf_conta(x = 10, y = 20)\n\n[1] 70\n\n\nVeja que o cálculo acima retorna exatamente o mesmo que o seguinte:\n\nf_conta(y = 20, x = 10)\n\n[1] 70\n\n\nIsso acontece porque a ordem dos argumentos foi alterada, porém, mantendo seus devidos nomes. Se não quiser colocar os nomes dos argumentos, precisa tomar cuidado para não errar a ordem deles. Isso porque:\n\nf_conta(10,20)\n\n[1] 70\n\n\né diferente de:\n\nf_conta(20,10)\n\n[1] 50\n\n\n\n\nA.5.13 Como obter ajuda no R\nListamos aqui 3 maneiras para buscar ajuda no R:\n\nHelp/documentação do R (comandos help(nome_da_funcao) ou ?nome_da_funcao). Como exemplo:\n\n\nhelp(mean) \n?mean\n\n\nGoogle: especificar a linguagem é de suma importância na pesquisa, além de deixar o problema ou a função bem claro.\n\n\n\n\n\n\nPesquisa no Google\n\n\n\n\n\nComunidade: O Stack Overflow e o Stack Overflow em Português são sites de perguntas e respostas amplamente utilizados por todas as linguagens de programação.\n\n\n\nA.5.14 Pacotes\nComo dito quando falamos “Sobre o R”, o R apresenta funções na sua base e também em forma de pacotes (conjunto de funções bem documentado), que precisam ser instalados (uma vez no seu computador) e carregados na sessão de utilização do R (carregado em toda sessão aberta).\nDificilmente uma análise será feita apenas com as funções básicas do R e dificilmente não vai existir um pacote com as funções que você precisa. Por esse motivo, falamos a seguir em como instalar e carregar pacotes.\n\nA.5.14.1 Instalação de pacotes\n\nVia CRAN:\n\n\ninstall.packages(\"nome-do-pacote\")\n\nExemplo: Instalação do pacote dplyr.\n\ninstall.packages(\"dplyr\")\n\nNote que o nome do pacote está entre aspas.\n\nVia Github: Para instalar via Github precisa primeiramente instalar o pacote devtools.\n\n\ndevtools::install_github(\"nome-do-repo/nome-do-pacote\")\n\nExemplo:\n\ndevtools::install_github(\"tidyverse/dplyr\")\n\n\n\nA.5.14.2 Carregar pacotes\nUma vez que um pacote de interesse está instalado em sua máquina, para carregá-lo na sessão atual do R é só rodar a seguinte linha de comando:\n\nlibrary(nome-do-pacote)\n\nVeja que para carregar o pacote não se usa aspas.\nComo exemplo, o carregamento do pacote dplyr:\n\nlibrary(dplyr)\n\nSó é necessário instalar o pacote uma vez, mas é necessário carregá-lo toda vez que começar uma nova sessão.\nDado que o pacote está carregado ao rodar a função library(), todas as funções desse pacote podem ser usadas sem problemas.\nCaso você não queira carregar o pacote e apenas usar uma função específica do pacote, você pode usar nome-do-pacote::nome-da-funcao. Por exemplo:\n\ndplyr::distinct(...)\n\nTendo carregado o pacote dplyr anteriormente (pela função library()), não seria necessário colocar dplyr:: antes da função distinct do pacote.\n\n\n\nA.5.15 Materiais complementares\nLivros e Artigos:\n\nCritical Thinking in Clinical Research. Felipe Fregni & Ben M. W. Illigens. 2018.\nCHAPTER 3: Selecting the Study Population. In: Critical Thinking in Clinical Research by Felipe Fregni and Ben Illigens. Oxford University Press 2018.\nFandino W. Formulating a good research question: Pearls and pitfalls. Indian J Anaesth. 2019;63(8):611–616. doi:10.4103/ija.IJA_198_19\nRiva JJ, Malik KM, Burnie SJ, Endicott AR, Busse JW. What is your research question? An introduction to the PICOT format for clinicians. J Can Chiropr Assoc. 2012;56(3):167–171.\nExternal validity, generalizability, and knowledge utilization. Ferguson L1. J Nurs Scholarsh. 2004;36(1):16-22.\nPeter M Rothwell; Commentary: External validity of results of randomized trials: disentangling a complex concept, International Journal of Epidemiology, Volume 39, Issue 1, 1 February 2010, Pages 94–96, https://doi.org/10.1093/ije/dyp305\n\nSites:\n\nhttps://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/1-data-display-and-summary\nhttp://www.sthda.com/english/wiki/statistical-tests-and-assumptions"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Anton, Howard, and Chris Rorres. 2001. Álgebra Linear\nCom Aplicações. Vol. 8. Bookman Porto\nAlegre.\n\n\nHartigan, John A, and Manchek A Wong. 1979. “Algorithm AS 136: A\nk-Means Clustering Algorithm.” Journal of the Royal\nStatistical Society. Series c (Applied Statistics) 28 (1): 100–108.\n\n\nJohnson, Richard Arnold, Dean W Wichern, et al. 2002. Applied\nMultivariate Statistical Analysis. Vol. 5. 8. Prentice hall Upper\nSaddle River, NJ.\n\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in\nData: An Introduction to Cluster Analysis. John Wiley & Sons.\n\n\nLloyd, Stuart. 1982. “Least Squares Quantization in PCM.”\nIEEE Transactions on Information Theory 28 (2): 129–37."
  }
]