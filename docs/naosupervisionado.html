<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="pt" xml:lang="pt"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Ciência de Dados Aplicada à Saúde Materno-Infantil - 10&nbsp; Aprendizado Não Supervisionado</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./PCA.html" rel="next">
<link href="./introducao_algebra.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Nenhum resultado",
    "search-matching-documents-text": "documentos correspondentes",
    "search-copy-link-title": "Copiar link para a busca",
    "search-hide-matches-text": "Esconder correspondências adicionais",
    "search-more-match-text": "mais correspondência neste documento",
    "search-more-matches-text": "mais correspondências neste documento",
    "search-clear-button-title": "Limpar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar",
    "search-label": "Procurar"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./naosupervisionado.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Aprendizado Não Supervisionado</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Alternar barra lateral" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Ciência de Dados Aplicada à Saúde Materno-Infantil</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Procurar"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Prefácio</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Sumário</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introdução</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./manipulacao_dados.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Manipulação de dados</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tabulacao.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tabulação de dados</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./descritiva.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Análise exploratória dos dados</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./correlacao.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Correlação</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimacao.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Estimação</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testes_np.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Testes não-paramétricos</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervisionada.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Aprendizado supervisionado</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introducao_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Alguns conceitos básicos de algebra</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./naosupervisionado.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Aprendizado Não Supervisionado</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PCA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Análise de Componentes Principais (PCA).</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Apêndices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Alternar seção">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tutorialr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Tutorial de R</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Referências</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Índice</h2>
   
  <ul>
  <li><a href="#clustering-ou-agrupamentos" id="toc-clustering-ou-agrupamentos" class="nav-link active" data-scroll-target="#clustering-ou-agrupamentos"><span class="header-section-number">10.0.1</span> Clustering ou Agrupamentos</a></li>
  <li><a href="#redução-de-dimensionalidade" id="toc-redução-de-dimensionalidade" class="nav-link" data-scroll-target="#redução-de-dimensionalidade"><span class="header-section-number">10.0.2</span> Redução de dimensionalidade</a></li>
  <li><a href="#análise-de-agrupamentos" id="toc-análise-de-agrupamentos" class="nav-link" data-scroll-target="#análise-de-agrupamentos"><span class="header-section-number">10.1</span> Análise de Agrupamentos</a>
  <ul class="collapse">
  <li><a href="#métodos-por-particionamento" id="toc-métodos-por-particionamento" class="nav-link" data-scroll-target="#métodos-por-particionamento"><span class="header-section-number">10.1.1</span> Métodos por Particionamento</a></li>
  <li><a href="#métodos-hierárquicos" id="toc-métodos-hierárquicos" class="nav-link" data-scroll-target="#métodos-hierárquicos"><span class="header-section-number">10.1.2</span> Métodos Hierárquicos</a></li>
  <li><a href="#dbscan" id="toc-dbscan" class="nav-link" data-scroll-target="#dbscan"><span class="header-section-number">10.1.3</span> DBSCAN</a></li>
  <li><a href="#agrupamento-espectral" id="toc-agrupamento-espectral" class="nav-link" data-scroll-target="#agrupamento-espectral"><span class="header-section-number">10.1.4</span> Agrupamento Espectral</a></li>
  <li><a href="#validação-do-modelo" id="toc-validação-do-modelo" class="nav-link" data-scroll-target="#validação-do-modelo"><span class="header-section-number">10.1.5</span> Validação do Modelo</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Aprendizado Não Supervisionado</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>O aprendizado de máquina pode ser dividido em duas categorias principais: aprendizado supervisionado e aprendizado não supervisionado. O aprendizado supervisionado é uma abordagem que utiliza um conjunto de dados rotulados para treinar um modelo de aprendizado de máquina. Por outro lado, o aprendizado não supervisionado não utiliza rótulos, mas em vez disso, tenta encontrar padrões e estruturas no conjunto de dados sem qualquer orientação externa.</p>
<p>Em geral, o aprendizado não supervisionado é usado quando não há classificação sobre os dados disponível ou quando os dados rotulados são muito escassos. Ele pode ser usado para várias tarefas, incluindo clustering (agrupamento) e redução de dimensionalidade.</p>
<section id="clustering-ou-agrupamentos" class="level3" data-number="10.0.1">
<h3 data-number="10.0.1" class="anchored" data-anchor-id="clustering-ou-agrupamentos"><span class="header-section-number">10.0.1</span> Clustering ou Agrupamentos</h3>
<p>O clustering é uma técnica de aprendizado não supervisionado em que os dados são agrupados com base em sua similaridade. Em outras palavras, os dados que são mais semelhantes são postos em uma mesma classificação, enquanto os dados que são diferentes são agrupados em classificações diferentes.</p>
<p>Existem vários algoritmos de clustering disponíveis, incluindo o algoritmo k-means, o clustering hierárquico e o DBSCAN. O algoritmo k-means é um dos algoritmos de clustering mais populares por conta da sua fácil compreensão e aplicação, além de sua eficiência.</p>
</section>
<section id="redução-de-dimensionalidade" class="level3" data-number="10.0.2">
<h3 data-number="10.0.2" class="anchored" data-anchor-id="redução-de-dimensionalidade"><span class="header-section-number">10.0.2</span> Redução de dimensionalidade</h3>
<p>A redução de dimensionalidade é uma técnica de aprendizado não supervisionado usada para reduzir o número de variáveis em um conjunto de dados. Ou seja, reduz a dimensionalidade do espaço de características dos dados, enquanto tenta preservar a maior parte da informação original. A redução de dimensionalidade pode ser usada para simplificar o modelo e, portanto, torná-lo mais fácil de interpretar ou para acelerar o treinamento do modelo.</p>
<p>Existem vários algoritmos de redução de dimensionalidade disponíveis, incluindo a Análise de Componentes Principais (PCA), a Análise Fatorial e a T-SNE. O PCA é um dos algoritmos de redução de dimensionalidade mais populares e será o aqui trabalhado.</p>
</section>
<section id="análise-de-agrupamentos" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="análise-de-agrupamentos"><span class="header-section-number">10.1</span> Análise de Agrupamentos</h2>
<p>Como descrito anteriormente e reforçado aqui, na análise de agrupamento, buscamos identificar regiões no espaço dos dados que possuam um grande número de observações próximas umas das outras. Essas regiões são chamadas de clusters. A ideia é agrupar indivíduos que sejam semelhantes entre si e diferentes dos indivíduos em outros clusters. Essa técnica é chamada de aprendizado não supervisionado, pois não utilizamos uma variável específica como referência para avaliar o resultado do agrupamento.</p>
<p>Formalmente, os clusters são definidos da seguinte forma:</p>
<ul>
<li><p>Cada cluster é um grupo de observações;</p></li>
<li><p>Todos os indivíduos pertencem a pelo menos um cluster;</p></li>
<li><p>Dois clusters diferentes não possuem observações em comum.</p></li>
</ul>
<p>Ao realizar o agrupamento de dados, é importante utilizar um método que maximize as diferenças entre os clusters, ao mesmo tempo que minimiza as diferenças dentro de cada cluster. Para isso, são utilizadas medidas de similaridade ou dissimilaridade, que quantificam as diferenças entre as observações.</p>
<p>As medidas de dissimilaridade mais comumente usadas são a distância euclidiana e a distância euclidiana quadrática, como apresentado abaixo respectivamente:</p>
<p><span class="math display">\[
\begin{split}
d(\mathbf{x}_i, \mathbf{x}_i') = \sqrt{\sum_{j=1}^{p} (x_{ij} - x_{i'j})^2}\\
d^2(\mathbf{x}_i, \mathbf{x}_i') = \sum_{j=1}^{p} (x_{ij} - x_{i'j})^2
\end{split}
\]</span></p>
<p>Outras medidas menos utilizadas incluem a distância absoluta e a distância de Mahalanobis, que leva em consideração a matriz de covariância, respectivamente representadas como:</p>
<p><span class="math display">\[
\begin{split}
d_a(\mathbf{x}_i, \mathbf{x}_i') = \sum_{j=1}^{p} |x_{ij} - x_{i'j}|\\
d_M(\mathbf{x}_i, \mathbf{x}_i') = \sqrt{(\mathbf{x}_i - \mathbf{x}_i')' \mathbf{S}^{-1} (\mathbf{x}_i - \mathbf{x}_i')}
\end{split}
\]</span></p>
<p>Uma maneira comum de representar as dissimilaridades entre os objetos em um conjunto de dados é por meio de uma matriz de dissimilaridade. Essa matriz mostra os valores de dissimilaridade <span class="math inline">\(a(x_i,x_j)\)</span> entre cada par de objetos <span class="math inline">\(x_i\)</span> e <span class="math inline">\(x_j\)</span> com <span class="math inline">\(i,j = 1,2,\dots,N.\)</span></p>
<p><span class="math display">\[
\begin{align}
A =
\begin{bmatrix}
          a(x_1,x_1) &amp; a(x_1,x_2) &amp; \cdots &amp;a(x_1,x_N) \\
         a(x_2,x_1) &amp; a(x_2,x_2) &amp; \cdots &amp; a(x_2,x_N)  \\
            \vdots &amp;\vdots &amp; \ddots &amp;\vdots \\
           a(x_N,x_1) &amp; a(x_N,x_2) &amp; \cdots &amp; a(x_N,x_N)
         \end{bmatrix}.
  \end{align}
\]</span></p>
<p>As matrizes de dissimilaridade podem ser obtidas com apoio da função <code>dist()</code>, onde o tipo de distância (Euclidiana por exemplo), é passada no parâmetro da função, <code>method</code> , veja a seguir, um exemplo aplicado ao conjunto de indicadores obstétricos, esse <em>DataSet</em> séra o referncial para a sessão atual, será considerado apenas as colunas dos indicadores.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>dist_euclidian <span class="ot">&lt;-</span> <span class="fu">dist</span>(<span class="fu">scale</span>(dados_indicadores[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]), <span class="at">method =</span> <span class="st">"euclidean"</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>O códio acima cria e armazena um objeto do tipo <code>dist</code> que será utilizado em exemplos futuros.</p>
<p>A análise de agrupamento é uma ferramenta valiosa que permite identificar estratos em uma população e detectar outliers. É importante considerar a escalabilidade do método, sua capacidade de lidar com diferentes tipos de variáveis e clusters de formatos variados. Além disso, a robustez em relação a outliers e a capacidade de agrupar dados de alta dimensionalidade são considerações essenciais. Existem diversos métodos de agrupamento na literatura, cada um com vantagens e desvantagens. Nas próximas sessões, exploraremos os métodos considerados e suas aplicações adequadas, bem como métodos de avaliação de qualidade para os agrupamentos.</p>
<p>Neste capítulo, vamos explorar diferentes maneiras de resolver o desafio do agrupamento de dados. Existem abordagens tradicionais, como o particionamento, que envolve dividir o conjunto de dados em grupos distintos. Além disso, temos os métodos hierárquicos, nos quais os grupos são organizados em uma estrutura de árvore.</p>
<p>Outra abordagem interessante é considerar a densidade dos pontos no espaço. Nesse caso, procuramos identificar regiões mais densas separadas por áreas menos povoadas. Esses métodos, conhecidos como baseados em densidade, oferecem uma perspectiva diferente na análise dos dados, aqui será considerado o DBSCAN.</p>
<p>Também existe uma classe de métodos que utiliza técnicas de decomposição espectral. Esses métodos reduzem a dimensionalidade dos dados, preservando as informações relevantes dos grupos presentes. São os chamados agrupamentos espectrais, que exploram as propriedades dos autovalores e autovetores da matriz de similaridade dos dados.</p>
<p>Cada uma dessas abordagens possui suas próprias características, vantagens e limitações.</p>
<section id="métodos-por-particionamento" class="level3" data-number="10.1.1">
<h3 data-number="10.1.1" class="anchored" data-anchor-id="métodos-por-particionamento"><span class="header-section-number">10.1.1</span> Métodos por Particionamento</h3>
<p>Os métodos por particionamento são comumente utilizados para agrupar dados, onde cada partição representa um cluster. Esses métodos são baseados em distância e envolvem a realocação iterativa das observações entre os clusters para obter um particionamento otimizado.</p>
<p>A escolha do número de clusters é um aspecto importante, pois influencia diretamente a qualidade do agrupamento. Uma abordagem comum é o método do cotovelo, que considera a relação entre a variância total intraclusters e o número de grupos criados. O método do cotovelo considera que aumentar o número de clusters reduz a variância, mas em algum ponto, não há melhora significativa na granularidade do agrupamento. Esse ponto ótimo, que indica o número adequado de clusters, é identificado no gráfico por uma curva tracejada.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figuras_naosupervisionado/cotovelo.png" class="img-fluid figure-img" alt="Screeplot para seleção de número de clusters"></p>
</figure>
</div>
<p>A variância total intraclusters é calculada utilizando as distâncias euclidianas quadráticas entre as observações e o centróide do respectivo grupo. O centróide <span class="math inline">\(c_l\)</span> de um grupo <span class="math inline">\(C_l\)</span> é obtido através da média das observações atribuídas a esse cluster, utilizando a fórmula:</p>
<p><span class="math display">\[c_l = \frac{1}{|\mathcal{C}_l|} \sum{i \in \mathcal{C}_l} \mathbf{x}_i\]</span></p>
<p>A variância total intraclusters é calculada como a soma das distâncias euclidianas quadráticas entre as observações e os respectivos centróides, utilizando a fórmula:</p>
<p><span class="math display">\[\sum_{l=1}^{K} \sum_{i \in \mathcal{C}_l} |\mathbf{x}_i - \mathbf{c}_l|^2\]</span></p>
<p>Essas são algumas das abordagens dos métodos por particionamento, aqui será considerado o k-médias e o k-medóides com os algorítmos PAM e CLARA, que serão apresentados a seguir com exemplos de aplicações.</p>
<section id="k-médias" class="level4" data-number="10.1.1.1">
<h4 data-number="10.1.1.1" class="anchored" data-anchor-id="k-médias"><span class="header-section-number">10.1.1.1</span> K-médias</h4>
<p>O K-médias é um método amplamente utilizado para agrupamento de dados. Ele busca encontrar K partições dos dados, minimizando a variância. O algoritmo de <span class="citation" data-cites="lloyd1982least">(<a href="references.html#ref-lloyd1982least" role="doc-biblioref">Lloyd 1982</a>)</span> é comumente usado para realizar o K-médias. Ele envolve os seguintes passos:</p>
<ol type="1">
<li><p>escolha dos K centróides iniciais;</p></li>
<li><p>particionamento dos dados com base na menor distância para cada centróide;</p></li>
<li><p>atualização dos centróides com as novas observações atribuídas a eles;</p></li>
<li><p>repetição dos passos 2 e 3 até que não haja mais mudança de agrupamento. É possível definir um número máximo de iterações para otimizar o método computacionalmente.</p></li>
</ol>
<p>Uma alternativa é o algoritmo de <span class="citation" data-cites="hartigan1979algorithm">(<a href="references.html#ref-hartigan1979algorithm" role="doc-biblioref">Hartigan e Wong 1979</a>)</span>, que adiciona uma etapa de validação para alterar os agrupamentos. A cada iteração, verifica-se se houve atualização nos centróides dos grupos. Nesse caso, um novo objeto só é atribuído a um cluster se a soma das distâncias quadráticas diminuir.</p>
<p>No entanto, o método K-médias apresenta limitações ao lidar com clusters de formas não convencionais ou grupos com tamanhos muito discrepantes. Além disso, ele é sensível a outliers, pois a inclusão de um dado extremo pode influenciar significativamente o valor do centróide. A aplicação para o software <em>R</em>, tanto do método de agrupamento quanto a escolha do número de clusters <em>K</em> pelo método do cotovelo, segue abaixo, será considerado os dados padronizados para retirar qualquer tendência em função da diferença de escala ou amplitude dos dados:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span> (<span class="dv">1122</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#BIBLIOTECAS</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="do">## padronizacao dos dados </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>dados_norm <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">scale</span>(dados_indicadores[,<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)]))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="do">## escolhendo k pelo metodo do cotovelo</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>cotovelo_kmedias <span class="ot">&lt;-</span> factoextra<span class="sc">::</span><span class="fu">fviz_nbclust</span>(dados_norm ,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a> kmeans,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a> <span class="at">method =</span> <span class="st">"wss"</span>) <span class="sc">+</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a> <span class="fu">geom_vline</span>( <span class="at">xintercept =</span> <span class="dv">7</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Numero de Grupos"</span>, <span class="at">y =</span> <span class="st">"Variancia Total Intragrupo"</span>, <span class="at">title =</span> <span class="st">"K-medias"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="do">## ajustando k-medias com o numero de grupos escolhido</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>k_medias <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(dados_norm,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>                        <span class="at">centers =</span> <span class="dv">7</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A função <code>kmeans</code> é uma ferramenta poderosa disponível no R para realizar o agrupamento de dados utilizando o método K-médias. A função kmeans retorna três principais objetos:</p>
<ul>
<li><p><code>Cluster_centers</code>: É uma matriz que representa os centróides finais de cada cluster. Cada linha dessa matriz representa um centróide, com as coordenadas correspondentes às variáveis do conjunto de dados.</p></li>
<li><p><code>Cluster_assignment</code>: É um vetor que contém as atribuições de cada observação a um determinado cluster. Cada elemento desse vetor representa o número do cluster ao qual a observação foi atribuída. O valor 1 representa o primeiro cluster, o valor 2 representa o segundo cluster e assim por diante.</p></li>
<li><p><code>Within_cluster_sum_of_squares</code>: É um valor que representa a soma dos quadrados das distâncias de cada observação em relação ao seu respectivo centróide. Essa medida indica a variabilidade dos dados dentro de cada cluster. Quanto menor o valor, mais compacto e homogêneo é o cluster.</p></li>
</ul>
</section>
<section id="k-medóides" class="level4" data-number="10.1.1.2">
<h4 data-number="10.1.1.2" class="anchored" data-anchor-id="k-medóides"><span class="header-section-number">10.1.1.2</span> K-medóides</h4>
<p>Em situações com valores extremos, os algoritmos K-medóides surgem como uma alternativa ao cálculo do centróide, evitando a influência excessiva desses valores na representação central de cada grupo. O algoritmo PAM (Partitioning Around Medoids) proposto por <span class="citation" data-cites="kaufman2009finding">(<a href="references.html#ref-kaufman2009finding" role="doc-biblioref">Kaufman e Rousseeuw 2009</a>)</span> considera um custo para as trocas de medóides a cada iteração. O custo é calculado como a diferença da variância total intragrupo considerando um novo medóide (observação não medóide) em comparação com o medóide atual. A variância total intragrupo é uma medida da dispersão dos pontos dentro de um grupo.</p>
<p>Para realizar o agrupamento, o algoritmo PAM segue os seguintes passos:</p>
<ol type="1">
<li><p>Escolha inicial dos <span class="math inline">\(K\)</span> medóides a partir do conjunto de dados;</p></li>
<li><p>As observações não selecionadas como medóides são atribuídas ao grupo cujo medóide é o mais próximo;</p></li>
<li><p>Selecionar aleatoriamente uma observação não medóide <span class="math inline">\(o_r\)</span>;</p></li>
<li><p>Calcular o custo de se mudar o medóide atual para <span class="math inline">\(o_r\)</span>;</p></li>
<li><p>Caso o custo seja menor que 0, realizar a troca de medóide;</p></li>
<li><p>Repetir os passos 2 a 5 até que não haja mais mudanças de agrupamento.</p></li>
</ol>
<p>O custo de mudança do medóide atual para outra observação é calculado como a diferença da variância total intragrupo considerando a nova observação como representante em comparação com o medóide atual.</p>
<p>Além disso, é comum utilizar a medida de distância absoluta no lugar da distância euclidiana quadrática para calcular a distância entre os pontos e os medóides. O método pode ser visto abaixo:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="do">## escolhendo k pelo metodo do cotovelo</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>cotovelo_pam <span class="ot">&lt;-</span> factoextra<span class="sc">::</span><span class="fu">fviz_nbclust</span>(dados_norm ,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>                      cluster<span class="sc">::</span>pam,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">method =</span> <span class="st">"wss"</span>) <span class="sc">+</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                <span class="fu">geom_vline</span>( <span class="at">xintercept =</span> <span class="dv">7</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>                <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Numero de Grupos"</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">y =</span> <span class="st">"Variancia Total Intragrupo"</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">title =</span> <span class="st">"PAM"</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>pam <span class="ot">&lt;-</span> cluster<span class="sc">::</span><span class="fu">pam</span>(dados_norm ,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>                      <span class="at">k =</span> <span class="dv">7</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A função <code>cluster::pam</code> no <em>R</em> retorna os seguintes elementos:</p>
<ol type="1">
<li><p><code>medoids</code>: Um vetor contendo os índices das observações selecionadas como medóides finais de cada cluster.</p></li>
<li><p><code>clustering</code>: Um vetor contendo os rótulos dos clusters aos quais cada observação foi atribuída.</p></li>
<li><p><code>objective</code>: O valor da medida de dissimilaridade total do agrupamento obtido.</p></li>
<li><p><code>isolation.distance</code>: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.</p></li>
<li><p><code>clusinfo</code>: Uma lista com informações adicionais sobre os clusters, incluindo o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.</p></li>
</ol>
<p>Esses elementos fornecem informações sobre os medóides finais selecionados, a atribuição de clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento e informações adicionais sobre cada cluster.</p>
<p>Para lidar com grandes conjuntos de dados, o algoritmo CLARA (Clustering Large Applications) divide o conjunto em amostras menores e aplica o PAM nessas amostras. Em seguida, calcula a variância total intragrupo para cada agrupamento gerado. A partição que apresentar menor variância total intragrupo é selecionada como o resultado final do algoritmo. Observe abaixo a aplicação para o <em>R</em>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>cotovelo_clara <span class="ot">&lt;-</span> factoextra<span class="sc">::</span><span class="fu">fviz_nbclust</span>(dados_norm ,</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>                      cluster<span class="sc">::</span>clara ,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>                      <span class="at">method =</span> <span class="st">"wss"</span>) <span class="sc">+</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">geom_vline</span>( <span class="at">xintercept =</span> <span class="dv">7</span>, <span class="at">linetype =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Numero de Grupos"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                        <span class="at">y =</span> <span class="st">"Variancia Total Intragrupo"</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                        <span class="at">title =</span> <span class="st">"CLARA"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>clara <span class="ot">&lt;-</span> cluster<span class="sc">::</span><span class="fu">clara</span>(dados_norm ,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                          <span class="at">k =</span> <span class="dv">7</span>, <span class="at">samples =</span> <span class="dv">10</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A função <code>cluster::clara</code> no <em>R</em> retorna os seguintes resultados:</p>
<ol type="1">
<li><p><code>medoids</code>: Um objeto pamobject contendo os medóides finais de cada cluster.</p></li>
<li><p><code>clustering</code>: Um vetor com os rótulos dos clusters atribuídos a cada observação.</p></li>
<li><p><code>objective</code>: O valor da medida de dissimilaridade total do agrupamento obtido.</p></li>
<li><p><code>isolation.distance</code>: Um vetor com as distâncias de isolamento de cada observação em relação ao seu medóide correspondente.</p></li>
<li><p><code>clusinfo</code>: Uma lista com informações adicionais sobre os clusters, como o número de observações em cada cluster e a soma das distâncias de dissimilaridade intracluster.</p></li>
<li><p><code>samples</code>: Uma lista contendo os índices das observações selecionadas em cada subamostra.</p></li>
<li><p><code>call</code>: A chamada original da função <code>cluster::clara</code> que foi utilizada.</p></li>
</ol>
<p>Esses resultados fornecem detalhes sobre os medóides finais escolhidos, a atribuição dos clusters para cada observação, o valor objetivo do agrupamento, as distâncias de isolamento, informações adicionais sobre os clusters, as subamostras utilizadas e a chamada original da função.</p>
</section>
</section>
<section id="métodos-hierárquicos" class="level3" data-number="10.1.2">
<h3 data-number="10.1.2" class="anchored" data-anchor-id="métodos-hierárquicos"><span class="header-section-number">10.1.2</span> Métodos Hierárquicos</h3>
<p>Os métodos hierárquicos são utilizados para agrupar dados em diferentes níveis de granularidade. Existem duas abordagens principais: aglomerativa e divisiva.</p>
<p>Na abordagem aglomerativa, os grupos são construídos a partir do nível mais baixo, onde cada observação forma um cluster separado, até atingir o nível mais alto, onde todos os dados estão em um único grupo. A fusão dos clusters ocorre com base na dissimilaridade entre eles. As medidas de dissimilaridade mais utilizadas, entre dois grupos, também conhecidas como linkages, podem ser definidas da seguinte forma:</p>
<ul>
<li>Método do vizinho mais próximo (Single linkages): Considera a menor distância entre todas as possíveis combinações de observações de dois grupos.</li>
</ul>
<p><span class="math display">\[
d(C_l,C_{l'}) = \min_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
\]</span></p>
<ul>
<li>Método do vizinho mais distante (Complete linkages): Utiliza a maior distância entre todas as possíveis combinações de observações de dois grupos.</li>
</ul>
<p><span class="math display">\[
d(C_l,C_{l'}) = \max_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
\]</span></p>
<ul>
<li>Método da média das distâncias (Average linkages): Calcula a média das distâncias entre as observações de dois grupos.</li>
</ul>
<p><span class="math display">\[
d(C_l,C_{l'}) = \frac{1}{|C_l||C_{l'}|}\sum_{x_i \in C_l ;x_k \in C_{l'}} d(x_i,x_j).
\]</span></p>
<ul>
<li>Método do centróide (Centróide linkages): Considera a distância entre os centróides de cada grupo como medida de dissimilaridade.</li>
</ul>
<p><span class="math display">\[
d(C_l,C_{l'}) = d^2(c_l,c_{l'}) .
\]</span></p>
<ul>
<li>Método de Ward: Minimiza a variância dentro dos grupos ao fundir os clusters que levam à menor variação possível.</li>
</ul>
<p><span class="math display">\[
d(C_l,C_{l'}) = \frac{n_ln_{l'}}{n_l + n_{l'}} d^2(c_l,c_{l'}) .
\]</span></p>
<p>No contexto dos métodos hierárquicos de agrupamento, a abordagem aglomerativa é amplamente utilizada e estudada. Isso se deve ao fato de que a abordagem divisiva apresenta um custo computacional mais elevado, uma vez que em cada iteração é necessário identificar a melhor divisão do grupo para maximizar a dissimilaridade. Portanto, o algoritmo para o agrupamento hierárquico aglomerativo consiste em:</p>
<ol type="1">
<li><p>Cada observação é inicialmente atribuída a um cluster separado.</p></li>
<li><p>Com base no método de dissimilaridade escolhido, calcula-se a dissimilaridade entre todos os pares de grupos.</p></li>
<li><p>Os dois grupos com a menor dissimilaridade são fundidos em um único grupo.</p></li>
<li><p>Repetem-se os passos 2 e 3 até que todas as observações estejam em um único grupo.</p></li>
</ol>
<p>Já na abordagem divisiva, tomando o algoritmo DIANA (Divisive Analysis), inicia-se com um grupo único que contém todas as observações e, em cada etapa, divide-se o grupo em dois com base na maior dissimilaridade entre as observações. O algoritmo DIANA segue os seguintes passos:</p>
<ol type="1">
<li><p>Todas as observações são agrupadas em um único grupo.</p></li>
<li><p>A observação com a maior dissimilaridade média em relação aos pontos do mesmo grupo é separada em um novo grupo.</p></li>
<li><p>Cada observação do grupo inicial é atribuída ao novo grupo se a dissimilaridade média em relação aos objetos desse grupo for menor do que a dissimilaridade média em relação aos demais pontos do grupo inicial.</p></li>
</ol>
<p>4.Calcula-se o diâmetro de todos os grupos (a maior dissimilaridade entre duas observações) e seleciona-se o grupo com o maior diâmetro.</p>
<ol start="5" type="1">
<li>Repetem-se os passos 2 a 4 até que cada observação esteja em um grupo separado.</li>
</ol>
<p>No agrupamento hierárquico, a visualização dos clusters é feita por meio de um dendrograma, um gráfico ramificado que mostra as junções e divisões dos clusters. A altura do ramo no primeiro nó do dendrograma representa a dissimilaridade entre os grupos divididos. Para determinar o número de grupos a partir do dendrograma, busca-se uma grande diferença de altura (dissimilaridade) ao adicionar um cluster aos dados. Uma característica dos métodos hierárquicos é que as decisões de agrupamento ou divisão não são desfeitas, ou seja, não há troca de observações entre os clusters. Decisões de união ou divisão mal feitas podem resultar em grupos de baixa qualidade. Além disso, esses métodos não são bem dimensionados, pois cada decisão de mesclagem ou divisão requer a avaliação de muitos objetos ou clusters. Pelo exemplo abaixo, uma possível resposta de número adequado de clusters seria de dois ou três grupos.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figuras_naosupervisionado/dendograma.png" class="img-fluid figure-img" alt="Exemplo de Dendrograma"></p>
</figure>
</div>
<p>Os agrupamentos hierárquicos podem ser obtidos de maneira rapida com o apoio computacional, onde inicialmente, com uso das funções <code>hclust</code> e <code>cluster::diana</code>, é obtido um objeto da classe “hclust” e da clase “diana” respectivamente, esses objetos contém informações sobre o agrupamento hierárquico realizado, incluindo a estrutura do dendrograma, as distâncias entre os objetos e outras propriedades relacionadas. Seguido pela seleção, com base no dendrograma, do número <em>K</em> ideal de clusters (Não necessariamente só um <em>K</em>), e o “corte” da árvore aglomerativa no valor ideal identificado. É apresentado abaixo para todos os métodos citados a aplicação para o <em>R</em>, supondo a distância utilizada como a euclidiana calculada no início do capítulo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggdendro)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#AGLOMERATIVOS ############</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#METODO WARD -----</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="do">##CRIAR O OBJETO HCLUST</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>agl_ward <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidian , <span class="at">method =</span> <span class="st">"ward.D2"</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># PLOT DO DENDROGRAMA </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>dendograma_ward <span class="ot">&lt;-</span>  <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(agl_ward), <span class="at">h =</span> <span class="dv">20</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a> <span class="at">main =</span> <span class="st">"Ward - cortado em H = 20"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># "corte" NOS RESPECTIVOS VALORES IDEAIS DE NUMERO DE CLUSTERS</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>agl_ward_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(agl_ward , <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># segue o mesmo para todos os outros metodos</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">#METODO SINGLE LINKAGE -----</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>agl_single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidian , <span class="at">method =</span> <span class="st">"single"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>dendograma_single <span class="ot">&lt;-</span> <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(agl_single), <span class="at">h =</span> <span class="dv">4</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>                        <span class="at">main =</span> <span class="st">"Vizinho mais Proximo - cortado em H = 4"</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>                          <span class="at">xlab =</span> <span class="st">""</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>agl_single_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(agl_single , <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co">#METODO COMPLETE LINKAGE -----</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>agl_complete <span class="ot">&lt;-</span>  <span class="fu">hclust</span>(dist_euclidian , <span class="at">method =</span> <span class="st">"complete"</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>dendograma_complete <span class="ot">&lt;-</span> <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(agl_complete), <span class="at">h =</span> <span class="dv">10</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>                             <span class="at">main =</span> <span class="st">"Vizinho mais Distante - cortado em H = 10"</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>agl_complete_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(agl_complete , <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co">#METODO AVARAGE LINKAGE -----</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>agl_ave <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidian , <span class="at">method =</span> <span class="st">"average"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>dendograma_ward <span class="ot">&lt;-</span> <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(agl_ave), <span class="at">h =</span> <span class="dv">15</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>                                <span class="at">main =</span> <span class="st">" Media das Distancias - cortado em H = 15"</span>)</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>agl_ave_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(agl_ave, <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="co">#METODO CENTROIDE LINKAGE -----</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>agl_cent <span class="ot">&lt;-</span> <span class="fu">hclust</span>(dist_euclidian , <span class="at">method =</span> <span class="st">"centroid"</span>)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>dendograma_ward <span class="ot">&lt;-</span> <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(agl_cent), <span class="at">h =</span> <span class="dv">15</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>                            <span class="at">main =</span> <span class="st">" Centroide - cortado em H = 15"</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>agl_cent_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(agl_cent , <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="do">####### DIVISIVO (DIANA) #########</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>diana <span class="ot">&lt;-</span> cluster<span class="sc">::</span><span class="fu">diana</span>(dist_euclidian ,</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                            <span class="at">diss =</span> <span class="cn">TRUE</span> ,</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                            <span class="at">metric =</span> <span class="st">"euclidean"</span>,</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep.diss =</span> <span class="cn">FALSE</span> ,</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep.data =</span> <span class="cn">FALSE</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>                            )</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>dendrograma_diana <span class="ot">&lt;-</span>  <span class="fu">plot</span>(<span class="fu">cut</span>(<span class="fu">as.dendrogram</span>(diana), <span class="at">h =</span> <span class="dv">7</span>)<span class="sc">$</span>upper ,</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>                                   <span class="at">main =</span> <span class="st">"Diana - cortado em H = 7"</span>)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>diana_res <span class="ot">&lt;-</span> <span class="fu">cutree</span>(diana , <span class="at">k =</span> <span class="dv">3</span><span class="sc">:</span><span class="dv">8</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dbscan" class="level3" data-number="10.1.3">
<h3 data-number="10.1.3" class="anchored" data-anchor-id="dbscan"><span class="header-section-number">10.1.3</span> DBSCAN</h3>
<p>O método DBSCAN (Density-Based Spatial Clustering of Applications with Noise) é um algoritmo de agrupamento que foi proposto para encontrar clusters com formas arbitrárias, ou seja, clusters que não necessariamente possuem uma forma esférica ou convexa. Ele busca identificar as regiões mais densas do espaço vetorial separadas por regiões com menos objetos, como é possível ver na figura abaixo sua eficiência em relação a outros métodos.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figuras_naosupervisionado/dbscan.png" class="img-fluid figure-img" alt="Aplicação DBSCAN, Fonte: Boyke et al. (2021)"></p>
</figure>
</div>
<p>A ideia geral do algoritmo DBSCAN é identificar os clusters de forma que a densidade de pontos ao redor de cada ponto de um grupo seja maior que um limite estabelecido. Ele utiliza dois parâmetros principais: <span class="math inline">\(\epsilon\)</span> e MinPts, para entendimento do agrupamento DBSCAN observe a definição dos seguintes termos:</p>
<ul>
<li><p><span class="math inline">\(\epsilon\)</span>-vizinhos: Os <span class="math inline">\(\epsilon\)</span>-vizinhos de um ponto <span class="math inline">\(x_j\)</span> são os objetos cuja distância para <span class="math inline">\(x_i\)</span> seja menor ou igual a <span class="math inline">\(\epsilon\)</span>, ou seja, a vizinhança de <span class="math inline">\(x_i\)</span> é composta pelos objetos que estão dentro de um raio <span class="math inline">\(\epsilon\)</span> ao redor de <span class="math inline">\(x_i\)</span>.</p></li>
<li><p>Ponto de Núcleo: Um ponto <span class="math inline">\(x_i\)</span> é considerado um ponto de núcleo se o número de seus <span class="math inline">\(\epsilon\)</span>-vizinhos (ou seja, os pontos dentro da vizinhança de <span class="math inline">\(x_i\)</span>) for maior ou igual a um valor mínimo estabelecido chamado MinPts.</p></li>
<li><p>Ponto de Borda: Um ponto <span class="math inline">\(x_i\)</span> é considerado um ponto de borda se o número de seus <span class="math inline">\(\epsilon\)</span> -vizinhos for menor do que MinPts, mas ele está dentro da vizinhança de algum ponto de núcleo.</p></li>
<li><p>Diretamente alcançável por densidade: Um ponto <span class="math inline">\(x_j\)</span> é diretamente alcançável por densidade por um ponto <span class="math inline">\(x_i\)</span> se <span class="math inline">\(x_j\)</span> é um <span class="math inline">\(\epsilon\)</span>-vizinho de <span class="math inline">\(x_i\)</span> e se <span class="math inline">\(x_i\)</span> é um ponto de núcleo.</p></li>
<li><p>Alcançável por densidade: Um ponto <span class="math inline">\(x_i\)</span> é alcançável por densidade por um ponto <span class="math inline">\(x_j\)</span> se existe uma cadeia de pontos <span class="math inline">\(x_{i'}\)</span> , onde $i’ N \mathbb{N} $, em que <span class="math inline">\(x_1 = x_j, x_N = x_i\)</span> e <span class="math inline">\(x_{i' + 1}\)</span> é diretamente alcançável por densidade por <span class="math inline">\(x_{i'}\)</span>.</p></li>
<li><p>Conectado por densidade: Um ponto <span class="math inline">\(x_i\)</span> é conectado por densidade a um ponto <span class="math inline">\(x_j\)</span> se existe um terceiro ponto <span class="math inline">\(x_{i'}\)</span> em que ambos são alcançáveis por densidade por <span class="math inline">\(x_{i'}\)</span>.</p></li>
</ul>
<p>Com base nesses critérios, o DBSCAN define os grupos da seguinte forma:</p>
<ul>
<li><p>Se um ponto <span class="math inline">\(x_i\)</span> é alcançável por densidade por um ponto <span class="math inline">\(x_j\)</span>, então ambos pertencem ao mesmo cluster <span class="math inline">\(C_l\)</span>.</p></li>
<li><p>Todos os pontos de um cluster são conectados por densidade entre si.</p></li>
</ul>
<p>Uma das vantagens do DBSCAN é sua capacidade de identificar outliers no conjunto de dados, uma vez que ele não atribui todos os objetos a grupos. Os outliers são definidos como os pontos que não são atribuídos a nenhum cluster. O algoritmo DBSCAN funciona da seguinte maneira:</p>
<ol type="1">
<li><p>Inicialmente, todas as observações da base de dados são classificadas como “não visitadas”.</p></li>
<li><p>Em seguida, os seguintes passos são executados repetidamente:</p>
<p>(a). Selecionar aleatoriamente uma observação “não visitada” <span class="math inline">\(x_i\)</span>.</p>
<p>(b). Verificar o número de <span class="math inline">\(epsilon\)</span>-vizinhos da observação <span class="math inline">\(x_i\)</span> selecionada:</p>
<ul>
<li><p>Se o número de <span class="math inline">\(\epsilon\)</span>-vizinhos <span class="math inline">\(|NEps(x_i)|\)</span> for maior ou igual a MinPts, um novo cluster é criado para <span class="math inline">\(x_i\)</span>, e todos os objetos na <span class="math inline">\(\epsilon\)</span>-vizinhança de <span class="math inline">\(x_i\)</span> são adicionados a um conjunto candidato, <span class="math inline">\(M\)</span>. O algoritmo adiciona iterativamente a <span class="math inline">\(C_l\)</span> todos os objetos em 𝑀 que não pertencem a nenhum cluster.</p></li>
<li><p>Se o número de <span class="math inline">\(\epsilon\)</span>-vizinhos <span class="math inline">\(|NEps(x_i)|\)</span> for menor do que MinPts, <span class="math inline">\(x_i\)</span> é marcado como um ponto de ruído (outlier).</p></li>
</ul></li>
<li><p>Repetir os passos 1 e 2 até que não haja mais observações “não visitadas”.</p></li>
</ol>
<p>Os parâmetros <span class="math inline">\(\epsilon\)</span> e MinPts afetam diretamente o resultado do agrupamento, determinando o tamanho mínimo de cada grupo e a distância entre os clusters. Uma das limitações do DBSCAN é o fato de que esses parâmetros são globais, ou seja, definidos para todos os grupos formados, o que pode não ser ideal quando diferentes clusters possuem densidades de pontos distintas. Para a seleção do valor de MinPts é comum que seja baseado no problema em si ou conhecimento prévio. Porém, uma proposta para seleção é o MinPts = 2×<span class="math inline">\(p\)</span> pode ser um bom valor para o parâmetro. Já o <span class="math inline">\(\epsilon\)</span> costuma ser baseado no número mínimo de pontos definido, em que é construído o gráfico das distâncias de cada ponto para os MinPts−1 vizinhos mais próximos e escolhido o valor “cotovelo”, ou seja, onde começa a ocorrer um crescimento mais acentuado nas distâncias ordenadas. Abaixo vemos como esse método se dá de forma aplicada para o software de programação tanto da seleção do melhor <span class="math inline">\(\epsilon\)</span> quanto da aplicação do modelo.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="do">## selecao do melhor eps</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a> dbscan<span class="sc">::</span><span class="fu">kNNdist</span>(dados_diss ,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a> <span class="at">k =</span> <span class="dv">3</span>, <span class="at">all =</span> <span class="cn">FALSE</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="do">## ajuste com o eps selecionado</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a> dbscan <span class="ot">&lt;-</span> dbscan<span class="sc">::</span><span class="fu">dbscan</span>(dados_norm , <span class="at">eps =</span> <span class="dv">2</span>, <span class="at">minPts =</span> <span class="dv">4</span>)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Essa função retorna os seguintes elementos:</p>
<ul>
<li><p><code>cluster</code>: É um vetor que atribui um número de cluster a cada ponto de dados no conjunto de entrada. Os pontos que não pertencem a nenhum cluster são rotulados como “0” ou como “outlier”.</p></li>
<li><p><code>eps</code>: É o valor do raio (epsilon) usado no algoritmo.</p></li>
<li><p><code>minPts</code>: É o número mínimo de pontos necessários para formar um cluster.</p></li>
<li><p><code>border</code>: É um vetor lógico que indica se cada ponto é um ponto de borda.</p></li>
<li><p><code>reachability</code>: É um vetor que armazena os valores de <em>reachability distance</em> de cada ponto.</p></li>
<li><p><code>core_dist</code>: É um vetor que contém as distâncias de densidade mínima( <em>core distance</em> ) para cada ponto.</p></li>
</ul>
<p>Uma técnica relacionada ao DBSCAN que lida com a limitação dos parâmetros globais é o algoritmo OPTICS (Ordering Points To Identify the Clustering Structure), proposto por <span class="citation" data-cites="ankerst1999optics">(<a href="references.html#ref-ankerst1999optics" role="doc-biblioref">Ankerst et al. 1999</a>)</span>. O OPTICS ordena os dados de forma que as observações em grupos mais densos estarão mais próximas na ordenação. Esse algoritmo pode ser uma opção para explorar diferentes densidades de pontos em diferentes clusters.</p>
</section>
<section id="agrupamento-espectral" class="level3" data-number="10.1.4">
<h3 data-number="10.1.4" class="anchored" data-anchor-id="agrupamento-espectral"><span class="header-section-number">10.1.4</span> Agrupamento Espectral</h3>
<p>Agrupar conjuntos de dados com alta dimensionalidade pode ser desafiador para alguns métodos de agrupamento. A grande quantidade de variáveis em alta dimensão pode aumentar significativamente o custo computacional e dificultar a separação adequada dos grupos. Nesse contexto, os métodos de agrupamento espectral surgem como uma abordagem promissora, pois permitem reduzir a dimensionalidade dos dados sem perder a informação contida nas variáveis, além de melhorar a separação entre grupos que podem não estar claramente distintos devido à alta dimensionalidade.</p>
<p>Os algoritmos de agrupamento espectral usam uma medida de dissimilaridade para representar o conjunto de dados como um grafo. Nessa representação, os vértices do grafo correspondem às observações (<span class="math inline">\(V = v_1, ..., v_N\)</span>), e as arestas (<span class="math inline">\(A\)</span>) são definidas por uma matriz de similaridade, onde <span class="math inline">\(a(x_i, x_{i'})\)</span> representa o peso da aresta que conecta os vértices <span class="math inline">\(x_i\)</span> e <span class="math inline">\(x_{i'}\)</span>. O agrupamento em <span class="math inline">\(K\)</span> clusters é então formulado como um problema de corte de arestas, que pode ser computacionalmente complexo. Para contornar essa complexidade, é aplicada a teoria espectral.</p>
<p>O algoritmo de Ng, Jordan e Weiss (NJW) é um exemplo de algoritmo de agrupamento espectral. Ele requer a definição prévia de um parâmetro, assim como o algoritmo K-médias. Esse parâmetro é o número de grupos <span class="math inline">\(K\)</span> a serem formados. O algoritmo NJW segue os seguintes passos:</p>
<ol type="1">
<li><p>Calcula-se a matriz de similaridade <span class="math inline">\(A\)</span>, usando um parâmetro escalar <span class="math inline">\(\sigma\)</span>. A similaridade entre cada par de pontos <span class="math inline">\(x_i\)</span> e <span class="math inline">\(x_{i'}\)</span> é medida usando uma função de similaridade, como a função Gaussiana. A matriz <span class="math inline">\(A\)</span> é preenchida com os valores calculados.</p></li>
<li><p>Calcula-se a matriz de graus <span class="math inline">\(D\)</span>, onde cada elemento <span class="math inline">\(d_{ij}\)</span> na diagonal principal representa a soma das similaridades da linha <span class="math inline">\(i\)</span> da matriz <span class="math inline">\(A\)</span>. A matriz <span class="math inline">\(D\)</span> captura a estrutura de conectividade do conjunto de dados.</p></li>
<li><p>Constrói-se a matriz Laplaciana <span class="math inline">\(L\)</span>, que é uma forma de normalização de <span class="math inline">\(A\)</span>. No algoritmo NJW, a matriz Laplaciana é definida como <span class="math inline">\(L = D^{(-1/2)} 𝐴 𝐷^{(-1/2)}\)</span>. Essa normalização realça as diferenças entre os grupos de dados.</p></li>
<li><p>Identificam-se os <span class="math inline">\(K\)</span> maiores autovalores de <span class="math inline">\(L\)</span> e seus respectivos autovetores associados, que são armazenados em uma matriz <span class="math inline">\(Z\)</span> de tamanho <span class="math inline">\(N \times K\)</span>.</p></li>
<li><p>Define-se uma nova matriz <span class="math inline">\(Y\)</span>, normalizada a partir de <span class="math inline">\(Z\)</span>. Cada elemento <span class="math inline">\(y_{ii'}\)</span> de <span class="math inline">\(Y\)</span> é calculado dividindo-se o elemento <span class="math inline">\(z_{ii'}\)</span> de <span class="math inline">\(Z\)</span> pelo produto da raiz quadrada do elemento diagonal correspondente a <span class="math inline">\(z_{ii}\)</span> e a raiz quadrada do elemento diagonal correspondente a <span class="math inline">\(z_{i'}\)</span>. Essa normalização ajusta as magnitudes dos dados.</p></li>
<li><p>Com a redução da dimensionalidade dos dados na matriz <span class="math inline">\(Y\)</span> (de tamanho <span class="math inline">\(N \times K\)</span>), pode-se aplicar um algoritmo de agrupamento, como o K-médias, para realizar o agrupamento dos dados.</p></li>
</ol>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>spectral_clustering <span class="ot">&lt;-</span> <span class="cf">function</span>(data , <span class="co"># dados</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                                k) <span class="co"># numero de grupos</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                                {</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matriz de dissimilaridade</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    A <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(KRLS<span class="sc">::</span><span class="fu">gausskernel</span> (data , <span class="dv">2</span>)) <span class="co"># sigma^2 = 1</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">diag</span>(A) <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matriz Laplaciana</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    L <span class="ot">=</span> <span class="fu">diag</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">rowSums</span>(A))) <span class="sc">%*%</span> A <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">rowSums</span>(A)))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matriz de autovetores associados aos k maiores autovalores</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    auto <span class="ot">&lt;-</span> <span class="fu">eigen</span>(L, <span class="at">symmetric =</span> <span class="cn">TRUE</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> auto<span class="sc">$</span>vectors[, <span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># matriz normalizada</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    Y <span class="ot">&lt;-</span> X <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">rowSums</span>(X <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># agrupamente k medias na matriz redimensionalizada</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span> (<span class="dv">1122</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    k_means <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(Y, <span class="at">centers =</span> k)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">clusters =</span> k_means<span class="sc">$</span>cluster , <span class="co">#retornar grupos</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">auto =</span> auto , <span class="co">#retornar autovalores e autovetores</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">autovetores =</span> X, <span class="co">#retornar autovetores dos K maiores autovalores</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">normalizada =</span> Y <span class="co">#retornar matriz normalizada</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A escolha dos parâmetros <span class="math inline">\(K\)</span> e <span class="math inline">\(\sigma\)</span> pode ser feita de forma arbitrária, dependendo do objetivo do estudo. No entanto, existem abordagens e técnicas disponíveis para auxiliar na seleção desses parâmetros. No exemplo acima , o parâmetro <span class="math inline">\(\sigma\)</span> foi fixado em 1, e o número de grupos <span class="math inline">\(K\)</span> foi escolhido com base na análise dos primeiros autovalores da matriz Laplaciana <span class="math inline">\(L\)</span>. O valor ótimo de <span class="math inline">\(K\)</span> é aquele em que ocorre uma queda significativa dos autovalores, uma vez que os maiores autovalores da matriz Laplaciana tendem a ser próximos de 1.</p>
</section>
<section id="validação-do-modelo" class="level3" data-number="10.1.5">
<h3 data-number="10.1.5" class="anchored" data-anchor-id="validação-do-modelo"><span class="header-section-number">10.1.5</span> Validação do Modelo</h3>
<p>No problema do agrupamento, não é possível verificar o grau de acerto do resultado obtido, uma vez que os verdadeiros grupos não são conhecidos a priori. Portanto, é necessário aplicar algum tipo de validação à partição final.</p>
<p>Quatro índices de avaliação interna da qualidade do agrupamento são os principais a serem utilizados atualmente: Davies-Bouldin (DB), Dunn (D), Silhueta (S) e Calinski-Harabasz (CH).</p>
<section id="davies-bouldin-db" class="level4" data-number="10.1.5.1">
<h4 data-number="10.1.5.1" class="anchored" data-anchor-id="davies-bouldin-db"><span class="header-section-number">10.1.5.1</span> Davies-Bouldin (DB):</h4>
<p>O índice DB mede a similaridade média entre cada grupo e seu grupo mais similar dentre os demais clusters. A distância média <span class="math inline">\(\delta_l\)</span> de um grupo <span class="math inline">\(l\)</span> às suas observações é calculada em relação a um valor referencial <span class="math inline">\(m_l\)</span>. A fórmula para <span class="math inline">\(\delta_l\)</span> é:</p>
<p><span class="math display">\[
\delta_{l} = \left(\frac{1}{n_{l}} \sum_{i=1}^{n_{l}} \left \| x_{i} - m_{l} \right \|^q\right)^{\frac{1}{q}}.
\]</span></p>
<p>onde <span class="math inline">\(n_l\)</span> é o número de observações no grupo <span class="math inline">\(l\)</span>, <span class="math inline">\(x_i\)</span> é uma observação, <span class="math inline">\(m_l\)</span> é o valor referencial (centróide ou medóide) e <span class="math inline">\(q\)</span> é um valor pré-definido, onde os valores mais utilizados são <span class="math inline">\(q = 1\)</span> e <span class="math inline">\(q= 2\)</span>. Para o índice, é utilizada também distância entre os grupos <span class="math inline">\(\Delta_{ll'}\)</span>, que é obtida como a distância entre os valores referenciais de cada grupo. A fórmula para <span class="math inline">\(\Delta_{ll'}\)</span> é:</p>
<p><span class="math display">\[
\Delta_{ll'} = \left( \sum_{j=1}^{p} |m_{jl} - m_{jl'}|^{t} \right)^{\frac{1}{t}}.
\]</span></p>
<p>onde <span class="math inline">\(t \in \mathbb{N}\)</span> é pré-definido e geralmente adotado como <span class="math inline">\(t = 1\)</span> (distâncias absolutas) ou <span class="math inline">\(t=2\)</span> (distância euclidiana). Assim, o índice de Davies-Bouldin (DB) fica definido por:</p>
<p><span class="math display">\[
DB = \frac{1}{K} \sum_{l=1}^{K} \max_{l \neq l'} \left( \frac{\delta_{l} + \delta_{l'}}{\Delta_{ll'}} \right).
\]</span> Nesse caso, buscamos agrupar observações de forma a minimizar a variância intragrupo e maximizar a diferença entre eles. Portanto, valores menores do índice de Davies-Bouldin são considerados melhores. Observe a métrica aplicada ao conjunto de dados de agrupamento obtido pelo k-médias (As medidas seguintes também serão aplicadas ao mesmo conjunto).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica DB usando Centroide</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>clusterSim<span class="sc">::</span><span class="fu">index.DB</span>(dados_norm ,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>                     k_medias<span class="sc">$</span>cluster)<span class="sc">$</span>DB</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica DB usando Medoide</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>clusterSim<span class="sc">::</span><span class="fu">index.DB</span>(dados_norm ,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>                      k_medias<span class="sc">$</span>cluster ,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">d =</span> dist_euclidian ,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                      <span class="at">centrotypes =</span> <span class="st">"medoids"</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>                      )<span class="sc">$</span>DB </span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dunn-d" class="level4" data-number="10.1.5.2">
<h4 data-number="10.1.5.2" class="anchored" data-anchor-id="dunn-d"><span class="header-section-number">10.1.5.2</span> Dunn (D):</h4>
<p>O índice D mede a razão entre a separação dos grupos e a variância dentro deles. A separação entre dois grupos <span class="math inline">\(l\)</span> e <span class="math inline">\(l'\)</span> é calculada pela distância do vizinho mais próximo, dessa forma buscamos valores grandes para essa medida de avaliação. A variância intragrupo é representada pelo diâmetro <span class="math inline">\(diam_l\)</span> do cluster. A fórmula para o índice D é:</p>
<p><span class="math display">\[
D = \frac{{\min_{l,l' \in \{1,...,K\}; l \neq l'} d(C_{l}, C_{l'})}}{{\max_{l \in \{1,...,K\}} diam_{l}}}.
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica D</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a> clValid<span class="sc">::</span><span class="fu">dunn</span>(<span class="at">distance =</span> dist_euclidian , k_medias<span class="sc">$</span>cluster)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="silhueta-s" class="level4" data-number="10.1.5.3">
<h4 data-number="10.1.5.3" class="anchored" data-anchor-id="silhueta-s"><span class="header-section-number">10.1.5.3</span> Silhueta (S):</h4>
<p>O índice de Silhueta(S) mede a qualidade do agrupamento considerando as distâncias de cada ponto em relação às observações do mesmo grupo e aos demais clusters formados. Para cada observação <span class="math inline">\(i\)</span> pertencente ao grupo <span class="math inline">\(C_l\)</span>, definimos as medidas <span class="math inline">\(a_i\)</span> e <span class="math inline">\(b_i\)</span> da seguinte maneira:</p>
$$
<span class="math display">\[\begin{split}
a_i = \frac{1}{n_{l}-1} \sum_{i' \neq i, i' \in C_{l}}^{n_l} d(x_{i}, x_{i'}),\\

b_i = \min_{l \neq l'} \left( \frac{1}{n_{l'}} \sum_{i' \in C_{l'}}^{n_{l'}} d(x_{i}, x_{i'}) \right).
\end{split}\]</span>
<p>$$</p>
<p>onde <span class="math inline">\(d(x_i,x_{i'})\)</span> representa a distância entre as observações <span class="math inline">\(x_i\)</span> e <span class="math inline">\(x_{i'}\)</span>. A partir dessas medidas, calculamos a silhueta <span class="math inline">\(s_i\)</span> da observação <span class="math inline">\(i\)</span>, com <span class="math inline">\(i = 1, ..., N\)</span>, usando a fórmula:</p>
<p><span class="math display">\[
s_i = \frac{b_i - a_i}{\max(a_i, b_i)}.
\]</span></p>
<p>O coeficiente de Silhueta S global é obtido por:</p>
<p><span class="math display">\[
S = \frac{1}{K} \sum_{l=1}^{K} \frac{1}{n_{l}} \sum_{i=1}^{n_{l}} s_i.
\]</span> Dessa forma, valores maiores de S indicam agrupamentos mais densos e separados, o que é considerado um cenário adequado para um agrupamento bem-sucedido.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica S</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>clusterSim<span class="sc">::</span><span class="fu">index.S</span>(dist_euclidian , k_medias<span class="sc">$</span>cluster)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="calinski-harabasz-ch" class="level4" data-number="10.1.5.4">
<h4 data-number="10.1.5.4" class="anchored" data-anchor-id="calinski-harabasz-ch"><span class="header-section-number">10.1.5.4</span> Calinski-Harabasz (CH):</h4>
<p>O índice de Calinski-Harabasz (CH) considera a variância intra-grupo, chamada de <span class="math inline">\(WGGS_l\)</span>, de cada cluster <span class="math inline">\(C_l\)</span> gerado, levando em conta a distância quadrática de cada observação em relação ao seu valor de referência <span class="math inline">\(m_l\)</span>, que pode ser um centróide ou medóide. A variância intra-grupo total,<span class="math inline">\(WGGS\)</span>, é calculada como a soma das variâncias intra-grupo de cada cluster:</p>
<p><span class="math display">\[
WGGS = \sum_{l=1}^{K} \sum_{i=1}^{n_{l}} d^2(x_{i}, m_l).
\]</span></p>
<p>Além disso, o cálculo do índice CH utiliza uma medida de dispersão <span class="math inline">\(BGSS\)</span> entre os grupos, que é obtida através da soma ponderada das distâncias quadráticas do valor de referência de cada cluster em relação a um valor central global <span class="math inline">\(m\)</span>. Essa medida é calculada da seguinte forma:</p>
<p><span class="math display">\[
BGSS = \sum_{l=1}^{K} n_{l} d^2(m_l, m).
\]</span></p>
<p>Dessa forma, o índice CH de Calinski-Harabasz é dado por:</p>
<p><span class="math display">\[
CH = \frac{(N - K) }{K - 1}\times \frac{BGSS}{WGSS}.
\]</span></p>
<p>Onde, valores maiores do índice CH indicam grupos com menor variância e bem separados, o que é considerado uma boa característica de um agrupamento.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica CH usando Medoide</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>clusterSim<span class="sc">::</span><span class="fu">index.G1</span>(dados_norm , k_medias<span class="sc">$</span>cluster , </span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">d =</span> dist_euclidian , <span class="at">centrotypes =</span> <span class="st">"medoids"</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">#Metrica CH usando centroide</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>clusterSim<span class="sc">::</span><span class="fu">index.G1</span>(dados_norm , k_medias<span class="sc">$</span>cluster)</span></code><button title="Copiar para a área de transferência" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-ankerst1999optics" class="csl-entry" role="listitem">
Ankerst, Mihael, Markus M Breunig, Hans-Peter Kriegel, e Jörg Sander. 1999. <span>«OPTICS: Ordering points to identify the clustering structure»</span>. <em>ACM Sigmod record</em> 28 (2): 49–60.
</div>
<div id="ref-hartigan1979algorithm" class="csl-entry" role="listitem">
Hartigan, John A, e Manchek A Wong. 1979. <span>«Algorithm AS 136: A k-means clustering algorithm»</span>. <em>Journal of the royal statistical society. series c (applied statistics)</em> 28 (1): 100–108.
</div>
<div id="ref-kaufman2009finding" class="csl-entry" role="listitem">
Kaufman, Leonard, e Peter J Rousseeuw. 2009. <em>Finding groups in data: an introduction to cluster analysis</em>. John Wiley &amp; Sons.
</div>
<div id="ref-lloyd1982least" class="csl-entry" role="listitem">
Lloyd, Stuart. 1982. <span>«Least squares quantization in PCM»</span>. <em>IEEE transactions on information theory</em> 28 (2): 129–37.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiada");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiada");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./introducao_algebra.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Alguns conceitos básicos de algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./PCA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Análise de Componentes Principais (PCA).</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>